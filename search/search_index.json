{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"automation/why-automate/","text":"Why Automate? \u00b6 Automation Overview \u00b6 The IBM Garage Methodology describes in detail why Automation is key in the Cloud world. In simple terms, Cloud computing is costly and rapidly scales beyond the capacity of human administrators. While compute, network, storage costs have continued to decrease, the required investment in operations personel has increased. \"The cost of infrastructure, even at scale, is negligible compared to just the average monthly overhead of an employee, regardless of the employee's country of residence.\" To estimate the cost of a solution, you can use a formula with three factors: (1) The number of people who are required to touch (2) some number of systems (3) some number of times. If you can reduce any of those three factors, you can decrease the cost of the solution and improve profitability. If you're required to log in to a server, you failed . Think in terms of operator-to-server ratio, which is another great metric in the cloud industry. The popular brands in cloud, such as Facebook and Google, claim openly to have operator-to-server ratios of something like 1:10,000. At that scale, operators don't have time to log in to individual servers. Instead, they are riding around data centers on scooters, looking for smoking blade servers and replacing them. Everything must be automated, from beginning to end. Servers are cattle, not pets. If the servers misbehave, they are replaced. While most organizations are not at the 1:10,000 scale, if you can focus your operators at the higher value tasks by using unattended automation to reduce the number of times people need to touch the systems, you can decrease the cost of the solution and improve profitability. Some might argue that you don't need to worry as much about decreasing the number of systems because an increasing number of systems in the solution is usually a sign of growth and health. Real profitability occurs when you can decrease the number of times people are required to touch the systems. This allows operators to focus on higher-value work and it also leads to an improvement in the operator-to-server ratio. Cloud Paks are no different to any other software system. Any time that an operator has to touch a system to install, configure or update a Cloud Pak, cost is incurred and the operator is taken away from a high value task. This guide focuses on automation of the entire Cloud Pak stack including Infrastructure, OpenShift, Cloud Paks and end user applications to enable customers to run Cloud Paks in Production in a low-touch, high value manner. Tools \u00b6 Terraform \u00b6 Terraform is an open-source infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files. Ansible \u00b6 Ansible is a universal language, unraveling the mystery of how work gets done. Turn tough tasks into repeatable playbooks. Roll out enterprise-wide protocols with the push of a button. OpenShift GitOps \u00b6 OpenShift GitOps is an OpenShift add-on which provides Argo CD and other tooling to enable teams to implement GitOps workflows for cluster configuration and application delivery. Red Hat OpenShift GitOps ensures consistency in applications when you deploy them to different clusters in different environments, such as: development, staging, and production. Red Hat OpenShift GitOps organizes the deployment process around the configuration repositories and makes them the central element. OpenShift Pipelines \u00b6 OpenShift Pipelines is a cloud-native, continuous integration and continuous delivery (CI/CD) solution based on Kubernetes resources. It uses Tekton building blocks to automate deployments across multiple platforms by abstracting away the underlying implementation details. Tekton introduces a number of standard custom resource definitions (CRDs) for defining CI/CD pipelines that are portable across Kubernetes distributions.","title":"Why Automate?"},{"location":"automation/why-automate/#why-automate","text":"","title":"Why Automate?"},{"location":"automation/why-automate/#automation-overview","text":"The IBM Garage Methodology describes in detail why Automation is key in the Cloud world. In simple terms, Cloud computing is costly and rapidly scales beyond the capacity of human administrators. While compute, network, storage costs have continued to decrease, the required investment in operations personel has increased. \"The cost of infrastructure, even at scale, is negligible compared to just the average monthly overhead of an employee, regardless of the employee's country of residence.\" To estimate the cost of a solution, you can use a formula with three factors: (1) The number of people who are required to touch (2) some number of systems (3) some number of times. If you can reduce any of those three factors, you can decrease the cost of the solution and improve profitability. If you're required to log in to a server, you failed . Think in terms of operator-to-server ratio, which is another great metric in the cloud industry. The popular brands in cloud, such as Facebook and Google, claim openly to have operator-to-server ratios of something like 1:10,000. At that scale, operators don't have time to log in to individual servers. Instead, they are riding around data centers on scooters, looking for smoking blade servers and replacing them. Everything must be automated, from beginning to end. Servers are cattle, not pets. If the servers misbehave, they are replaced. While most organizations are not at the 1:10,000 scale, if you can focus your operators at the higher value tasks by using unattended automation to reduce the number of times people need to touch the systems, you can decrease the cost of the solution and improve profitability. Some might argue that you don't need to worry as much about decreasing the number of systems because an increasing number of systems in the solution is usually a sign of growth and health. Real profitability occurs when you can decrease the number of times people are required to touch the systems. This allows operators to focus on higher-value work and it also leads to an improvement in the operator-to-server ratio. Cloud Paks are no different to any other software system. Any time that an operator has to touch a system to install, configure or update a Cloud Pak, cost is incurred and the operator is taken away from a high value task. This guide focuses on automation of the entire Cloud Pak stack including Infrastructure, OpenShift, Cloud Paks and end user applications to enable customers to run Cloud Paks in Production in a low-touch, high value manner.","title":"Automation Overview"},{"location":"automation/why-automate/#tools","text":"","title":"Tools"},{"location":"automation/why-automate/#terraform","text":"Terraform is an open-source infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files.","title":"Terraform"},{"location":"automation/why-automate/#ansible","text":"Ansible is a universal language, unraveling the mystery of how work gets done. Turn tough tasks into repeatable playbooks. Roll out enterprise-wide protocols with the push of a button.","title":"Ansible"},{"location":"automation/why-automate/#openshift-gitops","text":"OpenShift GitOps is an OpenShift add-on which provides Argo CD and other tooling to enable teams to implement GitOps workflows for cluster configuration and application delivery. Red Hat OpenShift GitOps ensures consistency in applications when you deploy them to different clusters in different environments, such as: development, staging, and production. Red Hat OpenShift GitOps organizes the deployment process around the configuration repositories and makes them the central element.","title":"OpenShift GitOps"},{"location":"automation/why-automate/#openshift-pipelines","text":"OpenShift Pipelines is a cloud-native, continuous integration and continuous delivery (CI/CD) solution based on Kubernetes resources. It uses Tekton building blocks to automate deployments across multiple platforms by abstracting away the underlying implementation details. Tekton introduces a number of standard custom resource definitions (CRDs) for defining CI/CD pipelines that are portable across Kubernetes distributions.","title":"OpenShift Pipelines"},{"location":"contribute/documentation/","text":"Setting up a Local Instance of the Site \u00b6 The site runs under a framework called MkDocs. On this page, you will find information about setting up a local instance of the web site to test your changes before submitting them as a pull request. You will also find information about authoring and formatting your content using Markdown tags and the Mkdocs framework. There are two alternative methods for running a local instance of the web site: Running it inside a Docker container Installing the MkDocs software on your local machine and running it there You can choose either method. The details are described below. Local Instance in a Docker Container \u00b6 You can use a Docker container and run MkDocs from the container, so no local installation is required: You need to have Docker installed and running on your system There are helper configurations installed if you have npm from Node.JS installed. Build the development docker container image. This is only needed once if the dependencies have not changed. npm run dev:build If this is the first time, you might see an error complaining about a missing \"package-lock.json\". If you see that, run \"npm install\" first and then run the \"npm run dev:build\" To start developing, run the following command in the root directory of the git repo (where package.json and mkdocs.yaml are located) npm run dev Open a browser to http://localhost:8000 , where you will see the documentation site. This will live update as you save changes to the Markdown files in the docs folder To stop developing, run the following command in another terminal window, which will terminate the docker container npm dev:stop To build the static HTML file and check all links and spelling run the command: npm test View the scripts section of package.json in the root folder of the git repo for additional options available Installing MkDocs and Running Locally \u00b6 You can install MkDocs and associated plugins on your development system and run the tools locally: Install Python 3 on your system Clone or Fork the documentation repository cd into the documentation directory Install the required python packages pip install -r requirements.txt Install the spell checker using command npm cspell Note The sudo command may be needed to install globally, depending on your system configuration. You now have all the tools installed to be able to create the static HTML site from the markdown documents. The documentation for MkDocs provides full instructions for using MkDocs, but the important commands are: mkdocs build will build the static site. This must be run in the root directory of the repo, where mkdocs.yml is located mkdocs serve will build the static site and launch a test server on http://localhost:8000 . Every time a document is modified the website will automatically be updated and any browser open will be refreshed to the latest. To check links in the built site ( mkdocs build must be run first), use the linkchecker, with command npm run dev:links . This command should be run in the root folder of the project, containing the linkcheckerrc file. To check spelling npm run dev:spell should be run in the root folder of the project, containing the cspell.json file. The developer guide is created using MkDocs with the Materials theme theme. MkDocs takes Markdown documents and turns them into a static website, that can be accessed from a filesystem or served from a web server. A link checker tool is also used to validate all links in the MkDocs generated website. Creating Content with MkDocs and Markdown \u00b6 MkDocs supports standard Markdown syntax and a set Markdown extensions provided by plugins. The exact Markdown syntax supported is based on the python implementation . MkDocs is configured using the mkdocs.yml file in the root of the git repository. For the prototype, the existing Gatsby content is still within the repository, with the prototype content located in the mkdocs directory. The mkdoc.yml file defines the top level navigation for the site. The level of indentation is configurable (this requires the theme to support this feature) with Markdown headings, levels 2 ( ## ) and 3 ( ### ) being used for the page navigation on the right of the page. Standard Markdown features \u00b6 The following markdown syntax is used within the documentation Todo Complete the table below Syntax Result # Title a level 1 heading containing. You can create up to 6 levels of headings by adding additional # characters, so ### is a level 3 heading **text** will display the word text in bold *text* will display the word text in italic HTML can be embedded in Markdown, but in the documentation it is preferred to stick with pure Markdown with the installed extensions. Links within MkDocs generated content \u00b6 MkDocs will warn of any internal broken links, so it is important that links within the documentation are recognized as internal links. a link starting with a protocol name, such as http or https, is an external link a link starting with / is an external link. This is because MkDocs generated content can be embedded into another web application, so links can point outside of the MkDocs generated site but hosted on the same website a link starting with a file name (including the .md extension) or relative directory (../directory/filename.md) is an internal link and will be verified by MkDocs Information Internal links should be to the Markdown file (with .md extension). When the site is generated the filename will be automatically converted to the correct URL As part of the build process a linkchecker application will check the generated html site for any broken links. You can run this linkchecker locally using the instructions. If any links in the documentation should be excluded from the link checker, such as links to localhost, then they should be added as a regex to the linkcheckerrc file, located in the root folder of the project - see linkchecker documentation for additional information Extensions used in the prototype \u00b6 There are a number of Markdown extensions being used in the prototype. See the mkdocs.yml file to see which extensions are configured. The documentation for the extensions can be found here Link configuration \u00b6 Links on the page or embedded images can be annotated to control the links and also the appearance of the links: Image \u00b6 Images are embedded in a page using the standard Markdown syntax ![description](URL) , but the image can be formatted with Attribute Lists . This is most commonly used to scale an image or center an image, e.g. ![ GitHub repo url ]( images/github-repo-url.png ){style=\"width: 80%\" .center } External Links \u00b6 External links can also use attribute lists to control behaviors, such as open in new tab or add a css class attribute to the generated HTML, such as external in the example below: [ MkDocs ]( http://mkdocs.org ){: target=\"_blank\" .external } Info You can use {: target=_blank} to create clickable images that launch to another site: [![Image description](Image URL)](target URL \"hover text\"){: target=_blank} YouTube videos \u00b6 It is possible to embed a YouTube video and have it play in place using pure markdown. You can use HTML to embed a video: < iframe width = \"100%\" height = \"500\" src = \"https://www.youtube-nocookie.com/embed/V-BFLaPdoPo\" title = \"YouTube video player\" frameborder = \"0\" allow = \"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen ></ iframe > Tabs \u00b6 Content can be organized into a set of horizontal tabs. === \"Tab 1\" Hello === \"Tab 2\" World produces : Tab 1 Hello Tab 2 World Information boxes \u00b6 The Admonition extension allows you to add themed information boxes using the !!! and ??? syntax: !!! note This is a note produces: Note This is a note and ??? note This is a note You can add a `+` character to force the box to be initially open `???+` produces a collapsible box: Note This is a collapsible note You can add a + character to force the box to be initially open ???+ You can override the title of the box by providing a title after the Admonition type. Example You can also nest different components as required note Note This is a note collapsible note Note This is a note custom title note Sample Title This is a note Markdown !!!Example You can also nest different components as required === \"note\" !!!note This is a note === \"collapsible note\" ???+note This is a note === \"custom title note\" !!!note \"Sample Title\" This is a note Supported Admonition Classes \u00b6 The Admonitions supported by the Material theme are : Note This is a note Abstract This is an abstract Info This is an info Tip This is a tip Success This is a success Question This is a question Warning This is a warning Failure This is a failure Danger This is a danger Bug This is a bug Example This is an example Quote This is a quote Code blocks \u00b6 Code blocks allow you to insert code or blocks of text in line or as a block. To use inline you simply enclose the text using a single back quote ` character. So a command can be included using `oc get pods` and will create oc get pods When you want to include a block of code you use a fence , which is 3 back quote character at the start and end of the block. After the opening quotes you should also specify the content type contained in the block. ```shell oc get pods ``` which will produce: oc get pods Notice that the block automatically gets the copy to clipboard link to allow easy copy and paste. Every code block needs to identify the content. Where there is no content type, then text should be used to identify the content as plain text. Some of the common content types are shown in the table below. However, a full link of supported content types can be found here , where the short name in the documentation should be used. type Content shell Shell script content powershell Windows Power Shell content bat Windows batch file (.bat or .cmd files) json JSON content yaml YAML content markdown or md Markdown content java Java programming language javascript or js JavaScript programming language typescript or ts TypeScript programming language text Plain text content Advanced highlighting of code blocks \u00b6 There are some additional features available due to the highlight plugin installed in MkDocs. Full details can be found in the MkDocs Materials documentation . Line numbers \u00b6 You can add line numbers to a code block with the linenums directive. You must specify the starting line number, 1 in the example below: ``` javascript linenums=\"1\" <script> document.getElementById(\"demo\").innerHTML = \"My First JavaScript\"; </script> ``` creates 1 2 3 < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; < /script> Info The line numbers do not get included when the copy to clipboard link is selected Code highlighting \u00b6 You can highlight specific lines of code using the hl_lines directive. The line numbers starts at 1 for the first line of code. If you want multiple lines highlighted, then provide the line numbers separated with a space. Below lines 1 and 3 are highlighted: ``` javascript hl_lines=\"1 3\" <script> document.getElementById(\"demo\").innerHTML = \"My First JavaScript\"; </script> ``` creates < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; < /script> It is possible to combine line number and highlighting. Below I start the line numbers at 10 and highlight the second line of code: ``` javascript linenums=\"10\" hl_lines=\"2\" <script> document.getElementById(\"demo\").innerHTML = \"My First JavaScript\"; </script> ``` creates 10 11 12 < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; < /script> Redirects \u00b6 To help external sites wanting to link to the documentation there are a number of vanity links maintained using the redirect plugin . The links are defined in the mkdocs.yml file and documented on the Additional Resources page. To ensure the auto-generated link page does not get reported by the link checker, an entry needs to be added to the nofollow section of the link checker config file, linkcheckerrc in the root directory of the project. E.g. if a link /help was created then an entry in the nofollow section should be public/help.html$ . Spell checking \u00b6 This project uses cSpell to check spelling within the markdown. The configuration included in the project automatically excludes content in a code block, enclosed in triple back quotes ```. The configuration file also specifies that US English is the language used in the documentation, so only US English spellings should be used for words where alternate international English spellings exist. You can add words to be considered valid either within a markdown document or within the cspell configuration file, cspell.json , in the root folder of the documentation repository. Words defined within a page only apply to that page, but words added to the configuration file apply to the entire project. Adding local words \u00b6 You can add a list of words to be considered valid for spell checking purposes as a comment in a Markdown file. The comment has a specific format to be picked up by the cSpell tool: <!--- cSpell:ignore linkchecker linkcheckerrc mkdocs mkdoc --> here the words linkchecker , linkcheckerrc , mkdocs and mkdoc are specified as words to be accepted by the spell checker within the file containing the comment. Adding global words \u00b6 The cSpell configuration file cspell.json contains a list of words that should always be considered valid when spell checking. The list of words applies to all files being checked.","title":"Running a Local Site Instance"},{"location":"contribute/documentation/#setting-up-a-local-instance-of-the-site","text":"The site runs under a framework called MkDocs. On this page, you will find information about setting up a local instance of the web site to test your changes before submitting them as a pull request. You will also find information about authoring and formatting your content using Markdown tags and the Mkdocs framework. There are two alternative methods for running a local instance of the web site: Running it inside a Docker container Installing the MkDocs software on your local machine and running it there You can choose either method. The details are described below.","title":"Setting up a Local Instance of the Site"},{"location":"contribute/documentation/#local-instance-in-a-docker-container","text":"You can use a Docker container and run MkDocs from the container, so no local installation is required: You need to have Docker installed and running on your system There are helper configurations installed if you have npm from Node.JS installed. Build the development docker container image. This is only needed once if the dependencies have not changed. npm run dev:build If this is the first time, you might see an error complaining about a missing \"package-lock.json\". If you see that, run \"npm install\" first and then run the \"npm run dev:build\" To start developing, run the following command in the root directory of the git repo (where package.json and mkdocs.yaml are located) npm run dev Open a browser to http://localhost:8000 , where you will see the documentation site. This will live update as you save changes to the Markdown files in the docs folder To stop developing, run the following command in another terminal window, which will terminate the docker container npm dev:stop To build the static HTML file and check all links and spelling run the command: npm test View the scripts section of package.json in the root folder of the git repo for additional options available","title":"Local Instance in a Docker Container"},{"location":"contribute/documentation/#installing-mkdocs-and-running-locally","text":"You can install MkDocs and associated plugins on your development system and run the tools locally: Install Python 3 on your system Clone or Fork the documentation repository cd into the documentation directory Install the required python packages pip install -r requirements.txt Install the spell checker using command npm cspell Note The sudo command may be needed to install globally, depending on your system configuration. You now have all the tools installed to be able to create the static HTML site from the markdown documents. The documentation for MkDocs provides full instructions for using MkDocs, but the important commands are: mkdocs build will build the static site. This must be run in the root directory of the repo, where mkdocs.yml is located mkdocs serve will build the static site and launch a test server on http://localhost:8000 . Every time a document is modified the website will automatically be updated and any browser open will be refreshed to the latest. To check links in the built site ( mkdocs build must be run first), use the linkchecker, with command npm run dev:links . This command should be run in the root folder of the project, containing the linkcheckerrc file. To check spelling npm run dev:spell should be run in the root folder of the project, containing the cspell.json file. The developer guide is created using MkDocs with the Materials theme theme. MkDocs takes Markdown documents and turns them into a static website, that can be accessed from a filesystem or served from a web server. A link checker tool is also used to validate all links in the MkDocs generated website.","title":"Installing MkDocs and Running Locally"},{"location":"contribute/documentation/#creating-content-with-mkdocs-and-markdown","text":"MkDocs supports standard Markdown syntax and a set Markdown extensions provided by plugins. The exact Markdown syntax supported is based on the python implementation . MkDocs is configured using the mkdocs.yml file in the root of the git repository. For the prototype, the existing Gatsby content is still within the repository, with the prototype content located in the mkdocs directory. The mkdoc.yml file defines the top level navigation for the site. The level of indentation is configurable (this requires the theme to support this feature) with Markdown headings, levels 2 ( ## ) and 3 ( ### ) being used for the page navigation on the right of the page.","title":"Creating Content with MkDocs and Markdown"},{"location":"contribute/documentation/#standard-markdown-features","text":"The following markdown syntax is used within the documentation Todo Complete the table below Syntax Result # Title a level 1 heading containing. You can create up to 6 levels of headings by adding additional # characters, so ### is a level 3 heading **text** will display the word text in bold *text* will display the word text in italic HTML can be embedded in Markdown, but in the documentation it is preferred to stick with pure Markdown with the installed extensions.","title":"Standard Markdown features"},{"location":"contribute/documentation/#links-within-mkdocs-generated-content","text":"MkDocs will warn of any internal broken links, so it is important that links within the documentation are recognized as internal links. a link starting with a protocol name, such as http or https, is an external link a link starting with / is an external link. This is because MkDocs generated content can be embedded into another web application, so links can point outside of the MkDocs generated site but hosted on the same website a link starting with a file name (including the .md extension) or relative directory (../directory/filename.md) is an internal link and will be verified by MkDocs Information Internal links should be to the Markdown file (with .md extension). When the site is generated the filename will be automatically converted to the correct URL As part of the build process a linkchecker application will check the generated html site for any broken links. You can run this linkchecker locally using the instructions. If any links in the documentation should be excluded from the link checker, such as links to localhost, then they should be added as a regex to the linkcheckerrc file, located in the root folder of the project - see linkchecker documentation for additional information","title":"Links within MkDocs generated content"},{"location":"contribute/documentation/#extensions-used-in-the-prototype","text":"There are a number of Markdown extensions being used in the prototype. See the mkdocs.yml file to see which extensions are configured. The documentation for the extensions can be found here","title":"Extensions used in the prototype"},{"location":"contribute/documentation/#link-configuration","text":"Links on the page or embedded images can be annotated to control the links and also the appearance of the links:","title":"Link configuration"},{"location":"contribute/documentation/#tabs","text":"Content can be organized into a set of horizontal tabs. === \"Tab 1\" Hello === \"Tab 2\" World produces : Tab 1 Hello Tab 2 World","title":"Tabs"},{"location":"contribute/documentation/#information-boxes","text":"The Admonition extension allows you to add themed information boxes using the !!! and ??? syntax: !!! note This is a note produces: Note This is a note and ??? note This is a note You can add a `+` character to force the box to be initially open `???+` produces a collapsible box: Note This is a collapsible note You can add a + character to force the box to be initially open ???+ You can override the title of the box by providing a title after the Admonition type. Example You can also nest different components as required note Note This is a note collapsible note Note This is a note custom title note Sample Title This is a note Markdown !!!Example You can also nest different components as required === \"note\" !!!note This is a note === \"collapsible note\" ???+note This is a note === \"custom title note\" !!!note \"Sample Title\" This is a note","title":"Information boxes"},{"location":"contribute/documentation/#code-blocks","text":"Code blocks allow you to insert code or blocks of text in line or as a block. To use inline you simply enclose the text using a single back quote ` character. So a command can be included using `oc get pods` and will create oc get pods When you want to include a block of code you use a fence , which is 3 back quote character at the start and end of the block. After the opening quotes you should also specify the content type contained in the block. ```shell oc get pods ``` which will produce: oc get pods Notice that the block automatically gets the copy to clipboard link to allow easy copy and paste. Every code block needs to identify the content. Where there is no content type, then text should be used to identify the content as plain text. Some of the common content types are shown in the table below. However, a full link of supported content types can be found here , where the short name in the documentation should be used. type Content shell Shell script content powershell Windows Power Shell content bat Windows batch file (.bat or .cmd files) json JSON content yaml YAML content markdown or md Markdown content java Java programming language javascript or js JavaScript programming language typescript or ts TypeScript programming language text Plain text content","title":"Code blocks"},{"location":"contribute/documentation/#redirects","text":"To help external sites wanting to link to the documentation there are a number of vanity links maintained using the redirect plugin . The links are defined in the mkdocs.yml file and documented on the Additional Resources page. To ensure the auto-generated link page does not get reported by the link checker, an entry needs to be added to the nofollow section of the link checker config file, linkcheckerrc in the root directory of the project. E.g. if a link /help was created then an entry in the nofollow section should be public/help.html$ .","title":"Redirects"},{"location":"contribute/documentation/#spell-checking","text":"This project uses cSpell to check spelling within the markdown. The configuration included in the project automatically excludes content in a code block, enclosed in triple back quotes ```. The configuration file also specifies that US English is the language used in the documentation, so only US English spellings should be used for words where alternate international English spellings exist. You can add words to be considered valid either within a markdown document or within the cspell configuration file, cspell.json , in the root folder of the documentation repository. Words defined within a page only apply to that page, but words added to the configuration file apply to the entire project.","title":"Spell checking"},{"location":"contribute/documentation/#adding-local-words","text":"You can add a list of words to be considered valid for spell checking purposes as a comment in a Markdown file. The comment has a specific format to be picked up by the cSpell tool: <!--- cSpell:ignore linkchecker linkcheckerrc mkdocs mkdoc --> here the words linkchecker , linkcheckerrc , mkdocs and mkdoc are specified as words to be accepted by the spell checker within the file containing the comment.","title":"Adding local words"},{"location":"contribute/documentation/#adding-global-words","text":"The cSpell configuration file cspell.json contains a list of words that should always be considered valid when spell checking. The list of words applies to all files being checked.","title":"Adding global words"},{"location":"contribute/githubflow/","text":"Contributing Content \u00b6 The Production Deployment Guides site is hosted in an internal IBM repo. If you are an IBM employee, and would like to contribute, there are two ways you can do that: Reporting bugs and making suggestions Contributing your own content Reporting bugs and making suggestions \u00b6 This can be done through the use of the Issues page. Before opening a new issue, please check the existing list to make sure your issue does not already exist. The more explicit you are in opening GitHub Issues, the more efficient your interaction with the maintainers will be. Contributing your own content \u00b6 We value content contributions, and accept them using the open source contribution model. Open source contributions are made by creating a fork of the site repository, adding your content in the forked repository, then creating a pull request back to the site repository. The detailed steps to do this are provided below. If you are new to open source contribution and would like some more pointers or guidance, you may want to check out Your First PR and First Timers Only . Forks and Pull Requests best practices \u00b6 Please follow these best practices after you read the instructions for contributing: One change / documentation update per pull request (PR) Always pull the latest changes from upstream and rebase before creating any pull request. New pull requests should be created against a branch of your forked repository. All new contributions should first be tested locally before PR submission. There is a way to run a local development instance of the web site on your own machine. Follow the instructions in the section below to set it up. The Open Source Contribution Process Step By Step \u00b6 Here is a compact guide to following the open source contribution model for our site. If you are new to github, or just rusty with the details, it is worth some minutes of your time to study this diagram, relating each step number to its description in the text below. This will help you to understand all the steps in the process, and understand where you are in it as you create your contribution. Create your own fork of the repo in github. This can be done in the GUI at https://github.ibm.com/cloudpakbringup/production-deployment-guides by clicking the Fork icon in the upper right corner. A fork or copy of the repo will be created under your own github id. The URL will be: https://github.ibm.com/<Your github ID>/production-deployment-guides Go to the URL of your forked repo, and use the green Code button to copy the repo information so you can paste it into your clone command in the command line to clone it to your local computer. Add a connection from the master branch of your local cloned repository to the master branch of the upstream repository. The details for this step are in Step 3 Configure Git to sync your fork ... of the link below. Do not miss this important step. Note You can find more detailed instructions for steps 1-3 by following this link: https://help.github.com/articles/fork-a-repo In a terminal window command line, working from the directory of your local cloned repository, create a new development branch off the targeted upstream master branch. git checkout -b <my-feature-branch> master Do your work: Write your contributions or make your changes Pass your tests locally (see the description of setting up your local test instance below) Commit your intermediate changes as you go and as appropriate Repeat until satisfied When you are finished with all your commits and ready to push your changes, fetch the latest upstream changes (in case other changes had been delivered upstream by others while you were working on your contribution). git fetch upstream Rebase your local cloned repo to the latest upstream changes, resolving any conflicts. This will 'replay' your local commits, one by one, after the changes delivered upstream while you were locally developing, letting you manually resolve any conflicts. Instructions on how to manually resolve a conflict and commit the new change or skip your local replayed commit will be presented on screen by the git CLI. git branch --set-upstream-to=upstream/master git rebase Push the changes you added and committed to your forked repository (see the diagram above to understand this step). git push origin <my-feature-branch> Create a pull request against the same targeted upstream branch. The easiest way to do this is through the GUI. If you go to the upstream repo URL https://github.ibm.com/cloudpakbringup/production-deployment-guides , after you have done the push above, you should see a notice of your pushed changes and a button to create a pull request. Click that button , fill in a general description of the changes in your pull request, and preferably choose one or more reviewers from the list on the right side. Your pull request needs at least one reviewer's approval before it can be merged. Once the pull request has been reviewed, accepted and merged into the main github repository, you should synchronize your remote and local forked github repository master branch with the upstream master branch. To do so, follow the steps below. Note More details on pull requests can be found at Creating a pull request In your local cloned repo command line, change your active branch to master . Then pull to your local cloned repo the latest changes from the upstream master branch (that is, the pull request). git checkout master git pull upstream master Now push those latest upstream changes that you just pulled locally to your forked repository. git push origin master Now your forked repository and your local cloned repository are all caught up and synced with the main upstream repository. What Happens to your Pull Request? \u00b6 This section is just for your information. You do not have to take any further steps unless you are requested to by the reviewer(s). All pull requests are automatically built and unit tested by a Travis-CI pipeline. The pipeline will highlight to the reviewer if there are any missing or mis-matched elements of your proposed change that could create problems for the web site. If any such problems are indicated, the reviewer or maintainer will contact you to request the necessary changes to resolve the issue. The repository maintainer will inspect the content your commit. if approved, they will merge your changes into the upstream master branch. Should a maintainer or reviewer ask for changes to be made to the pull request, these can be made locally and pushed to your forked repository and branch. This uses the same git add, commit, and push steps on the same development branch that you used when you first created the contribution. After the maintainer approves and merges your pull request, your changes will appear on the web site shortly thereafter. Testing Your Changes Locally \u00b6 Please follow the instructions in the Running a Local Site Instance page below to set up your own local instance of the web site. This can be used to test the changes you make locally before submitting them as a pull request.","title":"GitHub Open Source Contribution"},{"location":"contribute/githubflow/#contributing-content","text":"The Production Deployment Guides site is hosted in an internal IBM repo. If you are an IBM employee, and would like to contribute, there are two ways you can do that: Reporting bugs and making suggestions Contributing your own content","title":"Contributing Content"},{"location":"contribute/githubflow/#reporting-bugs-and-making-suggestions","text":"This can be done through the use of the Issues page. Before opening a new issue, please check the existing list to make sure your issue does not already exist. The more explicit you are in opening GitHub Issues, the more efficient your interaction with the maintainers will be.","title":"Reporting bugs and making suggestions"},{"location":"contribute/githubflow/#contributing-your-own-content","text":"We value content contributions, and accept them using the open source contribution model. Open source contributions are made by creating a fork of the site repository, adding your content in the forked repository, then creating a pull request back to the site repository. The detailed steps to do this are provided below. If you are new to open source contribution and would like some more pointers or guidance, you may want to check out Your First PR and First Timers Only .","title":"Contributing your own content"},{"location":"contribute/githubflow/#forks-and-pull-requests-best-practices","text":"Please follow these best practices after you read the instructions for contributing: One change / documentation update per pull request (PR) Always pull the latest changes from upstream and rebase before creating any pull request. New pull requests should be created against a branch of your forked repository. All new contributions should first be tested locally before PR submission. There is a way to run a local development instance of the web site on your own machine. Follow the instructions in the section below to set it up.","title":"Forks and Pull Requests best practices"},{"location":"contribute/githubflow/#the-open-source-contribution-process-step-by-step","text":"Here is a compact guide to following the open source contribution model for our site. If you are new to github, or just rusty with the details, it is worth some minutes of your time to study this diagram, relating each step number to its description in the text below. This will help you to understand all the steps in the process, and understand where you are in it as you create your contribution. Create your own fork of the repo in github. This can be done in the GUI at https://github.ibm.com/cloudpakbringup/production-deployment-guides by clicking the Fork icon in the upper right corner. A fork or copy of the repo will be created under your own github id. The URL will be: https://github.ibm.com/<Your github ID>/production-deployment-guides Go to the URL of your forked repo, and use the green Code button to copy the repo information so you can paste it into your clone command in the command line to clone it to your local computer. Add a connection from the master branch of your local cloned repository to the master branch of the upstream repository. The details for this step are in Step 3 Configure Git to sync your fork ... of the link below. Do not miss this important step. Note You can find more detailed instructions for steps 1-3 by following this link: https://help.github.com/articles/fork-a-repo In a terminal window command line, working from the directory of your local cloned repository, create a new development branch off the targeted upstream master branch. git checkout -b <my-feature-branch> master Do your work: Write your contributions or make your changes Pass your tests locally (see the description of setting up your local test instance below) Commit your intermediate changes as you go and as appropriate Repeat until satisfied When you are finished with all your commits and ready to push your changes, fetch the latest upstream changes (in case other changes had been delivered upstream by others while you were working on your contribution). git fetch upstream Rebase your local cloned repo to the latest upstream changes, resolving any conflicts. This will 'replay' your local commits, one by one, after the changes delivered upstream while you were locally developing, letting you manually resolve any conflicts. Instructions on how to manually resolve a conflict and commit the new change or skip your local replayed commit will be presented on screen by the git CLI. git branch --set-upstream-to=upstream/master git rebase Push the changes you added and committed to your forked repository (see the diagram above to understand this step). git push origin <my-feature-branch> Create a pull request against the same targeted upstream branch. The easiest way to do this is through the GUI. If you go to the upstream repo URL https://github.ibm.com/cloudpakbringup/production-deployment-guides , after you have done the push above, you should see a notice of your pushed changes and a button to create a pull request. Click that button , fill in a general description of the changes in your pull request, and preferably choose one or more reviewers from the list on the right side. Your pull request needs at least one reviewer's approval before it can be merged. Once the pull request has been reviewed, accepted and merged into the main github repository, you should synchronize your remote and local forked github repository master branch with the upstream master branch. To do so, follow the steps below. Note More details on pull requests can be found at Creating a pull request In your local cloned repo command line, change your active branch to master . Then pull to your local cloned repo the latest changes from the upstream master branch (that is, the pull request). git checkout master git pull upstream master Now push those latest upstream changes that you just pulled locally to your forked repository. git push origin master Now your forked repository and your local cloned repository are all caught up and synced with the main upstream repository.","title":"The Open Source Contribution Process Step By Step"},{"location":"contribute/githubflow/#what-happens-to-your-pull-request","text":"This section is just for your information. You do not have to take any further steps unless you are requested to by the reviewer(s). All pull requests are automatically built and unit tested by a Travis-CI pipeline. The pipeline will highlight to the reviewer if there are any missing or mis-matched elements of your proposed change that could create problems for the web site. If any such problems are indicated, the reviewer or maintainer will contact you to request the necessary changes to resolve the issue. The repository maintainer will inspect the content your commit. if approved, they will merge your changes into the upstream master branch. Should a maintainer or reviewer ask for changes to be made to the pull request, these can be made locally and pushed to your forked repository and branch. This uses the same git add, commit, and push steps on the same development branch that you used when you first created the contribution. After the maintainer approves and merges your pull request, your changes will appear on the web site shortly thereafter.","title":"What Happens to your Pull Request?"},{"location":"contribute/githubflow/#testing-your-changes-locally","text":"Please follow the instructions in the Running a Local Site Instance page below to set up your own local instance of the web site. This can be used to test the changes you make locally before submitting them as a pull request.","title":"Testing Your Changes Locally"},{"location":"gitops/personas/","text":"Personas \u00b6 Personas \u00b6 Persona Responsibilities Not responsible for Required skills OpenShift cluster administrator - Provision/install the cluster - Configure compute, network, storage (i.e. compute infrastructure) - Install gitOps and pipelines operator (ArgoCD and Tekton technologies) - Creates required cluster-wide resources - Install common pipelines and tasks used by product tenants - - Very little knowledge of products installed in the cluster, for example MQ or ACE - Delegates cluster security admin to Cluster security administrator - Strong skills in Kubernetes administration and Kubernetes operations - Working knowledge of DevOps: practices, tools, implementation OpenShift cluster security administrator - Responsible for administering OCP cluster security - Responsible for implementing organization wide security practices Works with both cluster admin and tenant product admin - Responsible for administering tenant product security - Security artifacts have the same structure for different tenant products - Unlikely to have knowledge of tenant products such as MQ, ACE - Strong security skills - Strong Kubernetes skills administration and operations OpenShift cluster tenant product administrator - Implements CICD DevOps for tenant product such as MQ or ACE - Can add tenant product specific cluster resources - Delegates cluster security admin to OCP cluster security administrator - Good skills in Kubernetes administration and Kubernetes operations - Good knowledge of DevOps: practices, tools, implementation - Good application development experience using containers and Kubernetes - Basic/working knowledge tenant product such as MQ or ACE Tenant product administrator - The product/integration administrator - May have some knowledge/experience of Kubernetes - Working/strong admin skills for tenant product, e.g. MQ or ACE - Basic knowledge of Kubernetes Tenant product developer - The developer who writes an application program that uses MQ - The integration developer who authors an ACE integration flow - Maintains development assets in source repository - No knowledge of Kubernetes required - Working/strong developer skills for tenant product, e.g. MQ or ACE DevOps leader - Is an expert in in DevOps - Establishes initial GitOps configuration at a customer - Works in close partnership with OCP Cluster administrator and cluster tenant administrator - Establishes/fixes/updates/evolves the DevOps resources deployed to the cluster - Ensures that DevOps infrastructure evolves in a well governed manner - Ensures the DevOps infrastructure is tested before it is rolled out - Strong Kubernetes skills administration and operations - Strong knowledge of DevOps: practices, tools, implementation - Basic work knowledge of how tenant products such as MQ, ACE interact with Kubernetes Site reliability engineer (SRE) - A Kubernetes engineer responsible for diagnosing and fixing issues with the cluster - Under time pressure to get failed service(s) back up and running - No knowledge of tenant products such as MQ, ACE - Strong Kubernetes skills administration and operations - Working knowledge of DevOps: practices, tools, implementation","title":"Personas"},{"location":"gitops/personas/#personas","text":"","title":"Personas"},{"location":"gitops/personas/#personas_1","text":"Persona Responsibilities Not responsible for Required skills OpenShift cluster administrator - Provision/install the cluster - Configure compute, network, storage (i.e. compute infrastructure) - Install gitOps and pipelines operator (ArgoCD and Tekton technologies) - Creates required cluster-wide resources - Install common pipelines and tasks used by product tenants - - Very little knowledge of products installed in the cluster, for example MQ or ACE - Delegates cluster security admin to Cluster security administrator - Strong skills in Kubernetes administration and Kubernetes operations - Working knowledge of DevOps: practices, tools, implementation OpenShift cluster security administrator - Responsible for administering OCP cluster security - Responsible for implementing organization wide security practices Works with both cluster admin and tenant product admin - Responsible for administering tenant product security - Security artifacts have the same structure for different tenant products - Unlikely to have knowledge of tenant products such as MQ, ACE - Strong security skills - Strong Kubernetes skills administration and operations OpenShift cluster tenant product administrator - Implements CICD DevOps for tenant product such as MQ or ACE - Can add tenant product specific cluster resources - Delegates cluster security admin to OCP cluster security administrator - Good skills in Kubernetes administration and Kubernetes operations - Good knowledge of DevOps: practices, tools, implementation - Good application development experience using containers and Kubernetes - Basic/working knowledge tenant product such as MQ or ACE Tenant product administrator - The product/integration administrator - May have some knowledge/experience of Kubernetes - Working/strong admin skills for tenant product, e.g. MQ or ACE - Basic knowledge of Kubernetes Tenant product developer - The developer who writes an application program that uses MQ - The integration developer who authors an ACE integration flow - Maintains development assets in source repository - No knowledge of Kubernetes required - Working/strong developer skills for tenant product, e.g. MQ or ACE DevOps leader - Is an expert in in DevOps - Establishes initial GitOps configuration at a customer - Works in close partnership with OCP Cluster administrator and cluster tenant administrator - Establishes/fixes/updates/evolves the DevOps resources deployed to the cluster - Ensures that DevOps infrastructure evolves in a well governed manner - Ensures the DevOps infrastructure is tested before it is rolled out - Strong Kubernetes skills administration and operations - Strong knowledge of DevOps: practices, tools, implementation - Basic work knowledge of how tenant products such as MQ, ACE interact with Kubernetes Site reliability engineer (SRE) - A Kubernetes engineer responsible for diagnosing and fixing issues with the cluster - Under time pressure to get failed service(s) back up and running - No knowledge of tenant products such as MQ, ACE - Strong Kubernetes skills administration and operations - Working knowledge of DevOps: practices, tools, implementation","title":"Personas"},{"location":"gitops/structure/","text":"GitOps Structure \u00b6 The GitOps concept originated from Weaveworks back in 2017 and the goal was to automate the operations of a Kubernetes (K8s) system using a model external to the system as the source of truth ( History of GitOps ). There are various GitOps workflows this is our opinionated point of view on how GitOps can be used to manage the infrastructure, services and application layers of K8s based systems. It takes into account the various personas interacting with the system and accounts for separation of duties. Refer to the https://github.com/cloud-native-toolkit/multi-tenancy-gitops repository for instructions to try out the GitOps workflow. It is focused around deploying IBM Cloud Paks on the Red Hat OpenShift platform. GitOps Principles \u00b6 With the ever growing adoption of GitOps, the OpenGitOps project was started in 2021 to define a set of open-source standards and best practices. These will help organizations adopt a standard and structured approach when implementing GitOps. It is currently a CNCF Sandbox project . The GitOps Working Group has released v0.1.0 of the GitOps Principles : The principle of declarative desired state : A system managed by GitOps must have its Desired State expressed declaratively as data in a format writable and readable by both humans and machines. The principle of immutable desired state versions : Desired State is stored in a way that supports versioning, immutability of versions, and retains a complete version history. The principle of continuous state reconciliation : Software agents continuously, and automatically, compare a system's Actual State to its Desired State. If the actual and desired states differ for any reason, automated actions to reconcile them are initiated. The principle of operations through declaration : The only mechanism through which the system is intentionally operated on is through these principles. GitOps Framework Video Overview \u00b6 This video explains the components used in our GitOps framework, and the structure of its GitOps repositories. GitOps Repository Structure \u00b6 There are a total of 4 Git repositories involved with the GitOps workflow. Main GitOps repository Infrastructure repository Services repository Application repository GitOps \u00b6 Main GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops ): This repository contains all the ArgoCD Applications for the infrastructure , services and application layers. Each ArgoCD Application will reference a specific K8s resource (yaml resides in a separate git repository), contain the configuration of the K8s resource, and determine where it will be deployed into the cluster. The various personas will interact with this repository to update the desired state of the OpenShift cluster. Directory structure for single-cluster or multi-cluster profiles: . \u251c\u2500\u2500 1 -infra \u251c\u2500\u2500 2 -services \u251c\u2500\u2500 3 -apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml The contents of the kustomization.yaml will determine which layer(s) will be deployed to the cluster. This is based on whether the resources are commented out or not. Each of the listed YAMLs contains an ArgoCD Application which in turn tracks all the K8s resources available to be deployed. This follows the ArgoCD app of apps pattern . resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml - 3-apps/3-apps.yaml Infrastructure Layer \u00b6 Infrastructure GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops-infra ): Contains the YAMLs for cluster-wide and/or infrastructure related K8s resources managed by a cluster administrator. This would include namespaces , clusterroles , clusterrolebindings , machinesets to name a few. 1 -infra \u251c\u2500\u2500 1 -infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Contents of the `kustomization.yaml` will determine which resources are deployed to the cluster resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/operator-ocs.yaml #- argocd/refarch-infraconfig.yaml #- argocd/refarch-machinesets.yaml Services Layer \u00b6 Services GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops-services ): Contains the YAMLs for K8s resources which will be used by the application layer. This could include subscriptions for Operators, YAMLs of custom resources provided, or Helm Charts for tools provided by a third party. These resource would usually be managed by the Administrator(s) and/or a DevOps team supporting application developers. 2 -services \u251c\u2500\u2500 2 -services.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 artifactory.yaml \u2502 \u2502 \u251c\u2500\u2500 cert-manager-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 chartmuseum.yaml \u2502 \u2502 \u251c\u2500\u2500 developer-dashboard.yaml \u2502 \u2502 \u251c\u2500\u2500 grafana-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-foundational-services-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-platform-navigator-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-process-mining-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 openshift-service-mesh-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 pact-broker.yaml \u2502 \u2502 \u251c\u2500\u2500 sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 sonarqube.yaml \u2502 \u2502 \u2514\u2500\u2500 swaggereditor.yaml \u2502 \u2514\u2500\u2500 operators \u2502 \u251c\u2500\u2500 cert-manager.yaml \u2502 \u251c\u2500\u2500 elasticsearch.yaml \u2502 \u251c\u2500\u2500 grafana-operator.yaml \u2502 \u251c\u2500\u2500 ibm-ace-operator.yaml \u2502 \u251c\u2500\u2500 ibm-apic-operator.yaml \u2502 \u251c\u2500\u2500 ibm-aspera-operator.yaml \u2502 \u251c\u2500\u2500 ibm-assetrepository-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-core-operator.yaml \u2502 \u251c\u2500\u2500 ibm-catalogs.yaml \u2502 \u251c\u2500\u2500 ibm-cp4i-operators.yaml \u2502 \u251c\u2500\u2500 ibm-datapower-operator.yaml \u2502 \u251c\u2500\u2500 ibm-eventstreams-operator.yaml \u2502 \u251c\u2500\u2500 ibm-foundations.yaml \u2502 \u251c\u2500\u2500 ibm-mq-operator.yaml \u2502 \u251c\u2500\u2500 ibm-opsdashboard-operator.yaml \u2502 \u251c\u2500\u2500 ibm-platform-navigator.yaml \u2502 \u251c\u2500\u2500 ibm-process-mining-operator.yaml \u2502 \u251c\u2500\u2500 jaeger.yaml \u2502 \u251c\u2500\u2500 kiali.yaml \u2502 \u251c\u2500\u2500 openshift-gitops.yaml \u2502 \u251c\u2500\u2500 openshift-pipelines.yaml \u2502 \u2514\u2500\u2500 openshift-service-mesh.yaml \u2514\u2500\u2500 kustomization.yaml Contents of the `kustomization.yaml` will determine which resources are deployed to the cluster resources : # IBM Software #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml # IBM Foundations / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml # IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount #- argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml Application Layer \u00b6 Application GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops-apps ): Contains the YAMLs for K8s resources required for Tekton pipelines and webhooks. It can also contains the YAMLs to deploy microservice(s), web application(s), instance(S) of the ACE integration server or queue manager(s). 3 -apps \u251c\u2500\u2500 3 -apps.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 <PRODUCT> \u2502 \u2502 \u251c\u2500\u2500 cicd.yaml \u2502 \u2502 \u251c\u2500\u2500 dev.yaml \u2502 \u2502 \u251c\u2500\u2500 prod.yaml \u2502 \u2502 \u2514\u2500\u2500 stage.yaml \u2514\u2500\u2500 kustomization.yaml Contents of the `kustomization.yaml` will determine which resources are deployed to the cluster resources : #- argocd/<PRODUCT>/cicd.yaml #- argocd/<PRODUCT>/dev.yaml #- argocd/<PRODUCT>/stage.yaml #- argocd/<PRODUCT>/prod.yaml","title":"GitOps Structure"},{"location":"gitops/structure/#gitops-structure","text":"The GitOps concept originated from Weaveworks back in 2017 and the goal was to automate the operations of a Kubernetes (K8s) system using a model external to the system as the source of truth ( History of GitOps ). There are various GitOps workflows this is our opinionated point of view on how GitOps can be used to manage the infrastructure, services and application layers of K8s based systems. It takes into account the various personas interacting with the system and accounts for separation of duties. Refer to the https://github.com/cloud-native-toolkit/multi-tenancy-gitops repository for instructions to try out the GitOps workflow. It is focused around deploying IBM Cloud Paks on the Red Hat OpenShift platform.","title":"GitOps Structure"},{"location":"gitops/structure/#gitops-principles","text":"With the ever growing adoption of GitOps, the OpenGitOps project was started in 2021 to define a set of open-source standards and best practices. These will help organizations adopt a standard and structured approach when implementing GitOps. It is currently a CNCF Sandbox project . The GitOps Working Group has released v0.1.0 of the GitOps Principles : The principle of declarative desired state : A system managed by GitOps must have its Desired State expressed declaratively as data in a format writable and readable by both humans and machines. The principle of immutable desired state versions : Desired State is stored in a way that supports versioning, immutability of versions, and retains a complete version history. The principle of continuous state reconciliation : Software agents continuously, and automatically, compare a system's Actual State to its Desired State. If the actual and desired states differ for any reason, automated actions to reconcile them are initiated. The principle of operations through declaration : The only mechanism through which the system is intentionally operated on is through these principles.","title":"GitOps Principles"},{"location":"gitops/structure/#gitops-framework-video-overview","text":"This video explains the components used in our GitOps framework, and the structure of its GitOps repositories.","title":"GitOps Framework Video Overview"},{"location":"gitops/structure/#gitops-repository-structure","text":"There are a total of 4 Git repositories involved with the GitOps workflow. Main GitOps repository Infrastructure repository Services repository Application repository","title":"GitOps Repository Structure"},{"location":"gitops/structure/#gitops","text":"Main GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops ): This repository contains all the ArgoCD Applications for the infrastructure , services and application layers. Each ArgoCD Application will reference a specific K8s resource (yaml resides in a separate git repository), contain the configuration of the K8s resource, and determine where it will be deployed into the cluster. The various personas will interact with this repository to update the desired state of the OpenShift cluster. Directory structure for single-cluster or multi-cluster profiles: . \u251c\u2500\u2500 1 -infra \u251c\u2500\u2500 2 -services \u251c\u2500\u2500 3 -apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml The contents of the kustomization.yaml will determine which layer(s) will be deployed to the cluster. This is based on whether the resources are commented out or not. Each of the listed YAMLs contains an ArgoCD Application which in turn tracks all the K8s resources available to be deployed. This follows the ArgoCD app of apps pattern . resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml - 3-apps/3-apps.yaml","title":"GitOps"},{"location":"gitops/structure/#infrastructure-layer","text":"Infrastructure GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops-infra ): Contains the YAMLs for cluster-wide and/or infrastructure related K8s resources managed by a cluster administrator. This would include namespaces , clusterroles , clusterrolebindings , machinesets to name a few. 1 -infra \u251c\u2500\u2500 1 -infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Contents of the `kustomization.yaml` will determine which resources are deployed to the cluster resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/operator-ocs.yaml #- argocd/refarch-infraconfig.yaml #- argocd/refarch-machinesets.yaml","title":"Infrastructure Layer"},{"location":"gitops/structure/#services-layer","text":"Services GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops-services ): Contains the YAMLs for K8s resources which will be used by the application layer. This could include subscriptions for Operators, YAMLs of custom resources provided, or Helm Charts for tools provided by a third party. These resource would usually be managed by the Administrator(s) and/or a DevOps team supporting application developers. 2 -services \u251c\u2500\u2500 2 -services.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 artifactory.yaml \u2502 \u2502 \u251c\u2500\u2500 cert-manager-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 chartmuseum.yaml \u2502 \u2502 \u251c\u2500\u2500 developer-dashboard.yaml \u2502 \u2502 \u251c\u2500\u2500 grafana-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-foundational-services-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-platform-navigator-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-process-mining-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 openshift-service-mesh-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 pact-broker.yaml \u2502 \u2502 \u251c\u2500\u2500 sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 sonarqube.yaml \u2502 \u2502 \u2514\u2500\u2500 swaggereditor.yaml \u2502 \u2514\u2500\u2500 operators \u2502 \u251c\u2500\u2500 cert-manager.yaml \u2502 \u251c\u2500\u2500 elasticsearch.yaml \u2502 \u251c\u2500\u2500 grafana-operator.yaml \u2502 \u251c\u2500\u2500 ibm-ace-operator.yaml \u2502 \u251c\u2500\u2500 ibm-apic-operator.yaml \u2502 \u251c\u2500\u2500 ibm-aspera-operator.yaml \u2502 \u251c\u2500\u2500 ibm-assetrepository-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-core-operator.yaml \u2502 \u251c\u2500\u2500 ibm-catalogs.yaml \u2502 \u251c\u2500\u2500 ibm-cp4i-operators.yaml \u2502 \u251c\u2500\u2500 ibm-datapower-operator.yaml \u2502 \u251c\u2500\u2500 ibm-eventstreams-operator.yaml \u2502 \u251c\u2500\u2500 ibm-foundations.yaml \u2502 \u251c\u2500\u2500 ibm-mq-operator.yaml \u2502 \u251c\u2500\u2500 ibm-opsdashboard-operator.yaml \u2502 \u251c\u2500\u2500 ibm-platform-navigator.yaml \u2502 \u251c\u2500\u2500 ibm-process-mining-operator.yaml \u2502 \u251c\u2500\u2500 jaeger.yaml \u2502 \u251c\u2500\u2500 kiali.yaml \u2502 \u251c\u2500\u2500 openshift-gitops.yaml \u2502 \u251c\u2500\u2500 openshift-pipelines.yaml \u2502 \u2514\u2500\u2500 openshift-service-mesh.yaml \u2514\u2500\u2500 kustomization.yaml Contents of the `kustomization.yaml` will determine which resources are deployed to the cluster resources : # IBM Software #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml # IBM Foundations / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml # IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount #- argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml","title":"Services Layer"},{"location":"gitops/structure/#application-layer","text":"Application GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops-apps ): Contains the YAMLs for K8s resources required for Tekton pipelines and webhooks. It can also contains the YAMLs to deploy microservice(s), web application(s), instance(S) of the ACE integration server or queue manager(s). 3 -apps \u251c\u2500\u2500 3 -apps.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 <PRODUCT> \u2502 \u2502 \u251c\u2500\u2500 cicd.yaml \u2502 \u2502 \u251c\u2500\u2500 dev.yaml \u2502 \u2502 \u251c\u2500\u2500 prod.yaml \u2502 \u2502 \u2514\u2500\u2500 stage.yaml \u2514\u2500\u2500 kustomization.yaml Contents of the `kustomization.yaml` will determine which resources are deployed to the cluster resources : #- argocd/<PRODUCT>/cicd.yaml #- argocd/<PRODUCT>/dev.yaml #- argocd/<PRODUCT>/stage.yaml #- argocd/<PRODUCT>/prod.yaml","title":"Application Layer"},{"location":"guides/ace-overview/","text":"App Connect Enterprise Guides Overview \u00b6 Table of Contents Overview Architecture GitOps Cloud Native Example: ../guides/cp4i/ace/cloud-native/example.md Environment Promotion","title":"App Connect Enterprise Guides Overview"},{"location":"guides/ace-overview/#app-connect-enterprise-guides-overview","text":"Table of Contents Overview Architecture GitOps Cloud Native Example: ../guides/cp4i/ace/cloud-native/example.md Environment Promotion","title":"App Connect Enterprise Guides Overview"},{"location":"guides/apic-overview/","text":"API Connect Guides Overview \u00b6 Table of Contents IBM API Connect IBM API Connect Multi-Cluster","title":"API Connect Guides Overview"},{"location":"guides/apic-overview/#api-connect-guides-overview","text":"Table of Contents IBM API Connect IBM API Connect Multi-Cluster","title":"API Connect Guides Overview"},{"location":"guides/cp4d-overview/","text":"Cloud Pak for Data Guides Overview \u00b6 Table of Contents Overview Base Cloud Pak: Overview Deployment on IBM Cloud Deployment on-prem Data Virtualization: Overview Deployment on IBM Cloud Deployment on-prem Spectrum Protect Plus: Overview Deployment on IBM Cloud Deployment on-prem Watson Knowledge Center: Overview Deployment on IBM Cloud Watson Studio: Overview Deployment on IBM Cloud","title":"Cloud Pak for Data Guides Overview"},{"location":"guides/cp4d-overview/#cloud-pak-for-data-guides-overview","text":"Table of Contents Overview Base Cloud Pak: Overview Deployment on IBM Cloud Deployment on-prem Data Virtualization: Overview Deployment on IBM Cloud Deployment on-prem Spectrum Protect Plus: Overview Deployment on IBM Cloud Deployment on-prem Watson Knowledge Center: Overview Deployment on IBM Cloud Watson Studio: Overview Deployment on IBM Cloud","title":"Cloud Pak for Data Guides Overview"},{"location":"guides/cp4s-overview/","text":"CP4S Guide Overview \u00b6 Table of Contents Using this guide Cloud Native MQ: Overview Target Architecture Creating the cluster: IBM Cloud Configuring the cluster: GitOps, Tekton & ArgoCD Getting started with GitOps Deploying Services Managing Lifecycle for CP4S","title":"CP4S Guide Overview"},{"location":"guides/cp4s-overview/#cp4s-guide-overview","text":"Table of Contents Using this guide Cloud Native MQ: Overview Target Architecture Creating the cluster: IBM Cloud Configuring the cluster: GitOps, Tekton & ArgoCD Getting started with GitOps Deploying Services Managing Lifecycle for CP4S","title":"CP4S Guide Overview"},{"location":"guides/guides-overview/","text":"Tutorials Overview \u00b6 Tutorials This section includes tutorials that demonstrate how to build a production level deployment based on GitOps, and add your own customizations as required. Included are tutorials to create the infrastructure, followed by cloud pak tutorials that show step-by-step how to deploy a cloud pak fully managed by GitOps. The infrastructure tutorials include examples of production ready Red Hat OpenShift clusters for different cloud providers. Please note, it is not necessary to create a production ready cluster in order to follow the step-by-step cloud pak tutorial.","title":"Overview"},{"location":"guides/guides-overview/#tutorials-overview","text":"Tutorials This section includes tutorials that demonstrate how to build a production level deployment based on GitOps, and add your own customizations as required. Included are tutorials to create the infrastructure, followed by cloud pak tutorials that show step-by-step how to deploy a cloud pak fully managed by GitOps. The infrastructure tutorials include examples of production ready Red Hat OpenShift clusters for different cloud providers. Please note, it is not necessary to create a production ready cluster in order to follow the step-by-step cloud pak tutorial.","title":"Tutorials Overview"},{"location":"guides/mq-overview/","text":"MQ Guide Overview \u00b6 Table of Contents Using this guide Cloud Native MQ: Overview Target Architecture Creating the cluster: IBM Cloud Configuring the cluster: GitOps, Tekton & ArgoCD Getting started with GitOps Deploying Services Build Queue Manager: Configuring the pipeline Running the pipeline Deploying and using Continuous updates Build MQ application: Sample application Configuring the pipeline Managing an application change High Availability: Overview In-Region Active-Passive Cross-Region Active-Passive In-Region Active-Active Architecture Decisions: Decision Index","title":"MQ Guide Overview"},{"location":"guides/mq-overview/#mq-guide-overview","text":"Table of Contents Using this guide Cloud Native MQ: Overview Target Architecture Creating the cluster: IBM Cloud Configuring the cluster: GitOps, Tekton & ArgoCD Getting started with GitOps Deploying Services Build Queue Manager: Configuring the pipeline Running the pipeline Deploying and using Continuous updates Build MQ application: Sample application Configuring the pipeline Managing an application change High Availability: Overview In-Region Active-Passive Cross-Region Active-Passive In-Region Active-Active Architecture Decisions: Decision Index","title":"MQ Guide Overview"},{"location":"guides/process-mining-overview/","text":"Process Mining Guides Overview \u00b6 Table of Contents Process Mining: Overview Architecture: IBM Cloud Deployment: IBM Cloud","title":"Process Mining Guides Overview"},{"location":"guides/process-mining-overview/#process-mining-guides-overview","text":"Table of Contents Process Mining: Overview Architecture: IBM Cloud Deployment: IBM Cloud","title":"Process Mining Guides Overview"},{"location":"guides/why-automate/","text":"Why Automate? \u00b6","title":"Why Automate?"},{"location":"guides/why-automate/#why-automate","text":"","title":"Why Automate?"},{"location":"guides/cp4ba/ads/architecture/ads/","text":"IBM Automation Decision Services Deployment Architecture \u00b6 Abstract This document describes the deployment of IBM Automation Decision Services on the RedHat OpenShift Kubernetes Service on IBM Cloud , known as ROKS, on Virtual Private Cloud (VPC) Gen 2 infrastructure. As we can see in the topology above, the RedHat OpenShift cluster has been deployed on a MultiZone Region (MZR) data center with three availability zones where Virtual Private Cloud Gen 2 is available. Note IBM Automation Decision Services requires ReadWriteMany (RWX) storage . In order to offer Read Write Many (RWX) storage for the applications running on our RedHat OpenShift cluster, we need to make OpenShift Data Foundation available in our RedHat OpenShift cluster . OpenShift Data Foundation (ODF) is a storage solution that consists of open source technologies Ceph , Noobaa , and Rook . ODF allows you to provision and manage File, Block, and Object storage for your containerized workloads in Red Hat\u00ae OpenShift\u00ae on IBM Cloud\u2122 clusters. Unlike other storage solutions where you might need to configure separate drivers and operators for each type of storage, ODF is a unified solution capable of adapting or scaling to your storage needs. The IBM Automation Decision Service runtime requires a MongoDB . For development systems, an embedded MongoDB is provided, however for Production usage an external MongoDB instance is required. In the deployment topology above we are using MongoDB as-a-service which is provided by the Cloud provider (in this case IBM Cloud). The Business Automation Navigator requires a Postgres DB . In the deployment topology above we are using Postgres as-a-service which is provided by the Cloud provider (in this case IBM Cloud). The User Management Service requires LDAP . For development systems, an embedded LDAP is provided, however for Production usage an external LDAP instance is required. In the deployment topology above we are using the VSIs to host the LDAP implementation, however this may not be required in an environment where an LDAP already exists.","title":"Architecture"},{"location":"guides/cp4ba/ads/architecture/ads/#ibm-automation-decision-services-deployment-architecture","text":"Abstract This document describes the deployment of IBM Automation Decision Services on the RedHat OpenShift Kubernetes Service on IBM Cloud , known as ROKS, on Virtual Private Cloud (VPC) Gen 2 infrastructure. As we can see in the topology above, the RedHat OpenShift cluster has been deployed on a MultiZone Region (MZR) data center with three availability zones where Virtual Private Cloud Gen 2 is available. Note IBM Automation Decision Services requires ReadWriteMany (RWX) storage . In order to offer Read Write Many (RWX) storage for the applications running on our RedHat OpenShift cluster, we need to make OpenShift Data Foundation available in our RedHat OpenShift cluster . OpenShift Data Foundation (ODF) is a storage solution that consists of open source technologies Ceph , Noobaa , and Rook . ODF allows you to provision and manage File, Block, and Object storage for your containerized workloads in Red Hat\u00ae OpenShift\u00ae on IBM Cloud\u2122 clusters. Unlike other storage solutions where you might need to configure separate drivers and operators for each type of storage, ODF is a unified solution capable of adapting or scaling to your storage needs. The IBM Automation Decision Service runtime requires a MongoDB . For development systems, an embedded MongoDB is provided, however for Production usage an external MongoDB instance is required. In the deployment topology above we are using MongoDB as-a-service which is provided by the Cloud provider (in this case IBM Cloud). The Business Automation Navigator requires a Postgres DB . In the deployment topology above we are using Postgres as-a-service which is provided by the Cloud provider (in this case IBM Cloud). The User Management Service requires LDAP . For development systems, an embedded LDAP is provided, however for Production usage an external LDAP instance is required. In the deployment topology above we are using the VSIs to host the LDAP implementation, however this may not be required in an environment where an LDAP already exists.","title":"IBM Automation Decision Services Deployment Architecture"},{"location":"guides/cp4ba/process-mining/architecture/ibm-cloud/","text":"IBM Process Mining Architecture on IBM Cloud \u00b6 Abstract This document describes the deployment of IBM Process Mining on the RedHat OpenShift Kubernetes Service on IBM Cloud , known as ROKS, on Virtual Private Cloud (VPC) Gen 2 infrastructure. Tip Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net As you can see in the topology above, the RedHat OpenShift Kubernetes Service cluster has been deployed on a MultiZone Region (MZR) data center with three availability zones (AZs) where Virtual Private Cloud (VPC) Gen 2 is available. Warning IBM Process Mining requires ReadWriteMany (RWX) storage . In order to offer Read Write Many (RWX) storage for the applications running on your RedHat OpenShift Kubernetes Service cluster on Virtual Private Cloud (VPC) Gen 2, you need to make OpenShift Data Foundation available in our RedHat OpenShift cluster . OpenShift Data Foundation (ODF) is a storage solution that consists of open source technologies Ceph , Noobaa , and Rook . ODF allows you to provision and manage File, Block, and Object storage for your containerized workloads in Red Hat\u00ae OpenShift\u00ae on IBM Cloud\u2122 clusters. Unlike other storage solutions where you might need to configure separate drivers and operators for each type of storage, ODF is a unified solution capable of adapting or scaling to your storage needs. In order to install OpenShift Data Foundation (ODF) in your RedHat OpenShift Kubernetes Service (ROKS) cluster on IBM Cloud on a Virtual Private Cloud (VPC) Gen 2, you need to make sure that your RedHat OpenShift Kubernetes Service cluster counts with at least three worker nodes. For high availability, you must create your RedHat OpenShift Kubernetes Service cluster with at least one worker node per zone across the three zones. Each worker node must have a minimum of 16 CPUs and 64 GB RAM . Important The storageClass used to configure OpenShift Data Foundation to request storage volumes must be of type metro . What metro means is that the volumeBindingMode of that storageClass will be set to WaitForFirstConsumer as opposed to the default Immediate . And what that means is that the Persistent Volume creation and allocation by the IBM Cloud Object Storage, as a result of its Persistent Volume Claim, will not happen until the pod linked to that Persistent Volume Claim is scheduled. This allows IBM Cloud Object Storage to know what Availability Zone of your MultiZone Region cluster the pod requesting block storage ended up on and, as a result, to be able to provision such storage in the appropriate place. Otherwise, if we used a storageClass whose volumeBindingMode was the default Immediate , IBM Cloud Object Storage would create and allocate the Persistent Volume in one of the Availability Zones which might not be the same Availability Zone the pod requiring such storage ends up on as a result of the OpenShift pod scheduler which would make the storage inaccessible to the pod. See Kubernetes official documentation here for further detail. Important The storageClass you need to configure OpenShift Data Foundation to use with must not have Retain Reclaim policy . If you retain the Persistent Volume, it might end up assigned to a pod in a different Availability Zone later, making that storage inaccessible to the pod allocated to. Therefore, the storageClassName you need to configure OpenShift Data Foundation to use with, in the deployment section , will need to be of either ibmc-vpc-block-metro-10iops-tier , ibmc-vpc-block-metro-5iops-tier or ibmc-vpc-block-metro-custom types. Storage \u00b6 The full install of IBM Process mining requires two mandatory persistent volumes and two optional persistent volumes. The mandatory persistent volumes are for storing process mining events and task mining events. The optional storage is for IBM DB2 and MongoDB. IBM Process Mining requires MongoDB for the process mining component and IBM DB2 (or optionally Mysql) for the task mining component. The IBM Process Mining Operator will automatically install an embedded MongoDB and IDM DB2 by default. However, this deployment is suitable for demonstration or evaluation use cases. For production environments, where performance is more important, please configure your process mining component and task mining component with a external database that you provisioned yourself. For production environments the following databases are required: Mongo DB v3.6 or higher for the process mining component. IBM DB2 11.5.6.0 for the task mining component. See the Links section at the bottom for more information on storage for IBM Process Mining. Security \u00b6 TLS certificates are mandatory to secure the exposed routes of the application. The certificates are required for the following routes: Process Mining public Rest API. Task Mining REST API for Agent and Designer integration. In a default installation, self-signed certificates are automatically created by the operator and no further action is required. However, for a production environment, your own certificates that are issued by a trusted CA should be provided within the ProcessMining CSV. See the Links section at the bottom for more information on security and certificates for IBM Process Mining. High Availability \u00b6 It is recommended that for production environments IBM Process Mining is installed highly available for better resiliency. All components of IBM Process Mining can be highly available deployed except from the embedded IBM DB2 and MongoDB components, which are not recommended for production environments and external self provisioned MongoDB and IBM DB2 databases are strongly recommended instead (see Storage section above). The highly available deployment of each of the IBM Process Mining components can be through the IBM Process Mining Custom Resource Definition (CRD) when installing your IBM Process Mining instance. See the Links section at the bottom for more information on deployment profiles for IBM Process Mining. Backup and Restore \u00b6 Back up and restore procedure for Process Mining \u00b6 The Process Mining component stores information in two places. Raw events and process analysis by directly accessing the file system, by using a persistent volume. More meta-information on processes, organizations, user-profiling on Mongo DB. The Mongo DB instance can be external or hosted in a cluster Pod. In the latter case, it stores the information on a persistent volume. Back up and Restore Procedure for Task Mining \u00b6 The Task Mining component stores information in two places: Task events and activity logs by directly accessing the file system, by using a persistent volume. Process and workflow metadata in IBM DB2 (or MySQL) DB. The IBM DB2 (or MySql) DB can be provided externally or it can be hosted in a cluster Pod. In the latter case, it will be an IBM DB2 DB and it will store the information on a persistent volume. See the Links section at the bottom for full detail on how to backup and restore the process mining and task mining components of IBM Process Mining. Sizing \u00b6 Listed below are two sizing configurations for IBM Process Mining. In order to appropriately sizing your Red Hat OpenShift cluster on IBM Cloud, you need to consider data volumes and data complexity (i.e. number of events), plus the number of concurrent users active on the application; it\u2019s not so important how many users are working, but what they are doing concurrently. Minimum Resources required for IBM Process Mining \u00b6 The following table details the minimum resources required for installing IBM Process Mining on Red Hat OpenShift for 10 concurrent users with up to 10 Million events. Software Memory (GB) CPU (cores) Disk (GB) Nodes Process Mining 64 16 100 1 Task Mining 32 8 100 1 Total 96 24 200 1 Highly Available Setup \u00b6 The following table details the minimum resources required for installing IBM Process Mining on Red Hat OpenShift for 10 concurrent users with up to 50 Million events with each of the IBM Process Mining components highly available Software Memory (GB) CPU (cores) Disk (GB) Nodes Process Mining 128 48 200 3 Task Mining 32 8 300 3 Total 160 56 500 3 See the Deployment Profiles section in the official IBM Process Mining documentation. Summary As you can see in the topology diagram above for the production reference architecture of IBM Process Mining on the RedHat OpenShift Kubernetes Service (ROKS) on IBM Cloud on Virtual Private Cloud (VPC) Gen 2, we strongly recommend you create your RedHat OpenShift Kubernetes Service cluster with 6 worker nodes , where three of them could be used for OpenShift Data Foundation only (see tip below) while the other three would be reserved to run the IBM Process Mining components. You can review above in this section the sizes for each of the worker nodes running OpenShift Data Foundation as well as the total sizing of the other three worker nodes running the IBM Process Mining components. Finally, make sure you are using a metro storage class with delete reclaim policy for OpenShift Data Foundation which will provide IBM Process Mining the required Read Write Many (RWX) storage. Tip You could use labels, tolerations and taints if you want that OpenShift Data Foundation specific workloads get deployed on completely separate worker nodes as your application workloads (IBM Process Mining in this case). Check the OpenShift Data Foundation documentation here and pay attention to the workerNodes parameter. Links \u00b6 IBM Process Mining Knowledge Center","title":"Process Mining"},{"location":"guides/cp4ba/process-mining/architecture/ibm-cloud/#ibm-process-mining-architecture-on-ibm-cloud","text":"Abstract This document describes the deployment of IBM Process Mining on the RedHat OpenShift Kubernetes Service on IBM Cloud , known as ROKS, on Virtual Private Cloud (VPC) Gen 2 infrastructure. Tip Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net As you can see in the topology above, the RedHat OpenShift Kubernetes Service cluster has been deployed on a MultiZone Region (MZR) data center with three availability zones (AZs) where Virtual Private Cloud (VPC) Gen 2 is available. Warning IBM Process Mining requires ReadWriteMany (RWX) storage . In order to offer Read Write Many (RWX) storage for the applications running on your RedHat OpenShift Kubernetes Service cluster on Virtual Private Cloud (VPC) Gen 2, you need to make OpenShift Data Foundation available in our RedHat OpenShift cluster . OpenShift Data Foundation (ODF) is a storage solution that consists of open source technologies Ceph , Noobaa , and Rook . ODF allows you to provision and manage File, Block, and Object storage for your containerized workloads in Red Hat\u00ae OpenShift\u00ae on IBM Cloud\u2122 clusters. Unlike other storage solutions where you might need to configure separate drivers and operators for each type of storage, ODF is a unified solution capable of adapting or scaling to your storage needs. In order to install OpenShift Data Foundation (ODF) in your RedHat OpenShift Kubernetes Service (ROKS) cluster on IBM Cloud on a Virtual Private Cloud (VPC) Gen 2, you need to make sure that your RedHat OpenShift Kubernetes Service cluster counts with at least three worker nodes. For high availability, you must create your RedHat OpenShift Kubernetes Service cluster with at least one worker node per zone across the three zones. Each worker node must have a minimum of 16 CPUs and 64 GB RAM . Important The storageClass used to configure OpenShift Data Foundation to request storage volumes must be of type metro . What metro means is that the volumeBindingMode of that storageClass will be set to WaitForFirstConsumer as opposed to the default Immediate . And what that means is that the Persistent Volume creation and allocation by the IBM Cloud Object Storage, as a result of its Persistent Volume Claim, will not happen until the pod linked to that Persistent Volume Claim is scheduled. This allows IBM Cloud Object Storage to know what Availability Zone of your MultiZone Region cluster the pod requesting block storage ended up on and, as a result, to be able to provision such storage in the appropriate place. Otherwise, if we used a storageClass whose volumeBindingMode was the default Immediate , IBM Cloud Object Storage would create and allocate the Persistent Volume in one of the Availability Zones which might not be the same Availability Zone the pod requiring such storage ends up on as a result of the OpenShift pod scheduler which would make the storage inaccessible to the pod. See Kubernetes official documentation here for further detail. Important The storageClass you need to configure OpenShift Data Foundation to use with must not have Retain Reclaim policy . If you retain the Persistent Volume, it might end up assigned to a pod in a different Availability Zone later, making that storage inaccessible to the pod allocated to. Therefore, the storageClassName you need to configure OpenShift Data Foundation to use with, in the deployment section , will need to be of either ibmc-vpc-block-metro-10iops-tier , ibmc-vpc-block-metro-5iops-tier or ibmc-vpc-block-metro-custom types.","title":"IBM Process Mining Architecture on IBM Cloud"},{"location":"guides/cp4ba/process-mining/architecture/ibm-cloud/#storage","text":"The full install of IBM Process mining requires two mandatory persistent volumes and two optional persistent volumes. The mandatory persistent volumes are for storing process mining events and task mining events. The optional storage is for IBM DB2 and MongoDB. IBM Process Mining requires MongoDB for the process mining component and IBM DB2 (or optionally Mysql) for the task mining component. The IBM Process Mining Operator will automatically install an embedded MongoDB and IDM DB2 by default. However, this deployment is suitable for demonstration or evaluation use cases. For production environments, where performance is more important, please configure your process mining component and task mining component with a external database that you provisioned yourself. For production environments the following databases are required: Mongo DB v3.6 or higher for the process mining component. IBM DB2 11.5.6.0 for the task mining component. See the Links section at the bottom for more information on storage for IBM Process Mining.","title":"Storage"},{"location":"guides/cp4ba/process-mining/architecture/ibm-cloud/#security","text":"TLS certificates are mandatory to secure the exposed routes of the application. The certificates are required for the following routes: Process Mining public Rest API. Task Mining REST API for Agent and Designer integration. In a default installation, self-signed certificates are automatically created by the operator and no further action is required. However, for a production environment, your own certificates that are issued by a trusted CA should be provided within the ProcessMining CSV. See the Links section at the bottom for more information on security and certificates for IBM Process Mining.","title":"Security"},{"location":"guides/cp4ba/process-mining/architecture/ibm-cloud/#high-availability","text":"It is recommended that for production environments IBM Process Mining is installed highly available for better resiliency. All components of IBM Process Mining can be highly available deployed except from the embedded IBM DB2 and MongoDB components, which are not recommended for production environments and external self provisioned MongoDB and IBM DB2 databases are strongly recommended instead (see Storage section above). The highly available deployment of each of the IBM Process Mining components can be through the IBM Process Mining Custom Resource Definition (CRD) when installing your IBM Process Mining instance. See the Links section at the bottom for more information on deployment profiles for IBM Process Mining.","title":"High Availability"},{"location":"guides/cp4ba/process-mining/architecture/ibm-cloud/#backup-and-restore","text":"","title":"Backup and Restore"},{"location":"guides/cp4ba/process-mining/architecture/ibm-cloud/#sizing","text":"Listed below are two sizing configurations for IBM Process Mining. In order to appropriately sizing your Red Hat OpenShift cluster on IBM Cloud, you need to consider data volumes and data complexity (i.e. number of events), plus the number of concurrent users active on the application; it\u2019s not so important how many users are working, but what they are doing concurrently.","title":"Sizing"},{"location":"guides/cp4ba/process-mining/architecture/ibm-cloud/#links","text":"IBM Process Mining Knowledge Center","title":"Links"},{"location":"guides/cp4ba/process-mining/cluster-config/gitops-config/","text":"GitOps Configuration \u00b6 In the cluster configuration of this chapter, you installed the fundamental components for continuous integration and continuous deployment into your cluster. These included a sample GitOps repository, and ArgoCD. In this section you're going to customize and enable the GitOps repository so that you can install IBM Process Mining. You will examine these components in more detail throughout this section of the tutorial as well. In this topic, you're going to: Explore the sample GitOps repository in a little more detail Customize the GitOps repository for your cluster Connect ArgoCD to the customized GitOps repository Bootstrap the cluster Explore how the ci namespace is created Try out some dynamic changes to the cluster with ArgoCD Explore how ArgoCD manages configuration drift By the end of this topic you'll have a cluster up and running, having used GitOps to do it. You'll fully understand how ArgoCD manages cluster change and configuration drift. Pre-requisites \u00b6 Before attempting this section, you must have completed the previous section of this tutorial where you created your Red Hat OpenShift cluster(s) and customized the GitOps repository and ArgoCD. Please see the previous sections of this guide for information on how to do these tasks. The sample GitOps repository \u00b6 Let's understand how the GitOps repository works by using it to install the components we've highlighted in the above diagram. In the first part of this topic, let's look at the high level structure of the GitOps repository. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Explore the high level folder structure Use the following command to display the folder structure: tree . -d -L 2 We can see the different folders in the GitOps repository: . \u251c\u2500\u2500 0-bootstrap \u2502 \u251c\u2500\u2500 others \u2502 \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 doc \u2502 \u251c\u2500\u2500 diagrams \u2502 \u251c\u2500\u2500 experimental \u2502 \u251c\u2500\u2500 images \u2502 \u2514\u2500\u2500 scenarios \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 bom \u2514\u2500\u2500 setup \u2514\u2500\u2500 ocp47 The 0-bootstrap folder is the key folder that contains all the available profiles. Within this folders are Cluster Profiles that control which resources are deployed to the cluster. There are other folders, but these contain utility scripts; and some document related information. Let us examine the 0-bootstrap folder structure: tree ./0-bootstrap/ -d -L 2 We can see the different folders: ./0-bootstrap/ \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u2514\u2500\u2500 3-apps If we consider single-cluster profile, you can see the three layers under that covering infrastructure, services, and applications. Use single-cluster profile to configure the cluster Use the following command to display the folder structure: tree ./0-bootstrap/single-cluster/ -L 3 We can see the different folders: ./0-bootstrap/single-cluster/ \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 1-infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 2-services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3-apps \u2502 \u251c\u2500\u2500 3-apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 cp4a \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Notice that each high level folder has an argocd folder. Within these argocd folders are ArgoCD applications that control which resources are deployed to the cluster. Sample ArgoCD application We'll see later how these ArgoCD applications work in detail. For now, notice how each layer of the architecture ( infrastructure , service , application ) has a set of ArgoCD applications. Type the following command: tree 0 -bootstrap/single-cluster/1-infra/ It shows a list of ArgoCD applications that are used to manage Kubernetes infrastructure resources: 0-bootstrap/single-cluster/1-infra/ \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Notice the namespace- YAMLs; we'll see in a moment how these each define an ArgoCD application dedicated to managing a Kubernetes namespace in our reference architecture. A word on terminology As we get started with ArgoCD, it can be easy to confuse the term ArgoCD application with your application. That's because ArgoCD uses the term application to refer to a Kubernetes custom resource that was initially designed to automate application management. However, an ArgoCD application can automate the deployment of any Kubernetes resource within a cluster, such as namespaces or queue managers, as we'll see a little later. Let's now customize this repository to deploy the highlighted Kubernetes resources to our cluster. Customizing the GitOps repositories \u00b6 The cloned GitOps configuration repositories (specifically, multi-tenancy-gitops and multi-tenancy-gitops-apps ) needs to be customized for your cluster. The ArgoCD Application and AppProject resources and kustomization.yaml resources contains environment variables which will need to be updated to your specific GitHub organization and branch. Update multi-tenancy-gitops repository \u00b6 Have a look at the following kustomization.yaml as an example of what will need to be updated. cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS_INFRA} - op: add path: /spec/source/targetRevision value: ${GIT_GITOPS_INFRA_BRANCH} Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Run the customization script Let's customize the cloned multi-tenancy-gitops repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. This script has also made some customizations that we'll use much later in the tutorial; we'll refer to those at the relevant time. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 199 files changed, 290 insertions ( + ) , 290 deletions ( - ) Set your GitOps repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (48/48), 4.70 KiB | 1.57 MiB/s, done. Total 48 (delta 32), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 23 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git d95eca5..a900c39 master -> master Update multi-tenancy-gitops-apps repository \u00b6 Similar to the previous section, there are corresponding ArgoCD Applications resources that will need to be updated in the multi-tenancy-gitops-apps repository. Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Locate the multi-tenancy-gitops-apps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops-apps Run the customization script Let's customize the cloned multi-tenancy-gitops-apps repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops-apps repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 21 files changed, 42 insertions ( + ) , 42 deletions ( - ) Set your repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops-apps repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (42/42), 4.70 KiB | 1.57 MiB/s, done. Total 42 (delta 42), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 21 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git d95eca5..a900c39 master -> master Connect ArgoCD to the GitOps repository \u00b6 Let's now connect your customized GitOps repository to the instance of ArgoCD running in the cluster. Once connected, ArgoCD will use the contents of the repository to create necessary resources. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Review ArgoCD infrastructure folder Let's examine the 0-bootstrap/single-cluster/1-infra/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml We can see the contents of the kustomization.yaml : resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Notice that the resources that needs to be applied to the cluster are all inactivate and commented out. Let us enable the resources that are needed by un-commenting them. Deploy Kubernetes resources with kustomization.yaml Open 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the below resources: argocd/consolelink.yaml argocd/consolenotification.yaml argocd/namespace-ibm-common-services.yaml argocd/namespace-ci.yaml argocd/namespace-dev.yaml argocd/namespace-staging.yaml argocd/namespace-prod.yaml argocd/namespace-sealed-secrets.yaml argocd/namespace-tools.yaml You will have the following resources un-commented for infrastructure: resources : - argocd/consolelink.yaml - argocd/consolenotification.yaml - argocd/namespace-ibm-common-services.yaml - argocd/namespace-ci.yaml - argocd/namespace-dev.yaml - argocd/namespace-staging.yaml - argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml - argocd/namespace-sealed-secrets.yaml - argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for infrastructure\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a900c39..e3f696d master -> master Examine bootstrap.yaml residing in 0-bootstrap/single-cluster/ . The bootstrap.yaml file is used to create our first ArgoCD application called bootstrap-single-cluster . This initial ArgoCD application will create all the other ArgoCD applications that control the application, service, and infrastructure resources (such as the ci and dev namespaces) deployed to the cluster. Examine the YAML that defines the ArgoCD bootstrap application: cat 0 -bootstrap/single-cluster/bootstrap.yaml Notice also how this ArgoCD application has been customized to use the GitOps repository repoURL: https://github.com/prod-ref-guide/multi-tenancy-gitops.git . apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-single-cluster namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/single-cluster repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Most importantly, see how path: 0-bootstrap/single-cluster refers to the 0-bootstrap/single-cluster folder within this repository. This will result in the creation of individual ArgoCD applications to manage our cluster resources. Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us for now only deploy infra resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and comment out the 2-services/2-services.yaml and 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml # - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Commit and push changes to your git repository: git add . git commit -s -m \"Using only infra\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 456 bytes | 456.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git e3f696d..ea3b43f master -> master Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. The repository contains a bootstrap-single-cluster ArgoCD application that is watching this repository and using its contents to manage the cluster. When the bootstrap-single-cluster ArgoCD application is applied to the cluster, it will continuously ensure that all the activated resources are applied to the cluster. Apply the bootstrap YAML to the cluster: oc apply -f 0 -bootstrap/single-cluster/bootstrap.yaml Kubernetes will confirm that the bootstrap ArgoCD application has been created: application.argoproj.io/bootstrap-single-cluster created The bootstrap ArgoCD application will watch the 0-bootstrap/single-cluster folder in our GitOps repository on GitHub. In this way, as resources are added to the infrastructure , service and application folders, they will be deployed to the cluster automatically. This is therefore the only direct cluster operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Verify the bootstrap deployment Verify that the bootstrap ArgoCD application is running with the following command: oc get app/bootstrap-single-cluster -n openshift-gitops You should see that the bootstrap application was recently updated: NAME SYNC STATUS HEALTH STATUS bootstrap-single-cluster Synced Healthy HEALTH_STATUS may temporarily show Missing ; simply re-issue the command to confirm it moves to Healthy . Using the UI to view the newly deployed ArgoCD applications In the previous section of this chapter you logged on to the ArgoCD web console. Switch back to that console, refresh the page and you should see the bootstrap-single-cluster ArgoCD application together with many other ArgoCD applications: (You may need to select List view rather than the Tiles view.) We can see that eleven ArgoCD applications have been deployed to the cluster as a result of applying bootstrap.yaml . In the next section of the tutorial, we'll examine these applications to see how and why they were created, but for now let's focus on one of them -- the namespace-ci ArgoCD application. Examining the namespace-ci ArgoCD application resources Let's examine the Kubernetes resources applied to the cluster by the namespace-ci ArgoCD application. In the ArgoCD application list, click on namespace-ci : (You may need to clear filters to see this screenshot.) The directed graph shows that the namespace-ci ArgoCD app has created 4 Kubernetes resources; our ci namespace and three role bindings. Verify the namespace using the oc CLI We've seen the new namespace definition in the GitOps repository and visually in the ArgoCD UI. Let's also verify it via the command line: Type the following command: oc get namespace ci -o yaml This will list the full details of the ci namespace: apiVersion : v1 kind : Namespace metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"namespace-ci\"},\"name\":\"ci\"},\"spec\":{}} openshift.io/sa.scc.mcs : s0:c27,c9 openshift.io/sa.scc.supplemental-groups : 1000720000/10000 openshift.io/sa.scc.uid-range : 1000720000/10000 creationTimestamp : \"2021-08-31T15:27:32Z\" labels : app.kubernetes.io/instance : namespace-ci managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:labels : . : {} f:app.kubernetes.io/instance : {} f:status : f:phase : {} manager : argocd-application-controller operation : Update time : \"2021-08-31T15:27:32Z\" - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : f:openshift.io/sa.scc.mcs : {} f:openshift.io/sa.scc.supplemental-groups : {} f:openshift.io/sa.scc.uid-range : {} manager : cluster-policy-controller operation : Update time : \"2021-08-31T15:27:32Z\" name : ci resourceVersion : \"2255607\" selfLink : /api/v1/namespaces/ci uid : fff6b82b-6318-4828-83bb-ade4e8e3c0cf spec : finalizers : - kubernetes status : phase : Active Notice how manager: argocd-application-controller identifies that this namespace was created by ArgoCD. It's important to understand the sequence of actions. We simply deployed the bootstrap-single-cluster ArgoCD application, and it ultimately resulted in the creation of the namespace-ci ArgoCD application which created the ci namespace. We don't directly apply resources to the cluster once the bootstrap-single-cluster ArgoCD application has been applied; the cluster state is determined by an ArgoCD application YAML in the corresponding application , service or infrastructure related folders. It's these ArgoCD applications that create and manage the underlying Kubernetes resources using the GitOps repository as the source of truth. Understanding the namespace-ci ArgoCD application Let's examine the ArgoCD application namespace-ci to see how it created the ci namespace and three role bindings in the cluster. Issue the following command to examine its YAML: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-ci.yaml Notice that apiVersion and kind identify this as an ArgoCD application: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ci labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" spec : destination : namespace : ci server : https://kubernetes.default.svc project : infra source : path : namespaces/ci syncPolicy : automated : prune : true selfHeal : true Most importantly, see how this namespace-ci ArgoCD application monitors the folder path: namespaces/ci in https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git ; it then applies the contents of this folder to the cluster whenever its content changes. Notice the SyncPolicy of automated ; any changes to this folder will automatically be applied to the cluster; we do not need to perform a manual Sync operation from the ArgoCD UI. Examine the ci namespace YAML To examine this, navigate to multi-tenancy-gitops-infra repository you cloned previously. cd $HOME /git cd multi-tenancy-gitops-infra Let's examine the ci namespace YAML in the namespaces/ci folder: cat namespaces/ci/namespace.yaml It's a very simple YAML: apiVersion : v1 kind : Namespace metadata : name : ci spec : {} This is a YAML that we would normally apply to the cluster manually or via a script. However, when we use GitOps, we push the ArgoCD application that uses this YAML to GitHub, and it applies the ci namespace YAML to the cluster. This is the essence of GitOps; we declare what we want to appear in the cluster using Git and ArgoCD synchronizes the cluster with this declaration. Examine the ci rolebinding YAML Now that we've seen how the namespace was created, let's see how the three other rolebindings were created by the namespace-ci ArgoCD application. In the same namespace/ci folder as the ci namespace YAML, there is a rolebinding.yaml file. This file will be also applied to the cluster by the namespace-ci ArgoCD application which is continuously watching this folder. Examine this file with the following command: cat namespaces/ci/rolebinding.yaml This YAML is slightly more complex than the namespace YAML: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-dev namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:dev --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-staging namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:staging --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-prod namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:prod --- However its structure is quite straightforward; look carefully and you'll see that there are indeed three rolebindings defined in this YAML. Each of these rolebindings binds different roles to different service accounts associated with components that will run within the ci namespace, such as Tekton pipelines for example. We'll see later how these rolebindings are important; they limit the operations that can be performed by the ci namespace, creating a well governed cluster. This confirms why we saw four resources created by the namespace-ci ArgoCD application in the ArgoCD UI: one namespace and three rolebindings. Again, notice the pattern: a single ArgoCD application manages one or more Kubernetes resources in the cluster -- using one or more YAML files in which those resources are defined. The bootstrap-single-cluster ArgoCD application in more detail In the ArgoCD UI Applications view, click on the bootstrap-single-cluster application: You can see the bootstrap application creates two Kubernetes resources, the infra ArgoCD application and the infra ArgoCD project. An ArgoCD project is a mechanism by which we can group related resources; we keep all our ArgoCD applications that manage infrastructure in the infra project. Later, we'll create a services project for the ArgoCD applications that manage the services we want deployed to the cluster such as the MQ application and queue manager pipelines. The infra ArgoCD application Let's examine the infra ArgoCD application in more detail to see how it works. In the ArgoCD UI Applications view, click on the open application icon for the infra application: We can see that the infra ArgoCD application creates 9 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It's the infra ArgoCD application that watches the 0-bootstrap/single-cluster/1-infra/argocd folder for ArgoCD applications that apply infrastructure resources to our cluster. It was the infra application that created the namespace-ci ArgoCD application which manages the ci namespace that we've been exploring in this section of the tutorial. We'll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it's not necessary, as you'll get lots of practice as we proceed. ArgoCD change management and governance \u00b6 In this final section about managing infrastructure resources, let's explore how ArgoCD provides some advanced resource management features: Dynamic Monitored Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Customize the web console banner Examine the banner in the OpenShift web console: We're going to use GitOps to modify this banner dynamically. The banner YAML This banner properties are defined by the YAML in /0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml . This YAML is currently being used by the cntk-consolenotification ArgoCD application that was deployed earlier. We can examine the YAML with the following command: cat 0 -bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml which shows the banner properties are part of the ConsoleNotification custom resource: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : cntk-consolenotification labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" finalizers : - resources-finalizer.argocd.argoproj.io spec : syncPolicy : automated : prune : true selfHeal : true destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : path : consolenotification helm : values : | ocp-console-notification: ## The name of the ConsoleNotification resource in the cluster name: \"banner-env\" ## The background color that should be used for the banner backgroundColor: teal ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text: \"Cluster Description\" See how the banner at the top of the screen: * contains the text Cluster Description * is located at top of the screen * has the color Teal Modify the YAML for this banner Let's now change this YAML In your editor, modify this YAML and change the below fields as follows: ocp-console-notification : ## The name of the ConsoleNotification resource in the cluster name : \"banner-env\" ## The background color that should be used for the banner backgroundColor : red ## The color of the text that will appear in the banner color : \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location : BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text : \"Production reference guide\" It's clear that our intention is to modify the banner's backgroundColor:red and text: Production reference guide to the newly specified values. If you look at the diff : git diff you should see the following diff --git a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml index 30adf1a..596e821 100644 --- a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml +++ b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml @@ -26,10 +26,10 @@ spec: name: \"banner-env\" ## The background color that should be used for the banner - backgroundColor: teal + backgroundColor: red ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created - text: \"Cluster Description\" + text: \"Production reference guide\" Make the web console YAML change active Let's make these changes visible to the cntk-consolenotification ArgoCD application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Modify console banner\" git push origin $GIT_BRANCH You'll see the changes being pushed to GitHub: Enumerating objects: 13, done. Counting objects: 100% (13/13), done. Delta compression using up to 8 threads Compressing objects: 100% (7/7), done. Writing objects: 100% (7/7), 670 bytes | 670.00 KiB/s, done. Total 7 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a1e8292..b49dff5 master -> master Let's see what effect they have on the web console. A dynamic change to the web console You can either wait for ArgoCD to automatically sync the cntk-consolenotification application or manually Sync it yourself: Returning to the OpenShift web console, you'll notice changes. Notice the dynamic nature of these changes; we updated the console YAML, pushed our changes to our GitOps repository and everything else happened automatically. As a result, our OpenShift console has a new banner color and text. Governing changes to the dev namespace Let's now look at how ArgoCD monitors Kubernetes resources for configuration drift, and what happens if it detects an unexpected change to a monitored resource. Don't worry about the following command; it might seem drastic and even reckless, but as you'll see, everything will be OK. Let's delete the dev namespace from the cluster: oc get namespace dev oc delete namespace dev See how the active namespace: NAME STATUS AGE dev Active 2d18h is deleted: namespace \"dev\" deleted We can see that the dev namespace has been manually deleted from the cluster. GitOps repository as a source of truth If you switch back to the ArgoCD UI, you may see that ArgoCD has detected a configuration drift: a resource is Missing (the dev namespace) namespace-dev therefore is OutOfSync namespace-dev is therefore Syncing with the GitOps repository After a while we'll see that namespace-dev is Healthy and Synced : ArgoCD has detected a configuration drift, and resynched with GitOps repository, re-applying the dev namespace to the cluster. Note You may miss seeing the first screenshot if ArgoCD detects and corrects the missing dev namespace before you get a chance to switch to the ArgoCD UI. Don't worry, you can try this operator again! The restored dev namespace Issue the following command to determine the status of the dev namespace: oc get namespace dev which confirms that the dev namespace has been re-instated: NAME STATUS AGE dev Active 115s Note that it is a different instance of the dev namespace; as indicated by its AGE value. Notice the governed nature of these changes; GitOps is our source of truth about the resources deployed to the cluster. ArgoCD restores any resources that suffer from configuration drift to their GitOps-defined configuration. Congratulations! You've used ArgoCD and the GitOps repository to set up ci , tools and dev namespaces. You've seen how to create ArgoCD applications that watch their respective GitOps namespace folders for details of the namespace resources they should apply to the cluster. You've seen how you can dynamically change deployed resources by updating the resource definition in the GitOps repository. Finally, you've experience how ArgoCD keeps the cluster synchronized with the GitOps repository as a source of truth; any unexpected configuration drift will be corrected without intervention.","title":"Getting started with GitOps"},{"location":"guides/cp4ba/process-mining/cluster-config/gitops-config/#gitops-configuration","text":"In the cluster configuration of this chapter, you installed the fundamental components for continuous integration and continuous deployment into your cluster. These included a sample GitOps repository, and ArgoCD. In this section you're going to customize and enable the GitOps repository so that you can install IBM Process Mining. You will examine these components in more detail throughout this section of the tutorial as well. In this topic, you're going to: Explore the sample GitOps repository in a little more detail Customize the GitOps repository for your cluster Connect ArgoCD to the customized GitOps repository Bootstrap the cluster Explore how the ci namespace is created Try out some dynamic changes to the cluster with ArgoCD Explore how ArgoCD manages configuration drift By the end of this topic you'll have a cluster up and running, having used GitOps to do it. You'll fully understand how ArgoCD manages cluster change and configuration drift.","title":"GitOps Configuration"},{"location":"guides/cp4ba/process-mining/cluster-config/gitops-config/#pre-requisites","text":"Before attempting this section, you must have completed the previous section of this tutorial where you created your Red Hat OpenShift cluster(s) and customized the GitOps repository and ArgoCD. Please see the previous sections of this guide for information on how to do these tasks.","title":"Pre-requisites"},{"location":"guides/cp4ba/process-mining/cluster-config/gitops-config/#the-sample-gitops-repository","text":"Let's understand how the GitOps repository works by using it to install the components we've highlighted in the above diagram. In the first part of this topic, let's look at the high level structure of the GitOps repository. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Explore the high level folder structure Use the following command to display the folder structure: tree . -d -L 2 We can see the different folders in the GitOps repository: . \u251c\u2500\u2500 0-bootstrap \u2502 \u251c\u2500\u2500 others \u2502 \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 doc \u2502 \u251c\u2500\u2500 diagrams \u2502 \u251c\u2500\u2500 experimental \u2502 \u251c\u2500\u2500 images \u2502 \u2514\u2500\u2500 scenarios \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 bom \u2514\u2500\u2500 setup \u2514\u2500\u2500 ocp47 The 0-bootstrap folder is the key folder that contains all the available profiles. Within this folders are Cluster Profiles that control which resources are deployed to the cluster. There are other folders, but these contain utility scripts; and some document related information. Let us examine the 0-bootstrap folder structure: tree ./0-bootstrap/ -d -L 2 We can see the different folders: ./0-bootstrap/ \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u2514\u2500\u2500 3-apps If we consider single-cluster profile, you can see the three layers under that covering infrastructure, services, and applications. Use single-cluster profile to configure the cluster Use the following command to display the folder structure: tree ./0-bootstrap/single-cluster/ -L 3 We can see the different folders: ./0-bootstrap/single-cluster/ \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 1-infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 2-services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3-apps \u2502 \u251c\u2500\u2500 3-apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 cp4a \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Notice that each high level folder has an argocd folder. Within these argocd folders are ArgoCD applications that control which resources are deployed to the cluster. Sample ArgoCD application We'll see later how these ArgoCD applications work in detail. For now, notice how each layer of the architecture ( infrastructure , service , application ) has a set of ArgoCD applications. Type the following command: tree 0 -bootstrap/single-cluster/1-infra/ It shows a list of ArgoCD applications that are used to manage Kubernetes infrastructure resources: 0-bootstrap/single-cluster/1-infra/ \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Notice the namespace- YAMLs; we'll see in a moment how these each define an ArgoCD application dedicated to managing a Kubernetes namespace in our reference architecture. A word on terminology As we get started with ArgoCD, it can be easy to confuse the term ArgoCD application with your application. That's because ArgoCD uses the term application to refer to a Kubernetes custom resource that was initially designed to automate application management. However, an ArgoCD application can automate the deployment of any Kubernetes resource within a cluster, such as namespaces or queue managers, as we'll see a little later. Let's now customize this repository to deploy the highlighted Kubernetes resources to our cluster.","title":"The sample GitOps repository"},{"location":"guides/cp4ba/process-mining/cluster-config/gitops-config/#customizing-the-gitops-repositories","text":"The cloned GitOps configuration repositories (specifically, multi-tenancy-gitops and multi-tenancy-gitops-apps ) needs to be customized for your cluster. The ArgoCD Application and AppProject resources and kustomization.yaml resources contains environment variables which will need to be updated to your specific GitHub organization and branch.","title":"Customizing the GitOps repositories"},{"location":"guides/cp4ba/process-mining/cluster-config/gitops-config/#update-multi-tenancy-gitops-repository","text":"Have a look at the following kustomization.yaml as an example of what will need to be updated. cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS_INFRA} - op: add path: /spec/source/targetRevision value: ${GIT_GITOPS_INFRA_BRANCH} Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Run the customization script Let's customize the cloned multi-tenancy-gitops repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. This script has also made some customizations that we'll use much later in the tutorial; we'll refer to those at the relevant time. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 199 files changed, 290 insertions ( + ) , 290 deletions ( - ) Set your GitOps repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (48/48), 4.70 KiB | 1.57 MiB/s, done. Total 48 (delta 32), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 23 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git d95eca5..a900c39 master -> master","title":"Update multi-tenancy-gitops repository"},{"location":"guides/cp4ba/process-mining/cluster-config/gitops-config/#update-multi-tenancy-gitops-apps-repository","text":"Similar to the previous section, there are corresponding ArgoCD Applications resources that will need to be updated in the multi-tenancy-gitops-apps repository. Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Locate the multi-tenancy-gitops-apps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops-apps Run the customization script Let's customize the cloned multi-tenancy-gitops-apps repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops-apps repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 21 files changed, 42 insertions ( + ) , 42 deletions ( - ) Set your repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops-apps repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (42/42), 4.70 KiB | 1.57 MiB/s, done. Total 42 (delta 42), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 21 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git d95eca5..a900c39 master -> master","title":"Update multi-tenancy-gitops-apps repository"},{"location":"guides/cp4ba/process-mining/cluster-config/gitops-config/#connect-argocd-to-the-gitops-repository","text":"Let's now connect your customized GitOps repository to the instance of ArgoCD running in the cluster. Once connected, ArgoCD will use the contents of the repository to create necessary resources. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Review ArgoCD infrastructure folder Let's examine the 0-bootstrap/single-cluster/1-infra/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml We can see the contents of the kustomization.yaml : resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Notice that the resources that needs to be applied to the cluster are all inactivate and commented out. Let us enable the resources that are needed by un-commenting them. Deploy Kubernetes resources with kustomization.yaml Open 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the below resources: argocd/consolelink.yaml argocd/consolenotification.yaml argocd/namespace-ibm-common-services.yaml argocd/namespace-ci.yaml argocd/namespace-dev.yaml argocd/namespace-staging.yaml argocd/namespace-prod.yaml argocd/namespace-sealed-secrets.yaml argocd/namespace-tools.yaml You will have the following resources un-commented for infrastructure: resources : - argocd/consolelink.yaml - argocd/consolenotification.yaml - argocd/namespace-ibm-common-services.yaml - argocd/namespace-ci.yaml - argocd/namespace-dev.yaml - argocd/namespace-staging.yaml - argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml - argocd/namespace-sealed-secrets.yaml - argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for infrastructure\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a900c39..e3f696d master -> master Examine bootstrap.yaml residing in 0-bootstrap/single-cluster/ . The bootstrap.yaml file is used to create our first ArgoCD application called bootstrap-single-cluster . This initial ArgoCD application will create all the other ArgoCD applications that control the application, service, and infrastructure resources (such as the ci and dev namespaces) deployed to the cluster. Examine the YAML that defines the ArgoCD bootstrap application: cat 0 -bootstrap/single-cluster/bootstrap.yaml Notice also how this ArgoCD application has been customized to use the GitOps repository repoURL: https://github.com/prod-ref-guide/multi-tenancy-gitops.git . apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-single-cluster namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/single-cluster repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Most importantly, see how path: 0-bootstrap/single-cluster refers to the 0-bootstrap/single-cluster folder within this repository. This will result in the creation of individual ArgoCD applications to manage our cluster resources. Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us for now only deploy infra resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and comment out the 2-services/2-services.yaml and 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml # - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Commit and push changes to your git repository: git add . git commit -s -m \"Using only infra\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 456 bytes | 456.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git e3f696d..ea3b43f master -> master Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. The repository contains a bootstrap-single-cluster ArgoCD application that is watching this repository and using its contents to manage the cluster. When the bootstrap-single-cluster ArgoCD application is applied to the cluster, it will continuously ensure that all the activated resources are applied to the cluster. Apply the bootstrap YAML to the cluster: oc apply -f 0 -bootstrap/single-cluster/bootstrap.yaml Kubernetes will confirm that the bootstrap ArgoCD application has been created: application.argoproj.io/bootstrap-single-cluster created The bootstrap ArgoCD application will watch the 0-bootstrap/single-cluster folder in our GitOps repository on GitHub. In this way, as resources are added to the infrastructure , service and application folders, they will be deployed to the cluster automatically. This is therefore the only direct cluster operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Verify the bootstrap deployment Verify that the bootstrap ArgoCD application is running with the following command: oc get app/bootstrap-single-cluster -n openshift-gitops You should see that the bootstrap application was recently updated: NAME SYNC STATUS HEALTH STATUS bootstrap-single-cluster Synced Healthy HEALTH_STATUS may temporarily show Missing ; simply re-issue the command to confirm it moves to Healthy . Using the UI to view the newly deployed ArgoCD applications In the previous section of this chapter you logged on to the ArgoCD web console. Switch back to that console, refresh the page and you should see the bootstrap-single-cluster ArgoCD application together with many other ArgoCD applications: (You may need to select List view rather than the Tiles view.) We can see that eleven ArgoCD applications have been deployed to the cluster as a result of applying bootstrap.yaml . In the next section of the tutorial, we'll examine these applications to see how and why they were created, but for now let's focus on one of them -- the namespace-ci ArgoCD application. Examining the namespace-ci ArgoCD application resources Let's examine the Kubernetes resources applied to the cluster by the namespace-ci ArgoCD application. In the ArgoCD application list, click on namespace-ci : (You may need to clear filters to see this screenshot.) The directed graph shows that the namespace-ci ArgoCD app has created 4 Kubernetes resources; our ci namespace and three role bindings. Verify the namespace using the oc CLI We've seen the new namespace definition in the GitOps repository and visually in the ArgoCD UI. Let's also verify it via the command line: Type the following command: oc get namespace ci -o yaml This will list the full details of the ci namespace: apiVersion : v1 kind : Namespace metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"namespace-ci\"},\"name\":\"ci\"},\"spec\":{}} openshift.io/sa.scc.mcs : s0:c27,c9 openshift.io/sa.scc.supplemental-groups : 1000720000/10000 openshift.io/sa.scc.uid-range : 1000720000/10000 creationTimestamp : \"2021-08-31T15:27:32Z\" labels : app.kubernetes.io/instance : namespace-ci managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:labels : . : {} f:app.kubernetes.io/instance : {} f:status : f:phase : {} manager : argocd-application-controller operation : Update time : \"2021-08-31T15:27:32Z\" - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : f:openshift.io/sa.scc.mcs : {} f:openshift.io/sa.scc.supplemental-groups : {} f:openshift.io/sa.scc.uid-range : {} manager : cluster-policy-controller operation : Update time : \"2021-08-31T15:27:32Z\" name : ci resourceVersion : \"2255607\" selfLink : /api/v1/namespaces/ci uid : fff6b82b-6318-4828-83bb-ade4e8e3c0cf spec : finalizers : - kubernetes status : phase : Active Notice how manager: argocd-application-controller identifies that this namespace was created by ArgoCD. It's important to understand the sequence of actions. We simply deployed the bootstrap-single-cluster ArgoCD application, and it ultimately resulted in the creation of the namespace-ci ArgoCD application which created the ci namespace. We don't directly apply resources to the cluster once the bootstrap-single-cluster ArgoCD application has been applied; the cluster state is determined by an ArgoCD application YAML in the corresponding application , service or infrastructure related folders. It's these ArgoCD applications that create and manage the underlying Kubernetes resources using the GitOps repository as the source of truth. Understanding the namespace-ci ArgoCD application Let's examine the ArgoCD application namespace-ci to see how it created the ci namespace and three role bindings in the cluster. Issue the following command to examine its YAML: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-ci.yaml Notice that apiVersion and kind identify this as an ArgoCD application: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ci labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" spec : destination : namespace : ci server : https://kubernetes.default.svc project : infra source : path : namespaces/ci syncPolicy : automated : prune : true selfHeal : true Most importantly, see how this namespace-ci ArgoCD application monitors the folder path: namespaces/ci in https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git ; it then applies the contents of this folder to the cluster whenever its content changes. Notice the SyncPolicy of automated ; any changes to this folder will automatically be applied to the cluster; we do not need to perform a manual Sync operation from the ArgoCD UI. Examine the ci namespace YAML To examine this, navigate to multi-tenancy-gitops-infra repository you cloned previously. cd $HOME /git cd multi-tenancy-gitops-infra Let's examine the ci namespace YAML in the namespaces/ci folder: cat namespaces/ci/namespace.yaml It's a very simple YAML: apiVersion : v1 kind : Namespace metadata : name : ci spec : {} This is a YAML that we would normally apply to the cluster manually or via a script. However, when we use GitOps, we push the ArgoCD application that uses this YAML to GitHub, and it applies the ci namespace YAML to the cluster. This is the essence of GitOps; we declare what we want to appear in the cluster using Git and ArgoCD synchronizes the cluster with this declaration. Examine the ci rolebinding YAML Now that we've seen how the namespace was created, let's see how the three other rolebindings were created by the namespace-ci ArgoCD application. In the same namespace/ci folder as the ci namespace YAML, there is a rolebinding.yaml file. This file will be also applied to the cluster by the namespace-ci ArgoCD application which is continuously watching this folder. Examine this file with the following command: cat namespaces/ci/rolebinding.yaml This YAML is slightly more complex than the namespace YAML: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-dev namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:dev --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-staging namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:staging --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-prod namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:prod --- However its structure is quite straightforward; look carefully and you'll see that there are indeed three rolebindings defined in this YAML. Each of these rolebindings binds different roles to different service accounts associated with components that will run within the ci namespace, such as Tekton pipelines for example. We'll see later how these rolebindings are important; they limit the operations that can be performed by the ci namespace, creating a well governed cluster. This confirms why we saw four resources created by the namespace-ci ArgoCD application in the ArgoCD UI: one namespace and three rolebindings. Again, notice the pattern: a single ArgoCD application manages one or more Kubernetes resources in the cluster -- using one or more YAML files in which those resources are defined. The bootstrap-single-cluster ArgoCD application in more detail In the ArgoCD UI Applications view, click on the bootstrap-single-cluster application: You can see the bootstrap application creates two Kubernetes resources, the infra ArgoCD application and the infra ArgoCD project. An ArgoCD project is a mechanism by which we can group related resources; we keep all our ArgoCD applications that manage infrastructure in the infra project. Later, we'll create a services project for the ArgoCD applications that manage the services we want deployed to the cluster such as the MQ application and queue manager pipelines. The infra ArgoCD application Let's examine the infra ArgoCD application in more detail to see how it works. In the ArgoCD UI Applications view, click on the open application icon for the infra application: We can see that the infra ArgoCD application creates 9 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It's the infra ArgoCD application that watches the 0-bootstrap/single-cluster/1-infra/argocd folder for ArgoCD applications that apply infrastructure resources to our cluster. It was the infra application that created the namespace-ci ArgoCD application which manages the ci namespace that we've been exploring in this section of the tutorial. We'll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it's not necessary, as you'll get lots of practice as we proceed.","title":"Connect ArgoCD to the GitOps repository"},{"location":"guides/cp4ba/process-mining/cluster-config/gitops-config/#argocd-change-management-and-governance","text":"In this final section about managing infrastructure resources, let's explore how ArgoCD provides some advanced resource management features: Dynamic Monitored Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Customize the web console banner Examine the banner in the OpenShift web console: We're going to use GitOps to modify this banner dynamically. The banner YAML This banner properties are defined by the YAML in /0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml . This YAML is currently being used by the cntk-consolenotification ArgoCD application that was deployed earlier. We can examine the YAML with the following command: cat 0 -bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml which shows the banner properties are part of the ConsoleNotification custom resource: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : cntk-consolenotification labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" finalizers : - resources-finalizer.argocd.argoproj.io spec : syncPolicy : automated : prune : true selfHeal : true destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : path : consolenotification helm : values : | ocp-console-notification: ## The name of the ConsoleNotification resource in the cluster name: \"banner-env\" ## The background color that should be used for the banner backgroundColor: teal ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text: \"Cluster Description\" See how the banner at the top of the screen: * contains the text Cluster Description * is located at top of the screen * has the color Teal Modify the YAML for this banner Let's now change this YAML In your editor, modify this YAML and change the below fields as follows: ocp-console-notification : ## The name of the ConsoleNotification resource in the cluster name : \"banner-env\" ## The background color that should be used for the banner backgroundColor : red ## The color of the text that will appear in the banner color : \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location : BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text : \"Production reference guide\" It's clear that our intention is to modify the banner's backgroundColor:red and text: Production reference guide to the newly specified values. If you look at the diff : git diff you should see the following diff --git a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml index 30adf1a..596e821 100644 --- a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml +++ b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml @@ -26,10 +26,10 @@ spec: name: \"banner-env\" ## The background color that should be used for the banner - backgroundColor: teal + backgroundColor: red ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created - text: \"Cluster Description\" + text: \"Production reference guide\" Make the web console YAML change active Let's make these changes visible to the cntk-consolenotification ArgoCD application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Modify console banner\" git push origin $GIT_BRANCH You'll see the changes being pushed to GitHub: Enumerating objects: 13, done. Counting objects: 100% (13/13), done. Delta compression using up to 8 threads Compressing objects: 100% (7/7), done. Writing objects: 100% (7/7), 670 bytes | 670.00 KiB/s, done. Total 7 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a1e8292..b49dff5 master -> master Let's see what effect they have on the web console. A dynamic change to the web console You can either wait for ArgoCD to automatically sync the cntk-consolenotification application or manually Sync it yourself: Returning to the OpenShift web console, you'll notice changes. Notice the dynamic nature of these changes; we updated the console YAML, pushed our changes to our GitOps repository and everything else happened automatically. As a result, our OpenShift console has a new banner color and text. Governing changes to the dev namespace Let's now look at how ArgoCD monitors Kubernetes resources for configuration drift, and what happens if it detects an unexpected change to a monitored resource. Don't worry about the following command; it might seem drastic and even reckless, but as you'll see, everything will be OK. Let's delete the dev namespace from the cluster: oc get namespace dev oc delete namespace dev See how the active namespace: NAME STATUS AGE dev Active 2d18h is deleted: namespace \"dev\" deleted We can see that the dev namespace has been manually deleted from the cluster. GitOps repository as a source of truth If you switch back to the ArgoCD UI, you may see that ArgoCD has detected a configuration drift: a resource is Missing (the dev namespace) namespace-dev therefore is OutOfSync namespace-dev is therefore Syncing with the GitOps repository After a while we'll see that namespace-dev is Healthy and Synced : ArgoCD has detected a configuration drift, and resynched with GitOps repository, re-applying the dev namespace to the cluster. Note You may miss seeing the first screenshot if ArgoCD detects and corrects the missing dev namespace before you get a chance to switch to the ArgoCD UI. Don't worry, you can try this operator again! The restored dev namespace Issue the following command to determine the status of the dev namespace: oc get namespace dev which confirms that the dev namespace has been re-instated: NAME STATUS AGE dev Active 115s Note that it is a different instance of the dev namespace; as indicated by its AGE value. Notice the governed nature of these changes; GitOps is our source of truth about the resources deployed to the cluster. ArgoCD restores any resources that suffer from configuration drift to their GitOps-defined configuration. Congratulations! You've used ArgoCD and the GitOps repository to set up ci , tools and dev namespaces. You've seen how to create ArgoCD applications that watch their respective GitOps namespace folders for details of the namespace resources they should apply to the cluster. You've seen how you can dynamically change deployed resources by updating the resource definition in the GitOps repository. Finally, you've experience how ArgoCD keeps the cluster synchronized with the GitOps repository as a source of truth; any unexpected configuration drift will be corrected without intervention.","title":"ArgoCD change management and governance"},{"location":"guides/cp4ba/process-mining/cluster-config/gitops-tekton-argocd/","text":"GitOps & ArgoCD \u00b6 Overview \u00b6 A GitOps model must be at the core of the methodology or approach used to deploy and manage any IBM capability in production. It ensures that the installation, configuration and changes of these IBM capabilities, as well as their configuration and citizens (applications, integration flows, APIs, etc), is only done through code. GitOps puts git repositories and git commands such as git push (to request a change) and git merge (to approve a change) at the heart of configuration management. A GitOps approach helps an organization implement best practices in version control and release governance based on a widely used open standard -- git . The following diagram outlines the major components in an IBM Process Mining GitOps model: Tip Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net ArgoCD (OpenShift GitOps) applications watch a Git config repository for changes. OpenShift GitOps applies the Kubernetes resources thus identified to the cluster, resulting in new or updated Kubernetes resources that represent the changed IBM Process Mining cluster and its ecosystem, such as pods, routes etc. OpenShift GitOps changes are durable; they remain as defined unless and until they are explicitly changed or deleted in the GitOps repository. Moreover, if the cluster resources drift from their Git config values, ArgoCD will restore them to their desired values; only changes that are applied to the Git config repository affect the long term state of the cluster. In general, in application development, OpenShift GitOps is used for Continuous Deployment . In this section of the tutorial, you're going to set up the GitOps repository, and install ArgoCD. More precisely, you are going to Create a Github Organization Download a sample GitOps repository and briefly review it Install ArgoCD Customize ArgoCD Login in to the ArgoCD UI By the end of this topic you'll have a all the basic components in place to perform GitOps in your cluster. Pre-requisites \u00b6 Before attempting this section, you must have completed the previous Create the Cluster section. Downloading the sample GitOps repository \u00b6 In this topic section, we're going to create the GitOps repository for our cluster. We'll use a sample GitOps repository as a starting point; we'll make our own copy and review its contents. Later in the tutorial, we'll customize it for our organization and cluster. ( If you'd like to re-familiarize yourself with git, GitHub or managing repositories, you'll find the following short video and short read helpful. ) Create a new GitHub organization Let's start by creating our own GitHub organization. In your browser, navigate to https://github.com/ . In the upper-right corner, click on the profile photo, and then click Settings . In user settings sidebar, click Organizations . In the \"Organizations\" section, click New organization. Create a new organization. Create the GitOps repositories in the new GitHub organization Let's start by making our own copy of the GitOps repository. In your browser, navigate to https://github.com/cloud-native-toolkit/multi-tenancy-gitops : Click on Use this template at the top RHS of the screen and you'll be asked where you'd like to use this repository: Select your organization, name the repository and click Create repository from template . After a few seconds the repository will be present in your chosen account: You now have a copy of the sample GitOps repository in your GitHub account. Create the infrastructure, services and applications GitOps repositories from the templates into the organization your GitHub organization As shown above, follow similar steps and copy the the repository templates into your organization. GitOps Infrastructure Repository GitOps Services Repository GitOps Application Repository Your organization now will have the below repositories in it. Set up environment variable for GitHub Organization This tutorial uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Replace <git-organization> in the following command: export GIT_ORG = <git-organization> You can verify your $GIT_ORG as follows: echo $GIT_ORG Change to your working git folder Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . We've used this default in the following command, but you may have a different location on your local machine. Change to your working git folder: cd $HOME /git You may need to create this folder if it's not already created. Clone the GitOps repository We're going to work on a local clone of the four repositories. Clone the GitOps repositories to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops'... remote: Enumerating objects: 132, done. remote: Counting objects: 100% (132/132), done. remote: Compressing objects: 100% (54/54), done. remote: Total 132 (delta 72), reused 118 (delta 67), pack-reused 0 Receiving objects: 100% (132/132), 310.92 KiB | 3.24 MiB/s, done. Resolving deltas: 100% (72/72), done. Clone the GitOps Infra repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-infra.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-infra'... remote: Enumerating objects: 62, done. remote: Counting objects: 100% (62/62), done. remote: Compressing objects: 100% (44/44), done. remote: Total 62 (delta 7), reused 53 (delta 3), pack-reused 0 Unpacking objects: 100% (62/62), done. Clone the GitOps Services repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-services.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-services'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Clone the GitOps Applications repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-apps'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Info Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the folder containing the local copy of your new repository: cd multi-tenancy-gitops Review the GitOps folder structure Show the folder structure with the following command: tree . -L 1 This will list all the top level folders and files: . \u251c\u2500\u2500 0-bootstrap \u251c\u2500\u2500 README.md \u251c\u2500\u2500 doc \u251c\u2500\u2500 scripts \u2514\u2500\u2500 setup The main folders perform the following roles: 0-bootstrap contains different profiles that can be used to initialize the cluster ready. We can choose one of the available profiles. scripts and setup contain useful scripts that we'll use during this tutorial. Feel free to explore this structure. We'll examine it in much more detail throughout this tutorial, and you'll become very familiar with it by trying it out. Installing ArgoCD for GitOps \u00b6 Now that we've had an initial view of the GitOps repository, let's install ArgoCD to make use of it. We will install ArgoCD using the Red Hat OpenShift GitOps operator. We will also configure it with the appropriate access rights to limit the resources it can create in the cluster. In so doing, we ensure that ArgoCD acts like a well-governed administrator of the cluster, only doing what it needs to do. Also, we will customize our ArgoCD instance by adding necessary custom checks. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". On successful login, you'll see: Logged into \"https://c100-e.jp-tok.containers.cloud.ibm.com:30749\" as \"IAM#hperabat@in.ibm.com\" using the token provided. You have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\" . This shows some basic information about your user within this cluster. (This user is different to your GitHub user.) Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. The following command creates the subscription: oc apply -f setup/ocp47/ The response confirms that the below resources has been created: clusterrole.rbac.authorization.k8s.io/custom-argocd-cluster-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-cntk-argocd-application-controller created subscription.operators.coreos.com/openshift-gitops-operator created Wait for the ArgoCD installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/gitops-operator-controller-manager -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/gitops-operator condition met ArgoCD is now installed and ready to use. If you see something like: Error from server (NotFound): deployments.apps \"gitops-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Alternatively, you can also validate as follows: while ! oc wait crd applications.argoproj.io --timeout = -1s --for = condition = Established 2 >/dev/null ; do sleep 30 ; done while ! oc wait pod --timeout = -1s --for = condition = Ready -l '!job-name' -n openshift-gitops > /dev/null ; do sleep 30 ; done ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are also setup along with the subscription. oc get clusterrole custom-argocd-cluster-argocd-application-controller oc get clusterrolebinding openshift-gitops-argocd-application-controller oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller You'll see the resources are created in the cluster: $ oc get clusterrole custom-argocd-cluster-argocd-application-controller NAME CREATED AT custom-argocd-cluster-argocd-application-controller 2021-08-27T13:35:13Z $ oc get clusterrolebinding openshift-gitops-argocd-application-controller NAME ROLE AGE openshift-gitops-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m43s $ oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller NAME ROLE AGE openshift-gitops-cntk-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m45s The clusterrole custom-argocd-cluster-argocd-application-controller defines a specific set of specific resources that are required by the ArgoCD service account and access rights over them. The cluster role bindings openshift-gitops-argocd-application-controller and openshift-gitops-cntk-argocd-application-controller binds the ArgoCD service account to the cluster role above. In combination, these definitions limit ArgoCD to perform the minimum set of operations required to manage the cluster. This is important; it means that ArgoCD behaves as a well governed administrator of the cluster. We'll also see later in the tutorial how ArgoCD creates resource in the cluster. Delete default ArgoCD instance Let us now delete the default ArgoCD instance that is created earlier. oc delete gitopsservice cluster -n openshift-gitops || true You will see the resources being deleted as follows: warning: deleting cluster-scoped resources, not scoped to the provided namespace gitopsservice.pipelines.openshift.io \"cluster\" deleted We are deleting the existing default instance and planning to make use of a customized ArgoCD instance with additional capabilities. Create a custom ArgoCD instance This customized ArgoCD instance implements custom checks. By default ArgoCD provides some built-in health checks for validating standard kubernetes resources. However, these checks will not be sufficient to validate the custom resources that belong to IBM Cloud Pak for Integration. Argo CD supports custom health checks that are defined in Lua . If you want explore more about the ArgoCD Resource Health, check this link out. Let us create a custom ArgoCD instance as follows: oc apply -f setup/ocp47/argocd-instance/ -n openshift-gitops The response confirms that the below resources has been created: argocd.argoproj.io/openshift-gitops-cntk created Wait till the ArgoCD instance pod is up and running. Issue the below command to verify the same: while ! oc wait pod --timeout = -1s --for = condition = ContainersReady -l app.kubernetes.io/name = openshift-gitops-cntk-server -n openshift-gitops > /dev/null ; do sleep 30 ; done Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops | grep openshift-gitops-cntk-server | awk '{print \"https://\"$2}' This will list the route to the ArgoCD we've just installed, for example: https://openshift-gitops-cntk-server-openshift-gitops.ibmcloud-roks-xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/openshift-gitops-cntk-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Once the UI launches, you'll see: See how there are no ArgoCD applications active at the moment. In the next section of the tutorial, we'll configure ArgoCD to create the ArgoCD applications that will in turn spin up infrastructure , service , and application resources to apply to the cluster. Congratulations! You have created the GitOps organization based on the sample GitOps repositories that will be used to manage the IBM capabilities and applications that you will install on your cluster. You have also briefly examined the structure of the main GitOps repository driving what will get installed. You have also installed ArgoCD. You have created a specific clusterrole and clusterrolebinding for the ArgoCD service account to ensure that it is able to manage the cluster in a well governed manner. Finally, you have launched the ArgoCD UI; you will make extensive use of it during this tutorial.","title":"GitOps & ArgoCD"},{"location":"guides/cp4ba/process-mining/cluster-config/gitops-tekton-argocd/#gitops-argocd","text":"","title":"GitOps &amp; ArgoCD"},{"location":"guides/cp4ba/process-mining/cluster-config/gitops-tekton-argocd/#overview","text":"A GitOps model must be at the core of the methodology or approach used to deploy and manage any IBM capability in production. It ensures that the installation, configuration and changes of these IBM capabilities, as well as their configuration and citizens (applications, integration flows, APIs, etc), is only done through code. GitOps puts git repositories and git commands such as git push (to request a change) and git merge (to approve a change) at the heart of configuration management. A GitOps approach helps an organization implement best practices in version control and release governance based on a widely used open standard -- git . The following diagram outlines the major components in an IBM Process Mining GitOps model: Tip Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net ArgoCD (OpenShift GitOps) applications watch a Git config repository for changes. OpenShift GitOps applies the Kubernetes resources thus identified to the cluster, resulting in new or updated Kubernetes resources that represent the changed IBM Process Mining cluster and its ecosystem, such as pods, routes etc. OpenShift GitOps changes are durable; they remain as defined unless and until they are explicitly changed or deleted in the GitOps repository. Moreover, if the cluster resources drift from their Git config values, ArgoCD will restore them to their desired values; only changes that are applied to the Git config repository affect the long term state of the cluster. In general, in application development, OpenShift GitOps is used for Continuous Deployment . In this section of the tutorial, you're going to set up the GitOps repository, and install ArgoCD. More precisely, you are going to Create a Github Organization Download a sample GitOps repository and briefly review it Install ArgoCD Customize ArgoCD Login in to the ArgoCD UI By the end of this topic you'll have a all the basic components in place to perform GitOps in your cluster.","title":"Overview"},{"location":"guides/cp4ba/process-mining/cluster-config/gitops-tekton-argocd/#pre-requisites","text":"Before attempting this section, you must have completed the previous Create the Cluster section.","title":"Pre-requisites"},{"location":"guides/cp4ba/process-mining/cluster-config/gitops-tekton-argocd/#downloading-the-sample-gitops-repository","text":"In this topic section, we're going to create the GitOps repository for our cluster. We'll use a sample GitOps repository as a starting point; we'll make our own copy and review its contents. Later in the tutorial, we'll customize it for our organization and cluster. ( If you'd like to re-familiarize yourself with git, GitHub or managing repositories, you'll find the following short video and short read helpful. ) Create a new GitHub organization Let's start by creating our own GitHub organization. In your browser, navigate to https://github.com/ . In the upper-right corner, click on the profile photo, and then click Settings . In user settings sidebar, click Organizations . In the \"Organizations\" section, click New organization. Create a new organization. Create the GitOps repositories in the new GitHub organization Let's start by making our own copy of the GitOps repository. In your browser, navigate to https://github.com/cloud-native-toolkit/multi-tenancy-gitops : Click on Use this template at the top RHS of the screen and you'll be asked where you'd like to use this repository: Select your organization, name the repository and click Create repository from template . After a few seconds the repository will be present in your chosen account: You now have a copy of the sample GitOps repository in your GitHub account. Create the infrastructure, services and applications GitOps repositories from the templates into the organization your GitHub organization As shown above, follow similar steps and copy the the repository templates into your organization. GitOps Infrastructure Repository GitOps Services Repository GitOps Application Repository Your organization now will have the below repositories in it. Set up environment variable for GitHub Organization This tutorial uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Replace <git-organization> in the following command: export GIT_ORG = <git-organization> You can verify your $GIT_ORG as follows: echo $GIT_ORG Change to your working git folder Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . We've used this default in the following command, but you may have a different location on your local machine. Change to your working git folder: cd $HOME /git You may need to create this folder if it's not already created. Clone the GitOps repository We're going to work on a local clone of the four repositories. Clone the GitOps repositories to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops'... remote: Enumerating objects: 132, done. remote: Counting objects: 100% (132/132), done. remote: Compressing objects: 100% (54/54), done. remote: Total 132 (delta 72), reused 118 (delta 67), pack-reused 0 Receiving objects: 100% (132/132), 310.92 KiB | 3.24 MiB/s, done. Resolving deltas: 100% (72/72), done. Clone the GitOps Infra repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-infra.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-infra'... remote: Enumerating objects: 62, done. remote: Counting objects: 100% (62/62), done. remote: Compressing objects: 100% (44/44), done. remote: Total 62 (delta 7), reused 53 (delta 3), pack-reused 0 Unpacking objects: 100% (62/62), done. Clone the GitOps Services repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-services.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-services'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Clone the GitOps Applications repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-apps'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Info Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the folder containing the local copy of your new repository: cd multi-tenancy-gitops Review the GitOps folder structure Show the folder structure with the following command: tree . -L 1 This will list all the top level folders and files: . \u251c\u2500\u2500 0-bootstrap \u251c\u2500\u2500 README.md \u251c\u2500\u2500 doc \u251c\u2500\u2500 scripts \u2514\u2500\u2500 setup The main folders perform the following roles: 0-bootstrap contains different profiles that can be used to initialize the cluster ready. We can choose one of the available profiles. scripts and setup contain useful scripts that we'll use during this tutorial. Feel free to explore this structure. We'll examine it in much more detail throughout this tutorial, and you'll become very familiar with it by trying it out.","title":"Downloading the sample GitOps repository"},{"location":"guides/cp4ba/process-mining/cluster-config/gitops-tekton-argocd/#installing-argocd-for-gitops","text":"Now that we've had an initial view of the GitOps repository, let's install ArgoCD to make use of it. We will install ArgoCD using the Red Hat OpenShift GitOps operator. We will also configure it with the appropriate access rights to limit the resources it can create in the cluster. In so doing, we ensure that ArgoCD acts like a well-governed administrator of the cluster, only doing what it needs to do. Also, we will customize our ArgoCD instance by adding necessary custom checks. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". On successful login, you'll see: Logged into \"https://c100-e.jp-tok.containers.cloud.ibm.com:30749\" as \"IAM#hperabat@in.ibm.com\" using the token provided. You have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\" . This shows some basic information about your user within this cluster. (This user is different to your GitHub user.) Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. The following command creates the subscription: oc apply -f setup/ocp47/ The response confirms that the below resources has been created: clusterrole.rbac.authorization.k8s.io/custom-argocd-cluster-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-cntk-argocd-application-controller created subscription.operators.coreos.com/openshift-gitops-operator created Wait for the ArgoCD installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/gitops-operator-controller-manager -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/gitops-operator condition met ArgoCD is now installed and ready to use. If you see something like: Error from server (NotFound): deployments.apps \"gitops-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Alternatively, you can also validate as follows: while ! oc wait crd applications.argoproj.io --timeout = -1s --for = condition = Established 2 >/dev/null ; do sleep 30 ; done while ! oc wait pod --timeout = -1s --for = condition = Ready -l '!job-name' -n openshift-gitops > /dev/null ; do sleep 30 ; done ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are also setup along with the subscription. oc get clusterrole custom-argocd-cluster-argocd-application-controller oc get clusterrolebinding openshift-gitops-argocd-application-controller oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller You'll see the resources are created in the cluster: $ oc get clusterrole custom-argocd-cluster-argocd-application-controller NAME CREATED AT custom-argocd-cluster-argocd-application-controller 2021-08-27T13:35:13Z $ oc get clusterrolebinding openshift-gitops-argocd-application-controller NAME ROLE AGE openshift-gitops-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m43s $ oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller NAME ROLE AGE openshift-gitops-cntk-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m45s The clusterrole custom-argocd-cluster-argocd-application-controller defines a specific set of specific resources that are required by the ArgoCD service account and access rights over them. The cluster role bindings openshift-gitops-argocd-application-controller and openshift-gitops-cntk-argocd-application-controller binds the ArgoCD service account to the cluster role above. In combination, these definitions limit ArgoCD to perform the minimum set of operations required to manage the cluster. This is important; it means that ArgoCD behaves as a well governed administrator of the cluster. We'll also see later in the tutorial how ArgoCD creates resource in the cluster. Delete default ArgoCD instance Let us now delete the default ArgoCD instance that is created earlier. oc delete gitopsservice cluster -n openshift-gitops || true You will see the resources being deleted as follows: warning: deleting cluster-scoped resources, not scoped to the provided namespace gitopsservice.pipelines.openshift.io \"cluster\" deleted We are deleting the existing default instance and planning to make use of a customized ArgoCD instance with additional capabilities. Create a custom ArgoCD instance This customized ArgoCD instance implements custom checks. By default ArgoCD provides some built-in health checks for validating standard kubernetes resources. However, these checks will not be sufficient to validate the custom resources that belong to IBM Cloud Pak for Integration. Argo CD supports custom health checks that are defined in Lua . If you want explore more about the ArgoCD Resource Health, check this link out. Let us create a custom ArgoCD instance as follows: oc apply -f setup/ocp47/argocd-instance/ -n openshift-gitops The response confirms that the below resources has been created: argocd.argoproj.io/openshift-gitops-cntk created Wait till the ArgoCD instance pod is up and running. Issue the below command to verify the same: while ! oc wait pod --timeout = -1s --for = condition = ContainersReady -l app.kubernetes.io/name = openshift-gitops-cntk-server -n openshift-gitops > /dev/null ; do sleep 30 ; done Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops | grep openshift-gitops-cntk-server | awk '{print \"https://\"$2}' This will list the route to the ArgoCD we've just installed, for example: https://openshift-gitops-cntk-server-openshift-gitops.ibmcloud-roks-xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/openshift-gitops-cntk-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Once the UI launches, you'll see: See how there are no ArgoCD applications active at the moment. In the next section of the tutorial, we'll configure ArgoCD to create the ArgoCD applications that will in turn spin up infrastructure , service , and application resources to apply to the cluster. Congratulations! You have created the GitOps organization based on the sample GitOps repositories that will be used to manage the IBM capabilities and applications that you will install on your cluster. You have also briefly examined the structure of the main GitOps repository driving what will get installed. You have also installed ArgoCD. You have created a specific clusterrole and clusterrolebinding for the ArgoCD service account to ensure that it is able to manage the cluster in a well governed manner. Finally, you have launched the ArgoCD UI; you will make extensive use of it during this tutorial.","title":"Installing ArgoCD for GitOps"},{"location":"guides/cp4ba/process-mining/cluster-create/ibm-cloud/","text":"Creating a cluster on IBM Cloud \u00b6 Abstract This document explains what are the options to create a Red Hat OpenShift cluster on IBM Cloud. Create a Red Hat OpenShift cluster instance. \u00b6 You can choose either IBM Technology Zone or your IBM Cloud account to create a Red Hat OpenShift cluster. You can use IBM Technology Zone to request a Red Hat OpenShift cluster on IBM Cloud. See the instructions here . The Red Hat OpenShift cluster on IBM Cloud you will be provided with will be hosted on classic infrastructure. You can use your IBM Cloud account to create a new Red Hat OpenShift cluster following these instructions here . You can choose to create your Red Hat OpenShift cluster on IBM Cloud either on classic infrastructure or on Virtual Private Cloud (VPC) infrastructure. Important Make sure you create your Red Hat OpenShift cluster on IBM Cloud with the following specs as explained here : OCP Version = 4.7 Worker Size (at least) = 24 CPU x 96 GB RAM Worker Node Count (at least) = 1 Storage = 200 GB Note Even though you do not require 3 worker nodes, since the IBM Process Mining instance you will deploy in this tutorial isn't a highly available IBM Process Mining instance due to the amount of resources required, the Red Hat OpenShift clusters you usually request or create come with 3 worker nodes for better high availability and resiliency purposes. Make sure you comply with the specs above when adding the resources of each worker node. Storage \u00b6 As explained in the IBM Process Mining on IBM Cloud architecture section here , IBM Process Mining requires ReadWriteMany (RWX) storage . Based on how you created your Red Hat OpenShift cluster on IBM Cloud explained in the previous section, If you requested your Red Hat OpenShift cluster through IBM Technology Zone, this will be a Red Hat OpenShift cluster on IBM Cloud on classic infrastructure which an NFS drive will be attached to. The storage class name will be managed-nfs-storage and it is the default in the GitOps repositories that will drive the deployment of your IBM Process Mining instance which are explained in the next sections. If you requested a Red Hat OpenShift cluster on IBM Cloud using your own IBM Cloud account, If you requested that Red Hat OpenShift cluster on IBM Cloud classic infrastructure, you will have RWX storage available out of the box . However, you must use those file storage classes whose name ends with -gid to allow non-root user access to persistent storage , which is a requirement for the embedded IBM DB2 DB that IBM Process Mining will deploy (if using external IBM DB2 database, as explained in the IBM Process Mining on IBM Cloud architecture section here , you would not need to use the storage classes whose name ends with -gid . Just any file storage class) If you requested that Red Hat OpenShift cluster on IBM Cloud Virtual Private Cloud (VPC) infrastructure, you must enable OpenShift Data Foundation for your Red Hat OpenShift cluster. Review the IBM Process Mining on IBM Cloud architecture section here for more details and go through the following OpenShift Data Foundation subsection to get it enabled on your Red Hat OpenShift cluster. Remember Do not forget the RWX storage class name based on the above as you will need to provide it when deploying your IBM Process Mining instance later in this tutorial. OpenShift Data Foundation \u00b6 Once your RedHat OpenShift cluster on IBM Cloud Virtual Private Cloud infrastructure is available on your IBM Cloud dashboard, you must install the OpenShift Data Foundation add-on on it: From the OpenShift clusters console, select the cluster where you want to install the add-on. On the cluster Overview page, scroll down to the Add-ons section. On the OpenShift Data Foundation card, click Install. On the Install OpenShift Data Foundation configuration panel that opens, make sure of The parameter ocsDeploy is set to true The parameter numOfOsd is set to 1 The parameter monStorageClassName and osdStorageClassName is set to any of the metro storage classes provided by IBM Cloud as explained in the IBM Process Mining Architecture on IBM Cloud page here Click Install. This should get OpenShift Data Foundation installation for your cluster started. After some time, you should see the OpenShift Data Foundation add-on under the installed subsection of the Add-ons section of your OpenShift cluster in the IBM Cloud web console with a green tick icon indicating the add-on is fine and ready (this process may take several minutes) If you go into your OpenShift cluster web console and go to Operators --> Installed Operators on the left hand side menu, you should see the OpenShift Container Storage (old name for OpenShift Data Foundation) operator installed Finally, if you go to Storage --> Storage Classes on the left hand side menu, you should see the following four new storage classes as a result of a successful OpenShift Data Foundation installation. These are the classes you should now use in your containerized workloads should you need File, Block or Object storage. And the storage class you would need to provide when deploying your IBM Process Mining instance later on in this tutorial would be ocs-storagecluster-cephfs Tools \u00b6 In order to interact with your Red Hat OpenShift cluster(s) and complete this tutorial successfully, we strongly recommend to install the following tools in your workstation. The oc command that matches the version of your cluster. Use these instructions to get the latest version of oc . Use oc version to confirm that you have Client Version: 4.6 or higher Server Version: 4.7 or higher The npm , git , tree and jq commands.","title":"IBM Cloud"},{"location":"guides/cp4ba/process-mining/cluster-create/ibm-cloud/#creating-a-cluster-on-ibm-cloud","text":"Abstract This document explains what are the options to create a Red Hat OpenShift cluster on IBM Cloud.","title":"Creating a cluster on IBM Cloud"},{"location":"guides/cp4ba/process-mining/cluster-create/ibm-cloud/#create-a-red-hat-openshift-cluster-instance","text":"You can choose either IBM Technology Zone or your IBM Cloud account to create a Red Hat OpenShift cluster. You can use IBM Technology Zone to request a Red Hat OpenShift cluster on IBM Cloud. See the instructions here . The Red Hat OpenShift cluster on IBM Cloud you will be provided with will be hosted on classic infrastructure. You can use your IBM Cloud account to create a new Red Hat OpenShift cluster following these instructions here . You can choose to create your Red Hat OpenShift cluster on IBM Cloud either on classic infrastructure or on Virtual Private Cloud (VPC) infrastructure. Important Make sure you create your Red Hat OpenShift cluster on IBM Cloud with the following specs as explained here : OCP Version = 4.7 Worker Size (at least) = 24 CPU x 96 GB RAM Worker Node Count (at least) = 1 Storage = 200 GB Note Even though you do not require 3 worker nodes, since the IBM Process Mining instance you will deploy in this tutorial isn't a highly available IBM Process Mining instance due to the amount of resources required, the Red Hat OpenShift clusters you usually request or create come with 3 worker nodes for better high availability and resiliency purposes. Make sure you comply with the specs above when adding the resources of each worker node.","title":"Create a Red Hat OpenShift cluster instance."},{"location":"guides/cp4ba/process-mining/cluster-create/ibm-cloud/#storage","text":"As explained in the IBM Process Mining on IBM Cloud architecture section here , IBM Process Mining requires ReadWriteMany (RWX) storage . Based on how you created your Red Hat OpenShift cluster on IBM Cloud explained in the previous section, If you requested your Red Hat OpenShift cluster through IBM Technology Zone, this will be a Red Hat OpenShift cluster on IBM Cloud on classic infrastructure which an NFS drive will be attached to. The storage class name will be managed-nfs-storage and it is the default in the GitOps repositories that will drive the deployment of your IBM Process Mining instance which are explained in the next sections. If you requested a Red Hat OpenShift cluster on IBM Cloud using your own IBM Cloud account, If you requested that Red Hat OpenShift cluster on IBM Cloud classic infrastructure, you will have RWX storage available out of the box . However, you must use those file storage classes whose name ends with -gid to allow non-root user access to persistent storage , which is a requirement for the embedded IBM DB2 DB that IBM Process Mining will deploy (if using external IBM DB2 database, as explained in the IBM Process Mining on IBM Cloud architecture section here , you would not need to use the storage classes whose name ends with -gid . Just any file storage class) If you requested that Red Hat OpenShift cluster on IBM Cloud Virtual Private Cloud (VPC) infrastructure, you must enable OpenShift Data Foundation for your Red Hat OpenShift cluster. Review the IBM Process Mining on IBM Cloud architecture section here for more details and go through the following OpenShift Data Foundation subsection to get it enabled on your Red Hat OpenShift cluster. Remember Do not forget the RWX storage class name based on the above as you will need to provide it when deploying your IBM Process Mining instance later in this tutorial.","title":"Storage"},{"location":"guides/cp4ba/process-mining/cluster-create/ibm-cloud/#openshift-data-foundation","text":"Once your RedHat OpenShift cluster on IBM Cloud Virtual Private Cloud infrastructure is available on your IBM Cloud dashboard, you must install the OpenShift Data Foundation add-on on it: From the OpenShift clusters console, select the cluster where you want to install the add-on. On the cluster Overview page, scroll down to the Add-ons section. On the OpenShift Data Foundation card, click Install. On the Install OpenShift Data Foundation configuration panel that opens, make sure of The parameter ocsDeploy is set to true The parameter numOfOsd is set to 1 The parameter monStorageClassName and osdStorageClassName is set to any of the metro storage classes provided by IBM Cloud as explained in the IBM Process Mining Architecture on IBM Cloud page here Click Install. This should get OpenShift Data Foundation installation for your cluster started. After some time, you should see the OpenShift Data Foundation add-on under the installed subsection of the Add-ons section of your OpenShift cluster in the IBM Cloud web console with a green tick icon indicating the add-on is fine and ready (this process may take several minutes) If you go into your OpenShift cluster web console and go to Operators --> Installed Operators on the left hand side menu, you should see the OpenShift Container Storage (old name for OpenShift Data Foundation) operator installed Finally, if you go to Storage --> Storage Classes on the left hand side menu, you should see the following four new storage classes as a result of a successful OpenShift Data Foundation installation. These are the classes you should now use in your containerized workloads should you need File, Block or Object storage. And the storage class you would need to provide when deploying your IBM Process Mining instance later on in this tutorial would be ocs-storagecluster-cephfs","title":"OpenShift Data Foundation"},{"location":"guides/cp4ba/process-mining/cluster-create/ibm-cloud/#tools","text":"In order to interact with your Red Hat OpenShift cluster(s) and complete this tutorial successfully, we strongly recommend to install the following tools in your workstation. The oc command that matches the version of your cluster. Use these instructions to get the latest version of oc . Use oc version to confirm that you have Client Version: 4.6 or higher Server Version: 4.7 or higher The npm , git , tree and jq commands.","title":"Tools"},{"location":"guides/cp4ba/process-mining/deployment/ibm-cloud/","text":"Deployment on IBM Cloud \u00b6 Important This deployment automation of the IBM Process Mining on RedHat OpenShift Kubernetes Service is only supported for clusters running OpenShift Container Platform version 4.7 Overview \u00b6 In the previous chapter of this tutorial, you have worked with ArgoCD and the GitOps repositories to understand what these are and how these work together and interact amongst them. You have seen how to create ArgoCD applications that watch their respective GitOps repository folders for details of the resources they should apply to the cluster. You have seen how you can dynamically change deployed resources by updating the resource definition in their GitOps repository. Finally, you've experienced how ArgoCD keeps the cluster synchronized with the GitOps repository as the source of truth; any unexpected configuration drift will be corrected without intervention. Now, in this section, you are going to look at what changes you need to do to that GitOps repository so that you get IBM Process Mining and all the components it needs/depends on deployed in your cluster. Services \u00b6 You have seen in the previous chapter of this tutorial that in the 0-bootstrap/single-cluster/kustomization.yaml file you have defined what layers out of infra , services and apps you want the main bootstrap-single-cluster ArgoCD application to watch. Before, you had it set up to watch only the infra layer. However, you now need it to watch the services and apps layers too in order to deploy IBM Process Mining and all the components it needs/depends on. Ensure you are logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Make sure the infra , services and apps layers are un-commented, and therefore active, for the main bootstrap-single-cluster ArgoCD application to watch them in the file 0-bootstrap/single-cluster/kustomization.yaml resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml - 3-apps/3-apps.yaml Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Adding services and applications layers\" git push origin $GIT_BRANCH If you go to your ArgoCD UI, you will see two new ArgoCD applications, one that will watch for resources on the services layer and another that will watch for resources on the apps layer. Tip You might want to manually sync your ArgoCD applications, instead of waiting for changes to get automatically picked up. For manually synchronize ArgoCD applications, you can click on the SYNC APPS button at the top. Then, select all the ArgoCD applications you want to get synched (or all for easiness) and click SYNC You are now all set to start deploying IBM Process Mining and all the components it needs/depends on. However, you will do it in two steps. In the first step, you will deploy all of the components that IBM Process Mining needs/depends on. These are things like the IBM Operators catalog to be able to install IBM Operators, the IBM Foundations, IBM Automation Foundation Core, IBM DB2 and IBM Process Mining operators where the first three are a dependency of the IBM Process Mining operator. To get all of that installed, all you need to do, in the same fashion you did for the components you wanted to get installed on the infra layer, is to un-comment these from the 0-bootstrap/single-cluster/2-services/kustomization.yaml file. Open the 0-bootstrap/single-cluster/2-services/kustomization.yaml file and un-comment the following resources: - argocd/operators/ibm-process-mining-operator.yaml - argocd/operators/ibm-db2u-operator.yaml - argocd/operators/ibm-foundations.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml - argocd/operators/ibm-catalogs.yaml Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Installing services\" git push origin $GIT_BRANCH If you go to your ArgoCD UI and filter by services project on the left hand side, you will now see the new ArgoCD applications you have just specified in your code above to be created. If you click on the services main ArgoCD application, you will see that it has created five new ArgoCD applications that correspond to each of the components we have un-commented and therefore bring to active state in the 0-bootstrap/single-cluster/2-services/kustomization.yaml file. After 5-10 mins, if you go to your Red Hat OpenShift web console and click on Operators --> Installed Operators on the right hand side menu and select the the tools project on the pull down menu at the top bar, you will see that the IBM Db2, IBM Automation Foundation Core, IBM Process Mining and IBM Cloud Pak foundational services have been successfully installed already (apart from the OpenShift GitOps operator that was installed previously). Not only that but should also be able to realise that the IBM Db2 and IBM Process Mining operators have been installed on the tools namespace as opposed to in all namespaces like the rest of the operators and this is, as already explained, a limitation of the IBM Db2 operator that must be installed per namespace. IBM Process Mining \u00b6 Last step, is to get an IBM Process Mining instance created through the IBM Process Mining operator. However, before being able to deploy any IBM capability we must have an IBM Entitlement Key to be able to pull IBM software down from IBM's software registry available in the Red Hat OpenShift project where we are deploying such IBM capability. Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. Create a secret containing your IBM Entitlement Key in the tools namespace (which is where the IBM Process Mining operator has been installed/deployed into and where the IBM Process Mining instance it will create afterwards will end up into as a result). oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io Link that docker-registry secret containing your IBM Entitlement Key with the default secret for pulling Docker images within your Red Hat OpenShift project oc secrets link default ibm-entitlement-key --for = pull Finally, to get an IBM Process Mining instance created, all you need to do is to make sure that the definition of the IBM Process Mining instance you want to deploy is correct on your GitOps repository and then, once again, tell the ArgoCD application that is watching over the services layer to activate such resource. This resource will, in turn, create another ArgoCD application that will watch over the resources specified in your definition of the IBM Process Mining instance. Open the 0-bootstrap/single-cluster/2-services/kustomization.yaml file and un-comment the following resource: - argocd/instances/ibm-process-mining-instance.yaml Make sure the storage settings for your IBM Process Mining instance you are about to deploy are correct based on where you requested/created your Red Hat OpenShift cluster on IBM Cloud. Please, review the Create the cluster section to understand what storage class you must ensure your IBM Process Mining instance definition is configured with . You can adjust storage settings in 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-process-mining-instance.yaml with the defaultStorageClassName property. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Installing an IBM Process Mining instance\" git push origin $GIT_BRANCH If you go to your ArgoCD UI, you will now see the new ibm-process-mining-instance ArgoCD application. If you go into that ArgoCD application, you can monitor the IBM Process Mining instance installation. You will see how Red Hat OpenShift resources are being created as a result of having the ibm-process-mining-instance ArgoCD application created the initial ProcessMining resource, which was then picked up by the IBM Process Mining operator. If you go to Operators --> Installed Operators under the tools project, click on the IBM Process Mining operator and then on the ProcessMining tab, you should see the processmining ProcessMining object. IBM Db2 IBM Entitlement Key \u00b6 IBM DB2 requires certain privileges in order to properly run in Red Hat OpenShift. As a result, the IBM Db2 operator creates a specific privileged Red Hat OpenShift Service Account that the IBM Db2 pods will use to run in. However, in order for that Red Hat OpenShift Service Account to be able to pull the IBM Software images from the IBM Software registry, it needs to be provided with the IBM Entitlement Key. You can see that IBM Db2 pods are not running on your cluster at the moment by getting the pods within the tools project with oc get pods . You should see the IBM Db2 pods with a ImagePullBackOff status: NAME READY STATUS RESTARTS AGE c-processmining-db2-instdb-n7mdp 0/1 ImagePullBackOff 0 8m17s c-processmining-db2-ldap-766c5bd4cf-8zkn7 0/1 ImagePullBackOff 0 8m19s ... If you describe any of those IBM Db2 pods with oc describe pod c-processmining-db2-xxxxx you should see the following events at the very bottom: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 20m default-scheduler Successfully assigned tools/c-processmining-db2-instdb-n7mdp to 10.13.177.6 Normal AddedInterface 20m multus Add eth0 [172.30.14.251/32] Normal Pulling 18m (x4 over 20m) kubelet Pulling image \"cp.icr.io/cp/db2u.instdb@sha256:17a854b803ba50b14ce332223a0023e49817bb919e961e829a9b219e5257efc6\" Warning Failed 18m (x4 over 20m) kubelet Failed to pull image \"cp.icr.io/cp/db2u.instdb@sha256:17a854b803ba50b14ce332223a0023e49817bb919e961e829a9b219e5257efc6\": rpc error: code = Unknown desc = unable to retrieve auth token: invalid username/password: unauthorized: The login credentials are not valid, or your IBM Cloud account is not active. Warning Failed 18m (x4 over 20m) kubelet Error: ErrImagePull Warning Failed 10m (x39 over 20m) kubelet Error: ImagePullBackOff Normal BackOff 37s (x82 over 20m) kubelet Back-off pulling image \"cp.icr.io/cp/db2u.instdb@sha256:17a854b803ba50b14ce332223a0023e49817bb919e961e829a9b219e5257efc6\" where you would find messages about not being able to pull the IBM Db2 images due to not being able to retrieve auth token in the first place. And what that means is that, as already explained, the Red Hat OpenShift Service Account the IBM Db2 creates to run its pods in needs to be provided with the IBM Entitlement Key. Ideally, when you created the IBM Entitlement Key secret above, you could have added such credentials to the Red Hat OpenShift Global Pull Secret that is available to all Red Hat OpenShift projects and Service Accounts which is what the IBM Db2 Operator official documentation in the IBM Knowledge Center here explains. However, IBM Red Hat OpenShift Kubernetes Service clusters, like the one you are using here, requires a reload of their worker nodes in order for that Red Hat OpenShift Global Pull Secret to be updated. Since that would add some unneeded complexity to this tutorial, you are going to simply add the existing IBM Entitlement Key credentials to the Red Hat OpenShift Service Account the IBM Db2 Operator created to run its pods in very much like you did for the default Red Hat OpenShift Service Account earlier when you created the IBM Entitlement Key secret. In order to find out what is the Red Hat OpenShift Service Account that the IBM Db2 operator created, execute the following command in your terminal: oc get ServiceAccount you should see a service account called account-<project>-<process_mining_instance_name>-db2 which in our case should be, if you did not change any of the defaults of this tutorial, account-tools-processmining-db2 . You can inspect what imagePullSecrets a service account is provided with to pull software down from container registries using oc get sa <service_account_name> -o jsonpath='{.imagePullSecrets}' If you use that command with the account-tools-processmining-db2 service account, you will see that the ibm-entitlement-key secret is not included. To include such secret into the imagePullSecrets property of the account-tools-processmining-db2 you can execute: oc secrets link account-tools-processmining-db2 ibm-entitlement-key --for=pull Once you have done that you can check again the imagePullSecrets property of the account-tools-processmining-db2 service account to make sure the ibm-entitlement-key is now listed in there. Last step to get the IBM Db2 pods to properly deploy is to delete the existing IBM Db2 pods so that the IBM Db2 operator re-deploys these but now the Red Hat OpenShift Service Account responsible for doing that should have the IBM Entitlement Key available to successfully pull down IBM Db2 container images. You can delete your existing IBM Db2 pods executing: for i in `oc get pods -n tools | grep processmining-db2 | awk '{print $1}'`; do oc delete pod $i -n tools; done After 10 minutes or so, you should see all your pods running fine: $ oc get pods -n tools NAME READY STATUS RESTARTS AGE c-processmining-db2-db2u-0 1/1 Running 0 2m8s c-processmining-db2-etcd-0 1/1 Running 0 7m34s c-processmining-db2-instdb-tzjjt 0/1 Completed 0 8m55s c-processmining-db2-ldap-766c5bd4cf-rc9st 1/1 Running 0 8m47s c-processmining-db2-restore-morph-4sxnf 1/1 Running 0 2m33s create-secrets-job-8cd9r 0/1 Completed 0 48m db2u-operator-manager-5f6b49c9b4-mxgvb 1/1 Running 0 175m iaf-zen-tour-job-lwd54 0/1 Completed 0 27m iaf-zen-tour-job-mnfsk 0/1 Completed 0 27m iam-config-job-n2gh9 0/1 Completed 0 32m ibm-nginx-7cd694dc97-2mzzl 1/1 Running 0 27m ibm-nginx-7cd694dc97-rzbcx 1/1 Running 0 27m processmining-analytics-9759f477-rjl6d 1/1 Running 0 50m processmining-bpa-694885c44f-jg6t6 1/1 Running 0 50m processmining-connectors-5c96d94966-c4tmd 1/1 Running 0 50m processmining-dr-749f85bc75-rsvnb 1/1 Running 0 50m processmining-engine-bdb99bb49-dh56c 0/1 Running 0 50m processmining-mongo-db-65888cb58b-p8zf2 1/1 Running 0 52m processmining-operator-controller-manager-7fd5669999-gdc5k 1/1 Running 0 138m processmining-processmining-nginx-7c8f8cb467-9l9t5 1/1 Running 0 52m processmining-processmining-um-84b5c7d9c6-wfkr2 1/1 Running 0 50m setup-nginx-job-tv87p 0/1 Completed 0 41m usermgmt-578588d5f8-8v7ml 1/1 Running 0 32m usermgmt-578588d5f8-jqwvz 1/1 Running 0 32m zen-audit-59546cb554-zl5j9 1/1 Running 0 37m zen-core-7bfbcbf949-bq2pj 1/1 Running 0 37m zen-core-7bfbcbf949-r7drl 1/1 Running 0 37m zen-core-api-557d8f574b-7xpss 1/1 Running 0 37m zen-core-api-557d8f574b-twkpm 1/1 Running 0 37m zen-metastoredb-0 1/1 Running 0 45m zen-metastoredb-1 1/1 Running 0 45m zen-metastoredb-2 1/1 Running 0 45m zen-metastoredb-certs-bh4dz 0/1 Completed 0 47m zen-metastoredb-init-pl8b2 0/1 Completed 0 45m zen-post-requisite-job-7l6ch 0/1 Completed 0 35m zen-pre-requisite-job-chgzf 0/1 Completed 0 37m zen-watcher-7b695c76d6-xd7j5 1/1 Running 0 37m Tip As you can see you have not deployed a highly available instance of IBM Process Mining. The reason for this is the amount of resources that it requires. However, the GitOps process and steps would exactly be the same for a highly available deployment of IBM Process Mining. If you wanted a highly available deployment of IBM Process Mining, you can find the ProcessMining Custom Resource Definition for it in the IBM Process Mining official documentation in the IBM Knowledge Center here and you would need to apply that in 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-process-mining-instance.yaml Accessing IBM Process Mining \u00b6 You can access the IBM Process Mining User Interface by first accessing the IBM Cloud Pak Dashboard. To find your IBM Cloud Pak Dashboard url you simple get the Red Hat OpenShift Routes in your tools Red Hat OpenShift Project, which is where you have deployed your IBM Process Mining Instance: $ oc get routes -n tools NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cpd cpd-tools.xxxx.containers.appdomain.cloud ibm-nginx-svc ibm-nginx-https-port passthrough/Redirect None processmining-pm processmining-pm-tools.xxxx.containers.appdomain.cloud processmining-service-web 7443-tcp reencrypt/Redirect None processmining-tm processmining-tm-tools.xxxx.containers.appdomain.cloud processmining-service-tm-nginx 8443-tcp reencrypt/Redirect None The route you are interested on that you should point your browser to is called cpd . Open that url on your web browser: Click on IBM provided credentials (admin only) to log in with your IBM Cloud Pak credentials. Your username will be admin and you can find the password with the following command: oc -n ibm-common-services get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_password}' | base64 -d && echo Once you are in the IBM Cloud Pak Dashboard, click on the hamburger icon on the top left corner to open the Navigation Menu . In there, click on Analyze and then in Process Mining . You should get redirected to your IBM Process Mining instance User Interface. Success You have now completed the in-depth IBM Process Mining tutorial. You have successfully deployed IBM Process Mining on a RedHat OpenShift Kubernetes Service (ROKS) cluster using GitOps","title":"IBM Cloud"},{"location":"guides/cp4ba/process-mining/deployment/ibm-cloud/#deployment-on-ibm-cloud","text":"Important This deployment automation of the IBM Process Mining on RedHat OpenShift Kubernetes Service is only supported for clusters running OpenShift Container Platform version 4.7","title":"Deployment on IBM Cloud"},{"location":"guides/cp4ba/process-mining/deployment/ibm-cloud/#overview","text":"In the previous chapter of this tutorial, you have worked with ArgoCD and the GitOps repositories to understand what these are and how these work together and interact amongst them. You have seen how to create ArgoCD applications that watch their respective GitOps repository folders for details of the resources they should apply to the cluster. You have seen how you can dynamically change deployed resources by updating the resource definition in their GitOps repository. Finally, you've experienced how ArgoCD keeps the cluster synchronized with the GitOps repository as the source of truth; any unexpected configuration drift will be corrected without intervention. Now, in this section, you are going to look at what changes you need to do to that GitOps repository so that you get IBM Process Mining and all the components it needs/depends on deployed in your cluster.","title":"Overview"},{"location":"guides/cp4ba/process-mining/deployment/ibm-cloud/#services","text":"You have seen in the previous chapter of this tutorial that in the 0-bootstrap/single-cluster/kustomization.yaml file you have defined what layers out of infra , services and apps you want the main bootstrap-single-cluster ArgoCD application to watch. Before, you had it set up to watch only the infra layer. However, you now need it to watch the services and apps layers too in order to deploy IBM Process Mining and all the components it needs/depends on. Ensure you are logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Make sure the infra , services and apps layers are un-commented, and therefore active, for the main bootstrap-single-cluster ArgoCD application to watch them in the file 0-bootstrap/single-cluster/kustomization.yaml resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml - 3-apps/3-apps.yaml Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Adding services and applications layers\" git push origin $GIT_BRANCH If you go to your ArgoCD UI, you will see two new ArgoCD applications, one that will watch for resources on the services layer and another that will watch for resources on the apps layer. Tip You might want to manually sync your ArgoCD applications, instead of waiting for changes to get automatically picked up. For manually synchronize ArgoCD applications, you can click on the SYNC APPS button at the top. Then, select all the ArgoCD applications you want to get synched (or all for easiness) and click SYNC You are now all set to start deploying IBM Process Mining and all the components it needs/depends on. However, you will do it in two steps. In the first step, you will deploy all of the components that IBM Process Mining needs/depends on. These are things like the IBM Operators catalog to be able to install IBM Operators, the IBM Foundations, IBM Automation Foundation Core, IBM DB2 and IBM Process Mining operators where the first three are a dependency of the IBM Process Mining operator. To get all of that installed, all you need to do, in the same fashion you did for the components you wanted to get installed on the infra layer, is to un-comment these from the 0-bootstrap/single-cluster/2-services/kustomization.yaml file. Open the 0-bootstrap/single-cluster/2-services/kustomization.yaml file and un-comment the following resources: - argocd/operators/ibm-process-mining-operator.yaml - argocd/operators/ibm-db2u-operator.yaml - argocd/operators/ibm-foundations.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml - argocd/operators/ibm-catalogs.yaml Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Installing services\" git push origin $GIT_BRANCH If you go to your ArgoCD UI and filter by services project on the left hand side, you will now see the new ArgoCD applications you have just specified in your code above to be created. If you click on the services main ArgoCD application, you will see that it has created five new ArgoCD applications that correspond to each of the components we have un-commented and therefore bring to active state in the 0-bootstrap/single-cluster/2-services/kustomization.yaml file. After 5-10 mins, if you go to your Red Hat OpenShift web console and click on Operators --> Installed Operators on the right hand side menu and select the the tools project on the pull down menu at the top bar, you will see that the IBM Db2, IBM Automation Foundation Core, IBM Process Mining and IBM Cloud Pak foundational services have been successfully installed already (apart from the OpenShift GitOps operator that was installed previously). Not only that but should also be able to realise that the IBM Db2 and IBM Process Mining operators have been installed on the tools namespace as opposed to in all namespaces like the rest of the operators and this is, as already explained, a limitation of the IBM Db2 operator that must be installed per namespace.","title":"Services"},{"location":"guides/cp4ba/process-mining/deployment/ibm-cloud/#ibm-process-mining","text":"Last step, is to get an IBM Process Mining instance created through the IBM Process Mining operator. However, before being able to deploy any IBM capability we must have an IBM Entitlement Key to be able to pull IBM software down from IBM's software registry available in the Red Hat OpenShift project where we are deploying such IBM capability. Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. Create a secret containing your IBM Entitlement Key in the tools namespace (which is where the IBM Process Mining operator has been installed/deployed into and where the IBM Process Mining instance it will create afterwards will end up into as a result). oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io Link that docker-registry secret containing your IBM Entitlement Key with the default secret for pulling Docker images within your Red Hat OpenShift project oc secrets link default ibm-entitlement-key --for = pull Finally, to get an IBM Process Mining instance created, all you need to do is to make sure that the definition of the IBM Process Mining instance you want to deploy is correct on your GitOps repository and then, once again, tell the ArgoCD application that is watching over the services layer to activate such resource. This resource will, in turn, create another ArgoCD application that will watch over the resources specified in your definition of the IBM Process Mining instance. Open the 0-bootstrap/single-cluster/2-services/kustomization.yaml file and un-comment the following resource: - argocd/instances/ibm-process-mining-instance.yaml Make sure the storage settings for your IBM Process Mining instance you are about to deploy are correct based on where you requested/created your Red Hat OpenShift cluster on IBM Cloud. Please, review the Create the cluster section to understand what storage class you must ensure your IBM Process Mining instance definition is configured with . You can adjust storage settings in 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-process-mining-instance.yaml with the defaultStorageClassName property. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Installing an IBM Process Mining instance\" git push origin $GIT_BRANCH If you go to your ArgoCD UI, you will now see the new ibm-process-mining-instance ArgoCD application. If you go into that ArgoCD application, you can monitor the IBM Process Mining instance installation. You will see how Red Hat OpenShift resources are being created as a result of having the ibm-process-mining-instance ArgoCD application created the initial ProcessMining resource, which was then picked up by the IBM Process Mining operator. If you go to Operators --> Installed Operators under the tools project, click on the IBM Process Mining operator and then on the ProcessMining tab, you should see the processmining ProcessMining object.","title":"IBM Process Mining"},{"location":"guides/cp4ba/process-mining/deployment/ibm-cloud/#accessing-ibm-process-mining","text":"You can access the IBM Process Mining User Interface by first accessing the IBM Cloud Pak Dashboard. To find your IBM Cloud Pak Dashboard url you simple get the Red Hat OpenShift Routes in your tools Red Hat OpenShift Project, which is where you have deployed your IBM Process Mining Instance: $ oc get routes -n tools NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cpd cpd-tools.xxxx.containers.appdomain.cloud ibm-nginx-svc ibm-nginx-https-port passthrough/Redirect None processmining-pm processmining-pm-tools.xxxx.containers.appdomain.cloud processmining-service-web 7443-tcp reencrypt/Redirect None processmining-tm processmining-tm-tools.xxxx.containers.appdomain.cloud processmining-service-tm-nginx 8443-tcp reencrypt/Redirect None The route you are interested on that you should point your browser to is called cpd . Open that url on your web browser: Click on IBM provided credentials (admin only) to log in with your IBM Cloud Pak credentials. Your username will be admin and you can find the password with the following command: oc -n ibm-common-services get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_password}' | base64 -d && echo Once you are in the IBM Cloud Pak Dashboard, click on the hamburger icon on the top left corner to open the Navigation Menu . In there, click on Analyze and then in Process Mining . You should get redirected to your IBM Process Mining instance User Interface. Success You have now completed the in-depth IBM Process Mining tutorial. You have successfully deployed IBM Process Mining on a RedHat OpenShift Kubernetes Service (ROKS) cluster using GitOps","title":"Accessing IBM Process Mining"},{"location":"guides/cp4ba/process-mining/overview/overview/","text":"IBM Process Mining \u00b6 IBM Process Mining powered by myInvenio \u00b6 IBM Process Mining is a process mining solution that automatically discovers, constantly monitors, and optimizes business processes. Process mining uses business system data to create and visualize an end-to-end process that includes all the process activities involved along with the various process paths. Businesses can easily analyze the discovered process to gain actionable insights for process improvement. Simulation is a feature of IBM Process Mining that businesses can use to test unlimited process changes. Simulation combines historical data with contextual data, like decision rules, to create what-if scenarios that are then analyzed for incredibly fast and simple results. Simulation shows how process changes affect future process behavior along with new benefits and reliable ROI. Businesses can also create simulations from scratch without the need for historical data so that processes can be tested and successfully introduced to business operations with immediate benefits. IBM Process Mining facilitates process automation by finding the best process candidates for automation, calculating expected ROI, and showing the impact of automation initiatives on the entire process before implementation. IBM Task Mining integrates with IBM Process Mining for a better understanding of how manual processes impact the business process. IBM Task Mining captures and sends real user interaction data to IBM Process Mining to create an augmented process model. The result is deeper insights for automation projects than what process mining alone can provide. IBM Process Mining is a component of the IBM Automation Foundation. What is IBM Automation foundation? \u00b6 IBM Automation foundation is an intelligent automation platform. IBM Automation foundation is aimed at providing Artificial Intelligence (AI) or Machine Learning (ML), and Robotic Process Automation (RPA) driven hyperautomation solutions to users who are looking to resolve challenges and inefficiencies in business and IT processes. IBM Automation foundation provides a foundation for the IBM Cloud Paks for Automation. Each Cloud Pak is a domain-specific set of capabilities that augments it. IBM Automation foundation provides the user interface which IBM Cloud Paks extend. IBM Cloud Paks with IBM Automation foundation together provide an intelligent automation platform that provides AI/ML-assisted automation recommendations, and RPA-driven automations for users who are looking to resolve their challenges and inefficiencies in business and IT processes. Red Hat\u00ae OpenShift\u00ae Container Platform provides the secure and stable infrastructure for Automation foundation. You are entitled to use process mining in the context of one of the IBM Cloud Pak (IBM Cloud Pak for Business Automation, IBM Cloud Pak for Integration, IBM Cloud Pak for Network Automation, IBM Cloud Pak for Watson AI Ops). If you do not want to install the Cloud Pak, you can still deploy the Process Mining. Links \u00b6 IBM Cloud Pak for Business Automation IBM Cloud Pak for Business Automation - Process Mining IBM Automation Foundation IBM Process Mining","title":"Overview"},{"location":"guides/cp4ba/process-mining/overview/overview/#ibm-process-mining","text":"","title":"IBM Process Mining"},{"location":"guides/cp4ba/process-mining/overview/overview/#ibm-process-mining-powered-by-myinvenio","text":"IBM Process Mining is a process mining solution that automatically discovers, constantly monitors, and optimizes business processes. Process mining uses business system data to create and visualize an end-to-end process that includes all the process activities involved along with the various process paths. Businesses can easily analyze the discovered process to gain actionable insights for process improvement. Simulation is a feature of IBM Process Mining that businesses can use to test unlimited process changes. Simulation combines historical data with contextual data, like decision rules, to create what-if scenarios that are then analyzed for incredibly fast and simple results. Simulation shows how process changes affect future process behavior along with new benefits and reliable ROI. Businesses can also create simulations from scratch without the need for historical data so that processes can be tested and successfully introduced to business operations with immediate benefits. IBM Process Mining facilitates process automation by finding the best process candidates for automation, calculating expected ROI, and showing the impact of automation initiatives on the entire process before implementation. IBM Task Mining integrates with IBM Process Mining for a better understanding of how manual processes impact the business process. IBM Task Mining captures and sends real user interaction data to IBM Process Mining to create an augmented process model. The result is deeper insights for automation projects than what process mining alone can provide. IBM Process Mining is a component of the IBM Automation Foundation.","title":"IBM Process Mining powered by myInvenio"},{"location":"guides/cp4ba/process-mining/overview/overview/#what-is-ibm-automation-foundation","text":"IBM Automation foundation is an intelligent automation platform. IBM Automation foundation is aimed at providing Artificial Intelligence (AI) or Machine Learning (ML), and Robotic Process Automation (RPA) driven hyperautomation solutions to users who are looking to resolve challenges and inefficiencies in business and IT processes. IBM Automation foundation provides a foundation for the IBM Cloud Paks for Automation. Each Cloud Pak is a domain-specific set of capabilities that augments it. IBM Automation foundation provides the user interface which IBM Cloud Paks extend. IBM Cloud Paks with IBM Automation foundation together provide an intelligent automation platform that provides AI/ML-assisted automation recommendations, and RPA-driven automations for users who are looking to resolve their challenges and inefficiencies in business and IT processes. Red Hat\u00ae OpenShift\u00ae Container Platform provides the secure and stable infrastructure for Automation foundation. You are entitled to use process mining in the context of one of the IBM Cloud Pak (IBM Cloud Pak for Business Automation, IBM Cloud Pak for Integration, IBM Cloud Pak for Network Automation, IBM Cloud Pak for Watson AI Ops). If you do not want to install the Cloud Pak, you can still deploy the Process Mining.","title":"What is IBM Automation foundation?"},{"location":"guides/cp4ba/process-mining/overview/overview/#links","text":"IBM Cloud Pak for Business Automation IBM Cloud Pak for Business Automation - Process Mining IBM Automation Foundation IBM Process Mining","title":"Links"},{"location":"guides/cp4ba/process-mining/using/this-guide/","text":"Using this guide \u00b6 Introduction \u00b6 The purpose of this guide is to teach architects, developers and operations staff how to implement a production-ready IBM Process Mining deployment on the OpenShift Container Platform . The guide also makes extensive use of other cloud native technologies such as Kustomize and ArgoCD. Using this tutorial, you will deploy a GitOps framework and IBM Process Mining in OpenShift gaining hands-on experience of these technologies and their benefits. The guide is structured as a tutorial and it is recommended that you follow the topics in order. There is an intuitive table of contents. Next and Previous links at the bottom of each page to help you navigate. The guide is intended as a complement to IBM Process Mining product documentation; where relevant the guide will refer to it and other documents. This guide is divided into chapters that help you deploy, step-by-step, an IBM Process Mining instance, based on a cloud native approach. Each chapter comprises topics which you should complete in order to gain a full understanding.","title":"Using this guide"},{"location":"guides/cp4ba/process-mining/using/this-guide/#using-this-guide","text":"","title":"Using this guide"},{"location":"guides/cp4ba/process-mining/using/this-guide/#introduction","text":"The purpose of this guide is to teach architects, developers and operations staff how to implement a production-ready IBM Process Mining deployment on the OpenShift Container Platform . The guide also makes extensive use of other cloud native technologies such as Kustomize and ArgoCD. Using this tutorial, you will deploy a GitOps framework and IBM Process Mining in OpenShift gaining hands-on experience of these technologies and their benefits. The guide is structured as a tutorial and it is recommended that you follow the topics in order. There is an intuitive table of contents. Next and Previous links at the bottom of each page to help you navigate. The guide is intended as a complement to IBM Process Mining product documentation; where relevant the guide will refer to it and other documents. This guide is divided into chapters that help you deploy, step-by-step, an IBM Process Mining instance, based on a cloud native approach. Each chapter comprises topics which you should complete in order to gain a full understanding.","title":"Introduction"},{"location":"guides/cp4d/cpdbase/deployment/cpd-ibmcloud/","text":"Cloud Pak for Data Deployment \u00b6 CPD Deployment on IBM Cloud \u00b6 To deploy IBM Cloud Pak for Data on an OpenShift cluster, we will use the IBM Cloud Native Toolkit GitOps Framework . There are only five steps to do: Prereqs - Make sure you have a RedHat OpenShift cluster and you are able to use the RedHat OpenShift CLI against it. Sealed Secrets - Provide the private key used to seal the secrets provided with the API Connect GitOps repository. RedHat OpenShift GitOps Operator - Install the RedHat OpenShift GitOps operator which provides the GitOps tools needed for installing and managing IBM Cloud Pak for Data instances using the GitOps approach already explained. IBM Cloud Pak for Data - Deploy an instance of IBM Cloud Pak for Data on the RedHat OpenShift cluster. IBM Cloud Pak for Data UI - Validate the installation of your IBM Cloud Pak for Data instance by making sure you are able to log into the IBM Cloud Pak for Data user interface. 1 - Prereqs \u00b6 Obtain a clean RedHat OpenShift cluster where nothing else has been installed on top. This RedHat OpenShift cluster must be composed of five worker nodes where three of these will be entirely dedicated to OpenShift Data Foundation (ODF). The storage nodes must be 16 CPUs and 64 GB RAM at least. Once your RedHat OpenShift cluster is available on your IBM Cloud dashboard, you must install the OpenShift Data Foundation add-on: From the OpenShift clusters console, select the cluster where you want to install the add-on. On the cluster Overview page, click Add-ons. On the OpenShift Data Foundation card, click Install. On the cluster Nodes page, select one node per availability zone and write down it's IP address. Log into your RedHat OpenShift web console using the button on the top right corner of your cluster dashboard on IBM Cloud. Go to the Compute --> Nodes section on the navigation index on the left hand side and for each of the nodes you picked in the previous step: Click on the node and in its Node Details dashboard, click on the Actions drop down menu that appears on the top right corner and select Edit Labels. Type node-role.kubernetes.io/storage=true and hit enter to add that label to the node. Click Save. Once you have completed the process above for the three nodes you selected to be entirely dedicated to storage, you should see such role in the Compute --> Nodes section of your RedHat OpenShift cluster. Log into your RedHat OpenShift cluster through the RedHat OpenShift CLI in order to execute commands to interact with it through your terminal. 2 - Sealed Secrets \u00b6 Create the sealed-secrets project. This project will host the Sealed Secrets operator that will allow us to decrypt sealed secrets stored in GitHub. oc new-project sealed-secrets Download the private key sealed-secrets-ibm-demo-key.yaml used to seal any secret contained in this demonstration and apply it to the cluster. In our case, we have included a demo IBM Entitlement Key within the GitOps GitHub repository so that we are able to pull down IBM Software. oc apply -f sealed-secrets-ibm-demo-key.yaml Delete the pod oc delete pod -n sealed-secrets -l app.kubernetes.io/name=sealed-secrets IMPORTANT WARNING: DO NOT CHECK THE FILE INTO GIT The private key MUST NOT be checked into GitHub under any circumstances. Please, remove the private key from your workstation to avoid any issues. rm sealed-secrets-ibm-demo-key.yaml For Cloud Pak to consume the entitlement key, restart the Platform Navigator pods oc delete pod -n tools -l app.kubernetes.io/name=ibm-integration-platform-navigator 3 - RedHat OpenShift GitOps Operator \u00b6 Clone the following GitHub repository that contains the GitOps structure that the Cloud Native Toolkit GitOps Framework understands. git clone https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-cp4d.git Change directory into multi-tenancy-gitops-cp4d . cd multi-tenancy-gitops-cp4d Install the RedHat OpenShift GitOps operator on your RedHat OpenShift cluster and wait for it to be available: If your RedHat OpenShift cluster version is 4.6 oc apply -f setup/ocp46/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done If your RedHat OpenShift cluster version is 4.7 oc apply -f setup/ocp47/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done Once the above command returns, you can open your RedHat OpenShift Web Console and check out that the RedHat OpenShift GitOps operator has been successfully installed in the openshift-gitops project. As you can see in the image, the RedHat OpenShift GitOps operator also installs the RedHat OpenShift Pipelines operator and ArgoCD (which will be that GitOps tool that synchronizes the Infrastructure/Configuration as Code we have stored in GitHub with the state of the RedHat OpenShift cluster). Important The RedHat OpenShift Pipelines operator gets installed by the RedHat OpenShift GitOps Subscription only for RedHat OpenShift version 4.6 . If your RedHat OpenShift cluster is version 4.7, you will need to install the RedHat OpenShift Pipelines operator as part of the GitOps process explained in this section. For getting such RedHat OpenShift Pipelines operator installed, you would need to specify that in the kustomize.yaml file for the services layer here . Once ArgoCD is deployed, get the admin password If your RedHat OpenShift cluster version is 4.6 oc extract secrets/argocd-cluster-cluster --keys=admin.password -n openshift-gitops --to=- If your RedHat OpenShift cluster version is 4.7 oc extract secrets/openshift-gitops-cluster --keys=admin.password -n openshift-gitops --to=- Open the ArgoCD web console by clicking on the ArgoCD console link you can see at the top of your RedHat OpenShift web console and log in. Once you login, you should see that your ArgoCD web console is empty as we have not deployed any Argo Application yet. 4 - IBM Cloud Pak for Data \u00b6 Before deploying an instance of IBM Cloud Pak for Data on the RedHat OpenShift cluster, we need to provide the storage node's IP addresses to the configuration of the OpenShift Data Foundation operator. You can provide custom configuration to the OpenShift Data Foundation operator in the ibm-odf.yaml file you can find in this IBM Cloud Pak for Data GitOps GitHub repository that was cloned at the beginning under multi-tenancy-gitops-process-mining/0-bootstrap/argocd/single-cluster/1-infra/argocd . Edit that file and provide your storage node's IP addresses to the worker_ip_X configuration parameters for the OpenShift Data Foundation operator. Install the ArgoCD Bootstrap Application oc apply -n openshift-gitops -f 0-bootstrap/argocd/bootstrap.yaml This ArgoCD Bootstrap Application will bootstrap the deployment of IBM Process Mining based on the configuration you have defined in the GitOps GitHub repository we cloned earlier. You can see that we integrate Kustomize for configuration management in the GitOps approach. As soon as you create this ArgoCD Bootstrap Application, the rest of the ArgoCD Applications and the respective RedHat Openshift resources these manage start to get created as a result of the synchronization process the GitOps approach is based on. You can see these ArgoCD Applications being created in the ArgoCD web console. If you go to the Operators --> Installed Operators section of your RedHat OpenShift cluster web console and select the openshift-storage project in the Project drop down list at the top, you will see that the OpenShift Container Storage operator (which has been recently renamed to OpenShift Data Foundation) is being installed. If you go to the Workloads --> Pods section of your RedHat OpenShift cluster web console you should see pods being created as a result of the OpenShift Container Storage operator being told to create an OpenShift Container Storage Cluster. After some time, you should see the OpenShift Container Storage operator successfully installed and the following new Storage Classes available on the Storage --> Storage Classes section of your RedHat OpenShift cluster web console that will be used by the IBM Process Mining operator to create an IBM Process Mining instance. If you go again to the Operators --> Installed Operators section of your RedHat OpenShift cluster web console and select the openshift-operators project in the Project drop down list at the top, you should see that the IBM Process Mining operator has been successfully installed as well as the IBM Automation Foundation Core and IBM Cloud Pak foundational services operators it depends on. If you go to the Operators --> Installed Operators section of your RedHat OpenShift cluster web console and select the prod project in the Project drop down list at the top, since in our IBM Process Mining GitOps process we have configured the IBM Process Mining instance to be deployed in the prod project, and click on the IBM Process Mining operator and then on the Process Mining tab, you should see the IBM Process Mining instance and that this is Running and Ready. If you go back to the ArgoCD web console, you should see all of the Argo Application in green. 5 - IBM Cloud Pak for Data UI \u00b6 Let's make sure that our IBM Cloud Pak for Data instance is up and running. Do that by logging into the IBM Cloud Pak for Data user interface. Obtain IBM Cloud Pak for Data console URL by executing echo https://`oc -n cloudpak get ZenService lite-cr -o jsonpath=\"{.status.url}{'\\n'}\"` Open the URL in a browser and you will be presented with the IBM Cloud Pak for Data user interface login option. Enter admin as username. Obtain admin password by executing oc -n cloudpak extract secret/admin-user-details --keys=initial_admin_password --to=- Log into the IBM Cloud Pak for Data UI using the password from previous step. Click on the navigation menu icon on the top left corner. Click on Services menu option to expand it, then select Services catalog. The various services installed with IBM Cloud Pak for Data will be displayed. That is it to get a working instance of IBM Cloud Pak for Data.","title":"Deployment on IBM Cloud"},{"location":"guides/cp4d/cpdbase/deployment/cpd-ibmcloud/#cloud-pak-for-data-deployment","text":"","title":"Cloud Pak for Data Deployment"},{"location":"guides/cp4d/cpdbase/deployment/cpd-ibmcloud/#cpd-deployment-on-ibm-cloud","text":"To deploy IBM Cloud Pak for Data on an OpenShift cluster, we will use the IBM Cloud Native Toolkit GitOps Framework . There are only five steps to do: Prereqs - Make sure you have a RedHat OpenShift cluster and you are able to use the RedHat OpenShift CLI against it. Sealed Secrets - Provide the private key used to seal the secrets provided with the API Connect GitOps repository. RedHat OpenShift GitOps Operator - Install the RedHat OpenShift GitOps operator which provides the GitOps tools needed for installing and managing IBM Cloud Pak for Data instances using the GitOps approach already explained. IBM Cloud Pak for Data - Deploy an instance of IBM Cloud Pak for Data on the RedHat OpenShift cluster. IBM Cloud Pak for Data UI - Validate the installation of your IBM Cloud Pak for Data instance by making sure you are able to log into the IBM Cloud Pak for Data user interface.","title":"CPD Deployment on IBM Cloud"},{"location":"guides/cp4d/cpdbase/deployment/cpd-ibmcloud/#1-prereqs","text":"Obtain a clean RedHat OpenShift cluster where nothing else has been installed on top. This RedHat OpenShift cluster must be composed of five worker nodes where three of these will be entirely dedicated to OpenShift Data Foundation (ODF). The storage nodes must be 16 CPUs and 64 GB RAM at least. Once your RedHat OpenShift cluster is available on your IBM Cloud dashboard, you must install the OpenShift Data Foundation add-on: From the OpenShift clusters console, select the cluster where you want to install the add-on. On the cluster Overview page, click Add-ons. On the OpenShift Data Foundation card, click Install. On the cluster Nodes page, select one node per availability zone and write down it's IP address. Log into your RedHat OpenShift web console using the button on the top right corner of your cluster dashboard on IBM Cloud. Go to the Compute --> Nodes section on the navigation index on the left hand side and for each of the nodes you picked in the previous step: Click on the node and in its Node Details dashboard, click on the Actions drop down menu that appears on the top right corner and select Edit Labels. Type node-role.kubernetes.io/storage=true and hit enter to add that label to the node. Click Save. Once you have completed the process above for the three nodes you selected to be entirely dedicated to storage, you should see such role in the Compute --> Nodes section of your RedHat OpenShift cluster. Log into your RedHat OpenShift cluster through the RedHat OpenShift CLI in order to execute commands to interact with it through your terminal.","title":"1 - Prereqs"},{"location":"guides/cp4d/cpdbase/deployment/cpd-ibmcloud/#2-sealed-secrets","text":"Create the sealed-secrets project. This project will host the Sealed Secrets operator that will allow us to decrypt sealed secrets stored in GitHub. oc new-project sealed-secrets Download the private key sealed-secrets-ibm-demo-key.yaml used to seal any secret contained in this demonstration and apply it to the cluster. In our case, we have included a demo IBM Entitlement Key within the GitOps GitHub repository so that we are able to pull down IBM Software. oc apply -f sealed-secrets-ibm-demo-key.yaml Delete the pod oc delete pod -n sealed-secrets -l app.kubernetes.io/name=sealed-secrets IMPORTANT WARNING: DO NOT CHECK THE FILE INTO GIT The private key MUST NOT be checked into GitHub under any circumstances. Please, remove the private key from your workstation to avoid any issues. rm sealed-secrets-ibm-demo-key.yaml For Cloud Pak to consume the entitlement key, restart the Platform Navigator pods oc delete pod -n tools -l app.kubernetes.io/name=ibm-integration-platform-navigator","title":"2 - Sealed Secrets"},{"location":"guides/cp4d/cpdbase/deployment/cpd-ibmcloud/#3-redhat-openshift-gitops-operator","text":"Clone the following GitHub repository that contains the GitOps structure that the Cloud Native Toolkit GitOps Framework understands. git clone https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-cp4d.git Change directory into multi-tenancy-gitops-cp4d . cd multi-tenancy-gitops-cp4d Install the RedHat OpenShift GitOps operator on your RedHat OpenShift cluster and wait for it to be available: If your RedHat OpenShift cluster version is 4.6 oc apply -f setup/ocp46/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done If your RedHat OpenShift cluster version is 4.7 oc apply -f setup/ocp47/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done Once the above command returns, you can open your RedHat OpenShift Web Console and check out that the RedHat OpenShift GitOps operator has been successfully installed in the openshift-gitops project. As you can see in the image, the RedHat OpenShift GitOps operator also installs the RedHat OpenShift Pipelines operator and ArgoCD (which will be that GitOps tool that synchronizes the Infrastructure/Configuration as Code we have stored in GitHub with the state of the RedHat OpenShift cluster). Important The RedHat OpenShift Pipelines operator gets installed by the RedHat OpenShift GitOps Subscription only for RedHat OpenShift version 4.6 . If your RedHat OpenShift cluster is version 4.7, you will need to install the RedHat OpenShift Pipelines operator as part of the GitOps process explained in this section. For getting such RedHat OpenShift Pipelines operator installed, you would need to specify that in the kustomize.yaml file for the services layer here . Once ArgoCD is deployed, get the admin password If your RedHat OpenShift cluster version is 4.6 oc extract secrets/argocd-cluster-cluster --keys=admin.password -n openshift-gitops --to=- If your RedHat OpenShift cluster version is 4.7 oc extract secrets/openshift-gitops-cluster --keys=admin.password -n openshift-gitops --to=- Open the ArgoCD web console by clicking on the ArgoCD console link you can see at the top of your RedHat OpenShift web console and log in. Once you login, you should see that your ArgoCD web console is empty as we have not deployed any Argo Application yet.","title":"3 - RedHat OpenShift GitOps Operator"},{"location":"guides/cp4d/cpdbase/deployment/cpd-ibmcloud/#4-ibm-cloud-pak-for-data","text":"Before deploying an instance of IBM Cloud Pak for Data on the RedHat OpenShift cluster, we need to provide the storage node's IP addresses to the configuration of the OpenShift Data Foundation operator. You can provide custom configuration to the OpenShift Data Foundation operator in the ibm-odf.yaml file you can find in this IBM Cloud Pak for Data GitOps GitHub repository that was cloned at the beginning under multi-tenancy-gitops-process-mining/0-bootstrap/argocd/single-cluster/1-infra/argocd . Edit that file and provide your storage node's IP addresses to the worker_ip_X configuration parameters for the OpenShift Data Foundation operator. Install the ArgoCD Bootstrap Application oc apply -n openshift-gitops -f 0-bootstrap/argocd/bootstrap.yaml This ArgoCD Bootstrap Application will bootstrap the deployment of IBM Process Mining based on the configuration you have defined in the GitOps GitHub repository we cloned earlier. You can see that we integrate Kustomize for configuration management in the GitOps approach. As soon as you create this ArgoCD Bootstrap Application, the rest of the ArgoCD Applications and the respective RedHat Openshift resources these manage start to get created as a result of the synchronization process the GitOps approach is based on. You can see these ArgoCD Applications being created in the ArgoCD web console. If you go to the Operators --> Installed Operators section of your RedHat OpenShift cluster web console and select the openshift-storage project in the Project drop down list at the top, you will see that the OpenShift Container Storage operator (which has been recently renamed to OpenShift Data Foundation) is being installed. If you go to the Workloads --> Pods section of your RedHat OpenShift cluster web console you should see pods being created as a result of the OpenShift Container Storage operator being told to create an OpenShift Container Storage Cluster. After some time, you should see the OpenShift Container Storage operator successfully installed and the following new Storage Classes available on the Storage --> Storage Classes section of your RedHat OpenShift cluster web console that will be used by the IBM Process Mining operator to create an IBM Process Mining instance. If you go again to the Operators --> Installed Operators section of your RedHat OpenShift cluster web console and select the openshift-operators project in the Project drop down list at the top, you should see that the IBM Process Mining operator has been successfully installed as well as the IBM Automation Foundation Core and IBM Cloud Pak foundational services operators it depends on. If you go to the Operators --> Installed Operators section of your RedHat OpenShift cluster web console and select the prod project in the Project drop down list at the top, since in our IBM Process Mining GitOps process we have configured the IBM Process Mining instance to be deployed in the prod project, and click on the IBM Process Mining operator and then on the Process Mining tab, you should see the IBM Process Mining instance and that this is Running and Ready. If you go back to the ArgoCD web console, you should see all of the Argo Application in green.","title":"4 - IBM Cloud Pak for Data"},{"location":"guides/cp4d/cpdbase/deployment/cpd-ibmcloud/#5-ibm-cloud-pak-for-data-ui","text":"Let's make sure that our IBM Cloud Pak for Data instance is up and running. Do that by logging into the IBM Cloud Pak for Data user interface. Obtain IBM Cloud Pak for Data console URL by executing echo https://`oc -n cloudpak get ZenService lite-cr -o jsonpath=\"{.status.url}{'\\n'}\"` Open the URL in a browser and you will be presented with the IBM Cloud Pak for Data user interface login option. Enter admin as username. Obtain admin password by executing oc -n cloudpak extract secret/admin-user-details --keys=initial_admin_password --to=- Log into the IBM Cloud Pak for Data UI using the password from previous step. Click on the navigation menu icon on the top left corner. Click on Services menu option to expand it, then select Services catalog. The various services installed with IBM Cloud Pak for Data will be displayed. That is it to get a working instance of IBM Cloud Pak for Data.","title":"5 - IBM Cloud Pak for Data UI"},{"location":"guides/cp4d/cpdbase/deployment/cpd-onprem/","text":"Cloud Pak for Data Deployment \u00b6 CPD Deployment on-premises \u00b6 Here we document the deployment of Cloud Pak for Data in an on-premises environment running RedHat OpenShift v4.6 or higher. We start with a large RedHat OpenShift cluster with 3 Master nodes and 5 Worker nodes, deployed on-prem. Three of those worker nodes are tagged as Storage nodes. To deploy IBM Cloud Pak for Data on an OpenShift cluster, we will use the IBM Cloud Native Toolkit GitOps Framework . There are only five steps to do: Prereqs - Make sure you have a RedHat OpenShift cluster and you are able to use the RedHat OpenShift CLI against it. Sealed Secrets - Provide the private key used to seal the secrets provided with the API Connect GitOps repository. RedHat OpenShift GitOps Operator - Install the RedHat OpenShift GitOps operator which provides the GitOps tools needed for installing and managing IBM Cloud Pak for Data instances using the GitOps approach already explained. IBM Cloud Pak for Data - Deploy an instance of IBM Cloud Pak for Data on the RedHat OpenShift cluster. IBM Cloud Pak for Data UI - Validate the installation of your IBM Cloud Pak for Data instance by making sure you are able to log into the IBM Cloud Pak for Data user interface. 1 - Prereqs \u00b6 Provision a Large OCP+ Cluster from Technology Zone Select Reserve now for immediate provisioning of cluster. Fill the form and Submit. Check the status of the cluster from My library > My reservations on the top left corner of your Technology Zone dashboard. Once the Status of your cluster is Ready , open the cluster tile from My reservations page, note down the URL for RedHat OpenShift web console, load balancer IP address and the password to the cluster. Your username is kubeadmin . Login to your cluster using oc CLI oc login -u kubeadmin -p <password> api.<clustername>.cp.fyre.ibm.com:6443 or using token obtained from RedHat OpenShift web console oc login --token = <token> --server = https://api.<clustername>.cp.fyre.ibm.com:6443 Set up local storage operator as per these instructions . Set up OpenShift Container Storage as per these instructions . 2 - Sealed Secrets \u00b6 Create the sealed-secrets project. This project will host the Sealed Secrets operator that will allow us to decrypt sealed secrets stored in GitHub. oc new-project sealed-secrets Download the private key sealed-secrets-ibm-demo-key.yaml used to seal any secret contained in this demonstration and apply it to the cluster. In our case, we have included a demo IBM Entitlement Key within the GitOps GitHub repository so that we are able to pull down IBM Software. oc apply -f sealed-secrets-ibm-demo-key.yaml Delete the pod oc delete pod -n sealed-secrets -l app.kubernetes.io/name=sealed-secrets IMPORTANT WARNING: DO NOT CHECK THE FILE INTO GIT The private key MUST NOT be checked into GitHub under any circumstances. Please, remove the private key from your workstation to avoid any issues. rm sealed-secrets-ibm-demo-key.yaml For Cloud Pak to consume the entitlement key, restart the Platform Navigator pods oc delete pod -n tools -l app.kubernetes.io/name=ibm-integration-platform-navigator 3 - RedHat OpenShift GitOps Operator \u00b6 Clone the following GitHub repository that contains the GitOps structure that the Cloud Native Toolkit GitOps Framework understands. git clone https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-cp4d.git Change directory into multi-tenancy-gitops-cp4d . cd multi-tenancy-gitops-cp4d Install the RedHat OpenShift GitOps operator on your RedHat OpenShift cluster and wait for it to be available: If your RedHat OpenShift cluster version is 4.6 oc apply -f setup/ocp46/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done If your RedHat OpenShift cluster version is 4.7 oc apply -f setup/ocp47/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done Once the above command returns, you can open your RedHat OpenShift Web Console and check out that the RedHat OpenShift GitOps operator has been successfully installed in the openshift-gitops project. As you can see in the image, the RedHat OpenShift GitOps operator also installs the RedHat OpenShift Pipelines operator and ArgoCD (which will be that GitOps tool that synchronizes the Infrastructure/Configuration as Code we have stored in GitHub with the state of the RedHat OpenShift cluster). Important The RedHat OpenShift Pipelines operator gets installed by the RedHat OpenShift GitOps Subscription only for RedHat OpenShift version 4.6 . If your RedHat OpenShift cluster is version 4.7, you will need to install the RedHat OpenShift Pipelines operator as part of the GitOps process explained in this section. For getting such RedHat OpenShift Pipelines operator installed, you would need to specify that in the kustomize.yaml file for the services layer here . Once ArgoCD is deployed, get the admin password If your RedHat OpenShift cluster version is 4.6 oc extract secrets/argocd-cluster-cluster --keys=admin.password -n openshift-gitops --to=- If your RedHat OpenShift cluster version is 4.7 oc extract secrets/openshift-gitops-cluster --keys=admin.password -n openshift-gitops --to=- Open the ArgoCD web console by clicking on the ArgoCD console link you can see at the top of your RedHat OpenShift web console and log in. Once you login, you should see that your ArgoCD web console is empty as we have not deployed any Argo Application yet. 4 - IBM Cloud Pak for Data \u00b6 Install the ArgoCD Bootstrap Application oc apply -n openshift-gitops -f 0-bootstrap/argocd/bootstrap.yaml This ArgoCD Bootstrap Application will bootstrap the deployment of IBM Cloud Pak for Data based on the configuration you have defined in the GitOps GitHub repository we cloned earlier. You can see that we integrate Kustomize for configuration management in the GitOps approach. As soon as you create this ArgoCD Bootstrap Application, the rest of the ArgoCD Applications and the respective RedHat Openshift resources these manage start to get created as a result of the synchronization process the GitOps approach is based on. You can see these ArgoCD Applications being created in the ArgoCD web console. If you go to the Operators > Installed Operators section of your RedHat OpenShift cluster web console and select the ibm-common-services project in the Project drop down list at the top, you should see that the Cloud Pak for Data Operator has been successfully installed as well as the IBM Cloud Pak foundational services. If you go to the Home > Search section of your RedHat OpenShift cluster web console and select the cloudpak project in the Project drop down list at the top, since in our Cloud Pak for Data GitOps process we have configured the IBM Cloud Pak for Data instance to be deployed in the cloudpak project, and search for ZenService in Resources , you should see ZenService listed. Select the listed ZenService resource and you should see lite-cr listed. Click on the lite-cr link and you should see it Running and Successful. If you go back to the ArgoCD web console, you should see all of the Argo Application in green. 5 - IBM Cloud Pak for Data UI \u00b6 Let's make sure that our IBM Cloud Pak for Data instance is up and running. Do that by logging into the IBM Cloud Pak for Data user interface. Obtain IBM Cloud Pak for Data console URL by executing echo https://`oc -n cloudpak get ZenService lite-cr -o jsonpath=\"{.status.url}{'\\n'}\"` Open the URL in a browser and you will be presented with the IBM Cloud Pak for Data user interface login option. Enter admin as username. Obtain admin password by executing oc -n cloudpak extract secret/admin-user-details --keys=initial_admin_password --to=- Log into the IBM Cloud Pak for Data UI using the password from previous step. Click on the navigation menu icon on the top left corner. Click on Services menu option to expand it, then select Services catalog. The various services installed with IBM Cloud Pak for Data will be displayed. That is it to get a working instance of IBM Cloud Pak for Data.","title":"Deployment on-prem"},{"location":"guides/cp4d/cpdbase/deployment/cpd-onprem/#cloud-pak-for-data-deployment","text":"","title":"Cloud Pak for Data Deployment"},{"location":"guides/cp4d/cpdbase/deployment/cpd-onprem/#cpd-deployment-on-premises","text":"Here we document the deployment of Cloud Pak for Data in an on-premises environment running RedHat OpenShift v4.6 or higher. We start with a large RedHat OpenShift cluster with 3 Master nodes and 5 Worker nodes, deployed on-prem. Three of those worker nodes are tagged as Storage nodes. To deploy IBM Cloud Pak for Data on an OpenShift cluster, we will use the IBM Cloud Native Toolkit GitOps Framework . There are only five steps to do: Prereqs - Make sure you have a RedHat OpenShift cluster and you are able to use the RedHat OpenShift CLI against it. Sealed Secrets - Provide the private key used to seal the secrets provided with the API Connect GitOps repository. RedHat OpenShift GitOps Operator - Install the RedHat OpenShift GitOps operator which provides the GitOps tools needed for installing and managing IBM Cloud Pak for Data instances using the GitOps approach already explained. IBM Cloud Pak for Data - Deploy an instance of IBM Cloud Pak for Data on the RedHat OpenShift cluster. IBM Cloud Pak for Data UI - Validate the installation of your IBM Cloud Pak for Data instance by making sure you are able to log into the IBM Cloud Pak for Data user interface.","title":"CPD Deployment on-premises"},{"location":"guides/cp4d/cpdbase/deployment/cpd-onprem/#1-prereqs","text":"Provision a Large OCP+ Cluster from Technology Zone Select Reserve now for immediate provisioning of cluster. Fill the form and Submit. Check the status of the cluster from My library > My reservations on the top left corner of your Technology Zone dashboard. Once the Status of your cluster is Ready , open the cluster tile from My reservations page, note down the URL for RedHat OpenShift web console, load balancer IP address and the password to the cluster. Your username is kubeadmin . Login to your cluster using oc CLI oc login -u kubeadmin -p <password> api.<clustername>.cp.fyre.ibm.com:6443 or using token obtained from RedHat OpenShift web console oc login --token = <token> --server = https://api.<clustername>.cp.fyre.ibm.com:6443 Set up local storage operator as per these instructions . Set up OpenShift Container Storage as per these instructions .","title":"1 - Prereqs"},{"location":"guides/cp4d/cpdbase/deployment/cpd-onprem/#2-sealed-secrets","text":"Create the sealed-secrets project. This project will host the Sealed Secrets operator that will allow us to decrypt sealed secrets stored in GitHub. oc new-project sealed-secrets Download the private key sealed-secrets-ibm-demo-key.yaml used to seal any secret contained in this demonstration and apply it to the cluster. In our case, we have included a demo IBM Entitlement Key within the GitOps GitHub repository so that we are able to pull down IBM Software. oc apply -f sealed-secrets-ibm-demo-key.yaml Delete the pod oc delete pod -n sealed-secrets -l app.kubernetes.io/name=sealed-secrets IMPORTANT WARNING: DO NOT CHECK THE FILE INTO GIT The private key MUST NOT be checked into GitHub under any circumstances. Please, remove the private key from your workstation to avoid any issues. rm sealed-secrets-ibm-demo-key.yaml For Cloud Pak to consume the entitlement key, restart the Platform Navigator pods oc delete pod -n tools -l app.kubernetes.io/name=ibm-integration-platform-navigator","title":"2 - Sealed Secrets"},{"location":"guides/cp4d/cpdbase/deployment/cpd-onprem/#3-redhat-openshift-gitops-operator","text":"Clone the following GitHub repository that contains the GitOps structure that the Cloud Native Toolkit GitOps Framework understands. git clone https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-cp4d.git Change directory into multi-tenancy-gitops-cp4d . cd multi-tenancy-gitops-cp4d Install the RedHat OpenShift GitOps operator on your RedHat OpenShift cluster and wait for it to be available: If your RedHat OpenShift cluster version is 4.6 oc apply -f setup/ocp46/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done If your RedHat OpenShift cluster version is 4.7 oc apply -f setup/ocp47/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done Once the above command returns, you can open your RedHat OpenShift Web Console and check out that the RedHat OpenShift GitOps operator has been successfully installed in the openshift-gitops project. As you can see in the image, the RedHat OpenShift GitOps operator also installs the RedHat OpenShift Pipelines operator and ArgoCD (which will be that GitOps tool that synchronizes the Infrastructure/Configuration as Code we have stored in GitHub with the state of the RedHat OpenShift cluster). Important The RedHat OpenShift Pipelines operator gets installed by the RedHat OpenShift GitOps Subscription only for RedHat OpenShift version 4.6 . If your RedHat OpenShift cluster is version 4.7, you will need to install the RedHat OpenShift Pipelines operator as part of the GitOps process explained in this section. For getting such RedHat OpenShift Pipelines operator installed, you would need to specify that in the kustomize.yaml file for the services layer here . Once ArgoCD is deployed, get the admin password If your RedHat OpenShift cluster version is 4.6 oc extract secrets/argocd-cluster-cluster --keys=admin.password -n openshift-gitops --to=- If your RedHat OpenShift cluster version is 4.7 oc extract secrets/openshift-gitops-cluster --keys=admin.password -n openshift-gitops --to=- Open the ArgoCD web console by clicking on the ArgoCD console link you can see at the top of your RedHat OpenShift web console and log in. Once you login, you should see that your ArgoCD web console is empty as we have not deployed any Argo Application yet.","title":"3 - RedHat OpenShift GitOps Operator"},{"location":"guides/cp4d/cpdbase/deployment/cpd-onprem/#4-ibm-cloud-pak-for-data","text":"Install the ArgoCD Bootstrap Application oc apply -n openshift-gitops -f 0-bootstrap/argocd/bootstrap.yaml This ArgoCD Bootstrap Application will bootstrap the deployment of IBM Cloud Pak for Data based on the configuration you have defined in the GitOps GitHub repository we cloned earlier. You can see that we integrate Kustomize for configuration management in the GitOps approach. As soon as you create this ArgoCD Bootstrap Application, the rest of the ArgoCD Applications and the respective RedHat Openshift resources these manage start to get created as a result of the synchronization process the GitOps approach is based on. You can see these ArgoCD Applications being created in the ArgoCD web console. If you go to the Operators > Installed Operators section of your RedHat OpenShift cluster web console and select the ibm-common-services project in the Project drop down list at the top, you should see that the Cloud Pak for Data Operator has been successfully installed as well as the IBM Cloud Pak foundational services. If you go to the Home > Search section of your RedHat OpenShift cluster web console and select the cloudpak project in the Project drop down list at the top, since in our Cloud Pak for Data GitOps process we have configured the IBM Cloud Pak for Data instance to be deployed in the cloudpak project, and search for ZenService in Resources , you should see ZenService listed. Select the listed ZenService resource and you should see lite-cr listed. Click on the lite-cr link and you should see it Running and Successful. If you go back to the ArgoCD web console, you should see all of the Argo Application in green.","title":"4 - IBM Cloud Pak for Data"},{"location":"guides/cp4d/cpdbase/deployment/cpd-onprem/#5-ibm-cloud-pak-for-data-ui","text":"Let's make sure that our IBM Cloud Pak for Data instance is up and running. Do that by logging into the IBM Cloud Pak for Data user interface. Obtain IBM Cloud Pak for Data console URL by executing echo https://`oc -n cloudpak get ZenService lite-cr -o jsonpath=\"{.status.url}{'\\n'}\"` Open the URL in a browser and you will be presented with the IBM Cloud Pak for Data user interface login option. Enter admin as username. Obtain admin password by executing oc -n cloudpak extract secret/admin-user-details --keys=initial_admin_password --to=- Log into the IBM Cloud Pak for Data UI using the password from previous step. Click on the navigation menu icon on the top left corner. Click on Services menu option to expand it, then select Services catalog. The various services installed with IBM Cloud Pak for Data will be displayed. That is it to get a working instance of IBM Cloud Pak for Data.","title":"5 - IBM Cloud Pak for Data UI"},{"location":"guides/cp4d/cpdbase/overview/architecture/","text":"IBM Cloud Pak for Data \u00b6 Putting your data to work quickly and efficiently \u00b6 IBM Cloud Pak for Data can run on a Red Hat\u00ae OpenShift\u00ae cluster, whether it's behind a firewall or on the cloud. Red Hat\u00ae OpenShift\u00ae Container Platform provides the secure and stable infrastructure. If most of the enterprise data lives behind a firewall, it makes sense to put the applications that access the data behind the firewall to prevent accidentally sharing the data. On the cloud Deploy Cloud Pak for Data on an OpenShift cluster on IBM Cloud, AWS, Microsoft Azure, or Google Cloud. On premises Run Cloud Pak for Data on your private, on-premises cluster. Objective \u00b6 The objective of these production deployments is to address non-functional requirements (NFRs) like continuous availability, disaster recovery, backup and restore, and security. There are 2 major steps to achieve that: Quickly and consistently deploy a production topology. Backup all data associated with the cluster and instance and restore that data in a new environment. The goal is to provide customers the ability to recover as quickly as possible in case of a major disaster like a ransomware attack. Data & Storage \u00b6 Given that this cloud pak is all about data, one of the main challenges in restoring to a new environment is to move all the data over. Backing up the data is part of the task, restoring that data in the proper schema and sequence is the harder task. For that we use OpenShift APIs for Data Protection (OADP), an operator that Red Hat has created to create backup and restore APIs in the OpenShift cluster. OADP installs Velero and OpenShift plugins for Velero while providing APIs to backup and restore OpenShift cluster resources (yaml files), internal images and persistent volume data. Red Hat has not added, removed or modified any of the APIs as documented in the Velero upstream project. We recommend using IBM Spectrum Protect Plus (SPP) to provide the complete backup and restore solution. IBM Spectrum Protect Plus is a data resilience solution that provides recovery, replication, retention, and reuse for VMs, databases, applications, file systems, SaaS workloads, and containers in hybrid cloud environments. Entitlements \u00b6 The following three license types are available for Cloud Pak for Data: Trial license for non-production use Standard Edition Enterprise Edition For starters, you can use the Red Hat\u00ae OpenShift\u00ae license bundled with your Cloud Pak for Data entitlement. A Cloud Pak for Data entitlement API key is needed to download images from the IBM entitled Cloud Pak registry. If you want to try things out, you can create a 60 day trial subscription key. You can retrieve your entitlement key from the container software library. Deployments \u00b6 Cloud Pak for Data can be deployed in various private cloud and public cloud environments. On-premises Private Cloud IBM Cloud Amazon Web Services (AWS) Infrastructure as a service Microsoft Azure Infrastructure as a service Google Cloud Cloud Pak for Data System Operators \u00b6 The GitOps approach is facilitated by the use of operators. Three Cloud Pak for Data operators are available: Cloud Pak for Data Foundational Services Operator Cloud Pak for Data Operator with Services Cloud Pak for Data Bootstrap Operator The steps to install either of these operators is the same. We recommend using the Cloud Pak for Data Bootstrap Operator because it allows administrators to install the service or services that they choose. GitOps \u00b6 All deployments are done using the GitOps framework. The GitOps approach is used to create and manage production environments. That way, any interaction with your production environment will be done through committing changes to Infrastructure, Configuration, etc., as Code. This is stored in a SCM repository such as GitHub that describes the desired state of your cluster. The task to apply any needed changes to the production environment is left to the GitOps tools, such as the RedHat OpenShift GitOps operator that uses ArgoCD for the mentioned task.","title":"Architecture"},{"location":"guides/cp4d/cpdbase/overview/architecture/#ibm-cloud-pak-for-data","text":"","title":"IBM Cloud Pak for Data"},{"location":"guides/cp4d/cpdbase/overview/architecture/#putting-your-data-to-work-quickly-and-efficiently","text":"IBM Cloud Pak for Data can run on a Red Hat\u00ae OpenShift\u00ae cluster, whether it's behind a firewall or on the cloud. Red Hat\u00ae OpenShift\u00ae Container Platform provides the secure and stable infrastructure. If most of the enterprise data lives behind a firewall, it makes sense to put the applications that access the data behind the firewall to prevent accidentally sharing the data. On the cloud Deploy Cloud Pak for Data on an OpenShift cluster on IBM Cloud, AWS, Microsoft Azure, or Google Cloud. On premises Run Cloud Pak for Data on your private, on-premises cluster.","title":"Putting your data to work quickly and efficiently"},{"location":"guides/cp4d/cpdbase/overview/architecture/#objective","text":"The objective of these production deployments is to address non-functional requirements (NFRs) like continuous availability, disaster recovery, backup and restore, and security. There are 2 major steps to achieve that: Quickly and consistently deploy a production topology. Backup all data associated with the cluster and instance and restore that data in a new environment. The goal is to provide customers the ability to recover as quickly as possible in case of a major disaster like a ransomware attack.","title":"Objective"},{"location":"guides/cp4d/cpdbase/overview/architecture/#data-storage","text":"Given that this cloud pak is all about data, one of the main challenges in restoring to a new environment is to move all the data over. Backing up the data is part of the task, restoring that data in the proper schema and sequence is the harder task. For that we use OpenShift APIs for Data Protection (OADP), an operator that Red Hat has created to create backup and restore APIs in the OpenShift cluster. OADP installs Velero and OpenShift plugins for Velero while providing APIs to backup and restore OpenShift cluster resources (yaml files), internal images and persistent volume data. Red Hat has not added, removed or modified any of the APIs as documented in the Velero upstream project. We recommend using IBM Spectrum Protect Plus (SPP) to provide the complete backup and restore solution. IBM Spectrum Protect Plus is a data resilience solution that provides recovery, replication, retention, and reuse for VMs, databases, applications, file systems, SaaS workloads, and containers in hybrid cloud environments.","title":"Data &amp; Storage"},{"location":"guides/cp4d/cpdbase/overview/architecture/#entitlements","text":"The following three license types are available for Cloud Pak for Data: Trial license for non-production use Standard Edition Enterprise Edition For starters, you can use the Red Hat\u00ae OpenShift\u00ae license bundled with your Cloud Pak for Data entitlement. A Cloud Pak for Data entitlement API key is needed to download images from the IBM entitled Cloud Pak registry. If you want to try things out, you can create a 60 day trial subscription key. You can retrieve your entitlement key from the container software library.","title":"Entitlements"},{"location":"guides/cp4d/cpdbase/overview/architecture/#deployments","text":"Cloud Pak for Data can be deployed in various private cloud and public cloud environments. On-premises Private Cloud IBM Cloud Amazon Web Services (AWS) Infrastructure as a service Microsoft Azure Infrastructure as a service Google Cloud Cloud Pak for Data System","title":"Deployments"},{"location":"guides/cp4d/cpdbase/overview/architecture/#operators","text":"The GitOps approach is facilitated by the use of operators. Three Cloud Pak for Data operators are available: Cloud Pak for Data Foundational Services Operator Cloud Pak for Data Operator with Services Cloud Pak for Data Bootstrap Operator The steps to install either of these operators is the same. We recommend using the Cloud Pak for Data Bootstrap Operator because it allows administrators to install the service or services that they choose.","title":"Operators"},{"location":"guides/cp4d/cpdbase/overview/architecture/#gitops","text":"All deployments are done using the GitOps framework. The GitOps approach is used to create and manage production environments. That way, any interaction with your production environment will be done through committing changes to Infrastructure, Configuration, etc., as Code. This is stored in a SCM repository such as GitHub that describes the desired state of your cluster. The task to apply any needed changes to the production environment is left to the GitOps tools, such as the RedHat OpenShift GitOps operator that uses ArgoCD for the mentioned task.","title":"GitOps"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/","text":"IBM Cloud Pak for Data \u00b6 Putting your data to work quickly and efficiently \u00b6 IBM Cloud Pak for Data can run on a Red Hat\u00ae OpenShift\u00ae cluster, whether it's behind a firewall or on the cloud. Red Hat\u00ae OpenShift\u00ae Container Platform provides the secure and stable infrastructure. If most of the enterprise data lives behind a firewall, it makes sense to put the applications that access the data behind the firewall to prevent accidentally sharing the data. On the cloud Deploy Cloud Pak for Data on an OpenShift cluster on IBM Cloud, AWS, Microsoft Azure, or Google Cloud. On premises Run Cloud Pak for Data on your private, on-premises cluster. Objective \u00b6 The objective of these production deployments is to address non-functional requirements (NFRs) like continuous availability, disaster recovery, backup and restore, and security. There are 2 major steps to achieve that: Quickly and consistently deploy a production topology. Backup all data associated with the cluster and instance and restore that data in a new environment. The goal is to provide customers the ability to recover as quickly as possible in case of a major disaster like a ransomware attack. Data & Storage \u00b6 Given that this cloud pak is all about data, one of the main challenges in restoring to a new environment is to move all the data over. Backing up the data is part of the task, restoring that data in the proper schema and sequence is the harder task. For that we use OpenShift APIs for Data Protection (OADP), an operator that Red Hat has created to create backup and restore APIs in the OpenShift cluster. OADP installs Velero and OpenShift plugins for Velero while providing APIs to backup and restore OpenShift cluster resources (yaml files), internal images and persistent volume data. Red Hat has not added, removed or modified any of the APIs as documented in the Velero upstream project. We recommend using IBM Spectrum Protect Plus (SPP) to provide the complete backup and restore solution. IBM Spectrum Protect Plus is a data resilience solution that provides recovery, replication, retention, and reuse for VMs, databases, applications, file systems, SaaS workloads, and containers in hybrid cloud environments. Entitlements \u00b6 The following three license types are available for Cloud Pak for Data: Trial license for non-production use Standard Edition Enterprise Edition For starters, you can use the Red Hat\u00ae OpenShift\u00ae license bundled with your Cloud Pak for Data entitlement. A Cloud Pak for Data entitlement API key is needed to download images from the IBM entitled Cloud Pak registry. If you want to try things out, you can create a 60 day trial subscription key. You can retrieve your entitlement key from the container software library. Deployments \u00b6 Cloud Pak for Data can be deployed in various private cloud and public cloud environments. On-premises Private Cloud IBM Cloud Amazon Web Services (AWS) Infrastructure as a service Microsoft Azure Infrastructure as a service Google Cloud Cloud Pak for Data System Operators \u00b6 The GitOps approach is facilitated by the use of operators. Three Cloud Pak for Data operators are available: Cloud Pak for Data Foundational Services Operator Cloud Pak for Data Operator with Services Cloud Pak for Data Bootstrap Operator The steps to install either of these operators is the same. We recommend using the Cloud Pak for Data Bootstrap Operator because it allows administrators to install the service or services that they choose. GitOps \u00b6 All deployments are done using the GitOps framework. The GitOps approach is used to create and manage production environments. That way, any interaction with your production environment will be done through committing changes to Infrastructure, Configuration, etc., as Code. This is stored in a SCM repository such as GitHub that describes the desired state of your cluster. The task to apply any needed changes to the production environment is left to the GitOps tools, such as the RedHat OpenShift GitOps operator that uses ArgoCD for the mentioned task. Pre-requisites \u00b6 Create a custom Git Organization \u00b6 Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization. Create a Git Personal Access Token (PAT) \u00b6 Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later. Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration \u00b6 Create the cluster \u00b6 Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For IBM Cloud Pak for Data we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 3 Worker Node Flavor = 8 CPU x 32 GB Use the cluster \u00b6 You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy. Links \u00b6 IBM Cloud Pak for Data IBM Cloud Pak for Data Knowledge Center IBM Cloud Pak for Data System IBM Spectrum Protect Plus","title":"Overview"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/#ibm-cloud-pak-for-data","text":"","title":"IBM Cloud Pak for Data"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/#putting-your-data-to-work-quickly-and-efficiently","text":"IBM Cloud Pak for Data can run on a Red Hat\u00ae OpenShift\u00ae cluster, whether it's behind a firewall or on the cloud. Red Hat\u00ae OpenShift\u00ae Container Platform provides the secure and stable infrastructure. If most of the enterprise data lives behind a firewall, it makes sense to put the applications that access the data behind the firewall to prevent accidentally sharing the data. On the cloud Deploy Cloud Pak for Data on an OpenShift cluster on IBM Cloud, AWS, Microsoft Azure, or Google Cloud. On premises Run Cloud Pak for Data on your private, on-premises cluster.","title":"Putting your data to work quickly and efficiently"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/#objective","text":"The objective of these production deployments is to address non-functional requirements (NFRs) like continuous availability, disaster recovery, backup and restore, and security. There are 2 major steps to achieve that: Quickly and consistently deploy a production topology. Backup all data associated with the cluster and instance and restore that data in a new environment. The goal is to provide customers the ability to recover as quickly as possible in case of a major disaster like a ransomware attack.","title":"Objective"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/#data-storage","text":"Given that this cloud pak is all about data, one of the main challenges in restoring to a new environment is to move all the data over. Backing up the data is part of the task, restoring that data in the proper schema and sequence is the harder task. For that we use OpenShift APIs for Data Protection (OADP), an operator that Red Hat has created to create backup and restore APIs in the OpenShift cluster. OADP installs Velero and OpenShift plugins for Velero while providing APIs to backup and restore OpenShift cluster resources (yaml files), internal images and persistent volume data. Red Hat has not added, removed or modified any of the APIs as documented in the Velero upstream project. We recommend using IBM Spectrum Protect Plus (SPP) to provide the complete backup and restore solution. IBM Spectrum Protect Plus is a data resilience solution that provides recovery, replication, retention, and reuse for VMs, databases, applications, file systems, SaaS workloads, and containers in hybrid cloud environments.","title":"Data &amp; Storage"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/#entitlements","text":"The following three license types are available for Cloud Pak for Data: Trial license for non-production use Standard Edition Enterprise Edition For starters, you can use the Red Hat\u00ae OpenShift\u00ae license bundled with your Cloud Pak for Data entitlement. A Cloud Pak for Data entitlement API key is needed to download images from the IBM entitled Cloud Pak registry. If you want to try things out, you can create a 60 day trial subscription key. You can retrieve your entitlement key from the container software library.","title":"Entitlements"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/#deployments","text":"Cloud Pak for Data can be deployed in various private cloud and public cloud environments. On-premises Private Cloud IBM Cloud Amazon Web Services (AWS) Infrastructure as a service Microsoft Azure Infrastructure as a service Google Cloud Cloud Pak for Data System","title":"Deployments"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/#operators","text":"The GitOps approach is facilitated by the use of operators. Three Cloud Pak for Data operators are available: Cloud Pak for Data Foundational Services Operator Cloud Pak for Data Operator with Services Cloud Pak for Data Bootstrap Operator The steps to install either of these operators is the same. We recommend using the Cloud Pak for Data Bootstrap Operator because it allows administrators to install the service or services that they choose.","title":"Operators"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/#gitops","text":"All deployments are done using the GitOps framework. The GitOps approach is used to create and manage production environments. That way, any interaction with your production environment will be done through committing changes to Infrastructure, Configuration, etc., as Code. This is stored in a SCM repository such as GitHub that describes the desired state of your cluster. The task to apply any needed changes to the production environment is left to the GitOps tools, such as the RedHat OpenShift GitOps operator that uses ArgoCD for the mentioned task.","title":"GitOps"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/#create-a-custom-git-organization","text":"Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization.","title":"Create a custom Git Organization"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/#create-a-git-personal-access-token-pat","text":"Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later.","title":"Create a Git Personal Access Token (PAT)"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/#red-hat-openshift-on-ibm-cloud-cluster-with-gitops-configuration","text":"","title":"Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/#create-the-cluster","text":"Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For IBM Cloud Pak for Data we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 3 Worker Node Flavor = 8 CPU x 32 GB","title":"Create the cluster"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/#use-the-cluster","text":"You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy.","title":"Use the cluster"},{"location":"guides/cp4d/cpdbase/overview/cpd-overview/#links","text":"IBM Cloud Pak for Data IBM Cloud Pak for Data Knowledge Center IBM Cloud Pak for Data System IBM Spectrum Protect Plus","title":"Links"},{"location":"guides/cp4d/datavirt/deployment/cpd-dv-ibmcloud/","text":"Deployment on IBM Cloud \u00b6 Overview \u00b6 Data Virtualization Coming soon...","title":"Deployment on IBM Cloud"},{"location":"guides/cp4d/datavirt/deployment/cpd-dv-ibmcloud/#deployment-on-ibm-cloud","text":"","title":"Deployment on IBM Cloud"},{"location":"guides/cp4d/datavirt/deployment/cpd-dv-ibmcloud/#overview","text":"Data Virtualization Coming soon...","title":"Overview"},{"location":"guides/cp4d/datavirt/deployment/cpd-dv-onprem/","text":"Data Virtualization Deployment \u00b6 DV Deployment on-premises \u00b6 Here we document the deployment of Data Virtualization in an on-premises environment running RedHat OpenShift v4.6 or higher. We start with a large RedHat OpenShift cluster with 3 Master nodes and 5 Worker nodes, deployed on-prem and then install the base Cloud Pak for Data as described in \"cpd_onprem.md\". 1 - Prereqs \u00b6 Provision a Large OCP+ Cluster from Technology Zone Select Reserve now for immediate provisioning of cluster. Fill the form and Submit. Check the status of the cluster from My library > My reservations on the top left corner of your Technology Zone dashboard. Once the Status of your cluster is Ready , open the cluster tile from My reservations page, note down the URL for RedHat OpenShift web console, load balancer IP address and the password to the cluster. Your username is kubeadmin . Login to your cluster using oc CLI oc login -u kubeadmin -p <password> api.<clustername>.cp.fyre.ibm.com:6443 or using token obtained from RedHat OpenShift web console oc login --token = <token> --server = https://api.<clustername>.cp.fyre.ibm.com:6443 Set up local storage operator as per these instructions . Set up OpenShift Container Storage as per these instructions . 2 - Sealed Secrets \u00b6 Create the sealed-secrets project. This project will host the Sealed Secrets operator that will allow us to decrypt sealed secrets stored in GitHub. oc new-project sealed-secrets Download the private key sealed-secrets-ibm-demo-key.yaml used to seal any secret contained in this demonstration and apply it to the cluster. In our case, we have included a demo IBM Entitlement Key within the GitOps GitHub repository so that we are able to pull down IBM Software. oc apply -f sealed-secrets-ibm-demo-key.yaml Delete the pod oc delete pod -n sealed-secrets -l app.kubernetes.io/name=sealed-secrets IMPORTANT WARNING: DO NOT CHECK THE FILE INTO GIT The private key MUST NOT be checked into GitHub under any circumstances. Please, remove the private key from your workstation to avoid any issues. rm sealed-secrets-ibm-demo-key.yaml For Cloud Pak to consume the entitlement key, restart the Platform Navigator pods oc delete pod -n tools -l app.kubernetes.io/name=ibm-integration-platform-navigator 3 - RedHat OpenShift GitOps Operator \u00b6 Clone the following GitHub repository that contains the GitOps structure that the Cloud Native Toolkit GitOps Framework understands. git clone https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-cp4d.git Change directory into multi-tenancy-gitops-cp4d . cd multi-tenancy-gitops-cp4d Install the RedHat OpenShift GitOps operator on your RedHat OpenShift cluster and wait for it to be available: If your RedHat OpenShift cluster version is 4.6 oc apply -f setup/ocp46/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done If your RedHat OpenShift cluster version is 4.7 oc apply -f setup/ocp47/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done Once the above command returns, you can open your RedHat OpenShift Web Console and check out that the RedHat OpenShift GitOps operator has been successfully installed in the openshift-gitops project. As you can see in the image, the RedHat OpenShift GitOps operator also installs the RedHat OpenShift Pipelines operator and ArgoCD (which will be that GitOps tool that synchronizes the Infrastructure/Configuration as Code we have stored in GitHub with the state of the RedHat OpenShift cluster). Important The RedHat OpenShift Pipelines operator gets installed by the RedHat OpenShift GitOps Subscription only for RedHat OpenShift version 4.6 . If your RedHat OpenShift cluster is version 4.7, you will need to install the RedHat OpenShift Pipelines operator as part of the GitOps process explained in this section. For getting such RedHat OpenShift Pipelines operator installed, you would need to specify that in the kustomize.yaml file for the services layer here . Once ArgoCD is deployed, get the admin password If your RedHat OpenShift cluster version is 4.6 oc extract secrets/argocd-cluster-cluster --keys=admin.password -n openshift-gitops --to=- If your RedHat OpenShift cluster version is 4.7 oc extract secrets/openshift-gitops-cluster --keys=admin.password -n openshift-gitops --to=- Open the ArgoCD web console by clicking on the ArgoCD console link you can see at the top of your RedHat OpenShift web console and log in. Once you login, you should see that your ArgoCD web console is empty as we have not deployed any Argo Application yet. 4 - IBM Cloud Pak for Data \u00b6 Install the ArgoCD Bootstrap Application oc apply -n openshift-gitops -f 0-bootstrap/argocd/bootstrap.yaml This ArgoCD Bootstrap Application will bootstrap the deployment of IBM Cloud Pak for Data based on the configuration you have defined in the GitOps GitHub repository we cloned earlier. You can see that we integrate Kustomize for configuration management in the GitOps approach. As soon as you create this ArgoCD Bootstrap Application, the rest of the ArgoCD Applications and the respective RedHat Openshift resources these manage start to get created as a result of the synchronization process the GitOps approach is based on. You can see these ArgoCD Applications being created in the ArgoCD web console. If you go to the Operators > Installed Operators section of your RedHat OpenShift cluster web console and select the ibm-common-services project in the Project drop down list at the top, you should see that the Cloud Pak for Data Operator has been successfully installed as well as the IBM Cloud Pak foundational services. If you go to the Home > Search section of your RedHat OpenShift cluster web console and select the cloudpak project in the Project drop down list at the top, since in our Cloud Pak for Data GitOps process we have configured the IBM Cloud Pak for Data instance to be deployed in the cloudpak project, and search for ZenService in Resources , you should see ZenService listed. Select the listed ZenService resource and you should see lite-cr listed. Click on the lite-cr link and you should see it Running and Successful. If you go back to the ArgoCD web console, you should see all of the Argo Application in green. 5 - IBM Cloud Pak for Data UI \u00b6 Let's make sure that our IBM Cloud Pak for Data instance is up and running. Do that by logging into the IBM Cloud Pak for Data user interface. Obtain IBM Cloud Pak for Data console URL by executing echo https://`oc -n cloudpak get ZenService lite-cr -o jsonpath=\"{.status.url}{'\\n'}\"` Open the URL in a browser and you will be presented with the IBM Cloud Pak for Data user interface login option. Enter admin as username. Obtain admin password by executing oc -n cloudpak extract secret/admin-user-details --keys=initial_admin_password --to=- Log into the IBM Cloud Pak for Data UI using the password from previous step. Click on the navigation menu icon on the top left corner. Click on Services menu option to expand it, then select Services catalog. The various services installed with IBM Cloud Pak for Data will be displayed. That is it to get a working instance of IBM Cloud Pak for Data. 6 - Data Virtualization \u00b6 Now we install Data Virtualization . Enable patch update oc patch NamespaceScope common-service \\ -n ibm-common-services \\ --type=merge \\ --patch='{\"spec\": {\"csvInjector\": {\"enable\": true} } }' Install db2u Operator Subscription cat <<EOF |oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: ibm-db2u-operator namespace: ibm-common-services # Pick the project that contains the Cloud Pak for Data operator spec: channel: v1.1 name: db2u-operator installPlanApproval: Automatic source: ibm-operator-catalog sourceNamespace: openshift-marketplace EOF Install Data Virtualization Operator Subscription cat <<EOF |oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: ibm-dv-operator-catalog-subscription namespace: ibm-common-services # Project that contains the Cloud Pak for Data operator spec: channel: v1.7 installPlanApproval: Automatic name: ibm-dv-operator source: ibm-operator-catalog sourceNamespace: openshift-marketplace EOF Install Data Virtualization CR cat <<EOF |oc apply -f - apiVersion: db2u.databases.ibm.com/v1 kind: DvService metadata: name: dv-service # This is the recommended name for DV CR namespace: cpd-instance # Project where you will install Data Virtualization spec: license: accept: true license: Enterprise # Specify the license you purchased version: 1.7.2 size: \"medium\" # Default size EOF After everything is completely configured you can go to the Red Hat OpenShift Console and under Operators->Installed Operators you should see the operators related to Data Virtualization. Data Virtualization requires kernel parameters and CRI-O configuration settings to be updated. 7 - Verifying the Installation \u00b6 Confirm DataVirtualization subscription was triggered oc get sub -n ibm-common-services ibm-dv-operator-catalog-subscription \\ -o jsonpath='{.status.installedCSV} {\"\\n\"}' Confirm DataVirtualization CSV is ready oc get csv -n ibm-common-services ibm-dv-operator.v1.7.2 \\ -o jsonpath='{ .status.phase } : { .status.message} {\"\\n\"}' Confirm DataVirtualization Operator is ready oc get deployments -n ibm-common-services -l olm.owner=\"ibm-dv-operator.v1.7.2\" \\ -o jsonpath=\"{.items[0].status.availableReplicas} {'\\n'}\" Get status of DataVirtualization service oc get dvservice dv-service Check if DataVirtualization service finished oc get DvService dv-service -o jsonpath=\"{.status.reconcileStatus}\" Data Virtualization service is ready when the command returns Completed . 6 - Data Virtualization UI \u00b6 If you haven't logged into the CP4D UI before, get the URL of the Cloud Pak for Data web client and open it in a browser. echo https://`oc get ZenService lite-cr -n tools -o jsonpath=\"{.status.url}{'\\n'}\"` The credentials for logging into the Cloud Pak for Data web client are admin/<password> where password is stored in a secret. oc extract secret/admin-user-details -n tools --keys=initial_admin_password --to=- Enter the user ID and password on the login screen. After you log into the IBM Cloud Pak for Data UI using the password from previous step, you will see the Welcome screen. Click on the navigation menu icon on the top left corner. Click on Services menu option to expand it, then select Services catalog. Under Status, select \"Enabled\" to display only the services that are installed and enabled in Cloud Pak for Data. You will see the Data Virtualization that says Enabled along with other enabled services.","title":"Deployment on-prem"},{"location":"guides/cp4d/datavirt/deployment/cpd-dv-onprem/#data-virtualization-deployment","text":"","title":"Data Virtualization Deployment"},{"location":"guides/cp4d/datavirt/deployment/cpd-dv-onprem/#dv-deployment-on-premises","text":"Here we document the deployment of Data Virtualization in an on-premises environment running RedHat OpenShift v4.6 or higher. We start with a large RedHat OpenShift cluster with 3 Master nodes and 5 Worker nodes, deployed on-prem and then install the base Cloud Pak for Data as described in \"cpd_onprem.md\".","title":"DV Deployment on-premises"},{"location":"guides/cp4d/datavirt/deployment/cpd-dv-onprem/#1-prereqs","text":"Provision a Large OCP+ Cluster from Technology Zone Select Reserve now for immediate provisioning of cluster. Fill the form and Submit. Check the status of the cluster from My library > My reservations on the top left corner of your Technology Zone dashboard. Once the Status of your cluster is Ready , open the cluster tile from My reservations page, note down the URL for RedHat OpenShift web console, load balancer IP address and the password to the cluster. Your username is kubeadmin . Login to your cluster using oc CLI oc login -u kubeadmin -p <password> api.<clustername>.cp.fyre.ibm.com:6443 or using token obtained from RedHat OpenShift web console oc login --token = <token> --server = https://api.<clustername>.cp.fyre.ibm.com:6443 Set up local storage operator as per these instructions . Set up OpenShift Container Storage as per these instructions .","title":"1 - Prereqs"},{"location":"guides/cp4d/datavirt/deployment/cpd-dv-onprem/#2-sealed-secrets","text":"Create the sealed-secrets project. This project will host the Sealed Secrets operator that will allow us to decrypt sealed secrets stored in GitHub. oc new-project sealed-secrets Download the private key sealed-secrets-ibm-demo-key.yaml used to seal any secret contained in this demonstration and apply it to the cluster. In our case, we have included a demo IBM Entitlement Key within the GitOps GitHub repository so that we are able to pull down IBM Software. oc apply -f sealed-secrets-ibm-demo-key.yaml Delete the pod oc delete pod -n sealed-secrets -l app.kubernetes.io/name=sealed-secrets IMPORTANT WARNING: DO NOT CHECK THE FILE INTO GIT The private key MUST NOT be checked into GitHub under any circumstances. Please, remove the private key from your workstation to avoid any issues. rm sealed-secrets-ibm-demo-key.yaml For Cloud Pak to consume the entitlement key, restart the Platform Navigator pods oc delete pod -n tools -l app.kubernetes.io/name=ibm-integration-platform-navigator","title":"2 - Sealed Secrets"},{"location":"guides/cp4d/datavirt/deployment/cpd-dv-onprem/#3-redhat-openshift-gitops-operator","text":"Clone the following GitHub repository that contains the GitOps structure that the Cloud Native Toolkit GitOps Framework understands. git clone https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-cp4d.git Change directory into multi-tenancy-gitops-cp4d . cd multi-tenancy-gitops-cp4d Install the RedHat OpenShift GitOps operator on your RedHat OpenShift cluster and wait for it to be available: If your RedHat OpenShift cluster version is 4.6 oc apply -f setup/ocp46/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done If your RedHat OpenShift cluster version is 4.7 oc apply -f setup/ocp47/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done Once the above command returns, you can open your RedHat OpenShift Web Console and check out that the RedHat OpenShift GitOps operator has been successfully installed in the openshift-gitops project. As you can see in the image, the RedHat OpenShift GitOps operator also installs the RedHat OpenShift Pipelines operator and ArgoCD (which will be that GitOps tool that synchronizes the Infrastructure/Configuration as Code we have stored in GitHub with the state of the RedHat OpenShift cluster). Important The RedHat OpenShift Pipelines operator gets installed by the RedHat OpenShift GitOps Subscription only for RedHat OpenShift version 4.6 . If your RedHat OpenShift cluster is version 4.7, you will need to install the RedHat OpenShift Pipelines operator as part of the GitOps process explained in this section. For getting such RedHat OpenShift Pipelines operator installed, you would need to specify that in the kustomize.yaml file for the services layer here . Once ArgoCD is deployed, get the admin password If your RedHat OpenShift cluster version is 4.6 oc extract secrets/argocd-cluster-cluster --keys=admin.password -n openshift-gitops --to=- If your RedHat OpenShift cluster version is 4.7 oc extract secrets/openshift-gitops-cluster --keys=admin.password -n openshift-gitops --to=- Open the ArgoCD web console by clicking on the ArgoCD console link you can see at the top of your RedHat OpenShift web console and log in. Once you login, you should see that your ArgoCD web console is empty as we have not deployed any Argo Application yet.","title":"3 - RedHat OpenShift GitOps Operator"},{"location":"guides/cp4d/datavirt/deployment/cpd-dv-onprem/#4-ibm-cloud-pak-for-data","text":"Install the ArgoCD Bootstrap Application oc apply -n openshift-gitops -f 0-bootstrap/argocd/bootstrap.yaml This ArgoCD Bootstrap Application will bootstrap the deployment of IBM Cloud Pak for Data based on the configuration you have defined in the GitOps GitHub repository we cloned earlier. You can see that we integrate Kustomize for configuration management in the GitOps approach. As soon as you create this ArgoCD Bootstrap Application, the rest of the ArgoCD Applications and the respective RedHat Openshift resources these manage start to get created as a result of the synchronization process the GitOps approach is based on. You can see these ArgoCD Applications being created in the ArgoCD web console. If you go to the Operators > Installed Operators section of your RedHat OpenShift cluster web console and select the ibm-common-services project in the Project drop down list at the top, you should see that the Cloud Pak for Data Operator has been successfully installed as well as the IBM Cloud Pak foundational services. If you go to the Home > Search section of your RedHat OpenShift cluster web console and select the cloudpak project in the Project drop down list at the top, since in our Cloud Pak for Data GitOps process we have configured the IBM Cloud Pak for Data instance to be deployed in the cloudpak project, and search for ZenService in Resources , you should see ZenService listed. Select the listed ZenService resource and you should see lite-cr listed. Click on the lite-cr link and you should see it Running and Successful. If you go back to the ArgoCD web console, you should see all of the Argo Application in green.","title":"4 - IBM Cloud Pak for Data"},{"location":"guides/cp4d/datavirt/deployment/cpd-dv-onprem/#5-ibm-cloud-pak-for-data-ui","text":"Let's make sure that our IBM Cloud Pak for Data instance is up and running. Do that by logging into the IBM Cloud Pak for Data user interface. Obtain IBM Cloud Pak for Data console URL by executing echo https://`oc -n cloudpak get ZenService lite-cr -o jsonpath=\"{.status.url}{'\\n'}\"` Open the URL in a browser and you will be presented with the IBM Cloud Pak for Data user interface login option. Enter admin as username. Obtain admin password by executing oc -n cloudpak extract secret/admin-user-details --keys=initial_admin_password --to=- Log into the IBM Cloud Pak for Data UI using the password from previous step. Click on the navigation menu icon on the top left corner. Click on Services menu option to expand it, then select Services catalog. The various services installed with IBM Cloud Pak for Data will be displayed. That is it to get a working instance of IBM Cloud Pak for Data.","title":"5 - IBM Cloud Pak for Data UI"},{"location":"guides/cp4d/datavirt/deployment/cpd-dv-onprem/#6-data-virtualization","text":"Now we install Data Virtualization . Enable patch update oc patch NamespaceScope common-service \\ -n ibm-common-services \\ --type=merge \\ --patch='{\"spec\": {\"csvInjector\": {\"enable\": true} } }' Install db2u Operator Subscription cat <<EOF |oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: ibm-db2u-operator namespace: ibm-common-services # Pick the project that contains the Cloud Pak for Data operator spec: channel: v1.1 name: db2u-operator installPlanApproval: Automatic source: ibm-operator-catalog sourceNamespace: openshift-marketplace EOF Install Data Virtualization Operator Subscription cat <<EOF |oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: ibm-dv-operator-catalog-subscription namespace: ibm-common-services # Project that contains the Cloud Pak for Data operator spec: channel: v1.7 installPlanApproval: Automatic name: ibm-dv-operator source: ibm-operator-catalog sourceNamespace: openshift-marketplace EOF Install Data Virtualization CR cat <<EOF |oc apply -f - apiVersion: db2u.databases.ibm.com/v1 kind: DvService metadata: name: dv-service # This is the recommended name for DV CR namespace: cpd-instance # Project where you will install Data Virtualization spec: license: accept: true license: Enterprise # Specify the license you purchased version: 1.7.2 size: \"medium\" # Default size EOF After everything is completely configured you can go to the Red Hat OpenShift Console and under Operators->Installed Operators you should see the operators related to Data Virtualization. Data Virtualization requires kernel parameters and CRI-O configuration settings to be updated.","title":"6 - Data Virtualization"},{"location":"guides/cp4d/datavirt/deployment/cpd-dv-onprem/#7-verifying-the-installation","text":"Confirm DataVirtualization subscription was triggered oc get sub -n ibm-common-services ibm-dv-operator-catalog-subscription \\ -o jsonpath='{.status.installedCSV} {\"\\n\"}' Confirm DataVirtualization CSV is ready oc get csv -n ibm-common-services ibm-dv-operator.v1.7.2 \\ -o jsonpath='{ .status.phase } : { .status.message} {\"\\n\"}' Confirm DataVirtualization Operator is ready oc get deployments -n ibm-common-services -l olm.owner=\"ibm-dv-operator.v1.7.2\" \\ -o jsonpath=\"{.items[0].status.availableReplicas} {'\\n'}\" Get status of DataVirtualization service oc get dvservice dv-service Check if DataVirtualization service finished oc get DvService dv-service -o jsonpath=\"{.status.reconcileStatus}\" Data Virtualization service is ready when the command returns Completed .","title":"7 - Verifying the Installation"},{"location":"guides/cp4d/datavirt/deployment/cpd-dv-onprem/#6-data-virtualization-ui","text":"If you haven't logged into the CP4D UI before, get the URL of the Cloud Pak for Data web client and open it in a browser. echo https://`oc get ZenService lite-cr -n tools -o jsonpath=\"{.status.url}{'\\n'}\"` The credentials for logging into the Cloud Pak for Data web client are admin/<password> where password is stored in a secret. oc extract secret/admin-user-details -n tools --keys=initial_admin_password --to=- Enter the user ID and password on the login screen. After you log into the IBM Cloud Pak for Data UI using the password from previous step, you will see the Welcome screen. Click on the navigation menu icon on the top left corner. Click on Services menu option to expand it, then select Services catalog. Under Status, select \"Enabled\" to display only the services that are installed and enabled in Cloud Pak for Data. You will see the Data Virtualization that says Enabled along with other enabled services.","title":"6 - Data Virtualization UI"},{"location":"guides/cp4d/datavirt/overview/cpd-dv-overview/","text":"Data Virtualization \u00b6 Virtual Data View \u00b6 Data Virtualization (DV) connects multiple data sources across locations and turns all of this data into one logical data view. DV service is used to create data sets from different data sources so that you can query and use the data as if it came from a single source. Information about Data Virtualization \u00b6 Data Virtualization service is included with IBM\u00ae Cloud Pak for Data. A project administrator can install Data Virtualization on the cloud pak. The requirements are: Data Virtualization requires a custom security context constraint (SCC). Data Virtualization must be installed in the same project as Cloud Pak for Data. Data Virtualization requires IBM Db2\u00ae Data Management Console and Cloud Pak for Data common core services. Data Virtualization uses one of these following storage classes: OpenShift Container Storage: ocs-storagecluster-cephfs NFS: managed-nfs-storage Portworx: portworx-db2-rwx-sc IBM Cloud File Storage: ibmc-file-gold-gid or ibm-file-custom-gold-gid Links \u00b6 IBM Cloud Pak for Data IBM Cloud Pak for Data Knowledge Center IBM Cloud Pak for Data System","title":"Overview"},{"location":"guides/cp4d/datavirt/overview/cpd-dv-overview/#data-virtualization","text":"","title":"Data Virtualization"},{"location":"guides/cp4d/datavirt/overview/cpd-dv-overview/#virtual-data-view","text":"Data Virtualization (DV) connects multiple data sources across locations and turns all of this data into one logical data view. DV service is used to create data sets from different data sources so that you can query and use the data as if it came from a single source.","title":"Virtual Data View"},{"location":"guides/cp4d/datavirt/overview/cpd-dv-overview/#information-about-data-virtualization","text":"Data Virtualization service is included with IBM\u00ae Cloud Pak for Data. A project administrator can install Data Virtualization on the cloud pak. The requirements are: Data Virtualization requires a custom security context constraint (SCC). Data Virtualization must be installed in the same project as Cloud Pak for Data. Data Virtualization requires IBM Db2\u00ae Data Management Console and Cloud Pak for Data common core services. Data Virtualization uses one of these following storage classes: OpenShift Container Storage: ocs-storagecluster-cephfs NFS: managed-nfs-storage Portworx: portworx-db2-rwx-sc IBM Cloud File Storage: ibmc-file-gold-gid or ibm-file-custom-gold-gid","title":"Information about Data Virtualization"},{"location":"guides/cp4d/datavirt/overview/cpd-dv-overview/#links","text":"IBM Cloud Pak for Data IBM Cloud Pak for Data Knowledge Center IBM Cloud Pak for Data System","title":"Links"},{"location":"guides/cp4d/spplus/deployment/spp-ibmcloud/","text":"Deployment on IBM Cloud \u00b6 Overview \u00b6 IBM Spectrum Protect Plus Links \u00b6 IBM Spectrum Protect Plus IBM Spectrum Protect Plus Knowledge Center IBM Spectrum Protect Plus Blueprints","title":"Deployment on IBM Cloud"},{"location":"guides/cp4d/spplus/deployment/spp-ibmcloud/#deployment-on-ibm-cloud","text":"","title":"Deployment on IBM Cloud"},{"location":"guides/cp4d/spplus/deployment/spp-ibmcloud/#overview","text":"IBM Spectrum Protect Plus","title":"Overview"},{"location":"guides/cp4d/spplus/deployment/spp-ibmcloud/#links","text":"IBM Spectrum Protect Plus IBM Spectrum Protect Plus Knowledge Center IBM Spectrum Protect Plus Blueprints","title":"Links"},{"location":"guides/cp4d/spplus/deployment/spp-onprem/","text":"Deployment on-premises \u00b6 Overview \u00b6 Deployment of IBM Spectrum Protect Plus (SPP) in an on-premises environment is detailed here. The graphic below shows SPP deployment across two geographical locations with data copy to cloud storage. Installing vSnap server \u00b6 Every installation of IBM Spectrum\u00ae Protect Plus v10.1.8 and earlier requires at least one vSnap server, which is the primary backup destination. The vSnap server can be a physical server, a virtual server in a VMware environment or a virtual server in a Hyper-V environment. The vSnap server is installed, registered in SPP and initialized. The vSnap server requirement will be lifted in v10.1.9. A physical vSnap server is installed Any vSnap server that is deployed virtually or installed physically must be registered in IBM Spectrum Protect Plus so that it can be recognized as a backup storage provider. The initialization process prepares a new vSnap server for use by loading and configuring software components and initializing the internal configuration. This is a one-time process. Installing container backup support \u00b6 To protect persistent volumes of containers and cluster-scoped and namespace-scoped resources, you must install and configure IBM Spectrum\u00ae Protect Plus Container Backup Support in a Kubernetes or Red Hat\u00ae OpenShift\u00ae Container Platform environment. Ensure that IBM Spectrum Protect Plus vSnap server is registered with an IP address or fully qualified domain name (FQDN). FQDN is recommended. The installation process for Container Backup Support uses a Helm 3 chart . The installation script that is provided with the installation package requires that the Helm 3 binary file is renamed to helm3 . To protect OpenShift cluster-scoped resources and namespace-scoped resources, you must use the OpenShift APIs for Data Protection (OADP) operator to install and configure the Velero tool in a dedicated namespace. The suggested name for the IBM Spectrum Protect Plus Velero namespace is spp-velero . The MinIO Object Store serves as an S3 object store for snapshot backups. The MinIO Pod is integrated in the Container Backup Support (BaaS) installation package and is deployed to the BaaS namespace. This Pod claims a persistent volume with a size of 10 GB, and uses the default Storage Class (minioStorageClass) based on the cluster configuration. Configuration parameters of the Container Backup Support Helm chart is specified in 2 files: baas-options.sh and baas-values.yaml Deploy \u00b6 The official IBM Spectrum Protect Plus deployment instructions can be found here . There are 3 parts - SPP Server setup, Backup as a Service (Baas) operator and setting up the vSnap server, which is a manual process. We strongly recommend using a GitOps approach for installing the SPP server and Baas. To deploy IBM Spectrum Protect Plus on an OpenShift cluster, we use the IBM Cloud Native Toolkit GitOps Framework . There are only five steps you need to take: Prereqs - Make sure you have a Red Hat OpenShift cluster and are able to use the Red Hat OpenShift CLI against it. Sealed Secrets - Provide the private key used to seal the secrets provided with the API Connect GitOps repository. Red Hat OpenShift GitOps Operator - Install the Red Hat OpenShift GitOps operator which provides the GitOps tools needed for installing and managing SPP instances through the GitOps approach already explained. IBM Spectrum Protect Plus - Deploy an instance of Spectrum Protect Plus on your cluster. IBM Spectrum Protect Plus UI - Validate the installation of your Spectrum Protect Plus instance by making sure you are able to log into the dashboard. 1 - Prereqs \u00b6 Get a clean RedHat OpenShift cluster. This RedHat OpenShift cluster must be composed of six worker nodes where three of these will be entirely dedicated to OpenShift Data Foundation. The storage nodes must be 16 CPUs and 64 GB RAM at least. 2 - Sealed Secrets \u00b6 Create the sealed-secrets project. This project will host the Sealed Secrets operator that will allow us to decrypt sealed secrets stored in GitHub. oc new-project sealed-secrets Download the private key sealed-secrets-ibm-demo-key.yaml used to seal any secret contained in this demonstration and apply it to the cluster. In our case, we have included a demo IBM Entitlement Key within the API Connect GitOps GitHub repository so that we are able to pull down IBM Software. oc apply -f sealed-secrets-ibm-demo-key.yaml IMPORTANT: DO NOT CHECK THE FILE INTO GIT . The private key MUST NOT be checked into GitHub under any circumstances. Please, remove the private key from your workstation to avoid any issues. rm sealed-secrets-ibm-demo-key.yaml 3 - Red Hat OpenShift GitOps Operator \u00b6 Clone the following GitHub repository that contains the GitOps structure that the Cloud Native Toolkit GitOps Framework understands. git clone https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-process-mining.git Change directory into multi-tenancy-gitops-process-mining . cd multi-tenancy-gitops-process-mining Install the Red Hat OpenShift GitOps operator on your RedHat OpenShift cluster and wait for it to be available: If your Red Hat OpenShift cluster version is 4.6 oc apply -f setup/ocp46/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done If your Red Hat OpenShift cluster version is 4.7 oc apply -f setup/ocp47/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done Once the above command returns, you can open your Red Hat OpenShift Web Console and check out that the RedHat OpenShift GitOps operator has been successfully installed in the openshift-gitops project. As you can see in the image, the Red Hat OpenShift GitOps operator also installs the Red Hat OpenShift Pipelines operator and ArgoCD (which will be that GitOps tool that synchronizes the Infrastructure/Configuration as Code we have stored in GitHub with the state of the Red Hat OpenShift cluster). Important: The Red Hat OpenShift Pipelines operator gets installed by the RedHat OpenShift GitOps Subscription only for RedHat OpenShift version 4.6 . If your RedHat OpenShift cluster is version 4.7, you will need to install the Red Hat OpenShift Pipelines operator as part of the GitOps process explained in this section. For getting such Red Hat OpenShift Pipelines operator installed, you would need to specify that in the kustomize.yaml file for the services layer here . Open the ArgoCD web console by clicking on the ArgoCD console link you can see at the top of your Red Hat OpenShift web console and log in. You can find your ArgoCD login password by executing: * If your RedHat OpenShift cluster version is 4.6 oc extract secrets/argocd-cluster-cluster --keys=admin.password -n openshift-gitops --to=- * If your RedHat OpenShift cluster version is 4.7 oc extract secrets/openshift-gitops-cluster --keys=admin.password -n openshift-gitops --to=- Once you login, you should see that your ArgoCD web console is empty as we have not deployed any Argo Application yet. 4 - IBM Spectrum Protect Plus \u00b6 Install the ArgoCD Bootstrap Application oc apply -n openshift-gitops -f 0-bootstrap/argocd/bootstrap.yaml This ArgoCD Bootstrap Application will bootstrap the deployment of IBM Spectrum Protect Plus based on the configuration you have defined in the GitOps GitHub repository we cloned earlier. You can see that we integrate Kustomize for configuration management in the GitOps approach. As soon as you create this ArgoCD Bootstrap Application, the rest of the ArgoCD Applications and the respective Red Hat Openshift managed resources start to get created as a result of the synchronization process the GitOps approach is based on. You can see these ArgoCD Applications being created in the ArgoCD web console. If you go to the Operators -> Installed Operators section of your Red Hat OpenShift cluster web console and select the openshift-storage project in the Project drop down list at the top, you will see that the OpenShift Container Storage operator (which has been recently renamed to OpenShift Data Foundation) is being installed. If you go to the Workloads -> Pods section of your Red Hat OpenShift cluster web console you should see pods being created as a result of the OpenShift Container Storage operator being told to create an OpenShift Container Storage Cluster. After some time, you should see the OpenShift Container Storage operator successfully installed and the following new Storage Classes available on the Storage -> Storage Classes section of your Red Hat OpenShift cluster web console that will be used by the IBM Spectrum Protect Plus operator to create an IBM Spectrum Protect Plus instance. If you go again to the Operators -> Installed Operators section of your Red Hat OpenShift cluster web console and select the openshift-operators project in the Project drop down list at the top, you should see that the IBM Spectrum Protect Plus operator has been successfully installed as well as the IBM Automation Foundation Core and IBM Cloud Pak foundational services operators it depends on. the IBM Spectrum Protect Plus instance should now be Running. Go to the Operators -> Installed Operators section of your Red Hat OpenShift cluster web console. Select the spp project in the Project drop down list at the top because in our IBM Spectrum Process Plus GitOps process we configured the IBM Spectrum Protect Plus instance to be deployed in the spp project. Click on the IBM Spectrum Protect Plus operator and then on the SPP tab, you should see the running instance. If you go back to the ArgoCD web console, you should see all of the Argo Application in green. 5 - IBM Spectrum Protect Plus UI \u00b6 Now, let's make sure that our IBM Spectrum Protect Plus instance is up and running. Log into the Spectrum Protect Plus UI using the initial credentials of admin/password. You will be asked to change the user ID and password. In this scenario they were changed to sppadmin/passw0rd Finally the default IBM Spectrum Protect Plus dashboard is displayed and you can start working with it. Links \u00b6 IBM Spectrum Protect Plus IBM Spectrum Protect Plus Knowledge Center IBM Spectrum Protect Plus Blueprints","title":"Deployment on-prem"},{"location":"guides/cp4d/spplus/deployment/spp-onprem/#deployment-on-premises","text":"","title":"Deployment on-premises"},{"location":"guides/cp4d/spplus/deployment/spp-onprem/#overview","text":"Deployment of IBM Spectrum Protect Plus (SPP) in an on-premises environment is detailed here. The graphic below shows SPP deployment across two geographical locations with data copy to cloud storage.","title":"Overview"},{"location":"guides/cp4d/spplus/deployment/spp-onprem/#installing-vsnap-server","text":"Every installation of IBM Spectrum\u00ae Protect Plus v10.1.8 and earlier requires at least one vSnap server, which is the primary backup destination. The vSnap server can be a physical server, a virtual server in a VMware environment or a virtual server in a Hyper-V environment. The vSnap server is installed, registered in SPP and initialized. The vSnap server requirement will be lifted in v10.1.9. A physical vSnap server is installed Any vSnap server that is deployed virtually or installed physically must be registered in IBM Spectrum Protect Plus so that it can be recognized as a backup storage provider. The initialization process prepares a new vSnap server for use by loading and configuring software components and initializing the internal configuration. This is a one-time process.","title":"Installing vSnap server"},{"location":"guides/cp4d/spplus/deployment/spp-onprem/#installing-container-backup-support","text":"To protect persistent volumes of containers and cluster-scoped and namespace-scoped resources, you must install and configure IBM Spectrum\u00ae Protect Plus Container Backup Support in a Kubernetes or Red Hat\u00ae OpenShift\u00ae Container Platform environment. Ensure that IBM Spectrum Protect Plus vSnap server is registered with an IP address or fully qualified domain name (FQDN). FQDN is recommended. The installation process for Container Backup Support uses a Helm 3 chart . The installation script that is provided with the installation package requires that the Helm 3 binary file is renamed to helm3 . To protect OpenShift cluster-scoped resources and namespace-scoped resources, you must use the OpenShift APIs for Data Protection (OADP) operator to install and configure the Velero tool in a dedicated namespace. The suggested name for the IBM Spectrum Protect Plus Velero namespace is spp-velero . The MinIO Object Store serves as an S3 object store for snapshot backups. The MinIO Pod is integrated in the Container Backup Support (BaaS) installation package and is deployed to the BaaS namespace. This Pod claims a persistent volume with a size of 10 GB, and uses the default Storage Class (minioStorageClass) based on the cluster configuration. Configuration parameters of the Container Backup Support Helm chart is specified in 2 files: baas-options.sh and baas-values.yaml","title":"Installing container backup support"},{"location":"guides/cp4d/spplus/deployment/spp-onprem/#deploy","text":"The official IBM Spectrum Protect Plus deployment instructions can be found here . There are 3 parts - SPP Server setup, Backup as a Service (Baas) operator and setting up the vSnap server, which is a manual process. We strongly recommend using a GitOps approach for installing the SPP server and Baas. To deploy IBM Spectrum Protect Plus on an OpenShift cluster, we use the IBM Cloud Native Toolkit GitOps Framework . There are only five steps you need to take: Prereqs - Make sure you have a Red Hat OpenShift cluster and are able to use the Red Hat OpenShift CLI against it. Sealed Secrets - Provide the private key used to seal the secrets provided with the API Connect GitOps repository. Red Hat OpenShift GitOps Operator - Install the Red Hat OpenShift GitOps operator which provides the GitOps tools needed for installing and managing SPP instances through the GitOps approach already explained. IBM Spectrum Protect Plus - Deploy an instance of Spectrum Protect Plus on your cluster. IBM Spectrum Protect Plus UI - Validate the installation of your Spectrum Protect Plus instance by making sure you are able to log into the dashboard.","title":"Deploy"},{"location":"guides/cp4d/spplus/deployment/spp-onprem/#1-prereqs","text":"Get a clean RedHat OpenShift cluster. This RedHat OpenShift cluster must be composed of six worker nodes where three of these will be entirely dedicated to OpenShift Data Foundation. The storage nodes must be 16 CPUs and 64 GB RAM at least.","title":"1 - Prereqs"},{"location":"guides/cp4d/spplus/deployment/spp-onprem/#2-sealed-secrets","text":"Create the sealed-secrets project. This project will host the Sealed Secrets operator that will allow us to decrypt sealed secrets stored in GitHub. oc new-project sealed-secrets Download the private key sealed-secrets-ibm-demo-key.yaml used to seal any secret contained in this demonstration and apply it to the cluster. In our case, we have included a demo IBM Entitlement Key within the API Connect GitOps GitHub repository so that we are able to pull down IBM Software. oc apply -f sealed-secrets-ibm-demo-key.yaml IMPORTANT: DO NOT CHECK THE FILE INTO GIT . The private key MUST NOT be checked into GitHub under any circumstances. Please, remove the private key from your workstation to avoid any issues. rm sealed-secrets-ibm-demo-key.yaml","title":"2 - Sealed Secrets"},{"location":"guides/cp4d/spplus/deployment/spp-onprem/#3-red-hat-openshift-gitops-operator","text":"Clone the following GitHub repository that contains the GitOps structure that the Cloud Native Toolkit GitOps Framework understands. git clone https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-process-mining.git Change directory into multi-tenancy-gitops-process-mining . cd multi-tenancy-gitops-process-mining Install the Red Hat OpenShift GitOps operator on your RedHat OpenShift cluster and wait for it to be available: If your Red Hat OpenShift cluster version is 4.6 oc apply -f setup/ocp46/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done If your Red Hat OpenShift cluster version is 4.7 oc apply -f setup/ocp47/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done Once the above command returns, you can open your Red Hat OpenShift Web Console and check out that the RedHat OpenShift GitOps operator has been successfully installed in the openshift-gitops project. As you can see in the image, the Red Hat OpenShift GitOps operator also installs the Red Hat OpenShift Pipelines operator and ArgoCD (which will be that GitOps tool that synchronizes the Infrastructure/Configuration as Code we have stored in GitHub with the state of the Red Hat OpenShift cluster). Important: The Red Hat OpenShift Pipelines operator gets installed by the RedHat OpenShift GitOps Subscription only for RedHat OpenShift version 4.6 . If your RedHat OpenShift cluster is version 4.7, you will need to install the Red Hat OpenShift Pipelines operator as part of the GitOps process explained in this section. For getting such Red Hat OpenShift Pipelines operator installed, you would need to specify that in the kustomize.yaml file for the services layer here . Open the ArgoCD web console by clicking on the ArgoCD console link you can see at the top of your Red Hat OpenShift web console and log in. You can find your ArgoCD login password by executing: * If your RedHat OpenShift cluster version is 4.6 oc extract secrets/argocd-cluster-cluster --keys=admin.password -n openshift-gitops --to=- * If your RedHat OpenShift cluster version is 4.7 oc extract secrets/openshift-gitops-cluster --keys=admin.password -n openshift-gitops --to=- Once you login, you should see that your ArgoCD web console is empty as we have not deployed any Argo Application yet.","title":"3 - Red Hat OpenShift GitOps Operator"},{"location":"guides/cp4d/spplus/deployment/spp-onprem/#4-ibm-spectrum-protect-plus","text":"Install the ArgoCD Bootstrap Application oc apply -n openshift-gitops -f 0-bootstrap/argocd/bootstrap.yaml This ArgoCD Bootstrap Application will bootstrap the deployment of IBM Spectrum Protect Plus based on the configuration you have defined in the GitOps GitHub repository we cloned earlier. You can see that we integrate Kustomize for configuration management in the GitOps approach. As soon as you create this ArgoCD Bootstrap Application, the rest of the ArgoCD Applications and the respective Red Hat Openshift managed resources start to get created as a result of the synchronization process the GitOps approach is based on. You can see these ArgoCD Applications being created in the ArgoCD web console. If you go to the Operators -> Installed Operators section of your Red Hat OpenShift cluster web console and select the openshift-storage project in the Project drop down list at the top, you will see that the OpenShift Container Storage operator (which has been recently renamed to OpenShift Data Foundation) is being installed. If you go to the Workloads -> Pods section of your Red Hat OpenShift cluster web console you should see pods being created as a result of the OpenShift Container Storage operator being told to create an OpenShift Container Storage Cluster. After some time, you should see the OpenShift Container Storage operator successfully installed and the following new Storage Classes available on the Storage -> Storage Classes section of your Red Hat OpenShift cluster web console that will be used by the IBM Spectrum Protect Plus operator to create an IBM Spectrum Protect Plus instance. If you go again to the Operators -> Installed Operators section of your Red Hat OpenShift cluster web console and select the openshift-operators project in the Project drop down list at the top, you should see that the IBM Spectrum Protect Plus operator has been successfully installed as well as the IBM Automation Foundation Core and IBM Cloud Pak foundational services operators it depends on. the IBM Spectrum Protect Plus instance should now be Running. Go to the Operators -> Installed Operators section of your Red Hat OpenShift cluster web console. Select the spp project in the Project drop down list at the top because in our IBM Spectrum Process Plus GitOps process we configured the IBM Spectrum Protect Plus instance to be deployed in the spp project. Click on the IBM Spectrum Protect Plus operator and then on the SPP tab, you should see the running instance. If you go back to the ArgoCD web console, you should see all of the Argo Application in green.","title":"4 - IBM Spectrum Protect Plus"},{"location":"guides/cp4d/spplus/deployment/spp-onprem/#5-ibm-spectrum-protect-plus-ui","text":"Now, let's make sure that our IBM Spectrum Protect Plus instance is up and running. Log into the Spectrum Protect Plus UI using the initial credentials of admin/password. You will be asked to change the user ID and password. In this scenario they were changed to sppadmin/passw0rd Finally the default IBM Spectrum Protect Plus dashboard is displayed and you can start working with it.","title":"5 - IBM Spectrum Protect Plus UI"},{"location":"guides/cp4d/spplus/deployment/spp-onprem/#links","text":"IBM Spectrum Protect Plus IBM Spectrum Protect Plus Knowledge Center IBM Spectrum Protect Plus Blueprints","title":"Links"},{"location":"guides/cp4d/spplus/overview/spp-overview/","text":"Spectrum Protect Plus \u00b6 Unified data protection for hybrid cloud environments \u00b6 IBM Spectrum\u00ae Protect Plus (SPP) is a data protection and availability solution for virtual environments and database applications. As a data resiliency solution it provides recovery, replication, retention, and reuse of data. The graphic below shows SPP deployed in two active locations. The IBM Spectrum Protect Plus server is deployed in only one of the sites. vSnap servers (with their corresponding disks) are deployed in each site to localize data movement in the context of the protected vSphere resources. Bi-directional replication is configured to take place between the vSnap servers at the two sites. Information about SPP \u00b6 IBM Spectrum Protect Plus is used to restore the snapshots from vSnap to the original or alternate hypervisor. In this production scenario SPP is used to backup and restore data from IBM\u00ae Cloud Pak for Data environments. SPP can be installed as: A virtual appliance A set of OpenShift containers SPP uses OADP (OpenShift APIs for Data Protection) which is an operator that Red Hat has created to create backup and restore APIs in the OpenShift cluster. Cloud Paks can backup the entire namespace whereas SPP also helps in backing up common services and cluster related resources. These resources can then be used to restore everything in a new environment. Links \u00b6 IBM Spectrum Protect Plus IBM Spectrum Protect Plus Knowledge Center IBM Spectrum Protect Plus Blueprints","title":"Overview"},{"location":"guides/cp4d/spplus/overview/spp-overview/#spectrum-protect-plus","text":"","title":"Spectrum Protect Plus"},{"location":"guides/cp4d/spplus/overview/spp-overview/#unified-data-protection-for-hybrid-cloud-environments","text":"IBM Spectrum\u00ae Protect Plus (SPP) is a data protection and availability solution for virtual environments and database applications. As a data resiliency solution it provides recovery, replication, retention, and reuse of data. The graphic below shows SPP deployed in two active locations. The IBM Spectrum Protect Plus server is deployed in only one of the sites. vSnap servers (with their corresponding disks) are deployed in each site to localize data movement in the context of the protected vSphere resources. Bi-directional replication is configured to take place between the vSnap servers at the two sites.","title":"Unified data protection for hybrid cloud environments"},{"location":"guides/cp4d/spplus/overview/spp-overview/#information-about-spp","text":"IBM Spectrum Protect Plus is used to restore the snapshots from vSnap to the original or alternate hypervisor. In this production scenario SPP is used to backup and restore data from IBM\u00ae Cloud Pak for Data environments. SPP can be installed as: A virtual appliance A set of OpenShift containers SPP uses OADP (OpenShift APIs for Data Protection) which is an operator that Red Hat has created to create backup and restore APIs in the OpenShift cluster. Cloud Paks can backup the entire namespace whereas SPP also helps in backing up common services and cluster related resources. These resources can then be used to restore everything in a new environment.","title":"Information about SPP"},{"location":"guides/cp4d/spplus/overview/spp-overview/#links","text":"IBM Spectrum Protect Plus IBM Spectrum Protect Plus Knowledge Center IBM Spectrum Protect Plus Blueprints","title":"Links"},{"location":"guides/cp4d/watsonkc/deployment/cpd-wkc-ibmcloud/","text":"Deployment on IBM Cloud \u00b6 Overview \u00b6 Watson Knowledge Catalog","title":"Deployment on IBM Cloud"},{"location":"guides/cp4d/watsonkc/deployment/cpd-wkc-ibmcloud/#deployment-on-ibm-cloud","text":"","title":"Deployment on IBM Cloud"},{"location":"guides/cp4d/watsonkc/deployment/cpd-wkc-ibmcloud/#overview","text":"Watson Knowledge Catalog","title":"Overview"},{"location":"guides/cp4d/watsonkc/overview/cpd-wkc-overview/","text":"Watson Knowledge Catalog \u00b6 Powers self-service discovery of data \u00b6 Watson Knowledge Catalog (WKC) provides a secure enterprise catalog management platform that is supported by a data governance framework. The data governance framework ensures that data access and data quality are compliant with your business rules and standards. Information about WKC \u00b6 Watson Knowledge Catalog is included with IBM\u00ae Cloud Pak for Data. A project administrator can install WKC on the cloud pak. The requirements are: WKC requires a custom security context constraint (SCC). WKC must be installed in the same project as Cloud Pak for Data. WKC requires the Cloud Pak for Data common core services. WKC uses one of these following storage classes: OpenShift Container Storage: ocs-storagecluster-cephfs ocs-storagecluster-ceph-rbd NFS: managed-nfs-storage Portworx: portworx-shared-gp3 portworx-cassandra-sc portworx-couchdb-sc portworx-db2-rwo-sc portworx-elastic-sc portworx-metastoredb-sc portworx-gp3-sc portworx-kafka-sc portworx-solr-sc IBM Cloud File Storage: ibmc-file-gold-gid or ibm-file-custom-gold-gid Links \u00b6 IBM Cloud Pak for Data IBM Cloud Pak for Data Knowledge Center IBM Cloud Pak for Data System","title":"Overview"},{"location":"guides/cp4d/watsonkc/overview/cpd-wkc-overview/#watson-knowledge-catalog","text":"","title":"Watson Knowledge Catalog"},{"location":"guides/cp4d/watsonkc/overview/cpd-wkc-overview/#powers-self-service-discovery-of-data","text":"Watson Knowledge Catalog (WKC) provides a secure enterprise catalog management platform that is supported by a data governance framework. The data governance framework ensures that data access and data quality are compliant with your business rules and standards.","title":"Powers self-service discovery of data"},{"location":"guides/cp4d/watsonkc/overview/cpd-wkc-overview/#information-about-wkc","text":"Watson Knowledge Catalog is included with IBM\u00ae Cloud Pak for Data. A project administrator can install WKC on the cloud pak. The requirements are: WKC requires a custom security context constraint (SCC). WKC must be installed in the same project as Cloud Pak for Data. WKC requires the Cloud Pak for Data common core services. WKC uses one of these following storage classes: OpenShift Container Storage: ocs-storagecluster-cephfs ocs-storagecluster-ceph-rbd NFS: managed-nfs-storage Portworx: portworx-shared-gp3 portworx-cassandra-sc portworx-couchdb-sc portworx-db2-rwo-sc portworx-elastic-sc portworx-metastoredb-sc portworx-gp3-sc portworx-kafka-sc portworx-solr-sc IBM Cloud File Storage: ibmc-file-gold-gid or ibm-file-custom-gold-gid","title":"Information about WKC"},{"location":"guides/cp4d/watsonkc/overview/cpd-wkc-overview/#links","text":"IBM Cloud Pak for Data IBM Cloud Pak for Data Knowledge Center IBM Cloud Pak for Data System","title":"Links"},{"location":"guides/cp4d/watsonstudio/deployment/cpd-ws-ibmcloud/","text":"Deployment on IBM Cloud \u00b6 Overview \u00b6 Watson Studio","title":"Deployment on IBM Cloud"},{"location":"guides/cp4d/watsonstudio/deployment/cpd-ws-ibmcloud/#deployment-on-ibm-cloud","text":"","title":"Deployment on IBM Cloud"},{"location":"guides/cp4d/watsonstudio/deployment/cpd-ws-ibmcloud/#overview","text":"Watson Studio","title":"Overview"},{"location":"guides/cp4d/watsonstudio/overview/cpd-ws-overview/","text":"Watson Studio \u00b6 Empowering data scientists, developers and analysts to build, run and manage AI models \u00b6 Watson Studio provides the environment and tools for data scientists and business analysts to collaborately work on data. Using analytics projects to organize, cleanse, analyze and visualize data. Watson Studio can also be used to create and train machine learning (ML) models. Information about Watson Studio \u00b6 Starting with v4, Watson Studio is included with IBM\u00ae Cloud Pak for Data. A project administrator can install Watson Studio on the cloud pak. The requirements are: Watson Studio needs only the restricted security context constraint (SCC). Watson Studio must be installed in the same project as Cloud Pak for Data. Watson Studio requires the Cloud Pak for Data common core services. Watson Studio uses one of these following storage classes: OpenShift Container Storage: ocs-storagecluster-cephfs NFS: managed-nfs-storage Portworx: portworx-shared-gp3 IBM Cloud File Storage: ibmc-file-gold-gid or ibm-file-custom-gold-gid Links \u00b6 IBM Cloud Pak for Data IBM Cloud Pak for Data Knowledge Center IBM Cloud Pak for Data System","title":"Overview"},{"location":"guides/cp4d/watsonstudio/overview/cpd-ws-overview/#watson-studio","text":"","title":"Watson Studio"},{"location":"guides/cp4d/watsonstudio/overview/cpd-ws-overview/#empowering-data-scientists-developers-and-analysts-to-build-run-and-manage-ai-models","text":"Watson Studio provides the environment and tools for data scientists and business analysts to collaborately work on data. Using analytics projects to organize, cleanse, analyze and visualize data. Watson Studio can also be used to create and train machine learning (ML) models.","title":"Empowering data scientists, developers and analysts to build, run and manage AI models"},{"location":"guides/cp4d/watsonstudio/overview/cpd-ws-overview/#information-about-watson-studio","text":"Starting with v4, Watson Studio is included with IBM\u00ae Cloud Pak for Data. A project administrator can install Watson Studio on the cloud pak. The requirements are: Watson Studio needs only the restricted security context constraint (SCC). Watson Studio must be installed in the same project as Cloud Pak for Data. Watson Studio requires the Cloud Pak for Data common core services. Watson Studio uses one of these following storage classes: OpenShift Container Storage: ocs-storagecluster-cephfs NFS: managed-nfs-storage Portworx: portworx-shared-gp3 IBM Cloud File Storage: ibmc-file-gold-gid or ibm-file-custom-gold-gid","title":"Information about Watson Studio"},{"location":"guides/cp4d/watsonstudio/overview/cpd-ws-overview/#links","text":"IBM Cloud Pak for Data IBM Cloud Pak for Data Knowledge Center IBM Cloud Pak for Data System","title":"Links"},{"location":"guides/cp4i/ace/build-backend-service/build-backend-service/","text":"Example Supporting Application \u00b6 Our sample ACE application is to integrate with an existing SOAP web service. We assume that this web service is deployed independently. But for the sake of completeness of this guide, we will go over a step-by-step guide on how to deploy the SOAP service on to your OpenShift cluster. Note We have chosen to use a GitOps approach to deploy the SOAP web service, and in turn making use of OpenShift Build (see more: OpenShift Build mechanism). You are free to chose another approach to deploy this service (for example, building image locally on your workstation, deploying to a registry and creating the necessary kubernetes manifests). The SOAP service is secured over mTLS. It serves traffic over HTTPS, and therefore requires identity certificate and key for the server. It also requires client certificate to be sent during requests. The service need to trust this certificate for communication to proceed. To simplify our set up, we will make use Certificate Authority (CA): The certificates used by both the SOAP service and the client will be signed by a CA. Both the SOAP service and the client will trust the CA certificate, and in turn will trust any certificate signed by the CA. This way the client and the SOAP service will trust each other. In this subsection we are using the general term, \"client\", to refer to the entity that is making the request to the SOAP service. Our ACE application is one such client. The service is implemented with Spring Boot Web Service. It needs a JKS based keystore that contains the server's certificate and key. It also needs a JKS based truststore to store the CA certificate. SealedSecret \u00b6 The JKS and passwords needed by the SOAP service pod will be mounted from a kubernetes Secret object at deploy time. As Secret store encoded contents and not encrypted contents, storing Secret s in a git repository is not safe. Instead we will make use of SealedSecret objects - which store encrypted content of JKS and passwords. To create the SealedSecret objects, we will first need to create plain Secret manifests that contains our data (keystores and passwords). Thereafter, we will use kubeseal to encrypt the Secret s to SealedSecret s When a SealedSecret object are created on the cluster, the SealedSecret operator will decrypt it back to plain Secret and create the Secret object on the cluster: The plain Secret will then be mounted on the on the SOAP service pod, in the usual way. Pre-requisite \u00b6 Sealed Secret operator is installed on your cluster. kubeseal CLI is installed on your local workstation. See kubeseal for installation instruction. yq (v4.x) CLI is installed on your local workstation. See yq for installation instruction. SoapUI SoapUI is installed. (Alternatively, you can use Postman or even curl ) Steps \u00b6 At start, the folder, soapServer , within gitOps repository looks like this: . \u251c\u2500\u2500 buildConfig \u2502 \u2514\u2500\u2500 buildConfig.yaml \u251c\u2500\u2500 deploymentConfig \u2502 \u2514\u2500\u2500 deploymentConfig.yaml \u251c\u2500\u2500 imageStream \u2502 \u2514\u2500\u2500 imageStream.yaml \u251c\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 route \u2502 \u2514\u2500\u2500 route.yaml \u251c\u2500\u2500 sealedSecret \u2514\u2500\u2500 service \u2514\u2500\u2500 service.yaml 6 directories, 10 files It contains the buildConfig that builds the container image for the SOAP service deploymentConfig that defines the deployment imageStream that names the container image service that define the service endpoints route that exposes the service Currently, the above sealedSecret folder is empty. This folder will contain the needed SealedSecret manifests. The kustomization.yaml does refer to these (non-existing) manifest files: ```yaml src=\"TODO-link-to-gitops\" apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - \"sealedSecret/ibm-soap-server-jks-enc.yaml\" <- need to be created - \"sealedSecret/ibm-ca-jks-enc.yaml\" <- need to be created - \"sealedSecret/ibm-soap-server-jks-enc-password.yaml\" <- need to be created - \"sealedSecret/ibm-ca-jks-enc-password.yaml\" <- need to be created - \"service/service.yaml\" - \"route/route.yaml\" - \"deploymentConfig/deploymentConfig.yaml\" - \"imageStream/imageStream.yaml\" - \"buildConfig/buildConfig.yaml\" <InlineNotification> We did not pre-create the certs and keys for the SOAP server (or any of the services we will deploy part of this guide). This is primarily because the hostname of the deployment host need to be part of the certificate's Common Name and Subject Alternative Names. The hostname depends on your OpenShift cluster. </InlineNotification> As part of the deployment, we will create the needed manifests. We will therefore follow the following steps: <AnchorLinks small> <AnchorLink>Step 1: Generate the required CA, keys and certificates</AnchorLink> <AnchorLink>Step 2: Create Sealed Secret manifests</AnchorLink> <AnchorLink>Step 3: Activate ArgoCD application</AnchorLink> <AnchorLink>Step 4: Test the deployment with SoapUI</AnchorLink> </AnchorLinks> ### Step 1: Generate the required CA, keys and certificates Follow the instructions for [cert-generation](https://github.com/saadlu/ace-rest-ws/tree/master/cert-generation). The script will generate all the certs, keys and stores that are required for this guide. For the SOAP service deployment, we only need the following files: ```sh ... \u2514\u2500\u2500 certs <--- directory where certs/keys are generated \u251c\u2500\u2500 ibm-ca.jks <---------- contains the CA certificate \u251c\u2500\u2500 ibm-client.jks <------ contains cert/key for a client application \u2514\u2500\u2500 ibm-soap-server.jks <- contains cert/key for the SOAP service ... Step 2: Create Sealed Secret manifests \u00b6 We will create manifests for four secrets containing the JKS files, ibm-soap-server.jks , ibm-ca.jks and their passwords: kind : Secret apiVersion : v1 metadata : name : ibm-soap-server.jks data : ibm-soap-server.jks : <BASE64 encoded content of ibm-soap-server.jks> kind : Secret apiVersion : v1 metadata : name : ibm-ca-jks data : ibm-ca-jks : <BASE64 encoded content of ibm-ca.jks> kind : Secret apiVersion : v1 metadata : name : ibm-soap-server-jks-password data : SERVER_SSL_KEY_STORE_PASSWORD : <BASE64 encoded password of ibm-soap-server.jks> kind : Secret apiVersion : v1 metadata : name : ibm-ca-jks-password data : SERVER_SSL_TRUST_STORE_PASSWORD : <BASE64 encoded password of ibm-ca.jks> You can manually create the above four secrets. Alternatively, you can make use of yq CLI as following (Optional) Use yq to create secrets Start by creating a directory, for example: mkdir -p ~/workspace/ace-ref-deployment/soap-secret cd ~/workspace/ace-ref-deployment/soap-secret We use secret.yaml as a starting point for our manifest file It only contains the kind and apiVersion of the manifests: kind : Secret apiVersion : v1 Use the following command to generate the secret manifests for the JKS files: yq e '.metadata.name=<name-of-the-secret>' \\ <path-to-secret.yaml> \\ > <name-of-secret-manifest>.yaml VALUE = \" $( base64 -w 0 <path-to-jks> ) \" \\ yq e '.data.\"<name-of-key>\"=strenv(VALUE)' \\ -i <name-of-secret-manifest>.yaml replacing <name-of-the-secret> , <path-to-secret.yaml> , <name-of-secret-manifest> , <path-to-jks> and <name-of-key> appropriately. for instance, to create a secret containing ibm-soap-server.jks : yq e '.metadata.name=\"ibm-soap-server.jks\"' \\ ../ace-rest-ws/appendix/secret.yaml \\ > ibm-soap-server-jks.yaml VALUE = \" $( base64 -w 0 ../ace-rest-ws/certs/ibm-soap-server.jks ) \" \\ yq e '.data.\"ibm-soap-server.jks\"=strenv(VALUE)' \\ -i ibm-soap-server-jks.yaml Similar commands can be used to create the secret containing ibm-ca.jks To create the Secret for password, you can use the following: yq e '.metadata.name=<name-of-the-secret>' \\ <path-to-secret.yaml> \\ > <name-of-secret-manifest>.yaml yq e '.stringData.\"<name-of-key>\"=<password>' \\ -i <name-of-secret-manifest>.yaml For instance: yq e '.metadata.name=\"ibm-soap-server-jks-password\"' \\ ../ace-rest-ws/appendix/secret.yaml \\ > ibm-soap-server-jks-password.yaml yq e '.stringData.SERVER_SSL_KEY_STORE_PASSWORD=\"passw0rd\"' \\ -i ibm-soap-server-jks-password.yaml The above will create a Secret manifest that contains the password for ibm-soap-server.jks . Similar command can be used to create the Secret manifest that contains the password for ibm-ca.jks . At the end of this process, you should have four Secret manifests: ibm-soap-server-jks.yaml ibm-soap-server-jks-password.yaml ibm-ca.yaml ibm-ca-jks-password.yaml Now that we have the four Secrets, we can encrypt them to SealedSecrets. To do so, we use kubeseal CLI. Make sure you are logged into the cluster: kubeseal --controller-name = sealedsecretcontroller-sealed-secrets \\ --controller-namespace = sealed-secrets \\ < <name-of-secret-manifest>.yaml \\ > <name-of-sealed-secret-manifest>.yaml for instance, to create a sealed secret containing ibm-soap-server.jks : kubeseal --controller-name = sealedsecretcontroller-sealed-secrets \\ --controller-namespace = sealed-secrets \\ < ibm-soap-server-jks.yaml \\ > ibm-soap-server-jks-enc.yaml Similar command can be used to create all four SealedSecrets: ibm-soap-server-jks-enc.yaml ibm-soap-server-jks-password-enc.yaml ibm-ca-enc.yaml ibm-ca-jks-password-enc.yaml Finally move the sealed secrets manifests to sealedSecret folder in the GitOps repos For instance: mv ibm-soap-server-jks-enc.yaml <git-ops-application-repo>/soapserver/base mv ibm-soap-server-jks-password-enc.yaml <git-ops-application-repo>/soapserver/base mv ibm-ca-enc.yaml <git-ops-application-repo>/soapserver/base mv ibm-ca-jks-password-enc.yaml <git-ops-application-repo>/soapserver/base You should now have all the required manifest in your gitOps repository: yaml\" repo \u251c\u2500\u2500 buildConfig \u2502 \u2514\u2500\u2500 buildConfig.yaml \u251c\u2500\u2500 deploymentConfig \u2502 \u2514\u2500\u2500 deploymentConfig.yaml \u251c\u2500\u2500 imageStream \u2502 \u2514\u2500\u2500 imageStream.yaml \u251c\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 route \u2502 \u2514\u2500\u2500 route.yaml \u251c\u2500\u2500 sealedSecret \u2502 \u251c\u2500\u2500 ibm-ca-jks-enc-password.yaml \u2502 \u251c\u2500\u2500 ibm-ca-jks-enc.yaml \u2502 \u251c\u2500\u2500 ibm-soap-server-jks-enc-password.yaml \u2502 \u2514\u2500\u2500 ibm-soap-server-jks-enc.yaml \u2514\u2500\u2500 service \u2514\u2500\u2500 service.yaml Step 3: Activate ArgoCD application \u00b6 Activate ArgoCD application to watch over SOAP service GitOps folder. The BuildConfig should start a build to create and push a container image for the SOAP service. At the same time, SealedSecret operator will create four Secrets based on the four SealedSecrets. Once the build is done, DeploymentConfig will deploy a pod, mounting the keystore files and plugging the password of them as environment variables. Service will create the endpoints and the service will be exposed via the Route. Step 4: Test the deployment with SoapUI \u00b6 We can use SoapUI application to test out our SOAP service. The application can take a WSDL file and generate a sample of the payload. Once the SOAP service is deployed, we can download the WSDL from it. The SOAP service was made with Spring-WS which uses a Contract-First approach. The XSD file, customer_details.xsd , defines the contract. The WSDL is generated by the service based on the contract XSD. Note that the SOAP service is protected over mTLS we would need client key, client certificate and CA certificate for any request, be in downloading the WSDL or making a service call. Download the WSDL via wget We can use wget CLI to download the WSDL file. Following steps covers it: Make a new folder to for the WSDL file sh mkdir wsdl cd wsdl Run the wget command: sh wget \\ --ca-certificate=<ca-cert-path> \\ --certificate=<client-cert-path> \\ --private-key=<client-key-path> \\ https://<hostname-of-SOAP-service>/ws/customerDetails.wsdl replacing <ca-cert-path> , <client-cert-path> , <client-key-path> and <hostname-of-SOAP-service> appropriately. <hostname-of-SOAP-service> can be found from the Route. For example: Make SOAP request via SoapUI Open SoapUI. Create a new project importing the WSDL. Open the project. Based on the WSDL, the SoapUI will create sample payload. Make change appropriately. Note that phone needs to have the pattern, [0-9]{3}-[0-9]{7} , for example, 555-5555555 . The URL will also be populated for you. Notice that it should be of the pattern https://<hostname>/ws As the service is protected over mTLS, you'd need use client keystore. Open Preference On the SSL Settings dialog, choose the client keystore. For instance, you can choose ibm-client.jks (created earlier above) Set the password. (if you using the certificates earlier, password is passw0rd ) Make the request. You can also test out the error handing: Remove service header from the payload. Service should return an error Use different phone number pattern, other than [0-9]{3}-[0-9]{7}","title":"Build backend service"},{"location":"guides/cp4i/ace/build-backend-service/build-backend-service/#example-supporting-application","text":"Our sample ACE application is to integrate with an existing SOAP web service. We assume that this web service is deployed independently. But for the sake of completeness of this guide, we will go over a step-by-step guide on how to deploy the SOAP service on to your OpenShift cluster. Note We have chosen to use a GitOps approach to deploy the SOAP web service, and in turn making use of OpenShift Build (see more: OpenShift Build mechanism). You are free to chose another approach to deploy this service (for example, building image locally on your workstation, deploying to a registry and creating the necessary kubernetes manifests). The SOAP service is secured over mTLS. It serves traffic over HTTPS, and therefore requires identity certificate and key for the server. It also requires client certificate to be sent during requests. The service need to trust this certificate for communication to proceed. To simplify our set up, we will make use Certificate Authority (CA): The certificates used by both the SOAP service and the client will be signed by a CA. Both the SOAP service and the client will trust the CA certificate, and in turn will trust any certificate signed by the CA. This way the client and the SOAP service will trust each other. In this subsection we are using the general term, \"client\", to refer to the entity that is making the request to the SOAP service. Our ACE application is one such client. The service is implemented with Spring Boot Web Service. It needs a JKS based keystore that contains the server's certificate and key. It also needs a JKS based truststore to store the CA certificate.","title":"Example Supporting Application"},{"location":"guides/cp4i/ace/build-backend-service/build-backend-service/#sealedsecret","text":"The JKS and passwords needed by the SOAP service pod will be mounted from a kubernetes Secret object at deploy time. As Secret store encoded contents and not encrypted contents, storing Secret s in a git repository is not safe. Instead we will make use of SealedSecret objects - which store encrypted content of JKS and passwords. To create the SealedSecret objects, we will first need to create plain Secret manifests that contains our data (keystores and passwords). Thereafter, we will use kubeseal to encrypt the Secret s to SealedSecret s When a SealedSecret object are created on the cluster, the SealedSecret operator will decrypt it back to plain Secret and create the Secret object on the cluster: The plain Secret will then be mounted on the on the SOAP service pod, in the usual way.","title":"SealedSecret"},{"location":"guides/cp4i/ace/build-backend-service/build-backend-service/#pre-requisite","text":"Sealed Secret operator is installed on your cluster. kubeseal CLI is installed on your local workstation. See kubeseal for installation instruction. yq (v4.x) CLI is installed on your local workstation. See yq for installation instruction. SoapUI SoapUI is installed. (Alternatively, you can use Postman or even curl )","title":"Pre-requisite"},{"location":"guides/cp4i/ace/build-backend-service/build-backend-service/#steps","text":"At start, the folder, soapServer , within gitOps repository looks like this: . \u251c\u2500\u2500 buildConfig \u2502 \u2514\u2500\u2500 buildConfig.yaml \u251c\u2500\u2500 deploymentConfig \u2502 \u2514\u2500\u2500 deploymentConfig.yaml \u251c\u2500\u2500 imageStream \u2502 \u2514\u2500\u2500 imageStream.yaml \u251c\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 route \u2502 \u2514\u2500\u2500 route.yaml \u251c\u2500\u2500 sealedSecret \u2514\u2500\u2500 service \u2514\u2500\u2500 service.yaml 6 directories, 10 files It contains the buildConfig that builds the container image for the SOAP service deploymentConfig that defines the deployment imageStream that names the container image service that define the service endpoints route that exposes the service Currently, the above sealedSecret folder is empty. This folder will contain the needed SealedSecret manifests. The kustomization.yaml does refer to these (non-existing) manifest files: ```yaml src=\"TODO-link-to-gitops\" apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - \"sealedSecret/ibm-soap-server-jks-enc.yaml\" <- need to be created - \"sealedSecret/ibm-ca-jks-enc.yaml\" <- need to be created - \"sealedSecret/ibm-soap-server-jks-enc-password.yaml\" <- need to be created - \"sealedSecret/ibm-ca-jks-enc-password.yaml\" <- need to be created - \"service/service.yaml\" - \"route/route.yaml\" - \"deploymentConfig/deploymentConfig.yaml\" - \"imageStream/imageStream.yaml\" - \"buildConfig/buildConfig.yaml\" <InlineNotification> We did not pre-create the certs and keys for the SOAP server (or any of the services we will deploy part of this guide). This is primarily because the hostname of the deployment host need to be part of the certificate's Common Name and Subject Alternative Names. The hostname depends on your OpenShift cluster. </InlineNotification> As part of the deployment, we will create the needed manifests. We will therefore follow the following steps: <AnchorLinks small> <AnchorLink>Step 1: Generate the required CA, keys and certificates</AnchorLink> <AnchorLink>Step 2: Create Sealed Secret manifests</AnchorLink> <AnchorLink>Step 3: Activate ArgoCD application</AnchorLink> <AnchorLink>Step 4: Test the deployment with SoapUI</AnchorLink> </AnchorLinks> ### Step 1: Generate the required CA, keys and certificates Follow the instructions for [cert-generation](https://github.com/saadlu/ace-rest-ws/tree/master/cert-generation). The script will generate all the certs, keys and stores that are required for this guide. For the SOAP service deployment, we only need the following files: ```sh ... \u2514\u2500\u2500 certs <--- directory where certs/keys are generated \u251c\u2500\u2500 ibm-ca.jks <---------- contains the CA certificate \u251c\u2500\u2500 ibm-client.jks <------ contains cert/key for a client application \u2514\u2500\u2500 ibm-soap-server.jks <- contains cert/key for the SOAP service ...","title":"Steps"},{"location":"guides/cp4i/ace/build-backend-service/build-backend-service/#step-2-create-sealed-secret-manifests","text":"We will create manifests for four secrets containing the JKS files, ibm-soap-server.jks , ibm-ca.jks and their passwords: kind : Secret apiVersion : v1 metadata : name : ibm-soap-server.jks data : ibm-soap-server.jks : <BASE64 encoded content of ibm-soap-server.jks> kind : Secret apiVersion : v1 metadata : name : ibm-ca-jks data : ibm-ca-jks : <BASE64 encoded content of ibm-ca.jks> kind : Secret apiVersion : v1 metadata : name : ibm-soap-server-jks-password data : SERVER_SSL_KEY_STORE_PASSWORD : <BASE64 encoded password of ibm-soap-server.jks> kind : Secret apiVersion : v1 metadata : name : ibm-ca-jks-password data : SERVER_SSL_TRUST_STORE_PASSWORD : <BASE64 encoded password of ibm-ca.jks> You can manually create the above four secrets. Alternatively, you can make use of yq CLI as following (Optional) Use yq to create secrets Start by creating a directory, for example: mkdir -p ~/workspace/ace-ref-deployment/soap-secret cd ~/workspace/ace-ref-deployment/soap-secret We use secret.yaml as a starting point for our manifest file It only contains the kind and apiVersion of the manifests: kind : Secret apiVersion : v1 Use the following command to generate the secret manifests for the JKS files: yq e '.metadata.name=<name-of-the-secret>' \\ <path-to-secret.yaml> \\ > <name-of-secret-manifest>.yaml VALUE = \" $( base64 -w 0 <path-to-jks> ) \" \\ yq e '.data.\"<name-of-key>\"=strenv(VALUE)' \\ -i <name-of-secret-manifest>.yaml replacing <name-of-the-secret> , <path-to-secret.yaml> , <name-of-secret-manifest> , <path-to-jks> and <name-of-key> appropriately. for instance, to create a secret containing ibm-soap-server.jks : yq e '.metadata.name=\"ibm-soap-server.jks\"' \\ ../ace-rest-ws/appendix/secret.yaml \\ > ibm-soap-server-jks.yaml VALUE = \" $( base64 -w 0 ../ace-rest-ws/certs/ibm-soap-server.jks ) \" \\ yq e '.data.\"ibm-soap-server.jks\"=strenv(VALUE)' \\ -i ibm-soap-server-jks.yaml Similar commands can be used to create the secret containing ibm-ca.jks To create the Secret for password, you can use the following: yq e '.metadata.name=<name-of-the-secret>' \\ <path-to-secret.yaml> \\ > <name-of-secret-manifest>.yaml yq e '.stringData.\"<name-of-key>\"=<password>' \\ -i <name-of-secret-manifest>.yaml For instance: yq e '.metadata.name=\"ibm-soap-server-jks-password\"' \\ ../ace-rest-ws/appendix/secret.yaml \\ > ibm-soap-server-jks-password.yaml yq e '.stringData.SERVER_SSL_KEY_STORE_PASSWORD=\"passw0rd\"' \\ -i ibm-soap-server-jks-password.yaml The above will create a Secret manifest that contains the password for ibm-soap-server.jks . Similar command can be used to create the Secret manifest that contains the password for ibm-ca.jks . At the end of this process, you should have four Secret manifests: ibm-soap-server-jks.yaml ibm-soap-server-jks-password.yaml ibm-ca.yaml ibm-ca-jks-password.yaml Now that we have the four Secrets, we can encrypt them to SealedSecrets. To do so, we use kubeseal CLI. Make sure you are logged into the cluster: kubeseal --controller-name = sealedsecretcontroller-sealed-secrets \\ --controller-namespace = sealed-secrets \\ < <name-of-secret-manifest>.yaml \\ > <name-of-sealed-secret-manifest>.yaml for instance, to create a sealed secret containing ibm-soap-server.jks : kubeseal --controller-name = sealedsecretcontroller-sealed-secrets \\ --controller-namespace = sealed-secrets \\ < ibm-soap-server-jks.yaml \\ > ibm-soap-server-jks-enc.yaml Similar command can be used to create all four SealedSecrets: ibm-soap-server-jks-enc.yaml ibm-soap-server-jks-password-enc.yaml ibm-ca-enc.yaml ibm-ca-jks-password-enc.yaml Finally move the sealed secrets manifests to sealedSecret folder in the GitOps repos For instance: mv ibm-soap-server-jks-enc.yaml <git-ops-application-repo>/soapserver/base mv ibm-soap-server-jks-password-enc.yaml <git-ops-application-repo>/soapserver/base mv ibm-ca-enc.yaml <git-ops-application-repo>/soapserver/base mv ibm-ca-jks-password-enc.yaml <git-ops-application-repo>/soapserver/base You should now have all the required manifest in your gitOps repository: yaml\" repo \u251c\u2500\u2500 buildConfig \u2502 \u2514\u2500\u2500 buildConfig.yaml \u251c\u2500\u2500 deploymentConfig \u2502 \u2514\u2500\u2500 deploymentConfig.yaml \u251c\u2500\u2500 imageStream \u2502 \u2514\u2500\u2500 imageStream.yaml \u251c\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 route \u2502 \u2514\u2500\u2500 route.yaml \u251c\u2500\u2500 sealedSecret \u2502 \u251c\u2500\u2500 ibm-ca-jks-enc-password.yaml \u2502 \u251c\u2500\u2500 ibm-ca-jks-enc.yaml \u2502 \u251c\u2500\u2500 ibm-soap-server-jks-enc-password.yaml \u2502 \u2514\u2500\u2500 ibm-soap-server-jks-enc.yaml \u2514\u2500\u2500 service \u2514\u2500\u2500 service.yaml","title":"Step 2: Create Sealed Secret manifests"},{"location":"guides/cp4i/ace/build-backend-service/build-backend-service/#step-3-activate-argocd-application","text":"Activate ArgoCD application to watch over SOAP service GitOps folder. The BuildConfig should start a build to create and push a container image for the SOAP service. At the same time, SealedSecret operator will create four Secrets based on the four SealedSecrets. Once the build is done, DeploymentConfig will deploy a pod, mounting the keystore files and plugging the password of them as environment variables. Service will create the endpoints and the service will be exposed via the Route.","title":"Step 3: Activate ArgoCD application"},{"location":"guides/cp4i/ace/build-backend-service/build-backend-service/#step-4-test-the-deployment-with-soapui","text":"We can use SoapUI application to test out our SOAP service. The application can take a WSDL file and generate a sample of the payload. Once the SOAP service is deployed, we can download the WSDL from it. The SOAP service was made with Spring-WS which uses a Contract-First approach. The XSD file, customer_details.xsd , defines the contract. The WSDL is generated by the service based on the contract XSD. Note that the SOAP service is protected over mTLS we would need client key, client certificate and CA certificate for any request, be in downloading the WSDL or making a service call. Download the WSDL via wget We can use wget CLI to download the WSDL file. Following steps covers it: Make a new folder to for the WSDL file sh mkdir wsdl cd wsdl Run the wget command: sh wget \\ --ca-certificate=<ca-cert-path> \\ --certificate=<client-cert-path> \\ --private-key=<client-key-path> \\ https://<hostname-of-SOAP-service>/ws/customerDetails.wsdl replacing <ca-cert-path> , <client-cert-path> , <client-key-path> and <hostname-of-SOAP-service> appropriately. <hostname-of-SOAP-service> can be found from the Route. For example: Make SOAP request via SoapUI Open SoapUI. Create a new project importing the WSDL. Open the project. Based on the WSDL, the SoapUI will create sample payload. Make change appropriately. Note that phone needs to have the pattern, [0-9]{3}-[0-9]{7} , for example, 555-5555555 . The URL will also be populated for you. Notice that it should be of the pattern https://<hostname>/ws As the service is protected over mTLS, you'd need use client keystore. Open Preference On the SSL Settings dialog, choose the client keystore. For instance, you can choose ibm-client.jks (created earlier above) Set the password. (if you using the certificates earlier, password is passw0rd ) Make the request. You can also test out the error handing: Remove service header from the payload. Service should return an error Use different phone number pattern, other than [0-9]{3}-[0-9]{7}","title":"Step 4: Test the deployment with SoapUI"},{"location":"guides/cp4i/ace/cloud-native-example/configuration-management/","text":"Configuration Management \u00b6 coming soon...","title":"Configuration Management"},{"location":"guides/cp4i/ace/cloud-native-example/configuration-management/#configuration-management","text":"coming soon...","title":"Configuration Management"},{"location":"guides/cp4i/ace/cloud-native-example/example/","text":"Sample App Connect Enterprise Application \u00b6 Overview \u00b6 The sample ACE application that we will build in this tutorial will integrate with an existing backend SOAP service and expose it as a RESTful Web Service. In this section, we're going to: Review the backend SOAP service Review the sample ACE application that integrates with the backend SOAP service Audience : Architects, Application developers, Administrators The SOAP Backend Service \u00b6 The SOAP backend takes payload to create Customer Details. It response with status indicating whether the request was successful customer_id identification number assigned to the customer record Backend SOAP Service Note This sample service does not persist customer details data. It simply returns the status and a customer id. The XSD for all the data models can be found here: customer_details.xsd . The (generated) WSDL can be found here: customerDetails.wsdl . As specified in the data model, there are two validations active Payload must contains a service_header Customer phone number must comply with regex, [0-9]{3}-[0-9]{7} An example payload can look like: <SOAP-ENV:Envelope xmlns:SOAP-ENV= 'http://schemas.xmlsoap.org/soap/envelope/' > <SOAP-ENV:Body> <cus:customerDetailsRequest xmlns:cus= 'http://ibm.com/CustomerDetails/' > <cus:customer_details> <cus:service_header> <cus:brand> IBM </cus:brand> </cus:service_header> <cus:personal_details> <cus:name_details> <cus:first_name> John </cus:first_name> <cus:last_name> Doe </cus:last_name> </cus:name_details> <cus:contact_details> <cus:phone> 555-5555555 </cus:phone> <cus:address> 123 Main Street </cus:address> </cus:contact_details> </cus:personal_details> </cus:customer_details> </cus:customerDetailsRequest> </SOAP-ENV:Body> </SOAP-ENV:Envelope> And the SOAP server will respond with, for example: <SOAP-ENV:Envelope xmlns:SOAP-ENV= \"http://schemas.xmlsoap.org/soap/envelope/\" > <SOAP-ENV:Header/> <SOAP-ENV:Body> <ns2:customerDetailsResponse xmlns:ns2= \"http://ibm.com/CustomerDetails/\" > <ns2:status> success </ns2:status> <ns2:customer_id> 55ed3857-0f19-4250-a36f-de42f2db96b2 </ns2:customer_id> </ns2:customerDetailsResponse> </SOAP-ENV:Body> </SOAP-ENV:Envelope> As for validation, without service_header , response would be: <SOAP-ENV:Envelope xmlns:SOAP-ENV= \"http://schemas.xmlsoap.org/soap/envelope/\" > <SOAP-ENV:Header/> <SOAP-ENV:Body> <SOAP-ENV:Fault> <faultcode> SOAP-ENV:Client </faultcode> <faultstring xml:lang= \"en\" > Validation error </faultstring> <detail> <spring-ws:ValidationError xmlns:spring-ws= \"http://springframework.org/spring-ws\" > cvc-complex-type.2.4.a: Invalid content was found starting with element 'cus:personal_details'. One of '{\"http://ibm.com/CustomerDetails/\":service_header}' is expected. </spring-ws:ValidationError> </detail> </SOAP-ENV:Fault> </SOAP-ENV:Body> </SOAP-ENV:Envelope> and without properly formatted phone number, the response would be: <SOAP-ENV:Envelope xmlns:SOAP-ENV= \"http://schemas.xmlsoap.org/soap/envelope/\" > <SOAP-ENV:Header/> <SOAP-ENV:Body> <SOAP-ENV:Fault> <faultcode> SOAP-ENV:Client </faultcode> <faultstring xml:lang= \"en\" > Validation error </faultstring> <detail> <spring-ws:ValidationError xmlns:spring-ws= \"http://springframework.org/spring-ws\" > cvc-pattern-valid: Value '555-555' is not facet-valid with respect to pattern '[0-9]{3}-[0-9]{7}' for type '#AnonType_phonecontact_detailspersonal_detailscustomer_details'. </spring-ws:ValidationError> <spring-ws:ValidationError xmlns:spring-ws= \"http://springframework.org/spring-ws\" > cvc-type.3.1.3: The value '555-555' of element 'cus:phone' is not valid. </spring-ws:ValidationError> </detail> </SOAP-ENV:Fault> </SOAP-ENV:Body> </SOAP-ENV:Envelope> Following is a video walk through of the SOAP service in action: ACE application \u00b6 With App Connect Enterprise, we create a REST message flow that integrates with the backend SOAP service. The flow takes a JSON payload, converts the payload to SOAP envelop, invokes the backend SOAP service, converts the SOAP response to JSON and returns to the client. ACE REST Service Source code of the message flow can be found here: createCustomer_REST_V1 Data models for the RESTful API can be found here: swagger.json Following is the subflow that defines our transformation and integrates with the SOAP service: ACE subflow JSONtoSOAP is a mapping node that maps the JSON payload to to a SOAP Envelop. The left hand side takes customer_details as defined in the data model (see customer_details in swagger) and the right hand side takes customerDetailsRequest (see customerDetailsRequest ) as defined by the WSDL of the SOAP service. JSON to SOAP mapping Set up destination is a custom Java Node that reads the URL of the backend SOAP service from a user-defined policy. This way, the URL of the SOAP service is not hard-coded in the flow. Instead, the URL is fed into the flow from a configuration at deployment time. URL is then fed into the SOAP Request node. An example of this policy can be found here: backendurl.policyxml . SOAP Request makes the call to the SOAP backend service, using the Payload from the JSONtoSOAP node and URL from Set up destination . From henceforth, there is two paths, a \"happy\" path where request is successful and a \"unhappy\" path where the request was not successful. SOAP Extract , SOAP Extract Fault extract out the SOAP envelop from the response of the SOAP Request node. HTTP Header , HTTP Header Fault set the header for the responses. When request is successful it sets the response to 200 , 400 otherwise. ToJSON , fault_mapping is a mapping node that maps the SOAP messages to JSON. For the successful path, the left hand side takes takes customerDetailsResponse (see customerDetailsResponse ) as defined by the WSDL of the SOAP service and the right hand side takes customer_details_response as defined in the data model (see customer_details_response in swagger) and the right hand side Mapping for successful call For the un-successful path, the left hand side takes takes SOAP Fault (see SOAP fault ) and the right hand side takes fault as defined in the data model (see fault in swagger). Mapping for faulty call Following is a video walk through of the ACE application:","title":"Cloud Native Example"},{"location":"guides/cp4i/ace/cloud-native-example/example/#sample-app-connect-enterprise-application","text":"","title":"Sample App Connect Enterprise Application"},{"location":"guides/cp4i/ace/cloud-native-example/example/#overview","text":"The sample ACE application that we will build in this tutorial will integrate with an existing backend SOAP service and expose it as a RESTful Web Service. In this section, we're going to: Review the backend SOAP service Review the sample ACE application that integrates with the backend SOAP service Audience : Architects, Application developers, Administrators","title":"Overview"},{"location":"guides/cp4i/ace/cloud-native-example/example/#the-soap-backend-service","text":"The SOAP backend takes payload to create Customer Details. It response with status indicating whether the request was successful customer_id identification number assigned to the customer record Backend SOAP Service Note This sample service does not persist customer details data. It simply returns the status and a customer id. The XSD for all the data models can be found here: customer_details.xsd . The (generated) WSDL can be found here: customerDetails.wsdl . As specified in the data model, there are two validations active Payload must contains a service_header Customer phone number must comply with regex, [0-9]{3}-[0-9]{7} An example payload can look like: <SOAP-ENV:Envelope xmlns:SOAP-ENV= 'http://schemas.xmlsoap.org/soap/envelope/' > <SOAP-ENV:Body> <cus:customerDetailsRequest xmlns:cus= 'http://ibm.com/CustomerDetails/' > <cus:customer_details> <cus:service_header> <cus:brand> IBM </cus:brand> </cus:service_header> <cus:personal_details> <cus:name_details> <cus:first_name> John </cus:first_name> <cus:last_name> Doe </cus:last_name> </cus:name_details> <cus:contact_details> <cus:phone> 555-5555555 </cus:phone> <cus:address> 123 Main Street </cus:address> </cus:contact_details> </cus:personal_details> </cus:customer_details> </cus:customerDetailsRequest> </SOAP-ENV:Body> </SOAP-ENV:Envelope> And the SOAP server will respond with, for example: <SOAP-ENV:Envelope xmlns:SOAP-ENV= \"http://schemas.xmlsoap.org/soap/envelope/\" > <SOAP-ENV:Header/> <SOAP-ENV:Body> <ns2:customerDetailsResponse xmlns:ns2= \"http://ibm.com/CustomerDetails/\" > <ns2:status> success </ns2:status> <ns2:customer_id> 55ed3857-0f19-4250-a36f-de42f2db96b2 </ns2:customer_id> </ns2:customerDetailsResponse> </SOAP-ENV:Body> </SOAP-ENV:Envelope> As for validation, without service_header , response would be: <SOAP-ENV:Envelope xmlns:SOAP-ENV= \"http://schemas.xmlsoap.org/soap/envelope/\" > <SOAP-ENV:Header/> <SOAP-ENV:Body> <SOAP-ENV:Fault> <faultcode> SOAP-ENV:Client </faultcode> <faultstring xml:lang= \"en\" > Validation error </faultstring> <detail> <spring-ws:ValidationError xmlns:spring-ws= \"http://springframework.org/spring-ws\" > cvc-complex-type.2.4.a: Invalid content was found starting with element 'cus:personal_details'. One of '{\"http://ibm.com/CustomerDetails/\":service_header}' is expected. </spring-ws:ValidationError> </detail> </SOAP-ENV:Fault> </SOAP-ENV:Body> </SOAP-ENV:Envelope> and without properly formatted phone number, the response would be: <SOAP-ENV:Envelope xmlns:SOAP-ENV= \"http://schemas.xmlsoap.org/soap/envelope/\" > <SOAP-ENV:Header/> <SOAP-ENV:Body> <SOAP-ENV:Fault> <faultcode> SOAP-ENV:Client </faultcode> <faultstring xml:lang= \"en\" > Validation error </faultstring> <detail> <spring-ws:ValidationError xmlns:spring-ws= \"http://springframework.org/spring-ws\" > cvc-pattern-valid: Value '555-555' is not facet-valid with respect to pattern '[0-9]{3}-[0-9]{7}' for type '#AnonType_phonecontact_detailspersonal_detailscustomer_details'. </spring-ws:ValidationError> <spring-ws:ValidationError xmlns:spring-ws= \"http://springframework.org/spring-ws\" > cvc-type.3.1.3: The value '555-555' of element 'cus:phone' is not valid. </spring-ws:ValidationError> </detail> </SOAP-ENV:Fault> </SOAP-ENV:Body> </SOAP-ENV:Envelope> Following is a video walk through of the SOAP service in action:","title":"The SOAP Backend Service"},{"location":"guides/cp4i/ace/cloud-native-example/example/#ace-application","text":"With App Connect Enterprise, we create a REST message flow that integrates with the backend SOAP service. The flow takes a JSON payload, converts the payload to SOAP envelop, invokes the backend SOAP service, converts the SOAP response to JSON and returns to the client. ACE REST Service Source code of the message flow can be found here: createCustomer_REST_V1 Data models for the RESTful API can be found here: swagger.json Following is the subflow that defines our transformation and integrates with the SOAP service: ACE subflow JSONtoSOAP is a mapping node that maps the JSON payload to to a SOAP Envelop. The left hand side takes customer_details as defined in the data model (see customer_details in swagger) and the right hand side takes customerDetailsRequest (see customerDetailsRequest ) as defined by the WSDL of the SOAP service. JSON to SOAP mapping Set up destination is a custom Java Node that reads the URL of the backend SOAP service from a user-defined policy. This way, the URL of the SOAP service is not hard-coded in the flow. Instead, the URL is fed into the flow from a configuration at deployment time. URL is then fed into the SOAP Request node. An example of this policy can be found here: backendurl.policyxml . SOAP Request makes the call to the SOAP backend service, using the Payload from the JSONtoSOAP node and URL from Set up destination . From henceforth, there is two paths, a \"happy\" path where request is successful and a \"unhappy\" path where the request was not successful. SOAP Extract , SOAP Extract Fault extract out the SOAP envelop from the response of the SOAP Request node. HTTP Header , HTTP Header Fault set the header for the responses. When request is successful it sets the response to 200 , 400 otherwise. ToJSON , fault_mapping is a mapping node that maps the SOAP messages to JSON. For the successful path, the left hand side takes takes customerDetailsResponse (see customerDetailsResponse ) as defined by the WSDL of the SOAP service and the right hand side takes customer_details_response as defined in the data model (see customer_details_response in swagger) and the right hand side Mapping for successful call For the un-successful path, the left hand side takes takes SOAP Fault (see SOAP fault ) and the right hand side takes fault as defined in the data model (see fault in swagger). Mapping for faulty call Following is a video walk through of the ACE application:","title":"ACE application"},{"location":"guides/cp4i/ace/cluster-config/gitops-config/","text":"IBM AppConnect Enterprise GitOps Configuration \u00b6 In the cluster configuration of this chapter, we installed the fundamental components for continuous integration and continuous deployment into our cluster. These included a sample GitOps repository, and ArgoCD. In this section we're going to customize and enable the GitOps repository, so that we can install IBM AppConnect Enterprise(ACE) and all of the components that go along with that. We will examine these components in more detail throughout this section of the tutorial as well. In this topic, we're going to: Explore the sample GitOps repository in a little more detail Customize the GitOps repository for our cluster Connect ArgoCD to the customized GitOps repository Bootstrap the cluster Explore how the ci namespace is created Try out some dynamic changes to the cluster with ArgoCD Explore how ArgoCD manages configuration drift By the end of this topic we'll have a cluster up and running, having used GitOps to do it. We'll fully understand how ArgoCD manages cluster change and configuration drift. Pre-requisites \u00b6 Before attempting this section, you must have completed the previous section of this tutorial where you created your Red Hat OpenShift cluster(s) and customized the GitOps repository and ArgoCD. Please see the previous sections of this guide for information on how to do these tasks. The sample GitOps repository \u00b6 Let's understand how the GitOps repository works by using it to install the components we've highlighted in the above diagram. In the first part of this topic, let's look at the high level structure of the GitOps repository. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Explore the high level folder structure Use the following command to display the folder structure: tree . -d -L 2 We can see the different folders in the GitOps repository: . \u251c\u2500\u2500 0-bootstrap \u2502 \u251c\u2500\u2500 others \u2502 \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 doc \u2502 \u251c\u2500\u2500 diagrams \u2502 \u251c\u2500\u2500 experimental \u2502 \u251c\u2500\u2500 images \u2502 \u2514\u2500\u2500 scenarios \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 bom \u2514\u2500\u2500 setup \u2514\u2500\u2500 ocp47 The 0-bootstrap folder is the key folder that contains all the available profiles. Within this folders are Cluster Profiles that control which resources are deployed to the cluster. There are other folders, but these contain utility scripts; and some document related information. Let us examine the 0-bootstrap folder structure: tree ./0-bootstrap/ -d -L 2 We can see the different folders: ./0-bootstrap/ \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u2514\u2500\u2500 3-apps If we consider single-cluster profile, you can see the three layers under that covering infrastructure, services, and applications. Use single-cluster profile to configure the cluster Use the following command to display the folder structure: tree ./0-bootstrap/single-cluster/ -L 3 We can see the different folders: ./0-bootstrap/single-cluster/ \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 1-infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 2-services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3-apps \u2502 \u251c\u2500\u2500 3-apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 cp4a \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Notice that each high level folder has an argocd folder. Within these argocd folders are ArgoCD applications that control which resources are deployed to the cluster. Sample ArgoCD application We'll see later how these ArgoCD applications work in detail. For now, notice how each layer of the architecture ( infrastructure , service , application ) has a set of ArgoCD applications. Type the following command: tree 0 -bootstrap/single-cluster/1-infra/ It shows a list of ArgoCD applications that are used to manage Kubernetes infrastructure resources: 0-bootstrap/single-cluster/1-infra/ \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Notice the namespace- YAMLs; we'll see in a moment how these each define an ArgoCD application dedicated to managing a Kubernetes namespace in our reference architecture. A word on terminology As we get started with ArgoCD, it can be easy to confuse the term ArgoCD application with your application. That's because ArgoCD uses the term application to refer to a Kubernetes custom resource that was initially designed to automate application management. However, an ArgoCD application can automate the deployment of any Kubernetes resource within a cluster, such as namespaces or queue managers, as we'll see a little later. Let's now customize this repository to deploy the highlighted Kubernetes resources to our cluster. Customizing the GitOps repositories \u00b6 The cloned GitOps configuration repositories (specifically, multi-tenancy-gitops and multi-tenancy-gitops-apps ) needs to be customized for your cluster. The ArgoCD Application and AppProject resources and kustomization.yaml resources contains environment variables which will need to be updated to your specific GitHub organization and branch. Update multi-tenancy-gitops repository \u00b6 Have a look at the following kustomization.yaml as an example of what will need to be updated. cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS_INFRA} - op: add path: /spec/source/targetRevision value: ${GIT_GITOPS_INFRA_BRANCH} Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Run the customization script Let's customize the cloned multi-tenancy-gitops repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. This script has also made some customizations that we'll use much later in the tutorial; we'll refer to those at the relevant time. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 199 files changed, 290 insertions ( + ) , 290 deletions ( - ) Set your GitOps repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (48/48), 4.70 KiB | 1.57 MiB/s, done. Total 48 (delta 32), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 23 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git d95eca5..a900c39 master -> master Update multi-tenancy-gitops-apps repository \u00b6 Similar to the previous section, there are corresponding ArgoCD Applications resources that will need to be updated in the multi-tenancy-gitops-apps repository. Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Locate the multi-tenancy-gitops-apps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops-apps Run the customization script Let's customize the cloned multi-tenancy-gitops-apps repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops-apps repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 21 files changed, 42 insertions ( + ) , 42 deletions ( - ) Set your repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops-apps repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (42/42), 4.70 KiB | 1.57 MiB/s, done. Total 42 (delta 42), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 21 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git d95eca5..a900c39 master -> master Connect ArgoCD to the GitOps repository \u00b6 Let's now connect your customized GitOps repository to the instance of ArgoCD running in the cluster. Once connected, ArgoCD will use the contents of the repository to create necessary resources. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Review ArgoCD infrastructure folder Let's examine the 0-bootstrap/single-cluster/1-infra/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml We can see the contents of the kustomization.yaml : resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Notice that the resources that needs to be applied to the cluster are all inactivate and commented out. Let us enable the resources that are needed by un-commenting them. Deploy Kubernetes resources with kustomization.yaml Open 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the below resources: argocd/consolelink.yaml argocd/consolenotification.yaml argocd/namespace-ibm-common-services.yaml argocd/namespace-ci.yaml argocd/namespace-dev.yaml argocd/namespace-staging.yaml argocd/namespace-prod.yaml argocd/namespace-sealed-secrets.yaml argocd/namespace-tools.yaml You will have the following resources un-commented for infrastructure: resources : - argocd/consolelink.yaml - argocd/consolenotification.yaml - argocd/namespace-ibm-common-services.yaml - argocd/namespace-ci.yaml - argocd/namespace-dev.yaml - argocd/namespace-staging.yaml - argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml - argocd/namespace-sealed-secrets.yaml - argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for infrastructure\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a900c39..e3f696d master -> master Examine bootstrap.yaml residing in 0-bootstrap/single-cluster/ . The bootstrap.yaml file is used to create our first ArgoCD application called bootstrap-single-cluster . This initial ArgoCD application will create all the other ArgoCD applications that control the application, service, and infrastructure resources (such as the ci and dev namespaces) deployed to the cluster. Examine the YAML that defines the ArgoCD bootstrap application: cat 0 -bootstrap/single-cluster/bootstrap.yaml Notice also how this ArgoCD application has been customized to use the GitOps repository repoURL: https://github.com/prod-ref-guide/multi-tenancy-gitops.git . apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-single-cluster namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/single-cluster repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Most importantly, see how path: 0-bootstrap/single-cluster refers to the 0-bootstrap/single-cluster folder within this repository. This will result in the creation of individual ArgoCD applications to manage our cluster resources. Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us for now only deploy infra resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and comment out the 2-services/2-services.yaml and 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml # - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Commit and push changes to your git repository: git add . git commit -s -m \"Using only infra\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 456 bytes | 456.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git e3f696d..ea3b43f master -> master Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. The repository contains a bootstrap-single-cluster ArgoCD application that is watching this repository and using its contents to manage the cluster. When the bootstrap-single-cluster ArgoCD application is applied to the cluster, it will continuously ensure that all the activated resources are applied to the cluster. Apply the bootstrap YAML to the cluster: oc apply -f 0 -bootstrap/single-cluster/bootstrap.yaml Kubernetes will confirm that the bootstrap ArgoCD application has been created: application.argoproj.io/bootstrap-single-cluster created The bootstrap ArgoCD application will watch the 0-bootstrap/single-cluster folder in our GitOps repository on GitHub. In this way, as resources are added to the infrastructure , service and application folders, they will be deployed to the cluster automatically. This is therefore the only direct cluster operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Verify the bootstrap deployment Verify that the bootstrap ArgoCD application is running with the following command: oc get app/bootstrap-single-cluster -n openshift-gitops You should see that the bootstrap application was recently updated: NAME SYNC STATUS HEALTH STATUS bootstrap-single-cluster Synced Healthy HEALTH_STATUS may temporarily show Missing ; simply re-issue the command to confirm it moves to Healthy . Using the UI to view the newly deployed ArgoCD applications In the previous section of this chapter you logged on to the ArgoCD web console. Switch back to that console, refresh the page and you should see the bootstrap-single-cluster ArgoCD application together with many other ArgoCD applications: (You may need to select List view rather than the Tiles view.) We can see that eleven ArgoCD applications have been deployed to the cluster as a result of applying bootstrap.yaml . In the next section of the tutorial, we'll examine these applications to see how and why they were created, but for now let's focus on one of them -- the namespace-ci ArgoCD application. Examining the namespace-ci ArgoCD application resources Let's examine the Kubernetes resources applied to the cluster by the namespace-ci ArgoCD application. In the ArgoCD application list, click on namespace-ci : (You may need to clear filters to see this screenshot.) The directed graph shows that the namespace-ci ArgoCD app has created 4 Kubernetes resources; our ci namespace and three role bindings. Verify the namespace using the oc CLI We've seen the new namespace definition in the GitOps repository and visually in the ArgoCD UI. Let's also verify it via the command line: Type the following command: oc get namespace ci -o yaml This will list the full details of the ci namespace: apiVersion : v1 kind : Namespace metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"namespace-ci\"},\"name\":\"ci\"},\"spec\":{}} openshift.io/sa.scc.mcs : s0:c27,c9 openshift.io/sa.scc.supplemental-groups : 1000720000/10000 openshift.io/sa.scc.uid-range : 1000720000/10000 creationTimestamp : \"2021-08-31T15:27:32Z\" labels : app.kubernetes.io/instance : namespace-ci managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:labels : . : {} f:app.kubernetes.io/instance : {} f:status : f:phase : {} manager : argocd-application-controller operation : Update time : \"2021-08-31T15:27:32Z\" - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : f:openshift.io/sa.scc.mcs : {} f:openshift.io/sa.scc.supplemental-groups : {} f:openshift.io/sa.scc.uid-range : {} manager : cluster-policy-controller operation : Update time : \"2021-08-31T15:27:32Z\" name : ci resourceVersion : \"2255607\" selfLink : /api/v1/namespaces/ci uid : fff6b82b-6318-4828-83bb-ade4e8e3c0cf spec : finalizers : - kubernetes status : phase : Active Notice how manager: argocd-application-controller identifies that this namespace was created by ArgoCD. It's important to understand the sequence of actions. We simply deployed the bootstrap-single-cluster ArgoCD application, and it ultimately resulted in the creation of the namespace-ci ArgoCD application which created the ci namespace. We don't directly apply resources to the cluster once the bootstrap-single-cluster ArgoCD application has been applied; the cluster state is determined by an ArgoCD application YAML in the corresponding application , service or infrastructure related folders. It's these ArgoCD applications that create and manage the underlying Kubernetes resources using the GitOps repository as the source of truth. Understanding the namespace-ci ArgoCD application Let's examine the ArgoCD application namespace-ci to see how it created the ci namespace and three role bindings in the cluster. Issue the following command to examine its YAML: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-ci.yaml Notice that apiVersion and kind identify this as an ArgoCD application: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ci labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" spec : destination : namespace : ci server : https://kubernetes.default.svc project : infra source : path : namespaces/ci syncPolicy : automated : prune : true selfHeal : true Most importantly, see how this namespace-ci ArgoCD application monitors the folder path: namespaces/ci in https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git ; it then applies the contents of this folder to the cluster whenever its content changes. Notice the SyncPolicy of automated ; any changes to this folder will automatically be applied to the cluster; we do not need to perform a manual Sync operation from the ArgoCD UI. Examine the ci namespace YAML To examine this, navigate to multi-tenancy-gitops-infra repository you cloned previously. cd $HOME /git cd multi-tenancy-gitops-infra Let's examine the ci namespace YAML in the namespaces/ci folder: cat namespaces/ci/namespace.yaml It's a very simple YAML: apiVersion : v1 kind : Namespace metadata : name : ci spec : {} This is a YAML that we would normally apply to the cluster manually or via a script. However, when we use GitOps, we push the ArgoCD application that uses this YAML to GitHub, and it applies the ci namespace YAML to the cluster. This is the essence of GitOps; we declare what we want to appear in the cluster using Git and ArgoCD synchronizes the cluster with this declaration. Examine the ci rolebinding YAML Now that we've seen how the namespace was created, let's see how the three other rolebindings were created by the namespace-ci ArgoCD application. In the same namespace/ci folder as the ci namespace YAML, there is a rolebinding.yaml file. This file will be also applied to the cluster by the namespace-ci ArgoCD application which is continuously watching this folder. Examine this file with the following command: cat namespaces/ci/rolebinding.yaml This YAML is slightly more complex than the namespace YAML: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-dev namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:dev --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-staging namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:staging --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-prod namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:prod --- However its structure is quite straightforward; look carefully and you'll see that there are indeed three rolebindings defined in this YAML. Each of these rolebindings binds different roles to different service accounts associated with components that will run within the ci namespace, such as Tekton pipelines for example. We'll see later how these rolebindings are important; they limit the operations that can be performed by the ci namespace, creating a well governed cluster. This confirms why we saw four resources created by the namespace-ci ArgoCD application in the ArgoCD UI: one namespace and three rolebindings. Again, notice the pattern: a single ArgoCD application manages one or more Kubernetes resources in the cluster -- using one or more YAML files in which those resources are defined. The bootstrap-single-cluster ArgoCD application in more detail In the ArgoCD UI Applications view, click on the bootstrap-single-cluster application: You can see the bootstrap application creates two Kubernetes resources, the infra ArgoCD application and the infra ArgoCD project. An ArgoCD project is a mechanism by which we can group related resources; we keep all our ArgoCD applications that manage infrastructure in the infra project. Later, we'll create a services project for the ArgoCD applications that manage the services we want deployed to the cluster such as the MQ application and queue manager pipelines. The infra ArgoCD application Let's examine the infra ArgoCD application in more detail to see how it works. In the ArgoCD UI Applications view, click on the open application icon for the infra application: We can see that the infra ArgoCD application creates 9 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It's the infra ArgoCD application that watches the 0-bootstrap/single-cluster/1-infra/argocd folder for ArgoCD applications that apply infrastructure resources to our cluster. It was the infra application that created the namespace-ci ArgoCD application which manages the ci namespace that we've been exploring in this section of the tutorial. We'll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it's not necessary, as you'll get lots of practice as we proceed. ArgoCD change management and governance \u00b6 In this final section about managing infrastructure resources, let's explore how ArgoCD provides some advanced resource management features: Dynamic Monitored Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Customize the web console banner Examine the banner in the OpenShift web console: We're going to use GitOps to modify this banner dynamically. The banner YAML This banner properties are defined by the YAML in /0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml . This YAML is currently being used by the cntk-consolenotification ArgoCD application that was deployed earlier. We can examine the YAML with the following command: cat 0 -bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml which shows the banner properties are part of the ConsoleNotification custom resource: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : cntk-consolenotification labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" finalizers : - resources-finalizer.argocd.argoproj.io spec : syncPolicy : automated : prune : true selfHeal : true destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : path : consolenotification helm : values : | ocp-console-notification: ## The name of the ConsoleNotification resource in the cluster name: \"banner-env\" ## The background color that should be used for the banner backgroundColor: teal ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text: \"Cluster Description\" See how the banner at the top of the screen: * contains the text Cluster Description * is located at top of the screen * has the color Teal Modify the YAML for this banner Let's now change this YAML In your editor, modify this YAML and change the below fields as follows: ocp-console-notification : ## The name of the ConsoleNotification resource in the cluster name : \"banner-env\" ## The background color that should be used for the banner backgroundColor : red ## The color of the text that will appear in the banner color : \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location : BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text : \"Production reference guide\" It's clear that our intention is to modify the banner's backgroundColor:red and text: Production reference guide to the newly specified values. If you look at the diff : git diff you should see the following diff --git a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml index 30adf1a..596e821 100644 --- a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml +++ b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml @@ -26,10 +26,10 @@ spec: name: \"banner-env\" ## The background color that should be used for the banner - backgroundColor: teal + backgroundColor: red ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created - text: \"Cluster Description\" + text: \"Production reference guide\" Make the web console YAML change active Let's make these changes visible to the cntk-consolenotification ArgoCD application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Modify console banner\" git push origin $GIT_BRANCH You'll see the changes being pushed to GitHub: Enumerating objects: 13, done. Counting objects: 100% (13/13), done. Delta compression using up to 8 threads Compressing objects: 100% (7/7), done. Writing objects: 100% (7/7), 670 bytes | 670.00 KiB/s, done. Total 7 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a1e8292..b49dff5 master -> master Let's see what effect they have on the web console. A dynamic change to the web console You can either wait for ArgoCD to automatically sync the cntk-consolenotification application or manually Sync it yourself: Returning to the OpenShift web console, you'll notice changes. Notice the dynamic nature of these changes; we updated the console YAML, pushed our changes to our GitOps repository and everything else happened automatically. As a result, our OpenShift console has a new banner color and text. Governing changes to the dev namespace Let's now look at how ArgoCD monitors Kubernetes resources for configuration drift, and what happens if it detects an unexpected change to a monitored resource. Don't worry about the following command; it might seem drastic and even reckless, but as you'll see, everything will be OK. Let's delete the dev namespace from the cluster: oc get namespace dev oc delete namespace dev See how the active namespace: NAME STATUS AGE dev Active 2d18h is deleted: namespace \"dev\" deleted We can see that the dev namespace has been manually deleted from the cluster. GitOps repository as a source of truth If you switch back to the ArgoCD UI, you may see that ArgoCD has detected a configuration drift: a resource is Missing (the dev namespace) namespace-dev therefore is OutOfSync namespace-dev is therefore Syncing with the GitOps repository After a while we'll see that namespace-dev is Healthy and Synced : ArgoCD has detected a configuration drift, and resynched with GitOps repository, re-applying the dev namespace to the cluster. Note You may miss seeing the first screenshot if ArgoCD detects and corrects the missing dev namespace before you get a chance to switch to the ArgoCD UI. Don't worry, you can try this operator again! The restored dev namespace Issue the following command to determine the status of the dev namespace: oc get namespace dev which confirms that the dev namespace has been re-instated: NAME STATUS AGE dev Active 115s Note that it is a different instance of the dev namespace; as indicated by its AGE value. Notice the governed nature of these changes; GitOps is our source of truth about the resources deployed to the cluster. ArgoCD restores any resources that suffer from configuration drift to their GitOps-defined configuration. Congratulations! You've used ArgoCD and the GitOps repository to set up ci , tools and dev namespaces. You've seen how to create ArgoCD applications that watch their respective GitOps namespace folders for details of the namespace resources they should apply to the cluster. You've seen how you can dynamically change deployed resources by updating the resource definition in the GitOps repository. Finally, you've experience how ArgoCD keeps the cluster synchronized with the GitOps repository as a source of truth; any unexpected configuration drift will be corrected without intervention.","title":"Getting started with GitOps"},{"location":"guides/cp4i/ace/cluster-config/gitops-config/#ibm-appconnect-enterprise-gitops-configuration","text":"In the cluster configuration of this chapter, we installed the fundamental components for continuous integration and continuous deployment into our cluster. These included a sample GitOps repository, and ArgoCD. In this section we're going to customize and enable the GitOps repository, so that we can install IBM AppConnect Enterprise(ACE) and all of the components that go along with that. We will examine these components in more detail throughout this section of the tutorial as well. In this topic, we're going to: Explore the sample GitOps repository in a little more detail Customize the GitOps repository for our cluster Connect ArgoCD to the customized GitOps repository Bootstrap the cluster Explore how the ci namespace is created Try out some dynamic changes to the cluster with ArgoCD Explore how ArgoCD manages configuration drift By the end of this topic we'll have a cluster up and running, having used GitOps to do it. We'll fully understand how ArgoCD manages cluster change and configuration drift.","title":"IBM AppConnect Enterprise GitOps Configuration"},{"location":"guides/cp4i/ace/cluster-config/gitops-config/#pre-requisites","text":"Before attempting this section, you must have completed the previous section of this tutorial where you created your Red Hat OpenShift cluster(s) and customized the GitOps repository and ArgoCD. Please see the previous sections of this guide for information on how to do these tasks.","title":"Pre-requisites"},{"location":"guides/cp4i/ace/cluster-config/gitops-config/#the-sample-gitops-repository","text":"Let's understand how the GitOps repository works by using it to install the components we've highlighted in the above diagram. In the first part of this topic, let's look at the high level structure of the GitOps repository. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Explore the high level folder structure Use the following command to display the folder structure: tree . -d -L 2 We can see the different folders in the GitOps repository: . \u251c\u2500\u2500 0-bootstrap \u2502 \u251c\u2500\u2500 others \u2502 \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 doc \u2502 \u251c\u2500\u2500 diagrams \u2502 \u251c\u2500\u2500 experimental \u2502 \u251c\u2500\u2500 images \u2502 \u2514\u2500\u2500 scenarios \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 bom \u2514\u2500\u2500 setup \u2514\u2500\u2500 ocp47 The 0-bootstrap folder is the key folder that contains all the available profiles. Within this folders are Cluster Profiles that control which resources are deployed to the cluster. There are other folders, but these contain utility scripts; and some document related information. Let us examine the 0-bootstrap folder structure: tree ./0-bootstrap/ -d -L 2 We can see the different folders: ./0-bootstrap/ \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u2514\u2500\u2500 3-apps If we consider single-cluster profile, you can see the three layers under that covering infrastructure, services, and applications. Use single-cluster profile to configure the cluster Use the following command to display the folder structure: tree ./0-bootstrap/single-cluster/ -L 3 We can see the different folders: ./0-bootstrap/single-cluster/ \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 1-infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 2-services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3-apps \u2502 \u251c\u2500\u2500 3-apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 cp4a \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Notice that each high level folder has an argocd folder. Within these argocd folders are ArgoCD applications that control which resources are deployed to the cluster. Sample ArgoCD application We'll see later how these ArgoCD applications work in detail. For now, notice how each layer of the architecture ( infrastructure , service , application ) has a set of ArgoCD applications. Type the following command: tree 0 -bootstrap/single-cluster/1-infra/ It shows a list of ArgoCD applications that are used to manage Kubernetes infrastructure resources: 0-bootstrap/single-cluster/1-infra/ \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Notice the namespace- YAMLs; we'll see in a moment how these each define an ArgoCD application dedicated to managing a Kubernetes namespace in our reference architecture. A word on terminology As we get started with ArgoCD, it can be easy to confuse the term ArgoCD application with your application. That's because ArgoCD uses the term application to refer to a Kubernetes custom resource that was initially designed to automate application management. However, an ArgoCD application can automate the deployment of any Kubernetes resource within a cluster, such as namespaces or queue managers, as we'll see a little later. Let's now customize this repository to deploy the highlighted Kubernetes resources to our cluster.","title":"The sample GitOps repository"},{"location":"guides/cp4i/ace/cluster-config/gitops-config/#customizing-the-gitops-repositories","text":"The cloned GitOps configuration repositories (specifically, multi-tenancy-gitops and multi-tenancy-gitops-apps ) needs to be customized for your cluster. The ArgoCD Application and AppProject resources and kustomization.yaml resources contains environment variables which will need to be updated to your specific GitHub organization and branch.","title":"Customizing the GitOps repositories"},{"location":"guides/cp4i/ace/cluster-config/gitops-config/#update-multi-tenancy-gitops-repository","text":"Have a look at the following kustomization.yaml as an example of what will need to be updated. cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS_INFRA} - op: add path: /spec/source/targetRevision value: ${GIT_GITOPS_INFRA_BRANCH} Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Run the customization script Let's customize the cloned multi-tenancy-gitops repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. This script has also made some customizations that we'll use much later in the tutorial; we'll refer to those at the relevant time. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 199 files changed, 290 insertions ( + ) , 290 deletions ( - ) Set your GitOps repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (48/48), 4.70 KiB | 1.57 MiB/s, done. Total 48 (delta 32), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 23 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git d95eca5..a900c39 master -> master","title":"Update multi-tenancy-gitops repository"},{"location":"guides/cp4i/ace/cluster-config/gitops-config/#update-multi-tenancy-gitops-apps-repository","text":"Similar to the previous section, there are corresponding ArgoCD Applications resources that will need to be updated in the multi-tenancy-gitops-apps repository. Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Locate the multi-tenancy-gitops-apps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops-apps Run the customization script Let's customize the cloned multi-tenancy-gitops-apps repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops-apps repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 21 files changed, 42 insertions ( + ) , 42 deletions ( - ) Set your repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops-apps repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (42/42), 4.70 KiB | 1.57 MiB/s, done. Total 42 (delta 42), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 21 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git d95eca5..a900c39 master -> master","title":"Update multi-tenancy-gitops-apps repository"},{"location":"guides/cp4i/ace/cluster-config/gitops-config/#connect-argocd-to-the-gitops-repository","text":"Let's now connect your customized GitOps repository to the instance of ArgoCD running in the cluster. Once connected, ArgoCD will use the contents of the repository to create necessary resources. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Review ArgoCD infrastructure folder Let's examine the 0-bootstrap/single-cluster/1-infra/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml We can see the contents of the kustomization.yaml : resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Notice that the resources that needs to be applied to the cluster are all inactivate and commented out. Let us enable the resources that are needed by un-commenting them. Deploy Kubernetes resources with kustomization.yaml Open 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the below resources: argocd/consolelink.yaml argocd/consolenotification.yaml argocd/namespace-ibm-common-services.yaml argocd/namespace-ci.yaml argocd/namespace-dev.yaml argocd/namespace-staging.yaml argocd/namespace-prod.yaml argocd/namespace-sealed-secrets.yaml argocd/namespace-tools.yaml You will have the following resources un-commented for infrastructure: resources : - argocd/consolelink.yaml - argocd/consolenotification.yaml - argocd/namespace-ibm-common-services.yaml - argocd/namespace-ci.yaml - argocd/namespace-dev.yaml - argocd/namespace-staging.yaml - argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml - argocd/namespace-sealed-secrets.yaml - argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for infrastructure\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a900c39..e3f696d master -> master Examine bootstrap.yaml residing in 0-bootstrap/single-cluster/ . The bootstrap.yaml file is used to create our first ArgoCD application called bootstrap-single-cluster . This initial ArgoCD application will create all the other ArgoCD applications that control the application, service, and infrastructure resources (such as the ci and dev namespaces) deployed to the cluster. Examine the YAML that defines the ArgoCD bootstrap application: cat 0 -bootstrap/single-cluster/bootstrap.yaml Notice also how this ArgoCD application has been customized to use the GitOps repository repoURL: https://github.com/prod-ref-guide/multi-tenancy-gitops.git . apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-single-cluster namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/single-cluster repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Most importantly, see how path: 0-bootstrap/single-cluster refers to the 0-bootstrap/single-cluster folder within this repository. This will result in the creation of individual ArgoCD applications to manage our cluster resources. Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us for now only deploy infra resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and comment out the 2-services/2-services.yaml and 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml # - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Commit and push changes to your git repository: git add . git commit -s -m \"Using only infra\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 456 bytes | 456.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git e3f696d..ea3b43f master -> master Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. The repository contains a bootstrap-single-cluster ArgoCD application that is watching this repository and using its contents to manage the cluster. When the bootstrap-single-cluster ArgoCD application is applied to the cluster, it will continuously ensure that all the activated resources are applied to the cluster. Apply the bootstrap YAML to the cluster: oc apply -f 0 -bootstrap/single-cluster/bootstrap.yaml Kubernetes will confirm that the bootstrap ArgoCD application has been created: application.argoproj.io/bootstrap-single-cluster created The bootstrap ArgoCD application will watch the 0-bootstrap/single-cluster folder in our GitOps repository on GitHub. In this way, as resources are added to the infrastructure , service and application folders, they will be deployed to the cluster automatically. This is therefore the only direct cluster operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Verify the bootstrap deployment Verify that the bootstrap ArgoCD application is running with the following command: oc get app/bootstrap-single-cluster -n openshift-gitops You should see that the bootstrap application was recently updated: NAME SYNC STATUS HEALTH STATUS bootstrap-single-cluster Synced Healthy HEALTH_STATUS may temporarily show Missing ; simply re-issue the command to confirm it moves to Healthy . Using the UI to view the newly deployed ArgoCD applications In the previous section of this chapter you logged on to the ArgoCD web console. Switch back to that console, refresh the page and you should see the bootstrap-single-cluster ArgoCD application together with many other ArgoCD applications: (You may need to select List view rather than the Tiles view.) We can see that eleven ArgoCD applications have been deployed to the cluster as a result of applying bootstrap.yaml . In the next section of the tutorial, we'll examine these applications to see how and why they were created, but for now let's focus on one of them -- the namespace-ci ArgoCD application. Examining the namespace-ci ArgoCD application resources Let's examine the Kubernetes resources applied to the cluster by the namespace-ci ArgoCD application. In the ArgoCD application list, click on namespace-ci : (You may need to clear filters to see this screenshot.) The directed graph shows that the namespace-ci ArgoCD app has created 4 Kubernetes resources; our ci namespace and three role bindings. Verify the namespace using the oc CLI We've seen the new namespace definition in the GitOps repository and visually in the ArgoCD UI. Let's also verify it via the command line: Type the following command: oc get namespace ci -o yaml This will list the full details of the ci namespace: apiVersion : v1 kind : Namespace metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"namespace-ci\"},\"name\":\"ci\"},\"spec\":{}} openshift.io/sa.scc.mcs : s0:c27,c9 openshift.io/sa.scc.supplemental-groups : 1000720000/10000 openshift.io/sa.scc.uid-range : 1000720000/10000 creationTimestamp : \"2021-08-31T15:27:32Z\" labels : app.kubernetes.io/instance : namespace-ci managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:labels : . : {} f:app.kubernetes.io/instance : {} f:status : f:phase : {} manager : argocd-application-controller operation : Update time : \"2021-08-31T15:27:32Z\" - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : f:openshift.io/sa.scc.mcs : {} f:openshift.io/sa.scc.supplemental-groups : {} f:openshift.io/sa.scc.uid-range : {} manager : cluster-policy-controller operation : Update time : \"2021-08-31T15:27:32Z\" name : ci resourceVersion : \"2255607\" selfLink : /api/v1/namespaces/ci uid : fff6b82b-6318-4828-83bb-ade4e8e3c0cf spec : finalizers : - kubernetes status : phase : Active Notice how manager: argocd-application-controller identifies that this namespace was created by ArgoCD. It's important to understand the sequence of actions. We simply deployed the bootstrap-single-cluster ArgoCD application, and it ultimately resulted in the creation of the namespace-ci ArgoCD application which created the ci namespace. We don't directly apply resources to the cluster once the bootstrap-single-cluster ArgoCD application has been applied; the cluster state is determined by an ArgoCD application YAML in the corresponding application , service or infrastructure related folders. It's these ArgoCD applications that create and manage the underlying Kubernetes resources using the GitOps repository as the source of truth. Understanding the namespace-ci ArgoCD application Let's examine the ArgoCD application namespace-ci to see how it created the ci namespace and three role bindings in the cluster. Issue the following command to examine its YAML: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-ci.yaml Notice that apiVersion and kind identify this as an ArgoCD application: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ci labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" spec : destination : namespace : ci server : https://kubernetes.default.svc project : infra source : path : namespaces/ci syncPolicy : automated : prune : true selfHeal : true Most importantly, see how this namespace-ci ArgoCD application monitors the folder path: namespaces/ci in https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git ; it then applies the contents of this folder to the cluster whenever its content changes. Notice the SyncPolicy of automated ; any changes to this folder will automatically be applied to the cluster; we do not need to perform a manual Sync operation from the ArgoCD UI. Examine the ci namespace YAML To examine this, navigate to multi-tenancy-gitops-infra repository you cloned previously. cd $HOME /git cd multi-tenancy-gitops-infra Let's examine the ci namespace YAML in the namespaces/ci folder: cat namespaces/ci/namespace.yaml It's a very simple YAML: apiVersion : v1 kind : Namespace metadata : name : ci spec : {} This is a YAML that we would normally apply to the cluster manually or via a script. However, when we use GitOps, we push the ArgoCD application that uses this YAML to GitHub, and it applies the ci namespace YAML to the cluster. This is the essence of GitOps; we declare what we want to appear in the cluster using Git and ArgoCD synchronizes the cluster with this declaration. Examine the ci rolebinding YAML Now that we've seen how the namespace was created, let's see how the three other rolebindings were created by the namespace-ci ArgoCD application. In the same namespace/ci folder as the ci namespace YAML, there is a rolebinding.yaml file. This file will be also applied to the cluster by the namespace-ci ArgoCD application which is continuously watching this folder. Examine this file with the following command: cat namespaces/ci/rolebinding.yaml This YAML is slightly more complex than the namespace YAML: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-dev namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:dev --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-staging namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:staging --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-prod namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:prod --- However its structure is quite straightforward; look carefully and you'll see that there are indeed three rolebindings defined in this YAML. Each of these rolebindings binds different roles to different service accounts associated with components that will run within the ci namespace, such as Tekton pipelines for example. We'll see later how these rolebindings are important; they limit the operations that can be performed by the ci namespace, creating a well governed cluster. This confirms why we saw four resources created by the namespace-ci ArgoCD application in the ArgoCD UI: one namespace and three rolebindings. Again, notice the pattern: a single ArgoCD application manages one or more Kubernetes resources in the cluster -- using one or more YAML files in which those resources are defined. The bootstrap-single-cluster ArgoCD application in more detail In the ArgoCD UI Applications view, click on the bootstrap-single-cluster application: You can see the bootstrap application creates two Kubernetes resources, the infra ArgoCD application and the infra ArgoCD project. An ArgoCD project is a mechanism by which we can group related resources; we keep all our ArgoCD applications that manage infrastructure in the infra project. Later, we'll create a services project for the ArgoCD applications that manage the services we want deployed to the cluster such as the MQ application and queue manager pipelines. The infra ArgoCD application Let's examine the infra ArgoCD application in more detail to see how it works. In the ArgoCD UI Applications view, click on the open application icon for the infra application: We can see that the infra ArgoCD application creates 9 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It's the infra ArgoCD application that watches the 0-bootstrap/single-cluster/1-infra/argocd folder for ArgoCD applications that apply infrastructure resources to our cluster. It was the infra application that created the namespace-ci ArgoCD application which manages the ci namespace that we've been exploring in this section of the tutorial. We'll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it's not necessary, as you'll get lots of practice as we proceed.","title":"Connect ArgoCD to the GitOps repository"},{"location":"guides/cp4i/ace/cluster-config/gitops-config/#argocd-change-management-and-governance","text":"In this final section about managing infrastructure resources, let's explore how ArgoCD provides some advanced resource management features: Dynamic Monitored Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Customize the web console banner Examine the banner in the OpenShift web console: We're going to use GitOps to modify this banner dynamically. The banner YAML This banner properties are defined by the YAML in /0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml . This YAML is currently being used by the cntk-consolenotification ArgoCD application that was deployed earlier. We can examine the YAML with the following command: cat 0 -bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml which shows the banner properties are part of the ConsoleNotification custom resource: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : cntk-consolenotification labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" finalizers : - resources-finalizer.argocd.argoproj.io spec : syncPolicy : automated : prune : true selfHeal : true destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : path : consolenotification helm : values : | ocp-console-notification: ## The name of the ConsoleNotification resource in the cluster name: \"banner-env\" ## The background color that should be used for the banner backgroundColor: teal ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text: \"Cluster Description\" See how the banner at the top of the screen: * contains the text Cluster Description * is located at top of the screen * has the color Teal Modify the YAML for this banner Let's now change this YAML In your editor, modify this YAML and change the below fields as follows: ocp-console-notification : ## The name of the ConsoleNotification resource in the cluster name : \"banner-env\" ## The background color that should be used for the banner backgroundColor : red ## The color of the text that will appear in the banner color : \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location : BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text : \"Production reference guide\" It's clear that our intention is to modify the banner's backgroundColor:red and text: Production reference guide to the newly specified values. If you look at the diff : git diff you should see the following diff --git a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml index 30adf1a..596e821 100644 --- a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml +++ b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml @@ -26,10 +26,10 @@ spec: name: \"banner-env\" ## The background color that should be used for the banner - backgroundColor: teal + backgroundColor: red ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created - text: \"Cluster Description\" + text: \"Production reference guide\" Make the web console YAML change active Let's make these changes visible to the cntk-consolenotification ArgoCD application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Modify console banner\" git push origin $GIT_BRANCH You'll see the changes being pushed to GitHub: Enumerating objects: 13, done. Counting objects: 100% (13/13), done. Delta compression using up to 8 threads Compressing objects: 100% (7/7), done. Writing objects: 100% (7/7), 670 bytes | 670.00 KiB/s, done. Total 7 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a1e8292..b49dff5 master -> master Let's see what effect they have on the web console. A dynamic change to the web console You can either wait for ArgoCD to automatically sync the cntk-consolenotification application or manually Sync it yourself: Returning to the OpenShift web console, you'll notice changes. Notice the dynamic nature of these changes; we updated the console YAML, pushed our changes to our GitOps repository and everything else happened automatically. As a result, our OpenShift console has a new banner color and text. Governing changes to the dev namespace Let's now look at how ArgoCD monitors Kubernetes resources for configuration drift, and what happens if it detects an unexpected change to a monitored resource. Don't worry about the following command; it might seem drastic and even reckless, but as you'll see, everything will be OK. Let's delete the dev namespace from the cluster: oc get namespace dev oc delete namespace dev See how the active namespace: NAME STATUS AGE dev Active 2d18h is deleted: namespace \"dev\" deleted We can see that the dev namespace has been manually deleted from the cluster. GitOps repository as a source of truth If you switch back to the ArgoCD UI, you may see that ArgoCD has detected a configuration drift: a resource is Missing (the dev namespace) namespace-dev therefore is OutOfSync namespace-dev is therefore Syncing with the GitOps repository After a while we'll see that namespace-dev is Healthy and Synced : ArgoCD has detected a configuration drift, and resynched with GitOps repository, re-applying the dev namespace to the cluster. Note You may miss seeing the first screenshot if ArgoCD detects and corrects the missing dev namespace before you get a chance to switch to the ArgoCD UI. Don't worry, you can try this operator again! The restored dev namespace Issue the following command to determine the status of the dev namespace: oc get namespace dev which confirms that the dev namespace has been re-instated: NAME STATUS AGE dev Active 115s Note that it is a different instance of the dev namespace; as indicated by its AGE value. Notice the governed nature of these changes; GitOps is our source of truth about the resources deployed to the cluster. ArgoCD restores any resources that suffer from configuration drift to their GitOps-defined configuration. Congratulations! You've used ArgoCD and the GitOps repository to set up ci , tools and dev namespaces. You've seen how to create ArgoCD applications that watch their respective GitOps namespace folders for details of the namespace resources they should apply to the cluster. You've seen how you can dynamically change deployed resources by updating the resource definition in the GitOps repository. Finally, you've experience how ArgoCD keeps the cluster synchronized with the GitOps repository as a source of truth; any unexpected configuration drift will be corrected without intervention.","title":"ArgoCD change management and governance"},{"location":"guides/cp4i/ace/cluster-config/gitops-tekton-argocd/","text":"Cloud Native AppConnect Enterprise GitOps \u00b6 Overview \u00b6 Continuous integration and continuous deployment (CICD) are at the core of a typical ACE deployment . They ensure that any changes to source applications and configurations are automatically built and tested before they are deployed, helping to ensure their correctness and the integrity of the cluster. ACE applications and queue managers are defined, configured and changed using a GitOps model. GitOps puts git repositories and git commands such as git push (to request a change) and git merge (to approve a change) at the heart of configuration management. A GitOps approach helps an organization implement best practices in version control and release governance based on a widely used open standard -- git . The following diagram outlines the major components in a GitOps ACE CICD process: Notice the clear separation of concerns: Tekton (OpenShift Pipelines) pipelines use Integration Application (message flows, esql, mapping, etc.) and configurations (serverconf, policy projects) source repositories to build and store successfully tested Kubernetes artifacts in a Git config repository or image registry. While Kubernetes resources (e.g. pods, routes...) can be created as part of the pipeline run to test the source change, these resources only last for the duration of the pipeline run. It is the resultant Git and container image registry resources that are used to affect changes to the cluster in a subsequent, asynchronous processing step controlled by ArgoCD. ArgoCD (OpenShift GitOps) applications watch a Git config repository for changes built as a result of successful pipeline runs. This repository identifies the latest version of the application using information stored in Git and an image repository. OpenShift GitOps applies the Kubernetes resources thus identified to the cluster, resulting in new or updated Kubernetes resources that represent the changed ACE workflow applications and Integration Servers and its ecosystem, such as pods, routes etc. In contrast to pipeline runs, OpenShift GitOps changes are durable; they remain as defined unless and until they are explicitly changed or deleted in the GitOps repository. Moreover, if the cluster resources drift from their Git config values, ArgoCD will restore them to their desired values; only changes that are applied to the Git config repository affect the long term state of the cluster. OpenShift Pipelines and OpenShift GitOps are used to separate Continuous Integration from Continuous Deployment . Often, the Tekton pipeline will perform its changes under a pull-request (PR) to provide an explicit approval mechanism for cluster changes. This is especially important in higher environments such as production which require a formal sign-off. Lower environments such as development often apply successful pipeline runs directly to the Git config repository that are immediately seen by ArgoCD and applied to the cluster. In this section of the tutorial, we're going to set up the GitOps repository, and install ArgoCD. Tekton will be installed in the next topic. Later in the guide, we'll customize our GitOps config repository, and use it to install the many other components in our reference architecture . Most importantly, it will include the ACE Integration Server, which will run the workflow application and configuration repositories and their pipelines that populate the cluster. In this topic, we're going to: Create a Github Organization Download a sample GitOps repository and briefly review it Install ArgoCD Customize ArgoCD Login in to the ArgoCD UI By the end of this topic we'll have a all the basic components in place to perform GitOps in our cluster. Pre-requisites \u00b6 Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed on your local machine the oc command that matches the version of your cluster. Use these instructions to get the latest version of oc . Use oc version to confirm that you have Client Version: 4.6 or higher Server Version: 4.7 or higher You have also installed npm , git , tree and jq commands. Please see the previous sections of this guide for information on how to do these tasks. Video Walkthrough \u00b6 This video demonstrates how to create a Github Organization and set up all the necessary repositories for this tutorial. It also goes through how to install ArgoCD. This is a video walkthrough and it takes you step by step through the below sections. Downloading the sample GitOps repository \u00b6 In this topic section, we're going to create the GitOps repository for our cluster. We'll use a sample GitOps repository as a starting point; we'll make our own copy and review its contents. Later in the tutorial, we'll customize it for our organization and cluster. ( If you'd like to re-familiarize yourself with git, GitHub or managing repositories, you'll find the following short video and short read helpful. ) Create a new GitHub organization Let's start by creating our own GitHub organization. In your browser, navigate to https://github.com/ . In the upper-right corner, click on the profile photo, and then click Settings . In user settings sidebar, click Organizations . In the \"Organizations\" section, click New organization. Create a new organization. Create the GitOps repositories in the new GitHub organization Let's start by making our own copy of the GitOps repository. In your browser, navigate to https://github.com/cloud-native-toolkit/multi-tenancy-gitops : Click on Use this template at the top RHS of the screen and you'll be asked where you'd like to use this repository: Select your organization, name the repository and click Create repository from template . After a few seconds the repository will be present in your chosen account: You now have a copy of the sample GitOps repository in your GitHub account. Create the infrastructure, services and applications GitOps repositories from the templates into the organization your GitHub organization As shown above, follow similar steps and copy the the repository templates into your organization. GitOps Infrastructure Repository GitOps Services Repository GitOps Application Repository Your organization now will have the below repositories in it. Set up environment variable for GitHub Organization This tutorial uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Replace <git-organization> in the following command: export GIT_ORG = <git-organization> You can verify your $GIT_ORG as follows: echo $GIT_ORG Change to your working git folder Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . We've used this default in the following command, but you may have a different location on your local machine. Change to your working git folder: cd $HOME /git You may need to create this folder if it's not already created. Clone the GitOps repository We're going to work on a local clone of the four repositories. Clone the GitOps repositories to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops'... remote: Enumerating objects: 132, done. remote: Counting objects: 100% (132/132), done. remote: Compressing objects: 100% (54/54), done. remote: Total 132 (delta 72), reused 118 (delta 67), pack-reused 0 Receiving objects: 100% (132/132), 310.92 KiB | 3.24 MiB/s, done. Resolving deltas: 100% (72/72), done. Clone the GitOps Infra repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-infra.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-infra'... remote: Enumerating objects: 62, done. remote: Counting objects: 100% (62/62), done. remote: Compressing objects: 100% (44/44), done. remote: Total 62 (delta 7), reused 53 (delta 3), pack-reused 0 Unpacking objects: 100% (62/62), done. Clone the GitOps Services repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-services.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-services'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Clone the GitOps Applications repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-apps'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Info Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the folder containing the local copy of your new repository: cd multi-tenancy-gitops Review the GitOps folder structure Show the folder structure with the following command: tree . -L 1 This will list all the top level folders and files: . \u251c\u2500\u2500 0-bootstrap \u251c\u2500\u2500 README.md \u251c\u2500\u2500 doc \u251c\u2500\u2500 scripts \u2514\u2500\u2500 setup The main folders perform the following roles: 0-bootstrap contains different profiles that can be used to initialize the cluster ready. We can choose one of the available profiles. scripts and setup contain useful scripts that we'll use during this tutorial. Feel free to explore this structure. We'll examine it in much more detail throughout this tutorial, and you'll become very familiar with it by trying it out. Installing ArgoCD for GitOps \u00b6 Now that we've had an initial view of the GitOps repository, let's install ArgoCD to make use of it. We will install ArgoCD using the Red Hat OpenShift GitOps operator. We will also configure it with the appropriate access rights to limit the resources it can create in the cluster. In so doing, we ensure that ArgoCD acts like a well-governed administrator of the cluster, only doing what it needs to do. Also, we will customize our ArgoCD instance by adding necessary custom checks. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". On successful login, you'll see: Logged into \"https://c100-e.jp-tok.containers.cloud.ibm.com:30749\" as \"IAM#hperabat@in.ibm.com\" using the token provided. You have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\" . This shows some basic information about your user within this cluster. (This user is different to your GitHub user.) Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. The following command creates the subscription: oc apply -f setup/ocp47/ The response confirms that the below resources has been created: clusterrole.rbac.authorization.k8s.io/custom-argocd-cluster-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-cntk-argocd-application-controller created subscription.operators.coreos.com/openshift-gitops-operator created Wait for the ArgoCD installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/gitops-operator-controller-manager -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/gitops-operator condition met ArgoCD is now installed and ready to use. If you see something like: Error from server (NotFound): deployments.apps \"gitops-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Alternatively, you can also validate as follows: while ! oc wait crd applications.argoproj.io --timeout = -1s --for = condition = Established 2 >/dev/null ; do sleep 30 ; done while ! oc wait pod --timeout = -1s --for = condition = Ready -l '!job-name' -n openshift-gitops > /dev/null ; do sleep 30 ; done ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are also setup along with the subscription. oc get clusterrole custom-argocd-cluster-argocd-application-controller oc get clusterrolebinding openshift-gitops-argocd-application-controller oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller You'll see the resources are created in the cluster: $ oc get clusterrole custom-argocd-cluster-argocd-application-controller NAME CREATED AT custom-argocd-cluster-argocd-application-controller 2021-08-27T13:35:13Z $ oc get clusterrolebinding openshift-gitops-argocd-application-controller NAME ROLE AGE openshift-gitops-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m43s $ oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller NAME ROLE AGE openshift-gitops-cntk-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m45s The clusterrole custom-argocd-cluster-argocd-application-controller defines a specific set of specific resources that are required by the ArgoCD service account and access rights over them. The cluster role bindings openshift-gitops-argocd-application-controller and openshift-gitops-cntk-argocd-application-controller binds the ArgoCD service account to the cluster role above. In combination, these definitions limit ArgoCD to perform the minimum set of operations required to manage the cluster. This is important; it means that ArgoCD behaves as a well governed administrator of the cluster. We'll also see later in the tutorial how ArgoCD creates resource in the cluster. Delete default ArgoCD instance Let us now delete the default ArgoCD instance that is created earlier. oc delete gitopsservice cluster -n openshift-gitops || true You will see the resources being deleted as follows: warning: deleting cluster-scoped resources, not scoped to the provided namespace gitopsservice.pipelines.openshift.io \"cluster\" deleted We are deleting the existing default instance and planning to make use of a customized ArgoCD instance with additional capabilities. Create a custom ArgoCD instance This customized ArgoCD instance implements custom checks. By default ArgoCD provides some built-in health checks for validating standard kubernetes resources. However, these checks will not be sufficient to validate the custom resources that belong to IBM Cloud Pak for Integration. Argo CD supports custom health checks that are defined in Lua . If you want explore more about the ArgoCD Resource Health, check this link out. Let us create a custom ArgoCD instance as follows: oc apply -f setup/ocp47/argocd-instance/ -n openshift-gitops The response confirms that the below resources has been created: argocd.argoproj.io/openshift-gitops-cntk created Wait till the ArgoCD instance pod is up and running. Issue the below command to verify the same: while ! oc wait pod --timeout = -1s --for = condition = ContainersReady -l app.kubernetes.io/name = openshift-gitops-cntk-server -n openshift-gitops > /dev/null ; do sleep 30 ; done Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops | grep openshift-gitops-cntk-server | awk '{print \"https://\"$2}' This will list the route to the ArgoCD we've just installed, for example: https://openshift-gitops-cntk-server-openshift-gitops.ibmcloud-roks-xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/openshift-gitops-cntk-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Once the UI launches, you'll see: See how there are no ArgoCD applications active at the moment. In the next section of the tutorial, we'll configure ArgoCD to create the ArgoCD applications that will in turn spin up infrastructure , service , and application resources to apply to the cluster. Congratulations! You've created the GitOps repository for your cluster and examined its high level structure. You also installed ArgoCD. You created a specific clusterrole and clusterrolebinding for the ArgoCD service account to ensure that it manages the cluster in a well governed manner. Finally, you launched the UI for ArgoCD; you'll make extensive use of it during this tutorial. In the next topic of this chapter, we're going to customize the GitOps repository for your cluster and use Tekton and ArgoCD to create and manage the Kubernetes resources for our MQ applications and queue managers.","title":"GitOps, Tekton & ArgoCD"},{"location":"guides/cp4i/ace/cluster-config/gitops-tekton-argocd/#cloud-native-appconnect-enterprise-gitops","text":"","title":"Cloud Native AppConnect Enterprise GitOps"},{"location":"guides/cp4i/ace/cluster-config/gitops-tekton-argocd/#overview","text":"Continuous integration and continuous deployment (CICD) are at the core of a typical ACE deployment . They ensure that any changes to source applications and configurations are automatically built and tested before they are deployed, helping to ensure their correctness and the integrity of the cluster. ACE applications and queue managers are defined, configured and changed using a GitOps model. GitOps puts git repositories and git commands such as git push (to request a change) and git merge (to approve a change) at the heart of configuration management. A GitOps approach helps an organization implement best practices in version control and release governance based on a widely used open standard -- git . The following diagram outlines the major components in a GitOps ACE CICD process: Notice the clear separation of concerns: Tekton (OpenShift Pipelines) pipelines use Integration Application (message flows, esql, mapping, etc.) and configurations (serverconf, policy projects) source repositories to build and store successfully tested Kubernetes artifacts in a Git config repository or image registry. While Kubernetes resources (e.g. pods, routes...) can be created as part of the pipeline run to test the source change, these resources only last for the duration of the pipeline run. It is the resultant Git and container image registry resources that are used to affect changes to the cluster in a subsequent, asynchronous processing step controlled by ArgoCD. ArgoCD (OpenShift GitOps) applications watch a Git config repository for changes built as a result of successful pipeline runs. This repository identifies the latest version of the application using information stored in Git and an image repository. OpenShift GitOps applies the Kubernetes resources thus identified to the cluster, resulting in new or updated Kubernetes resources that represent the changed ACE workflow applications and Integration Servers and its ecosystem, such as pods, routes etc. In contrast to pipeline runs, OpenShift GitOps changes are durable; they remain as defined unless and until they are explicitly changed or deleted in the GitOps repository. Moreover, if the cluster resources drift from their Git config values, ArgoCD will restore them to their desired values; only changes that are applied to the Git config repository affect the long term state of the cluster. OpenShift Pipelines and OpenShift GitOps are used to separate Continuous Integration from Continuous Deployment . Often, the Tekton pipeline will perform its changes under a pull-request (PR) to provide an explicit approval mechanism for cluster changes. This is especially important in higher environments such as production which require a formal sign-off. Lower environments such as development often apply successful pipeline runs directly to the Git config repository that are immediately seen by ArgoCD and applied to the cluster. In this section of the tutorial, we're going to set up the GitOps repository, and install ArgoCD. Tekton will be installed in the next topic. Later in the guide, we'll customize our GitOps config repository, and use it to install the many other components in our reference architecture . Most importantly, it will include the ACE Integration Server, which will run the workflow application and configuration repositories and their pipelines that populate the cluster. In this topic, we're going to: Create a Github Organization Download a sample GitOps repository and briefly review it Install ArgoCD Customize ArgoCD Login in to the ArgoCD UI By the end of this topic we'll have a all the basic components in place to perform GitOps in our cluster.","title":"Overview"},{"location":"guides/cp4i/ace/cluster-config/gitops-tekton-argocd/#pre-requisites","text":"Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed on your local machine the oc command that matches the version of your cluster. Use these instructions to get the latest version of oc . Use oc version to confirm that you have Client Version: 4.6 or higher Server Version: 4.7 or higher You have also installed npm , git , tree and jq commands. Please see the previous sections of this guide for information on how to do these tasks.","title":"Pre-requisites"},{"location":"guides/cp4i/ace/cluster-config/gitops-tekton-argocd/#video-walkthrough","text":"This video demonstrates how to create a Github Organization and set up all the necessary repositories for this tutorial. It also goes through how to install ArgoCD. This is a video walkthrough and it takes you step by step through the below sections.","title":"Video Walkthrough"},{"location":"guides/cp4i/ace/cluster-config/gitops-tekton-argocd/#downloading-the-sample-gitops-repository","text":"In this topic section, we're going to create the GitOps repository for our cluster. We'll use a sample GitOps repository as a starting point; we'll make our own copy and review its contents. Later in the tutorial, we'll customize it for our organization and cluster. ( If you'd like to re-familiarize yourself with git, GitHub or managing repositories, you'll find the following short video and short read helpful. ) Create a new GitHub organization Let's start by creating our own GitHub organization. In your browser, navigate to https://github.com/ . In the upper-right corner, click on the profile photo, and then click Settings . In user settings sidebar, click Organizations . In the \"Organizations\" section, click New organization. Create a new organization. Create the GitOps repositories in the new GitHub organization Let's start by making our own copy of the GitOps repository. In your browser, navigate to https://github.com/cloud-native-toolkit/multi-tenancy-gitops : Click on Use this template at the top RHS of the screen and you'll be asked where you'd like to use this repository: Select your organization, name the repository and click Create repository from template . After a few seconds the repository will be present in your chosen account: You now have a copy of the sample GitOps repository in your GitHub account. Create the infrastructure, services and applications GitOps repositories from the templates into the organization your GitHub organization As shown above, follow similar steps and copy the the repository templates into your organization. GitOps Infrastructure Repository GitOps Services Repository GitOps Application Repository Your organization now will have the below repositories in it. Set up environment variable for GitHub Organization This tutorial uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Replace <git-organization> in the following command: export GIT_ORG = <git-organization> You can verify your $GIT_ORG as follows: echo $GIT_ORG Change to your working git folder Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . We've used this default in the following command, but you may have a different location on your local machine. Change to your working git folder: cd $HOME /git You may need to create this folder if it's not already created. Clone the GitOps repository We're going to work on a local clone of the four repositories. Clone the GitOps repositories to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops'... remote: Enumerating objects: 132, done. remote: Counting objects: 100% (132/132), done. remote: Compressing objects: 100% (54/54), done. remote: Total 132 (delta 72), reused 118 (delta 67), pack-reused 0 Receiving objects: 100% (132/132), 310.92 KiB | 3.24 MiB/s, done. Resolving deltas: 100% (72/72), done. Clone the GitOps Infra repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-infra.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-infra'... remote: Enumerating objects: 62, done. remote: Counting objects: 100% (62/62), done. remote: Compressing objects: 100% (44/44), done. remote: Total 62 (delta 7), reused 53 (delta 3), pack-reused 0 Unpacking objects: 100% (62/62), done. Clone the GitOps Services repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-services.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-services'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Clone the GitOps Applications repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-apps'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Info Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the folder containing the local copy of your new repository: cd multi-tenancy-gitops Review the GitOps folder structure Show the folder structure with the following command: tree . -L 1 This will list all the top level folders and files: . \u251c\u2500\u2500 0-bootstrap \u251c\u2500\u2500 README.md \u251c\u2500\u2500 doc \u251c\u2500\u2500 scripts \u2514\u2500\u2500 setup The main folders perform the following roles: 0-bootstrap contains different profiles that can be used to initialize the cluster ready. We can choose one of the available profiles. scripts and setup contain useful scripts that we'll use during this tutorial. Feel free to explore this structure. We'll examine it in much more detail throughout this tutorial, and you'll become very familiar with it by trying it out.","title":"Downloading the sample GitOps repository"},{"location":"guides/cp4i/ace/cluster-config/gitops-tekton-argocd/#installing-argocd-for-gitops","text":"Now that we've had an initial view of the GitOps repository, let's install ArgoCD to make use of it. We will install ArgoCD using the Red Hat OpenShift GitOps operator. We will also configure it with the appropriate access rights to limit the resources it can create in the cluster. In so doing, we ensure that ArgoCD acts like a well-governed administrator of the cluster, only doing what it needs to do. Also, we will customize our ArgoCD instance by adding necessary custom checks. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". On successful login, you'll see: Logged into \"https://c100-e.jp-tok.containers.cloud.ibm.com:30749\" as \"IAM#hperabat@in.ibm.com\" using the token provided. You have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\" . This shows some basic information about your user within this cluster. (This user is different to your GitHub user.) Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. The following command creates the subscription: oc apply -f setup/ocp47/ The response confirms that the below resources has been created: clusterrole.rbac.authorization.k8s.io/custom-argocd-cluster-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-cntk-argocd-application-controller created subscription.operators.coreos.com/openshift-gitops-operator created Wait for the ArgoCD installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/gitops-operator-controller-manager -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/gitops-operator condition met ArgoCD is now installed and ready to use. If you see something like: Error from server (NotFound): deployments.apps \"gitops-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Alternatively, you can also validate as follows: while ! oc wait crd applications.argoproj.io --timeout = -1s --for = condition = Established 2 >/dev/null ; do sleep 30 ; done while ! oc wait pod --timeout = -1s --for = condition = Ready -l '!job-name' -n openshift-gitops > /dev/null ; do sleep 30 ; done ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are also setup along with the subscription. oc get clusterrole custom-argocd-cluster-argocd-application-controller oc get clusterrolebinding openshift-gitops-argocd-application-controller oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller You'll see the resources are created in the cluster: $ oc get clusterrole custom-argocd-cluster-argocd-application-controller NAME CREATED AT custom-argocd-cluster-argocd-application-controller 2021-08-27T13:35:13Z $ oc get clusterrolebinding openshift-gitops-argocd-application-controller NAME ROLE AGE openshift-gitops-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m43s $ oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller NAME ROLE AGE openshift-gitops-cntk-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m45s The clusterrole custom-argocd-cluster-argocd-application-controller defines a specific set of specific resources that are required by the ArgoCD service account and access rights over them. The cluster role bindings openshift-gitops-argocd-application-controller and openshift-gitops-cntk-argocd-application-controller binds the ArgoCD service account to the cluster role above. In combination, these definitions limit ArgoCD to perform the minimum set of operations required to manage the cluster. This is important; it means that ArgoCD behaves as a well governed administrator of the cluster. We'll also see later in the tutorial how ArgoCD creates resource in the cluster. Delete default ArgoCD instance Let us now delete the default ArgoCD instance that is created earlier. oc delete gitopsservice cluster -n openshift-gitops || true You will see the resources being deleted as follows: warning: deleting cluster-scoped resources, not scoped to the provided namespace gitopsservice.pipelines.openshift.io \"cluster\" deleted We are deleting the existing default instance and planning to make use of a customized ArgoCD instance with additional capabilities. Create a custom ArgoCD instance This customized ArgoCD instance implements custom checks. By default ArgoCD provides some built-in health checks for validating standard kubernetes resources. However, these checks will not be sufficient to validate the custom resources that belong to IBM Cloud Pak for Integration. Argo CD supports custom health checks that are defined in Lua . If you want explore more about the ArgoCD Resource Health, check this link out. Let us create a custom ArgoCD instance as follows: oc apply -f setup/ocp47/argocd-instance/ -n openshift-gitops The response confirms that the below resources has been created: argocd.argoproj.io/openshift-gitops-cntk created Wait till the ArgoCD instance pod is up and running. Issue the below command to verify the same: while ! oc wait pod --timeout = -1s --for = condition = ContainersReady -l app.kubernetes.io/name = openshift-gitops-cntk-server -n openshift-gitops > /dev/null ; do sleep 30 ; done Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops | grep openshift-gitops-cntk-server | awk '{print \"https://\"$2}' This will list the route to the ArgoCD we've just installed, for example: https://openshift-gitops-cntk-server-openshift-gitops.ibmcloud-roks-xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/openshift-gitops-cntk-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Once the UI launches, you'll see: See how there are no ArgoCD applications active at the moment. In the next section of the tutorial, we'll configure ArgoCD to create the ArgoCD applications that will in turn spin up infrastructure , service , and application resources to apply to the cluster. Congratulations! You've created the GitOps repository for your cluster and examined its high level structure. You also installed ArgoCD. You created a specific clusterrole and clusterrolebinding for the ArgoCD service account to ensure that it manages the cluster in a well governed manner. Finally, you launched the UI for ArgoCD; you'll make extensive use of it during this tutorial. In the next topic of this chapter, we're going to customize the GitOps repository for your cluster and use Tekton and ArgoCD to create and manage the Kubernetes resources for our MQ applications and queue managers.","title":"Installing ArgoCD for GitOps"},{"location":"guides/cp4i/ace/cluster-config/services/","text":"Installing services with ArgoCD \u00b6 Overview \u00b6 In the previous section of this chapter, we used GitOps to create the ArgoCD applications that installed and managed the ci , tools and dev namespaces in the cluster. In this section we're going to complete the installation of all the necessary services required by our IBM AppConnect Enterprise(ACE) CICD process: We'll examine these highlighted components in more detail throughout this section of the tutorial; here's an overview of their function. Tekton is used for Continuous Integration. Often, the Tekton pipeline will perform its changes under a pull-request (PR) to provide an explicit approval mechanism or through a push based on the requirements for cluster changes. ArgoCD applications will be created for each of these resources. Specifically, ArgoCD applications will keep the cluster synchronized with the application YAML definitions stored in Git. Note how these application services are installed in the ci and tools namespaces we created in the previous topic. This section will reinforce our understanding of GitOps. We will then be ready to create ACE applications that use the infrastructure we previously created and these services we are going to install further. In this topic, we're going to: Deploy services to the cluster using GitOps Explore the ArgoCD applications that manage these services Review how ArgoCD projects work See how infra and services ArgoCD applications manage the infrastructure and services layers in our architecture. By the end of this topic we'll have a fully configured cluster which is ready for us to deploy ACE applications. Pre-requisites \u00b6 Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed on your local machine the oc command that matches the version of your cluster. You have installed npm , git and tree commands. You have completed the tutorial section to customize the GitOps repository, and install ArgoCD. You have completed the tutorial section to create the ci , tools and dev namespaces using GitOps. Please see the previous sections of this guide for information on how to do these tasks. Video Walkthrough \u00b6 This video demonstrates how to install Tekton. It also shows how to use the GitOps repository to set up different service related components. This is a video walkthrough and it takes you step by step through the below sections. Post cluster provisioning tasks \u00b6 Red Hat OpenShift cluster \u00b6 An OpenShift v4.7+ cluster is required. CLI tools \u00b6 Install the OpenShift CLI oc (version 4.7+) . The binary can be downloaded from the Help menu from the OpenShift Console. Download oc cli Log in from a terminal window. oc login --token = <token> --server = <server> IBM Entitlement Key \u00b6 The IBM Entitlement Key is required to pull IBM Cloud Pak specific container images from the IBM Entitled Registry. To get an entitlement key, Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. A Secret containing the entitlement key is created in the tools namespace. oc new-project tools || true oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io Installing Tekton for GitOps \u00b6 Tekton is made available to your Red Hat OpenShift cluster through the Red Hat OpenShift Pipelines operator. Hence, let's see how to get that operator installed on your cluster. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Install Tekton into the cluster We use the Red Hat Pipelines operator to install Tekton into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: - argocd/operators/openshift-pipelines.yaml Your kustomization.yaml for services should match the following: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Install tekton using Red Hat OpenShift Pipelines Operator\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 524 bytes | 524.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 85a4c46..61e15b0 master -> master Activate the services in the GitOps repo Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us only deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see openshift-pipelines ArgoCD application have been created as follows: Wait for the Tekton installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/openshift-pipelines-operator -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/openshift-pipelines-operator condition met Tekton is now installed and ready to use. We\u2019ll explore Tekton in later topics. If you see something like: Error from server (NotFound): deployments.apps \"openshift-pipelines-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Deploy services to the cluster \u00b6 We've just had our first successful GitOps experience, using an ArgoCD application to create the ci , tools and dev namespaces in our cluster. There are few more components to create: IBM ACE, IBM Platform navigator, IBM foundations, IBM automation foundations, IBM Catalog and Sealed secrets. These components are part of the services layer in our architecture, and that requires us to access /0-bootstrap/single-cluster/2-services within our GitOps repository. How to deploy services As we saw earlier, the bootstrap-single-cluster application uses the contents of the /0-bootstrap/single-cluster/2-services folder to determine which Kubernetes resources should be deployed in the cluster. Issue the following command to see what's currently deployed in the cluster: tree 0 -bootstrap/single-cluster/2-services/ -L 3 The resources currently deployed to the cluster map directly to this folder structure: 0 -bootstrap/single-cluster/ \u251c\u2500\u2500 1 -infra \u2502 \u251c\u2500\u2500 1 -infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2 -services \u2502 \u251c\u2500\u2500 2 -services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3 -apps \u2502 \u251c\u2500\u2500 3 -apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml 13 directories, 23 files This structure shows how only infrastructure ArgoCD applications -- and therefore their managed resources such as ci , tools and dev namespaces -- are deployed in the cluster. Review ArgoCD services folder Let\u2019s examine the 0-bootstrap/single-cluster/2-services/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/kustomization.yaml We can see the contents of the kustomization.yaml : resources: # IBM Software #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml # IBM Foundations / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml # IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount #- argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml patches: - target: group: argoproj.io kind: Application labelSelector: \"gitops.tier.layer=services\" patch: | - - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master Add the services layer to the cluster Note The IBM Platform Navigator instance requires a RWX storageclass and it is set to managed-nfs-storage by default in the ArgoCD Application 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-platform-navigator-instance.yaml . This storageclass is available for Red Hat OpenShift on IBM Cloud cluster provisioned from IBM Technology Zone with NFS storage selected. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: - argocd/operators/ibm-ace-operator.yaml - argocd/operators/ibm-platform-navigator.yaml - argocd/instances/ibm-platform-navigator-instance.yaml # IBM Foundations / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml # IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml If you take a diff, and just look for the added services: git diff | grep \"^\\+\" | grep -v \"^\\+++\" You will have the following +- argocd/operators/ibm-ace-operator.yaml +- argocd/operators/ibm-platform-navigator.yaml +- argocd/instances/ibm-platform-navigator-instance.yaml +- argocd/operators/ibm-foundations.yaml +- argocd/instances/ibm-foundational-services-instance.yaml +- argocd/operators/ibm-automation-foundation-core-operator.yaml +- argocd/operators/ibm-catalogs.yaml +- argocd/instances/sealed-secrets.yaml which shows the resources to be deployed for services. Commit and push changes to your git repository: git add . git commit -s -m \"Initial boostrap setup for services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11 , done . Counting objects: 100 % ( 11 /11 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 6 /6 ) , done . Writing objects: 100 % ( 6 /6 ) , 564 bytes | 564 .00 KiB/s, done . Total 6 ( delta 5 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 5 /5 ) , completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git b49dff5..533602c master -> master The intention of this operation is to indicate that we'd like the resources declared in 0-bootstrap/single-cluster/2-services/kustomization.yaml to be deployed in the cluster. Like the infra ArgoCD application, the resources created by the services ArgoCD application will manage the Kubernetes relevant services resources applied to the cluster. Optional: Activate the services in the GitOps repo Run this step if you skip installing tekton in the previous section. If you did not skip that step go to Step 7 . Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources: - 1 -infra/1-infra.yaml - 2 -services/2-services.yaml # - 3-apps/3-apps.yaml patches: - target: group: argoproj.io kind: Application labelSelector: \"gitops.tier.layer=gitops\" patch: | - - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target: group: argoproj.io kind: AppProject labelSelector: \"gitops.tier.layer=infra\" patch: | - - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target: group: argoproj.io kind: AppProject labelSelector: \"gitops.tier.layer=services\" patch: | - - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target: group: argoproj.io kind: AppProject labelSelector: \"gitops.tier.layer=applications\" patch: | - - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9 , done . Counting objects: 100 % ( 9 /9 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 5 /5 ) , done . Writing objects: 100 % ( 5 /5 ) , 431 bytes | 431 .00 KiB/s, done . Total 5 ( delta 4 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 4 /4 ) , completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see lots of new ArgoCD applications have been created to manage the services we have deployed by modifying kustomization.yaml under 0-bootstrap/single-cluster/2-services folder: See how most ArgoCD applications are Synced almost immediately, but some spend time in Progressing . After a few minutes you'll see that all ArgoCD applications become Healthy and Synced : Notice how many more Synched ArgoCD applications are now in the cluster; these are as a result of the newly added services layer in our architecture. The services application Let's examine the ArgoCD application that manage the services in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the services application: We can see that the services ArgoCD application creates several ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It\u2019s the services ArgoCD application that watches the 0-bootstrap/single-cluster/2-services/argocd folder for ArgoCD applications that apply service resources to our cluster. It was the services application that created the artifactory ArgoCD application which manages the Artifactory instance that is used for application configuration management that we will be exploring later in this section of the tutorial. We\u2019ll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it\u2019s not necessary, as you\u2019ll get lots of practice as we proceed. The services ArgoCD project As we've seen in the ArgoCD UI, the services ArgoCD application is responsible for creating the ArgoCD applications that manage the services within the cluster. Let's examine their definitions to see how they do this. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/2-services.yaml The following YAML may initially look a little intimidating; we'll discuss the major elements below: --- apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: services labels: gitops.tier.layer: services spec: sourceRepos: [] # Populated by kustomize patches in 2-services/kustomization.yaml destinations: - namespace: tools server: https://kubernetes.default.svc - namespace: ibm-common-services server: https://kubernetes.default.svc - namespace: redhat-operators server: https://kubernetes.default.svc - namespace: openshift-operators server: https://kubernetes.default.svc - namespace: openshift-marketplace server: https://kubernetes.default.svc - namespace: ci server: https://kubernetes.default.svc - namespace: dev server: https://kubernetes.default.svc - namespace: staging server: https://kubernetes.default.svc - namespace: prod server: https://kubernetes.default.svc - namespace: sealed-secrets server: https://kubernetes.default.svc - namespace: istio-system server: https://kubernetes.default.svc - namespace: openldap server: https://kubernetes.default.svc - namespace: instana-agent server: https://kubernetes.default.svc - namespace: openshift-gitops server: https://kubernetes.default.svc clusterResourceWhitelist: # TODO: SCC needs to be moved to 1-infra, here for now for artifactory - group: \"security.openshift.io\" kind: SecurityContextConstraints - group: \"console.openshift.io\" kind: ConsoleLink - group: \"apps\" kind: statefulsets - group: \"apps\" kind: deployments - group: \"\" kind: services - group: \"\" kind: configmaps - group: \"\" kind: secrets - group: \"\" kind: serviceaccounts - group: \"batch\" kind: jobs - group: \"\" kind: roles - group: \"route.openshift.io\" kind: routes - group: \"\" kind: RoleBinding - group: \"rbac.authorization.k8s.io\" kind: ClusterRoleBinding - group: \"rbac.authorization.k8s.io\" kind: ClusterRole - group: apiextensions.k8s.io kind: CustomResourceDefinition roles: # A role which provides read-only access to all applications in the project - name: read-only description: Read-only privileges to my-project policies: - p, proj:my-project:read-only, applications, get, my-project/*, allow groups: - argocd-admins --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: services annotations: argocd.argoproj.io/sync-wave: \"200\" labels: gitops.tier.layer: gitops spec: destination: namespace: openshift-gitops server: https://kubernetes.default.svc project: services source: # repoURL and targetRevision populated by kustomize patches in 2-services/kustomization.yaml path: 0 -bootstrap/single-cluster/2-services syncPolicy: automated: prune: true selfHeal: true Notice how this YAML defines three ArgoCD resources: a services project which manages all the necessary services that are needed by the applications. Notice how the destinations for the services project are limited to the ci , tools and dev namespaces -- as well as a few others that we'll use in the tutorial. These destinations restrict the namespaces where ArgoCD applications in the services project can manage resources. The same is true for clusterResourceWhiteList . It limits the Kubernetes resources that can be managed to configmaps , deployments and rolebindings amongst others. In summary, we see that the service project is used to group all the ArgoCD applications that will manage the services in our cluster. These ArgoCD applications can only perform specific actions on specific resource types in specific namespaces. See how ArgoCD is acting as a well-governed administrator. The similar structure of services and infra ArgoCD applications Even though we didn't closely examine the infra ArgoCD application YAML in the previous topic, it has has a very similar structure to the ArgoCD services applications we've just examined. Type the following command to list the ArgoCD infra app YAML. cat 0 -bootstrap/single-cluster/1-infra/1-infra.yaml Again, although this YAML might look a little intimidating, the overall structure is the same as for services : --- apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: infra labels: gitops.tier.layer: infra spec: sourceRepos: [] # Populated by kustomize patches in 1-infra/kustomization.yaml destinations: - namespace: ci server: https://kubernetes.default.svc - namespace: dev server: https://kubernetes.default.svc - namespace: staging server: https://kubernetes.default.svc - namespace: prod server: https://kubernetes.default.svc - namespace: sealed-secrets server: https://kubernetes.default.svc - namespace: tools server: https://kubernetes.default.svc - namespace: ibm-common-services server: https://kubernetes.default.svc - namespace: istio-system server: https://kubernetes.default.svc - namespace: openldap server: https://kubernetes.default.svc - namespace: instana-agent server: https://kubernetes.default.svc - namespace: openshift-gitops server: https://kubernetes.default.svc clusterResourceWhitelist: - group: \"\" kind: Namespace - group: \"\" kind: RoleBinding - group: \"security.openshift.io\" kind: SecurityContextConstraints - group: \"console.openshift.io\" kind: ConsoleNotification - group: \"console.openshift.io\" kind: ConsoleLink roles: # A role which provides read-only access to all applications in the project - name: read-only description: Read-only privileges to my-project policies: - p, proj:my-project:read-only, applications, get, my-project/*, allow groups: - argocd-admins --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: infra annotations: argocd.argoproj.io/sync-wave: \"100\" labels: gitops.tier.layer: gitops spec: destination: namespace: openshift-gitops server: https://kubernetes.default.svc project: infra source: # repoURL and targetRevision populated by kustomize patches in 1-infra/kustomization.yaml path: 0 -bootstrap/single-cluster/1-infra syncPolicy: automated: prune: true selfHeal: true As with the 2-services.yaml , we can see An ArgoCD project called infra . ArgoCD applications defined in this project will be limited by the destinations: and clusterResourceWhitelist: specified in the YAML. An ArgoCD app called infra . This is the ArgoCD application that we used in the previous section of the tutorial. It watches the path: 0-bootstrap/single-cluster/1-infra folder for ArgoCD applications that it applied to the cluster. It was these applications that managed the ci , tools and dev namespaces for example. The installation of the operators will take approximately 30 - 45 minutes. Verify the deployment of IBM App Connect operator sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/ibm-appconnect -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/ibm-appconnect condition met If you see something like: Error from server ( NotFound ) : deployments.apps \"ibm-appconnect\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Validate the deployment of IBM Cloud Pak for Integration Platform Navigator operator sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/ibm-integration-platform-navigator-operator -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/ibm-integration-platform-navigator-operator condition met If you see something like: Error from server ( NotFound ) : deployments.apps \"ibm-integration-platform-navigator-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Verify the IBM Platform Navigator has been deployed successfully. oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' Expected output = True The Platform Navigator console has been installed as part of the IBM Cloud Pak for Integration. We use the Platform Navigator to view our App Connect Enterprise dashboard. Access the IBM Platform Navigator console Retrieve the URL for the IBM Platform Navigator console: oc get route integration-navigator-pn -n tools Copy the URL from the HOST/PORT column and paste into a browser, prefixed with https:// . Retrieve the admin password: oc get secret ibm-iam-bindinfo-platform-auth-idp-credentials -n dev -o json -n ibm-common-services | jq -r '.data.\"admin_password\"' | base64 -D Log in to the IBM Platform Navigator console with the admin credentials. Other important ArgoCD features \u00b6 In this final section of this chapter, let's explore ArgoCD features you may have noticed as you explored different YAML files in this chapter: SyncWave \u00b6 Using SyncWave to control deployment sequencing When ArgoCD uses a GitOps repository to apply ArgoCD applications to the cluster, it applies all ArgoCD applications concurrently. Even though Kubernetes use of eventual consistency means that resources which depend on each other can be deployed in any order, it often makes sense to help deploy certain resources in a certain order. This can avoid spurious error messages or wasted compute resources for example. Let's compare two ArgoCD YAML files. Firstly, let's examine the namespace-tools in the infrastructure layer: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-tools.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: tools labels: gitops.tier.layer: infra annotations: argocd.argoproj.io/sync-wave: \"100\" ... Then examine the artifactory in the services layer: cat 0 -bootstrap/single-cluster/2-services/argocd/instances/artifactory.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: artifactory annotations: argocd.argoproj.io/sync-wave: \"250\" labels: gitops.tier.group: cntk gitops.tier.layer: services finalizers: - resources-finalizer.argocd.argoproj.io ... Notice the use of sync-wave: \"100\" for tools and how it contrasts with sync-wave: \"250\" for artifactory . The lower 100 will be deployed before the higher sync-wave 250 because it only makes sense to deploy the pipelines into the tools namespace, after the tools namespace has been created. You are free to choose any number for sync-wave . In our deployment, we have chosen 100-199 for infrastructure, 200-299 for services and 300-399 for applications; it provides alignment with high level folder numbering such as 1-infra and so on. In our tutorial, we're going to incrementally add infrastructure, services and applications so that you can understand how everything fits together incrementally. This makes sync-wave less important than in a real-world system, where you might be making updates to each of these deployed layers simultaneously. In such cases the use of sync-wave provides a high degree of confidence in the effectiveness of a deployment change because all changes go through in the correct order. Congratulations! You've installed tekton and configured the key services in the cluster to support continuous integration and continuous delivery. These have included services like Sonarqube and Artifactory. We installed them into the tools namespaces we created previously. This chapter is now complete. In the following chapters, we're going to use these components to deploy MQ applications and queue managers to the cluster.","title":"Deploying Services"},{"location":"guides/cp4i/ace/cluster-config/services/#installing-services-with-argocd","text":"","title":"Installing services with ArgoCD"},{"location":"guides/cp4i/ace/cluster-config/services/#overview","text":"In the previous section of this chapter, we used GitOps to create the ArgoCD applications that installed and managed the ci , tools and dev namespaces in the cluster. In this section we're going to complete the installation of all the necessary services required by our IBM AppConnect Enterprise(ACE) CICD process: We'll examine these highlighted components in more detail throughout this section of the tutorial; here's an overview of their function. Tekton is used for Continuous Integration. Often, the Tekton pipeline will perform its changes under a pull-request (PR) to provide an explicit approval mechanism or through a push based on the requirements for cluster changes. ArgoCD applications will be created for each of these resources. Specifically, ArgoCD applications will keep the cluster synchronized with the application YAML definitions stored in Git. Note how these application services are installed in the ci and tools namespaces we created in the previous topic. This section will reinforce our understanding of GitOps. We will then be ready to create ACE applications that use the infrastructure we previously created and these services we are going to install further. In this topic, we're going to: Deploy services to the cluster using GitOps Explore the ArgoCD applications that manage these services Review how ArgoCD projects work See how infra and services ArgoCD applications manage the infrastructure and services layers in our architecture. By the end of this topic we'll have a fully configured cluster which is ready for us to deploy ACE applications.","title":"Overview"},{"location":"guides/cp4i/ace/cluster-config/services/#pre-requisites","text":"Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed on your local machine the oc command that matches the version of your cluster. You have installed npm , git and tree commands. You have completed the tutorial section to customize the GitOps repository, and install ArgoCD. You have completed the tutorial section to create the ci , tools and dev namespaces using GitOps. Please see the previous sections of this guide for information on how to do these tasks.","title":"Pre-requisites"},{"location":"guides/cp4i/ace/cluster-config/services/#video-walkthrough","text":"This video demonstrates how to install Tekton. It also shows how to use the GitOps repository to set up different service related components. This is a video walkthrough and it takes you step by step through the below sections.","title":"Video Walkthrough"},{"location":"guides/cp4i/ace/cluster-config/services/#post-cluster-provisioning-tasks","text":"","title":"Post cluster provisioning tasks"},{"location":"guides/cp4i/ace/cluster-config/services/#red-hat-openshift-cluster","text":"An OpenShift v4.7+ cluster is required.","title":"Red Hat OpenShift cluster"},{"location":"guides/cp4i/ace/cluster-config/services/#cli-tools","text":"Install the OpenShift CLI oc (version 4.7+) . The binary can be downloaded from the Help menu from the OpenShift Console. Download oc cli Log in from a terminal window. oc login --token = <token> --server = <server>","title":"CLI tools"},{"location":"guides/cp4i/ace/cluster-config/services/#ibm-entitlement-key","text":"The IBM Entitlement Key is required to pull IBM Cloud Pak specific container images from the IBM Entitled Registry. To get an entitlement key, Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. A Secret containing the entitlement key is created in the tools namespace. oc new-project tools || true oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io","title":"IBM Entitlement Key"},{"location":"guides/cp4i/ace/cluster-config/services/#installing-tekton-for-gitops","text":"Tekton is made available to your Red Hat OpenShift cluster through the Red Hat OpenShift Pipelines operator. Hence, let's see how to get that operator installed on your cluster. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Install Tekton into the cluster We use the Red Hat Pipelines operator to install Tekton into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: - argocd/operators/openshift-pipelines.yaml Your kustomization.yaml for services should match the following: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Install tekton using Red Hat OpenShift Pipelines Operator\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 524 bytes | 524.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 85a4c46..61e15b0 master -> master Activate the services in the GitOps repo Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us only deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see openshift-pipelines ArgoCD application have been created as follows: Wait for the Tekton installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/openshift-pipelines-operator -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/openshift-pipelines-operator condition met Tekton is now installed and ready to use. We\u2019ll explore Tekton in later topics. If you see something like: Error from server (NotFound): deployments.apps \"openshift-pipelines-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected.","title":"Installing Tekton for GitOps"},{"location":"guides/cp4i/ace/cluster-config/services/#deploy-services-to-the-cluster","text":"We've just had our first successful GitOps experience, using an ArgoCD application to create the ci , tools and dev namespaces in our cluster. There are few more components to create: IBM ACE, IBM Platform navigator, IBM foundations, IBM automation foundations, IBM Catalog and Sealed secrets. These components are part of the services layer in our architecture, and that requires us to access /0-bootstrap/single-cluster/2-services within our GitOps repository. How to deploy services As we saw earlier, the bootstrap-single-cluster application uses the contents of the /0-bootstrap/single-cluster/2-services folder to determine which Kubernetes resources should be deployed in the cluster. Issue the following command to see what's currently deployed in the cluster: tree 0 -bootstrap/single-cluster/2-services/ -L 3 The resources currently deployed to the cluster map directly to this folder structure: 0 -bootstrap/single-cluster/ \u251c\u2500\u2500 1 -infra \u2502 \u251c\u2500\u2500 1 -infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2 -services \u2502 \u251c\u2500\u2500 2 -services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3 -apps \u2502 \u251c\u2500\u2500 3 -apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml 13 directories, 23 files This structure shows how only infrastructure ArgoCD applications -- and therefore their managed resources such as ci , tools and dev namespaces -- are deployed in the cluster. Review ArgoCD services folder Let\u2019s examine the 0-bootstrap/single-cluster/2-services/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/kustomization.yaml We can see the contents of the kustomization.yaml : resources: # IBM Software #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml # IBM Foundations / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml # IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount #- argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml patches: - target: group: argoproj.io kind: Application labelSelector: \"gitops.tier.layer=services\" patch: | - - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master Add the services layer to the cluster Note The IBM Platform Navigator instance requires a RWX storageclass and it is set to managed-nfs-storage by default in the ArgoCD Application 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-platform-navigator-instance.yaml . This storageclass is available for Red Hat OpenShift on IBM Cloud cluster provisioned from IBM Technology Zone with NFS storage selected. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: - argocd/operators/ibm-ace-operator.yaml - argocd/operators/ibm-platform-navigator.yaml - argocd/instances/ibm-platform-navigator-instance.yaml # IBM Foundations / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml # IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml If you take a diff, and just look for the added services: git diff | grep \"^\\+\" | grep -v \"^\\+++\" You will have the following +- argocd/operators/ibm-ace-operator.yaml +- argocd/operators/ibm-platform-navigator.yaml +- argocd/instances/ibm-platform-navigator-instance.yaml +- argocd/operators/ibm-foundations.yaml +- argocd/instances/ibm-foundational-services-instance.yaml +- argocd/operators/ibm-automation-foundation-core-operator.yaml +- argocd/operators/ibm-catalogs.yaml +- argocd/instances/sealed-secrets.yaml which shows the resources to be deployed for services. Commit and push changes to your git repository: git add . git commit -s -m \"Initial boostrap setup for services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11 , done . Counting objects: 100 % ( 11 /11 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 6 /6 ) , done . Writing objects: 100 % ( 6 /6 ) , 564 bytes | 564 .00 KiB/s, done . Total 6 ( delta 5 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 5 /5 ) , completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git b49dff5..533602c master -> master The intention of this operation is to indicate that we'd like the resources declared in 0-bootstrap/single-cluster/2-services/kustomization.yaml to be deployed in the cluster. Like the infra ArgoCD application, the resources created by the services ArgoCD application will manage the Kubernetes relevant services resources applied to the cluster. Optional: Activate the services in the GitOps repo Run this step if you skip installing tekton in the previous section. If you did not skip that step go to Step 7 . Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources: - 1 -infra/1-infra.yaml - 2 -services/2-services.yaml # - 3-apps/3-apps.yaml patches: - target: group: argoproj.io kind: Application labelSelector: \"gitops.tier.layer=gitops\" patch: | - - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target: group: argoproj.io kind: AppProject labelSelector: \"gitops.tier.layer=infra\" patch: | - - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target: group: argoproj.io kind: AppProject labelSelector: \"gitops.tier.layer=services\" patch: | - - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target: group: argoproj.io kind: AppProject labelSelector: \"gitops.tier.layer=applications\" patch: | - - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9 , done . Counting objects: 100 % ( 9 /9 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 5 /5 ) , done . Writing objects: 100 % ( 5 /5 ) , 431 bytes | 431 .00 KiB/s, done . Total 5 ( delta 4 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 4 /4 ) , completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see lots of new ArgoCD applications have been created to manage the services we have deployed by modifying kustomization.yaml under 0-bootstrap/single-cluster/2-services folder: See how most ArgoCD applications are Synced almost immediately, but some spend time in Progressing . After a few minutes you'll see that all ArgoCD applications become Healthy and Synced : Notice how many more Synched ArgoCD applications are now in the cluster; these are as a result of the newly added services layer in our architecture. The services application Let's examine the ArgoCD application that manage the services in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the services application: We can see that the services ArgoCD application creates several ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It\u2019s the services ArgoCD application that watches the 0-bootstrap/single-cluster/2-services/argocd folder for ArgoCD applications that apply service resources to our cluster. It was the services application that created the artifactory ArgoCD application which manages the Artifactory instance that is used for application configuration management that we will be exploring later in this section of the tutorial. We\u2019ll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it\u2019s not necessary, as you\u2019ll get lots of practice as we proceed. The services ArgoCD project As we've seen in the ArgoCD UI, the services ArgoCD application is responsible for creating the ArgoCD applications that manage the services within the cluster. Let's examine their definitions to see how they do this. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/2-services.yaml The following YAML may initially look a little intimidating; we'll discuss the major elements below: --- apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: services labels: gitops.tier.layer: services spec: sourceRepos: [] # Populated by kustomize patches in 2-services/kustomization.yaml destinations: - namespace: tools server: https://kubernetes.default.svc - namespace: ibm-common-services server: https://kubernetes.default.svc - namespace: redhat-operators server: https://kubernetes.default.svc - namespace: openshift-operators server: https://kubernetes.default.svc - namespace: openshift-marketplace server: https://kubernetes.default.svc - namespace: ci server: https://kubernetes.default.svc - namespace: dev server: https://kubernetes.default.svc - namespace: staging server: https://kubernetes.default.svc - namespace: prod server: https://kubernetes.default.svc - namespace: sealed-secrets server: https://kubernetes.default.svc - namespace: istio-system server: https://kubernetes.default.svc - namespace: openldap server: https://kubernetes.default.svc - namespace: instana-agent server: https://kubernetes.default.svc - namespace: openshift-gitops server: https://kubernetes.default.svc clusterResourceWhitelist: # TODO: SCC needs to be moved to 1-infra, here for now for artifactory - group: \"security.openshift.io\" kind: SecurityContextConstraints - group: \"console.openshift.io\" kind: ConsoleLink - group: \"apps\" kind: statefulsets - group: \"apps\" kind: deployments - group: \"\" kind: services - group: \"\" kind: configmaps - group: \"\" kind: secrets - group: \"\" kind: serviceaccounts - group: \"batch\" kind: jobs - group: \"\" kind: roles - group: \"route.openshift.io\" kind: routes - group: \"\" kind: RoleBinding - group: \"rbac.authorization.k8s.io\" kind: ClusterRoleBinding - group: \"rbac.authorization.k8s.io\" kind: ClusterRole - group: apiextensions.k8s.io kind: CustomResourceDefinition roles: # A role which provides read-only access to all applications in the project - name: read-only description: Read-only privileges to my-project policies: - p, proj:my-project:read-only, applications, get, my-project/*, allow groups: - argocd-admins --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: services annotations: argocd.argoproj.io/sync-wave: \"200\" labels: gitops.tier.layer: gitops spec: destination: namespace: openshift-gitops server: https://kubernetes.default.svc project: services source: # repoURL and targetRevision populated by kustomize patches in 2-services/kustomization.yaml path: 0 -bootstrap/single-cluster/2-services syncPolicy: automated: prune: true selfHeal: true Notice how this YAML defines three ArgoCD resources: a services project which manages all the necessary services that are needed by the applications. Notice how the destinations for the services project are limited to the ci , tools and dev namespaces -- as well as a few others that we'll use in the tutorial. These destinations restrict the namespaces where ArgoCD applications in the services project can manage resources. The same is true for clusterResourceWhiteList . It limits the Kubernetes resources that can be managed to configmaps , deployments and rolebindings amongst others. In summary, we see that the service project is used to group all the ArgoCD applications that will manage the services in our cluster. These ArgoCD applications can only perform specific actions on specific resource types in specific namespaces. See how ArgoCD is acting as a well-governed administrator. The similar structure of services and infra ArgoCD applications Even though we didn't closely examine the infra ArgoCD application YAML in the previous topic, it has has a very similar structure to the ArgoCD services applications we've just examined. Type the following command to list the ArgoCD infra app YAML. cat 0 -bootstrap/single-cluster/1-infra/1-infra.yaml Again, although this YAML might look a little intimidating, the overall structure is the same as for services : --- apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: infra labels: gitops.tier.layer: infra spec: sourceRepos: [] # Populated by kustomize patches in 1-infra/kustomization.yaml destinations: - namespace: ci server: https://kubernetes.default.svc - namespace: dev server: https://kubernetes.default.svc - namespace: staging server: https://kubernetes.default.svc - namespace: prod server: https://kubernetes.default.svc - namespace: sealed-secrets server: https://kubernetes.default.svc - namespace: tools server: https://kubernetes.default.svc - namespace: ibm-common-services server: https://kubernetes.default.svc - namespace: istio-system server: https://kubernetes.default.svc - namespace: openldap server: https://kubernetes.default.svc - namespace: instana-agent server: https://kubernetes.default.svc - namespace: openshift-gitops server: https://kubernetes.default.svc clusterResourceWhitelist: - group: \"\" kind: Namespace - group: \"\" kind: RoleBinding - group: \"security.openshift.io\" kind: SecurityContextConstraints - group: \"console.openshift.io\" kind: ConsoleNotification - group: \"console.openshift.io\" kind: ConsoleLink roles: # A role which provides read-only access to all applications in the project - name: read-only description: Read-only privileges to my-project policies: - p, proj:my-project:read-only, applications, get, my-project/*, allow groups: - argocd-admins --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: infra annotations: argocd.argoproj.io/sync-wave: \"100\" labels: gitops.tier.layer: gitops spec: destination: namespace: openshift-gitops server: https://kubernetes.default.svc project: infra source: # repoURL and targetRevision populated by kustomize patches in 1-infra/kustomization.yaml path: 0 -bootstrap/single-cluster/1-infra syncPolicy: automated: prune: true selfHeal: true As with the 2-services.yaml , we can see An ArgoCD project called infra . ArgoCD applications defined in this project will be limited by the destinations: and clusterResourceWhitelist: specified in the YAML. An ArgoCD app called infra . This is the ArgoCD application that we used in the previous section of the tutorial. It watches the path: 0-bootstrap/single-cluster/1-infra folder for ArgoCD applications that it applied to the cluster. It was these applications that managed the ci , tools and dev namespaces for example. The installation of the operators will take approximately 30 - 45 minutes. Verify the deployment of IBM App Connect operator sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/ibm-appconnect -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/ibm-appconnect condition met If you see something like: Error from server ( NotFound ) : deployments.apps \"ibm-appconnect\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Validate the deployment of IBM Cloud Pak for Integration Platform Navigator operator sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/ibm-integration-platform-navigator-operator -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/ibm-integration-platform-navigator-operator condition met If you see something like: Error from server ( NotFound ) : deployments.apps \"ibm-integration-platform-navigator-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Verify the IBM Platform Navigator has been deployed successfully. oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' Expected output = True The Platform Navigator console has been installed as part of the IBM Cloud Pak for Integration. We use the Platform Navigator to view our App Connect Enterprise dashboard. Access the IBM Platform Navigator console Retrieve the URL for the IBM Platform Navigator console: oc get route integration-navigator-pn -n tools Copy the URL from the HOST/PORT column and paste into a browser, prefixed with https:// . Retrieve the admin password: oc get secret ibm-iam-bindinfo-platform-auth-idp-credentials -n dev -o json -n ibm-common-services | jq -r '.data.\"admin_password\"' | base64 -D Log in to the IBM Platform Navigator console with the admin credentials.","title":"Deploy services to the cluster"},{"location":"guides/cp4i/ace/cluster-config/services/#other-important-argocd-features","text":"In this final section of this chapter, let's explore ArgoCD features you may have noticed as you explored different YAML files in this chapter:","title":"Other important ArgoCD features"},{"location":"guides/cp4i/ace/cluster-config/services/#syncwave","text":"Using SyncWave to control deployment sequencing When ArgoCD uses a GitOps repository to apply ArgoCD applications to the cluster, it applies all ArgoCD applications concurrently. Even though Kubernetes use of eventual consistency means that resources which depend on each other can be deployed in any order, it often makes sense to help deploy certain resources in a certain order. This can avoid spurious error messages or wasted compute resources for example. Let's compare two ArgoCD YAML files. Firstly, let's examine the namespace-tools in the infrastructure layer: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-tools.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: tools labels: gitops.tier.layer: infra annotations: argocd.argoproj.io/sync-wave: \"100\" ... Then examine the artifactory in the services layer: cat 0 -bootstrap/single-cluster/2-services/argocd/instances/artifactory.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: artifactory annotations: argocd.argoproj.io/sync-wave: \"250\" labels: gitops.tier.group: cntk gitops.tier.layer: services finalizers: - resources-finalizer.argocd.argoproj.io ... Notice the use of sync-wave: \"100\" for tools and how it contrasts with sync-wave: \"250\" for artifactory . The lower 100 will be deployed before the higher sync-wave 250 because it only makes sense to deploy the pipelines into the tools namespace, after the tools namespace has been created. You are free to choose any number for sync-wave . In our deployment, we have chosen 100-199 for infrastructure, 200-299 for services and 300-399 for applications; it provides alignment with high level folder numbering such as 1-infra and so on. In our tutorial, we're going to incrementally add infrastructure, services and applications so that you can understand how everything fits together incrementally. This makes sync-wave less important than in a real-world system, where you might be making updates to each of these deployed layers simultaneously. In such cases the use of sync-wave provides a high degree of confidence in the effectiveness of a deployment change because all changes go through in the correct order. Congratulations! You've installed tekton and configured the key services in the cluster to support continuous integration and continuous delivery. These have included services like Sonarqube and Artifactory. We installed them into the tools namespaces we created previously. This chapter is now complete. In the following chapters, we're going to use these components to deploy MQ applications and queue managers to the cluster.","title":"SyncWave"},{"location":"guides/cp4i/ace/cluster-create/aws-setup/","text":"Overview \u00b6 This sprint does not support AWS clusters Later sprints will support clusters provisioned on AWS","title":"Creating a cluster on AWS"},{"location":"guides/cp4i/ace/cluster-create/aws-setup/#overview","text":"This sprint does not support AWS clusters Later sprints will support clusters provisioned on AWS","title":"Overview"},{"location":"guides/cp4i/ace/cluster-create/azure-setup/","text":"Overview \u00b6 This sprint does not support Azure clusters Later sprints will support clusters on different platform","title":"Creating a cluster on Azure"},{"location":"guides/cp4i/ace/cluster-create/azure-setup/#overview","text":"This sprint does not support Azure clusters Later sprints will support clusters on different platform","title":"Overview"},{"location":"guides/cp4i/ace/cluster-create/ibm-setup/","text":"Creating a cluster on IBM Cloud \u00b6 Overview \u00b6 IBM Technology Zone is the one-stop shop to get access to technical environments and software for demos, prototyping, and deployment. OpenShift on IBM Cloud \u00b6 Navigate to the IBM Technology Zone In your browser, navigate to the IBM Technology Zone to create your cluster. You will need to sign-in with your IBM credentials if your browser has not authenticated. You will need to read and accept Terms and conditions to proceed to the IBM Technology Zone. Reserve a cluster You can reserve a cluster immediately on IBM Cloud. You'll see the following options: Click the Reserve now radio button. It takes between 30 and 60 minutes to provision a cluster. Moreover, once a cluster has been provisioned, by default, it will be deleted after 3 days -- unless you renew it. It can therefore be helpful to schedule the environment to be created later if you are not going to be using it for a while -- such as for a future demonstration or classroom session. Come back and explore the Schedule for later option another time. Complete reservation details - step 1 You will be asked to complete your reservations details in two steps. Firstly, when you make a cluster reservation, you can supply a set of details that describe: The cluster name The purpose of the reservation from a pre-defined set A description, which will appear for a Practice/Self-Education reservation Notes to help you identify this reservation In the following web page: Complete the details as follows: Type a helpful Name such as My cluster 1 Select Practice/Self-Education from the Purpose drop-down list Type a helpful Purpose Description such as Education Do not select Preferred Geography yet; we will do that in a moment Add some helpful Notes such as Learn how to build a cloud native production reference deployment using a CloudPak and OpenShift on IBM Cloud Complete reservation details - step 2 The second step in registration is to specify the physical details of the compute infrastructure for your cluster. When you make a cluster reservation, you can supply a set of details that describe: The cluster's geographical location The version of OpenShift Container Platform used by the cluster The number of CPUs in the cluster and their memory size The number of Kubernetes worker nodes in the cluster The NFS storage size for the cluster The End date and time for the cluster In the following web page: Complete the details as follows: Select your Preferred Geography such as London 5 , HongKong 2 or San Jose 04 Under End date and time , select a date and specify the time along with its time zone. Select Worker Node Count as 3 from the drop-down list Select Worker Node Flavor as 8CPU x 32GB from the drop-down list Select NFS Size as 500 GB from the drop-down list Select OpenShift Version as 4.8 from the drop-down-list Info By default, your cluster will be reserved for three days , after which time it will be deleted. This is more than enough time to complete this tutorial. If required, you can increase the cluster lifetime by configuring the End date and time . Don't do this unless you really need to do so; as you'll learn in the tutorial, the use of GitOps makes it quick and easy to provision a cluster when you need to do so. Create the cluster Check your reservation details and press Submit when you're ready to create the cluster: The cluster provisioning process will take between 30 and 60 minutes to complete on IBM Cloud. Provisioning email As the provisioning process starts, you will receive an email to confirm that the provisioning process has started: Checking the cluster provisioning status You can also view the status of your provisioning request on IBM Technology Zone. Click on My reservations to see: Note: The Status: field will be Ready when the cluster has been provisioned. The creation and expiry date-time for your cluster. You can return to the reservation at any time to get the details of your cluster. Cluster provisioned email You will receive another email once your cluster is ready: Your cluster is now provisioned and ready to use. Note the expiry date-time. Navigate to your cluster You can use the information in the notification email to access your cluster. Click on the Cluster URL link to login to IBM Cloud and locate your OpenShift cluster: Note: The Cluster ID that has been assigned by IBM Cloud. The Zone matches your requested geography. The OCP Version matches your requested version of OCP. The Node status identifies the 3 worker nodes in the Normal state. Feel free to explore this web page. Receiving cluster deletion email and extending your cluster's lifetime When your cluster is within 3 days of deletion, you will receive a daily email: Note: You can extend your cluster if required. This option may be limited to certain types of reservations. The My reservations page allows you to manage your cluster reservation","title":"IBM Cloud"},{"location":"guides/cp4i/ace/cluster-create/ibm-setup/#creating-a-cluster-on-ibm-cloud","text":"","title":"Creating a cluster on IBM Cloud"},{"location":"guides/cp4i/ace/cluster-create/ibm-setup/#overview","text":"IBM Technology Zone is the one-stop shop to get access to technical environments and software for demos, prototyping, and deployment.","title":"Overview"},{"location":"guides/cp4i/ace/cluster-create/ibm-setup/#openshift-on-ibm-cloud","text":"Navigate to the IBM Technology Zone In your browser, navigate to the IBM Technology Zone to create your cluster. You will need to sign-in with your IBM credentials if your browser has not authenticated. You will need to read and accept Terms and conditions to proceed to the IBM Technology Zone. Reserve a cluster You can reserve a cluster immediately on IBM Cloud. You'll see the following options: Click the Reserve now radio button. It takes between 30 and 60 minutes to provision a cluster. Moreover, once a cluster has been provisioned, by default, it will be deleted after 3 days -- unless you renew it. It can therefore be helpful to schedule the environment to be created later if you are not going to be using it for a while -- such as for a future demonstration or classroom session. Come back and explore the Schedule for later option another time. Complete reservation details - step 1 You will be asked to complete your reservations details in two steps. Firstly, when you make a cluster reservation, you can supply a set of details that describe: The cluster name The purpose of the reservation from a pre-defined set A description, which will appear for a Practice/Self-Education reservation Notes to help you identify this reservation In the following web page: Complete the details as follows: Type a helpful Name such as My cluster 1 Select Practice/Self-Education from the Purpose drop-down list Type a helpful Purpose Description such as Education Do not select Preferred Geography yet; we will do that in a moment Add some helpful Notes such as Learn how to build a cloud native production reference deployment using a CloudPak and OpenShift on IBM Cloud Complete reservation details - step 2 The second step in registration is to specify the physical details of the compute infrastructure for your cluster. When you make a cluster reservation, you can supply a set of details that describe: The cluster's geographical location The version of OpenShift Container Platform used by the cluster The number of CPUs in the cluster and their memory size The number of Kubernetes worker nodes in the cluster The NFS storage size for the cluster The End date and time for the cluster In the following web page: Complete the details as follows: Select your Preferred Geography such as London 5 , HongKong 2 or San Jose 04 Under End date and time , select a date and specify the time along with its time zone. Select Worker Node Count as 3 from the drop-down list Select Worker Node Flavor as 8CPU x 32GB from the drop-down list Select NFS Size as 500 GB from the drop-down list Select OpenShift Version as 4.8 from the drop-down-list Info By default, your cluster will be reserved for three days , after which time it will be deleted. This is more than enough time to complete this tutorial. If required, you can increase the cluster lifetime by configuring the End date and time . Don't do this unless you really need to do so; as you'll learn in the tutorial, the use of GitOps makes it quick and easy to provision a cluster when you need to do so. Create the cluster Check your reservation details and press Submit when you're ready to create the cluster: The cluster provisioning process will take between 30 and 60 minutes to complete on IBM Cloud. Provisioning email As the provisioning process starts, you will receive an email to confirm that the provisioning process has started: Checking the cluster provisioning status You can also view the status of your provisioning request on IBM Technology Zone. Click on My reservations to see: Note: The Status: field will be Ready when the cluster has been provisioned. The creation and expiry date-time for your cluster. You can return to the reservation at any time to get the details of your cluster. Cluster provisioned email You will receive another email once your cluster is ready: Your cluster is now provisioned and ready to use. Note the expiry date-time. Navigate to your cluster You can use the information in the notification email to access your cluster. Click on the Cluster URL link to login to IBM Cloud and locate your OpenShift cluster: Note: The Cluster ID that has been assigned by IBM Cloud. The Zone matches your requested geography. The OCP Version matches your requested version of OCP. The Node status identifies the 3 worker nodes in the Normal state. Feel free to explore this web page. Receiving cluster deletion email and extending your cluster's lifetime When your cluster is within 3 days of deletion, you will receive a daily email: Note: You can extend your cluster if required. This option may be limited to certain types of reservations. The My reservations page allows you to manage your cluster reservation","title":"OpenShift on IBM Cloud"},{"location":"guides/cp4i/ace/cluster-create/vmware-setup/","text":"Overview \u00b6 This sprint does not support VMWare clusters Later sprints will allow clusters on different platform","title":"Creating a cluster on VMWare"},{"location":"guides/cp4i/ace/cluster-create/vmware-setup/#overview","text":"This sprint does not support VMWare clusters Later sprints will allow clusters on different platform","title":"Overview"},{"location":"guides/cp4i/ace/deploy-backend-service/deploy-backend-service/","text":"Deploy Backend Service \u00b6 Overview \u00b6 In the section Cloud Native Example we explained that our sample ACE integration application will integrate with a SOAP-based backend service. In this section, we will deploy this backend service. Note Scope of this tutorial does not include the SOAP service, but rather the ACE integration application. But for the sake of completeness of this tutorial, we will deploy this SOAP backend service within our cluster so that we can demonstrate our sample ACE integration application. In this section, we will deploy pre-built image of the SOAP service. In a later section, we will show how to use OpenShift Build to deploy from the source. Deploy Backend SOAP Service \u00b6 Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Deploy the applications layer in the GitOps repo Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy apps resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 3-apps/3-apps.yaml as follows: resources: - 1 -infra/1-infra.yaml - 2 -services/2-services.yaml - 3 -apps/3-apps.yaml ... # omitted for brevity if you take a diff: git diff --unified = 0 diff --git a/0-bootstrap/single-cluster/kustomization.yaml b/0-bootstrap/single-cluster/kustomization.yaml index 3d7f7d8..4bac6d7 100644 --- a/0-bootstrap/single-cluster/kustomization.yaml +++ b/0-bootstrap/single-cluster/kustomization.yaml @@ -4 +4 @@ resources: -# - 3-apps/3-apps.yaml +- 3-apps/3-apps.yaml Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the changes will be applied to the cluster.: git add . git commit -s -m \"Intial boostrap setup for applications\" git push origin $GIT_BRANCH The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the changes: Note the the commit message in sync status. It should be of commit from the last step. The bootstrap-single-cluster application deployed the \"applications\" ArgoCD Application and AppProject Note the \"applications\" ArgoCD application icon which should appears new: We can examine the this \"applications\" ArgoCD application, either from Applications > applications top menu, or simply clicking the link sub-icon within the applications icon: Select resources to deploy At this time, the applications ArgoCD application has not yet deploy any other applications in turn. The reason is that it is kustomize based application definition, where all resources are commented out at this time. Access the 0-bootstrap/single-cluster/3-apps/kustomization.yaml : cat 0 -bootstrap/single-cluster/3-apps/kustomization.yaml and you should notice all resources are commented out. Open 0-bootstrap/single-cluster/3-apps/kustomization.yaml and uncomment the following: - argocd/soapserver/soapserver.yaml if you take a diff now: git diff --unified = 0 you should see the following: diff --git a/0-bootstrap/single-cluster/3-apps/kustomization.yaml b/0-bootstrap/single-cluster/3-apps/kustomization.yaml index 7a3bce4..c695ae5 100644 --- a/0-bootstrap/single-cluster/3-apps/kustomization.yaml +++ b/0-bootstrap/single-cluster/3-apps/kustomization.yaml @@ -22 +22 @@ resources: -#- argocd/soapserver/soapserver.yaml +- argocd/soapserver/soapserver.yaml Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying apps\" git push origin $GIT_BRANCH The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the changes: Note the the commit message in sync status. It should be of commit from the last step. The applications application Switch to \"applications\" ArgoCD application on the ArgoCD UI (either by clicking on the link sub-icon from the \"applications\" resource icon, or from the top menu Applications -> applications ) The \"applications\" ArgoCD application will be updated to have apps-soapserver-tools application: apps-soapserver-tools application* Open the apps-soapserver-tools application (either by clicking on the link sub-icon from the \"apps-soapserver-tools\" resource icon, or from the top menu Applications > apps-soapserver-tools ) The apps-soapserver-tools application, in turn deploys prod-app-soapserver application: Open the application (either by clicking on the link sub-icon from the \"applications\" resource icon, or from the top menu Applications > prod-app-soapserver ) Notice the resources deployed by the application: Deployment, soapserver-nonsecure Service, soapserver-nonsecure Route, soapserver-nonsecure Click on the resource icons to see details of their deployments. For instance, the Deployment, soapserver-nonsecure , provides a summary page showing the status and health: events: and logs: Verify deployment of backend SOAP service \u00b6 Now that the service has been deployed, you can see the resource instances in your OCP cluster. From the Route object under tools namespace, you can see the exposed route, soapserver-nonsecure , and you can find the host name of the SOAP service. The host name can be used externally outside the cluster to send SOAP request. Following video shows how you can use OCP and SOAP UI to verify the service deployment:","title":"Deploy Backend Service"},{"location":"guides/cp4i/ace/deploy-backend-service/deploy-backend-service/#deploy-backend-service","text":"","title":"Deploy Backend Service"},{"location":"guides/cp4i/ace/deploy-backend-service/deploy-backend-service/#overview","text":"In the section Cloud Native Example we explained that our sample ACE integration application will integrate with a SOAP-based backend service. In this section, we will deploy this backend service. Note Scope of this tutorial does not include the SOAP service, but rather the ACE integration application. But for the sake of completeness of this tutorial, we will deploy this SOAP backend service within our cluster so that we can demonstrate our sample ACE integration application. In this section, we will deploy pre-built image of the SOAP service. In a later section, we will show how to use OpenShift Build to deploy from the source.","title":"Overview"},{"location":"guides/cp4i/ace/deploy-backend-service/deploy-backend-service/#deploy-backend-soap-service","text":"Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Deploy the applications layer in the GitOps repo Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy apps resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 3-apps/3-apps.yaml as follows: resources: - 1 -infra/1-infra.yaml - 2 -services/2-services.yaml - 3 -apps/3-apps.yaml ... # omitted for brevity if you take a diff: git diff --unified = 0 diff --git a/0-bootstrap/single-cluster/kustomization.yaml b/0-bootstrap/single-cluster/kustomization.yaml index 3d7f7d8..4bac6d7 100644 --- a/0-bootstrap/single-cluster/kustomization.yaml +++ b/0-bootstrap/single-cluster/kustomization.yaml @@ -4 +4 @@ resources: -# - 3-apps/3-apps.yaml +- 3-apps/3-apps.yaml Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the changes will be applied to the cluster.: git add . git commit -s -m \"Intial boostrap setup for applications\" git push origin $GIT_BRANCH The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the changes: Note the the commit message in sync status. It should be of commit from the last step. The bootstrap-single-cluster application deployed the \"applications\" ArgoCD Application and AppProject Note the \"applications\" ArgoCD application icon which should appears new: We can examine the this \"applications\" ArgoCD application, either from Applications > applications top menu, or simply clicking the link sub-icon within the applications icon: Select resources to deploy At this time, the applications ArgoCD application has not yet deploy any other applications in turn. The reason is that it is kustomize based application definition, where all resources are commented out at this time. Access the 0-bootstrap/single-cluster/3-apps/kustomization.yaml : cat 0 -bootstrap/single-cluster/3-apps/kustomization.yaml and you should notice all resources are commented out. Open 0-bootstrap/single-cluster/3-apps/kustomization.yaml and uncomment the following: - argocd/soapserver/soapserver.yaml if you take a diff now: git diff --unified = 0 you should see the following: diff --git a/0-bootstrap/single-cluster/3-apps/kustomization.yaml b/0-bootstrap/single-cluster/3-apps/kustomization.yaml index 7a3bce4..c695ae5 100644 --- a/0-bootstrap/single-cluster/3-apps/kustomization.yaml +++ b/0-bootstrap/single-cluster/3-apps/kustomization.yaml @@ -22 +22 @@ resources: -#- argocd/soapserver/soapserver.yaml +- argocd/soapserver/soapserver.yaml Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying apps\" git push origin $GIT_BRANCH The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the changes: Note the the commit message in sync status. It should be of commit from the last step. The applications application Switch to \"applications\" ArgoCD application on the ArgoCD UI (either by clicking on the link sub-icon from the \"applications\" resource icon, or from the top menu Applications -> applications ) The \"applications\" ArgoCD application will be updated to have apps-soapserver-tools application: apps-soapserver-tools application* Open the apps-soapserver-tools application (either by clicking on the link sub-icon from the \"apps-soapserver-tools\" resource icon, or from the top menu Applications > apps-soapserver-tools ) The apps-soapserver-tools application, in turn deploys prod-app-soapserver application: Open the application (either by clicking on the link sub-icon from the \"applications\" resource icon, or from the top menu Applications > prod-app-soapserver ) Notice the resources deployed by the application: Deployment, soapserver-nonsecure Service, soapserver-nonsecure Route, soapserver-nonsecure Click on the resource icons to see details of their deployments. For instance, the Deployment, soapserver-nonsecure , provides a summary page showing the status and health: events: and logs:","title":"Deploy Backend SOAP Service"},{"location":"guides/cp4i/ace/deploy-backend-service/deploy-backend-service/#verify-deployment-of-backend-soap-service","text":"Now that the service has been deployed, you can see the resource instances in your OCP cluster. From the Route object under tools namespace, you can see the exposed route, soapserver-nonsecure , and you can find the host name of the SOAP service. The host name can be used externally outside the cluster to send SOAP request. Following video shows how you can use OCP and SOAP UI to verify the service deployment:","title":"Verify deployment of backend SOAP service"},{"location":"guides/cp4i/ace/disaster-recovery/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/ace/disaster-recovery/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/","text":"Promoting between Environments \u00b6 Basic Promotion Process \u00b6 Audience : Architects, Application developers The sample pipeline flow that we built will demonstrate the set of practices we designed to effectively deliver the sample ACE code across environments. In this topic, we're going to discuss: Deployment environments Promoting deliverables across environments Deployment environments \u00b6 For the basic promotion process, in our CI/CD process, typical environments include development, staging and production. Also, we are using a dedicated environment to run our CI/CD pipelines. These environments are defined as distinct projects within a cluster. Dev, Stage and Prod environments \u00b6 Dev environment is the primary environment and it is used for sandboxing. During the development process, developers use this environment to test their code. This will allow them to thoroughly test the sample ACE application code, its configuration code as well as the infrastructure code. Stage environment is sort of pre prod environment. All the sample ACE application deliverables that are ready to be pushed to the prod environment should be thoroughly tested here. Prod environment will host all live services that belong to the ACE workloads. These deliverables will be consumed by the end user. CI environment \u00b6 CI environment is used run all the ace pipelines. This includes all the tasks (custom / reusable task from cloud native toolkit ), pipelines, necessary triggers etc. Promoting deliverables across environments \u00b6 For our sample usecase, end to end basic promotion process is as follows. For this sample usecase, we are maintaining three source code repositories for maintaining ACE configuration, infrastructure and a sample application that helps us to demonstrate the work flow. Webhooks are configured for all these three repositories hooking them up with corresponding pipelines. If there are any updates to the source code, you can push changes to the respective repositories. When the changes are pushed, underlying webhooks will trigger corresponding pipelines. When we make changes to the ace-config repository, it will trigger ace-config-pipeline . Similarly, when we make changes to ace-infra repository or ace-sample-application , it will trigger ace-bar-pipeline . Once these pipelines are run successfully, the updated resources are pushed to the dev destination in the gitops repository. Subsequently, a dedicated ArgoCD application watches for the updates, which it deploys to the cluster in the DEV environment. Once the deployment is successful, the ArgoCD post-sync trigger will fire the ace-functional-test-pipeline . If ace-functional-test-pipeline runs successfully, it will make changes to the stage resources and leave a Pull Request in the gitops repository. Then the Pull Request will be reviewed manually and the changes to the stage resources are merged into the gitops repository. Subsequently, a dedicated ArgoCD application watches for the updates, which it deploys to the cluster in the STAGE environment. Once the deployment is successful, the ArgoCD post-sync trigger will fire the ace-performance-test-pipeline . If ace-performance-test-pipeline runs successfully, it will make changes to the prod resources and leave a Pull Request in the gitops repository. Then the Pull Request will be reviewed manually and the changes to the prod resources are merged into the gitops repository. Subsequently, a dedicated ArgoCD application watches for the updates, which it deploys to the cluster in the PROD environment. Development Environment \u00b6 Audience : Architects, Application developers In this topic, we're going to discuss: Development environment Triggers in dev environment Functional test Pipeline Development environment \u00b6 When the changes are made to the source repositories, respective pipelines will run and build the necessary deliverables. These deliverables will be deployed to the development environment and will be subjected to first phase of testing allowing to validate the deployment. This deployment in development environment is done by ArgoCD. ArgoCD\u2019s role is to perform continuous deployment, ensuring that the cluster is always kept up-to-date with the latest built application. This is the first environment where the deliverable is deployed. If this deployment is a success, then this will be the first step that ensures that this deliverable can be promoted to production at some point of time. Once the deliverable is successfully deployed, we can validate different properties of the system which may functional or non-functional. Not only, we can also validate the configurations of the deliverable. In our sample promotion process, we showed an example of functional tests that are part of functional test pipeline. Depending on the usecases this pipeline can be further extended or modified. If everything works fine, we can promote the deliverable to next environment and deploy it there. Triggers in dev environment \u00b6 In our sample dev environment, like mentioned earlier, we set up argo resource hooks. Resource hooks will help us to run a job before, during and after a ArgoCD sync operation is done. In our workflow, we used a PostSync hook. This PostSync hook is responsible for running the functional test pipeline when a new deliverable is deployed in the dev environment. This PostSync hook is a simple Kubernetes job annotated with argocd.argoproj.io/hook . Our PostSync hook definition is as follows. apiVersion: batch/v1 kind: Job metadata: name: trigger-functional-test annotations: argocd.argoproj.io/hook: PostSync spec: template: spec: serviceAccountName: pipeline containers: - name: functional-test image: quay.io/rhcanada/tkn-cli command: - tkn args: - -n - ci - pipeline - start - ace-functional-test - --param - is-source-repo-url=git@github.ibm.com:cpat-int-samples/createCustomer_REST_V1.git - --param - is-source-repo-private-key=id_create-customer-rest-v1 - --param - source-env=dev - --param - destination-env=staging - --workspace - name=shared-workspace,claimName=ace-test-pvc - --serviceaccount - pipeline restartPolicy: Never backoffLimit: 0 Functional Test Pipeline \u00b6 clone-is-source \u00b6 Clones the source repositories. resolve-props \u00b6 Initially, it will grab all the source properties values. Then, it also resolves the necessary ocp properties. functional-test \u00b6 Waits for the deliverables to be successfully deployed and get active in the development environment. Grabs the route of the deliverable. Performs functional testing using newman test scripts. gitops-for-ace \u00b6 Clones the gitops repository. Copies the deliverables from dev to staging. Update the kustomization.yaml with latest resources. Pushes the changes to gitops repository. Functional Tests \u00b6 Audience : Architects, Application developers In this topic, we're going to discuss: Postman Test scripts Newman Advantages Postman \u00b6 Postman is a tool that is used for API automation. This tools helps us to automate many tests like unit tests, integration tests, end to end tests etc. Based on the test scripts we define, this validates the respective APIs by making sure that the responses received are valid. It also allows us to save a bunch of API requests as a collection. Grouping the requests together will allow easier access. These collections can be exported as JSON files and stored. Test scripts \u00b6 For our sample, the Postman collection is exported as a test.json and you can have a look at it here . In this test script, we defined couple of automated APIs as follows. Create customer No Service Record Bad Phone Number Newman \u00b6 Newman is the command-line collection runner. It allows us to run the test scripts from the command line. This one can be easily used in the continuous integration and build systems. Like mentioned earlier, the collection that is exported as json files from Postman can be used to run the tests using Newman. We are using the below command as part of our functional testing task. newman run \\ --ssl-extra-ca-certs /client-certs/$(params.ca-cert) \\ --ssl-client-key /client-certs/$(params.client-key) \\ --ssl-client-cert /client-certs/$(params.client-cert) \\ --env-var username=$BASIC_AUTH_USERNAME \\ --env-var password=$BASIC_AUTH_PASSWORD \\ --env-var host=$route \\ $test_file We are passing the necessary client certificates, authentication information, host information along with the test json file. Advantages \u00b6 Some advantages are: Ensures all the APIs are working properly. Detects if any bugs exists. Helps us to improve the application and make it better. Test Environment \u00b6 Audience : Architects, Application developers In this topic, we're going to discuss: Staging environment Triggers in staging environment Performance test pipeline Staging environment \u00b6 When the Pull Request with the changes in staging resources get merged into the GitOps repository, the deliverables for staging environment will be updated. These deliverables will be deployed to the stage environment and will be subjected to second phase of testing allowing to validate the deployment. This deployment in staging environment is done by ArgoCD. ArgoCD\u2019s role is to perform continuous deployment, ensuring that the cluster is always kept up-to-date with the latest built application. This is the next environment after dev where the deliverable is deployed and is exact replica of production environment. If this deployment is a success, then this will ensure that this deliverable can be promoted to production environment. Once the deliverable is successfully deployed, we can perform different tests like User Acceptance Tests (UAT), load/stress testing, chaos engineering tests etc. In our sample promotion process, we showed an example of load tests that are part of performance test pipeline. Depending on the usecases this pipeline can be further extended or modified. If everything works fine, we can promote the deliverable to next environment and deploy it there. Triggers in staging environment \u00b6 In our sample stage environment, like mentioned earlier, we set up argo resource hooks. Resource hooks will help us to run a job before, during and after a ArgoCD sync operation is done. In our workflow, we used a PostSync hook. This PostSync hook is responsible for running the performance test pipeline when a new deliverable is deployed in the stage environment. This PostSync hook is a simple Kubernetes job annotated with argocd.argoproj.io/hook . Our PostSync hook definition is as follows. apiVersion: batch/v1 kind: Job metadata: name: trigger-performance-test namespace: ci annotations: argocd.argoproj.io/hook: PostSync spec: template: spec: serviceAccountName: pipeline containers: - name: performance-test image: quay.io/rhcanada/tkn-cli command: - tkn args: - -n - ci - pipeline - start - ace-performance-test - --param - is-source-repo-url=git@github.ibm.com:cpat-int-samples/createCustomer_REST_V1.git - --param - is-source-repo-private-key=id_create-customer-rest-v1 - --param - source-env=staging - --param - destination-env=prod - --param - jmeter-users=500 - --workspace - name=shared-workspace,claimName=ace-test-pvc - --serviceaccount - pipeline restartPolicy: Never backoffLimit: 0 Performance Test Pipeline \u00b6 clone-is-source \u00b6 Clones the source repositories. resolve-props \u00b6 Initially, it will grab all the source properties values. Then, it also resolves the necessary ocp properties. performance-test \u00b6 Performs load testing using jmeter test scripts. gitops-for-ace \u00b6 Clones the gitops repository. Copies the deliverables from staging to prod. Update the kustomization.yaml with latest resources. Pushes the changes to gitops repository. Performance Tests \u00b6 Audience : Architects, Application developers In this topic, we're going to discuss: Introduction Apache Jmeter Datastore Grafana Validating our sample application Advantages Note The contents in this guide are only examples. We demonstrated some of the practices using some sample usecases. For real world scenarios, it should be used in conjunction with existing performance practices and this content is not a replacement. For assistance, CSMs should reach out to performance SMEs. Introduction \u00b6 In our sample performance test task, we are using Jmeter as our performance load testing tool. The live data from Jmeter will be fed into a datastore. Based on this data, the metrics can be visualized using Grafana dashboard. Apache Jmeter \u00b6 Jmeter is an open source java based tool. It helps us to measure the performance of the application. We can basically analyze the load functional behavior of the application. Jmeter simulates the load, for instance it simulates a group of users and keep sending requests to the target server. We can define a test plan script. All the necessary configurations will be provided here. Once we run the Jmeter tests, results will be available. Sample test plan \u00b6 Our sample app test plan can be accessed here . If you have a look at the test plan, you will see different configuration elements. For example, you can see __P(jmeter_users,3) in one of the ThreadGroup elements. The default number of users we are providing here is 3, but number of users is parameterized. We will be passing this information as part of pipeline. In our sample load tests, we are creating a user load of about 500 and testing things out. Running jmeter tests \u00b6 Below is the jmeter command we used as part of our tests. jmeter.sh \\ -Djavax.net.ssl.keyStoreType=jks \\ -Djavax.net.ssl.keyStore=/client-certs/ibm-client.jks \\ -Djavax.net.ssl.keyStorePassword=${KEYSTORE_PASSWORD} \\ -n -t $test-plan \\ -Jjmeter_users=$jmeter_users \\ -l /tmp/report.jtl Here we are providing the keystore information and test plan script along with the user load information to the Jmeter. Once the command is executed successfully, it loads the results into /tmp/report.jtl file. Datastore \u00b6 Jmeter writes the real time test results into a datastore which will be further used for analysis, metric generation etc. In order to integrate Jmeter with datastore, we use a Backend Listener . Currently, in our sample work flow, we are using InfluxDB as our datastore. InfluxDB is a time series database. It handles time-stamped data. The backend listener configuration for the same is as follows. <BackendListener guiclass=\"BackendListenerGui\" testclass=\"BackendListener\" testname=\"Backend Listener\" enabled=\"true\"> <elementProp name=\"arguments\" elementType=\"Arguments\" guiclass=\"ArgumentsPanel\" testclass=\"Arguments\" enabled=\"true\"> <collectionProp name=\"Arguments.arguments\"> <elementProp name=\"influxdbMetricsSender\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">influxdbMetricsSender</stringProp> <stringProp name=\"Argument.value\">org.apache.jmeter.visualizers.backend.influxdb.HttpMetricsSender</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"influxdbUrl\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">influxdbUrl</stringProp> <stringProp name=\"Argument.value\">http://influxdb-service.tools.svc.cluster.local:8086/write?db=gatling</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"application\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">application</stringProp> <stringProp name=\"Argument.value\">application name</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"measurement\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">measurement</stringProp> <stringProp name=\"Argument.value\">jmeter</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"summaryOnly\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">summaryOnly</stringProp> <stringProp name=\"Argument.value\">true</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"samplersRegex\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">samplersRegex</stringProp> <stringProp name=\"Argument.value\">.*</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"percentiles\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">percentiles</stringProp> <stringProp name=\"Argument.value\">90;95;99</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"testTitle\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">testTitle</stringProp> <stringProp name=\"Argument.value\">Test name</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"eventTags\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">eventTags</stringProp> <stringProp name=\"Argument.value\"></stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> </collectionProp> </elementProp> <stringProp name=\"classname\">org.apache.jmeter.visualizers.backend.influxdb.InfluxdbBackendListenerClient</stringProp> </BackendListener> Note: We are working on replacing the InfluxDB with Prometheus. Once we have the implementation, we will document the steps here. Grafana \u00b6 We are using Grafana for visualization and metric analysis. Grafana will read the information from the datastore and display it on the dashboard. Validating our sample application \u00b6 If we run the performance test pipeline, what basically happens is Jmeter writes the real time test results into the datastore using a backend listener and Grafana will read the data from it and then in turn display it on the dashboard. In our sample workflow, we are using a user load of about 500 with a replica of 4 integration servers as follows. When the performance test pipeline is run, Jmeter tests will run. It validates the user load and passes successfully as shown if the servers can efficiently handle the load generated. Once the jmeter tests complete, the data will be loaded into the datastore and eventually displayed in Grafana as follows. Similarly, now we are using a user load of about 500 and reducing the integration server replica to 2 . When the performance test pipeline is run, Jmeter tests will run. It validates the user load and fails as follows if the servers can't efficiently handle the load generated. Once the jmeter tests complete, the data will be loaded into the datastore and eventually displayed in Grafana as follows. Advantages \u00b6 Some of the advantages are: Allows us to identify performance bottlenecks. Identify bugs. System downtime can be reduced. Production Environment \u00b6 Audience : Architects, Application developers In this topic, we're going to discuss: Production environment Production environment \u00b6 When the Pull Request with the changes in prod resources get merged into the GitOps repository, the deliverables for prod environment will be updated. These deliverables will be deployed to the prod environment. This deployment in production environment is done by ArgoCD. ArgoCD\u2019s role is to perform continuous deployment, ensuring that the cluster is always kept up-to-date with the latest built application. This is the next environment after stage where the deliverable is finally deployed. Once the deliverable is successfully deployed, it will be made available to the end user.","title":"Environment Promotion"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#promoting-between-environments","text":"","title":"Promoting between Environments"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#basic-promotion-process","text":"Audience : Architects, Application developers The sample pipeline flow that we built will demonstrate the set of practices we designed to effectively deliver the sample ACE code across environments. In this topic, we're going to discuss: Deployment environments Promoting deliverables across environments","title":"Basic Promotion Process"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#deployment-environments","text":"For the basic promotion process, in our CI/CD process, typical environments include development, staging and production. Also, we are using a dedicated environment to run our CI/CD pipelines. These environments are defined as distinct projects within a cluster.","title":"Deployment environments"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#promoting-deliverables-across-environments","text":"For our sample usecase, end to end basic promotion process is as follows. For this sample usecase, we are maintaining three source code repositories for maintaining ACE configuration, infrastructure and a sample application that helps us to demonstrate the work flow. Webhooks are configured for all these three repositories hooking them up with corresponding pipelines. If there are any updates to the source code, you can push changes to the respective repositories. When the changes are pushed, underlying webhooks will trigger corresponding pipelines. When we make changes to the ace-config repository, it will trigger ace-config-pipeline . Similarly, when we make changes to ace-infra repository or ace-sample-application , it will trigger ace-bar-pipeline . Once these pipelines are run successfully, the updated resources are pushed to the dev destination in the gitops repository. Subsequently, a dedicated ArgoCD application watches for the updates, which it deploys to the cluster in the DEV environment. Once the deployment is successful, the ArgoCD post-sync trigger will fire the ace-functional-test-pipeline . If ace-functional-test-pipeline runs successfully, it will make changes to the stage resources and leave a Pull Request in the gitops repository. Then the Pull Request will be reviewed manually and the changes to the stage resources are merged into the gitops repository. Subsequently, a dedicated ArgoCD application watches for the updates, which it deploys to the cluster in the STAGE environment. Once the deployment is successful, the ArgoCD post-sync trigger will fire the ace-performance-test-pipeline . If ace-performance-test-pipeline runs successfully, it will make changes to the prod resources and leave a Pull Request in the gitops repository. Then the Pull Request will be reviewed manually and the changes to the prod resources are merged into the gitops repository. Subsequently, a dedicated ArgoCD application watches for the updates, which it deploys to the cluster in the PROD environment.","title":"Promoting deliverables across environments"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#development-environment","text":"Audience : Architects, Application developers In this topic, we're going to discuss: Development environment Triggers in dev environment Functional test Pipeline","title":"Development Environment"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#development-environment_1","text":"When the changes are made to the source repositories, respective pipelines will run and build the necessary deliverables. These deliverables will be deployed to the development environment and will be subjected to first phase of testing allowing to validate the deployment. This deployment in development environment is done by ArgoCD. ArgoCD\u2019s role is to perform continuous deployment, ensuring that the cluster is always kept up-to-date with the latest built application. This is the first environment where the deliverable is deployed. If this deployment is a success, then this will be the first step that ensures that this deliverable can be promoted to production at some point of time. Once the deliverable is successfully deployed, we can validate different properties of the system which may functional or non-functional. Not only, we can also validate the configurations of the deliverable. In our sample promotion process, we showed an example of functional tests that are part of functional test pipeline. Depending on the usecases this pipeline can be further extended or modified. If everything works fine, we can promote the deliverable to next environment and deploy it there.","title":"Development environment"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#triggers-in-dev-environment","text":"In our sample dev environment, like mentioned earlier, we set up argo resource hooks. Resource hooks will help us to run a job before, during and after a ArgoCD sync operation is done. In our workflow, we used a PostSync hook. This PostSync hook is responsible for running the functional test pipeline when a new deliverable is deployed in the dev environment. This PostSync hook is a simple Kubernetes job annotated with argocd.argoproj.io/hook . Our PostSync hook definition is as follows. apiVersion: batch/v1 kind: Job metadata: name: trigger-functional-test annotations: argocd.argoproj.io/hook: PostSync spec: template: spec: serviceAccountName: pipeline containers: - name: functional-test image: quay.io/rhcanada/tkn-cli command: - tkn args: - -n - ci - pipeline - start - ace-functional-test - --param - is-source-repo-url=git@github.ibm.com:cpat-int-samples/createCustomer_REST_V1.git - --param - is-source-repo-private-key=id_create-customer-rest-v1 - --param - source-env=dev - --param - destination-env=staging - --workspace - name=shared-workspace,claimName=ace-test-pvc - --serviceaccount - pipeline restartPolicy: Never backoffLimit: 0","title":"Triggers in dev environment"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#functional-test-pipeline","text":"","title":"Functional Test Pipeline"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#functional-tests","text":"Audience : Architects, Application developers In this topic, we're going to discuss: Postman Test scripts Newman Advantages","title":"Functional Tests"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#postman","text":"Postman is a tool that is used for API automation. This tools helps us to automate many tests like unit tests, integration tests, end to end tests etc. Based on the test scripts we define, this validates the respective APIs by making sure that the responses received are valid. It also allows us to save a bunch of API requests as a collection. Grouping the requests together will allow easier access. These collections can be exported as JSON files and stored.","title":"Postman"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#test-scripts","text":"For our sample, the Postman collection is exported as a test.json and you can have a look at it here . In this test script, we defined couple of automated APIs as follows. Create customer No Service Record Bad Phone Number","title":"Test scripts"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#newman","text":"Newman is the command-line collection runner. It allows us to run the test scripts from the command line. This one can be easily used in the continuous integration and build systems. Like mentioned earlier, the collection that is exported as json files from Postman can be used to run the tests using Newman. We are using the below command as part of our functional testing task. newman run \\ --ssl-extra-ca-certs /client-certs/$(params.ca-cert) \\ --ssl-client-key /client-certs/$(params.client-key) \\ --ssl-client-cert /client-certs/$(params.client-cert) \\ --env-var username=$BASIC_AUTH_USERNAME \\ --env-var password=$BASIC_AUTH_PASSWORD \\ --env-var host=$route \\ $test_file We are passing the necessary client certificates, authentication information, host information along with the test json file.","title":"Newman"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#advantages","text":"Some advantages are: Ensures all the APIs are working properly. Detects if any bugs exists. Helps us to improve the application and make it better.","title":"Advantages"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#test-environment","text":"Audience : Architects, Application developers In this topic, we're going to discuss: Staging environment Triggers in staging environment Performance test pipeline","title":"Test Environment"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#staging-environment","text":"When the Pull Request with the changes in staging resources get merged into the GitOps repository, the deliverables for staging environment will be updated. These deliverables will be deployed to the stage environment and will be subjected to second phase of testing allowing to validate the deployment. This deployment in staging environment is done by ArgoCD. ArgoCD\u2019s role is to perform continuous deployment, ensuring that the cluster is always kept up-to-date with the latest built application. This is the next environment after dev where the deliverable is deployed and is exact replica of production environment. If this deployment is a success, then this will ensure that this deliverable can be promoted to production environment. Once the deliverable is successfully deployed, we can perform different tests like User Acceptance Tests (UAT), load/stress testing, chaos engineering tests etc. In our sample promotion process, we showed an example of load tests that are part of performance test pipeline. Depending on the usecases this pipeline can be further extended or modified. If everything works fine, we can promote the deliverable to next environment and deploy it there.","title":"Staging environment"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#triggers-in-staging-environment","text":"In our sample stage environment, like mentioned earlier, we set up argo resource hooks. Resource hooks will help us to run a job before, during and after a ArgoCD sync operation is done. In our workflow, we used a PostSync hook. This PostSync hook is responsible for running the performance test pipeline when a new deliverable is deployed in the stage environment. This PostSync hook is a simple Kubernetes job annotated with argocd.argoproj.io/hook . Our PostSync hook definition is as follows. apiVersion: batch/v1 kind: Job metadata: name: trigger-performance-test namespace: ci annotations: argocd.argoproj.io/hook: PostSync spec: template: spec: serviceAccountName: pipeline containers: - name: performance-test image: quay.io/rhcanada/tkn-cli command: - tkn args: - -n - ci - pipeline - start - ace-performance-test - --param - is-source-repo-url=git@github.ibm.com:cpat-int-samples/createCustomer_REST_V1.git - --param - is-source-repo-private-key=id_create-customer-rest-v1 - --param - source-env=staging - --param - destination-env=prod - --param - jmeter-users=500 - --workspace - name=shared-workspace,claimName=ace-test-pvc - --serviceaccount - pipeline restartPolicy: Never backoffLimit: 0","title":"Triggers in staging environment"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#performance-test-pipeline","text":"","title":"Performance Test Pipeline"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#performance-tests","text":"Audience : Architects, Application developers In this topic, we're going to discuss: Introduction Apache Jmeter Datastore Grafana Validating our sample application Advantages Note The contents in this guide are only examples. We demonstrated some of the practices using some sample usecases. For real world scenarios, it should be used in conjunction with existing performance practices and this content is not a replacement. For assistance, CSMs should reach out to performance SMEs.","title":"Performance Tests"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#introduction","text":"In our sample performance test task, we are using Jmeter as our performance load testing tool. The live data from Jmeter will be fed into a datastore. Based on this data, the metrics can be visualized using Grafana dashboard.","title":"Introduction"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#apache-jmeter","text":"Jmeter is an open source java based tool. It helps us to measure the performance of the application. We can basically analyze the load functional behavior of the application. Jmeter simulates the load, for instance it simulates a group of users and keep sending requests to the target server. We can define a test plan script. All the necessary configurations will be provided here. Once we run the Jmeter tests, results will be available.","title":"Apache Jmeter"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#datastore","text":"Jmeter writes the real time test results into a datastore which will be further used for analysis, metric generation etc. In order to integrate Jmeter with datastore, we use a Backend Listener . Currently, in our sample work flow, we are using InfluxDB as our datastore. InfluxDB is a time series database. It handles time-stamped data. The backend listener configuration for the same is as follows. <BackendListener guiclass=\"BackendListenerGui\" testclass=\"BackendListener\" testname=\"Backend Listener\" enabled=\"true\"> <elementProp name=\"arguments\" elementType=\"Arguments\" guiclass=\"ArgumentsPanel\" testclass=\"Arguments\" enabled=\"true\"> <collectionProp name=\"Arguments.arguments\"> <elementProp name=\"influxdbMetricsSender\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">influxdbMetricsSender</stringProp> <stringProp name=\"Argument.value\">org.apache.jmeter.visualizers.backend.influxdb.HttpMetricsSender</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"influxdbUrl\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">influxdbUrl</stringProp> <stringProp name=\"Argument.value\">http://influxdb-service.tools.svc.cluster.local:8086/write?db=gatling</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"application\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">application</stringProp> <stringProp name=\"Argument.value\">application name</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"measurement\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">measurement</stringProp> <stringProp name=\"Argument.value\">jmeter</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"summaryOnly\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">summaryOnly</stringProp> <stringProp name=\"Argument.value\">true</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"samplersRegex\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">samplersRegex</stringProp> <stringProp name=\"Argument.value\">.*</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"percentiles\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">percentiles</stringProp> <stringProp name=\"Argument.value\">90;95;99</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"testTitle\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">testTitle</stringProp> <stringProp name=\"Argument.value\">Test name</stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> <elementProp name=\"eventTags\" elementType=\"Argument\"> <stringProp name=\"Argument.name\">eventTags</stringProp> <stringProp name=\"Argument.value\"></stringProp> <stringProp name=\"Argument.metadata\">=</stringProp> </elementProp> </collectionProp> </elementProp> <stringProp name=\"classname\">org.apache.jmeter.visualizers.backend.influxdb.InfluxdbBackendListenerClient</stringProp> </BackendListener> Note: We are working on replacing the InfluxDB with Prometheus. Once we have the implementation, we will document the steps here.","title":"Datastore"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#grafana","text":"We are using Grafana for visualization and metric analysis. Grafana will read the information from the datastore and display it on the dashboard.","title":"Grafana"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#validating-our-sample-application","text":"If we run the performance test pipeline, what basically happens is Jmeter writes the real time test results into the datastore using a backend listener and Grafana will read the data from it and then in turn display it on the dashboard. In our sample workflow, we are using a user load of about 500 with a replica of 4 integration servers as follows. When the performance test pipeline is run, Jmeter tests will run. It validates the user load and passes successfully as shown if the servers can efficiently handle the load generated. Once the jmeter tests complete, the data will be loaded into the datastore and eventually displayed in Grafana as follows. Similarly, now we are using a user load of about 500 and reducing the integration server replica to 2 . When the performance test pipeline is run, Jmeter tests will run. It validates the user load and fails as follows if the servers can't efficiently handle the load generated. Once the jmeter tests complete, the data will be loaded into the datastore and eventually displayed in Grafana as follows.","title":"Validating our sample application"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#advantages_1","text":"Some of the advantages are: Allows us to identify performance bottlenecks. Identify bugs. System downtime can be reduced.","title":"Advantages"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#production-environment","text":"Audience : Architects, Application developers In this topic, we're going to discuss: Production environment","title":"Production Environment"},{"location":"guides/cp4i/ace/environment-promotion/promoting-environments/#production-environment_1","text":"When the Pull Request with the changes in prod resources get merged into the GitOps repository, the deliverables for prod environment will be updated. These deliverables will be deployed to the prod environment. This deployment in production environment is done by ArgoCD. ArgoCD\u2019s role is to perform continuous deployment, ensuring that the cluster is always kept up-to-date with the latest built application. This is the next environment after stage where the deliverable is finally deployed. Once the deliverable is successfully deployed, it will be made available to the end user.","title":"Production environment"},{"location":"guides/cp4i/ace/gitops/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/ace/gitops/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/ace/high-availability/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/ace/high-availability/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/ace/monitoring/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/ace/monitoring/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/ace/overview/architecture/","text":"App Connect Enterprise Architecture \u00b6 Introduction \u00b6 Audience : Architects, Integration developers, Integration Administrators, Site Reliability Engineers In this topic, we're going to: Examine a high-level production deployment for ACE Examine the Kubernetes runtime component Understand the the GitOps model for ACE configuration and management Identify the different users of an ACE deployment At the end of the topic, you will understand the major components for a production-ready, cloud native App Connect Enterprise deployment. Target architecture \u00b6 The following diagram shows a typical ACE deployment: You'll notice the major components that are essential to a production-ready ACE cloud native deployment: A Kubernetes cluster containing: ACE applications Cloud native components such as Tekton, ArgoCD, Kibana and Grafana which will help operate this environment GitHub as a source of truth for the cluster runtime containing: ACE application source (one Github repository per application) ACE shared library source (one Github repository per shared library) ACE application configuration artifacts CI/CD artifacts describing Tekton tasks and Pipelines This use of Git as a source of truth follows the GitOps model for the configuration and management of an MQ deployment in a Kubernetes cluster. We'll learn more about this model throughout the guide. Notice the set of users who interact with these components: Integration Developers Integration Administrators DevOps specialists and Site Reliability Engineers Architects Business analysts ACE application users In this guide, we'll see how these users work within this environment.","title":"Architecture"},{"location":"guides/cp4i/ace/overview/architecture/#app-connect-enterprise-architecture","text":"","title":"App Connect Enterprise Architecture"},{"location":"guides/cp4i/ace/overview/architecture/#introduction","text":"Audience : Architects, Integration developers, Integration Administrators, Site Reliability Engineers In this topic, we're going to: Examine a high-level production deployment for ACE Examine the Kubernetes runtime component Understand the the GitOps model for ACE configuration and management Identify the different users of an ACE deployment At the end of the topic, you will understand the major components for a production-ready, cloud native App Connect Enterprise deployment.","title":"Introduction"},{"location":"guides/cp4i/ace/overview/architecture/#target-architecture","text":"The following diagram shows a typical ACE deployment: You'll notice the major components that are essential to a production-ready ACE cloud native deployment: A Kubernetes cluster containing: ACE applications Cloud native components such as Tekton, ArgoCD, Kibana and Grafana which will help operate this environment GitHub as a source of truth for the cluster runtime containing: ACE application source (one Github repository per application) ACE shared library source (one Github repository per shared library) ACE application configuration artifacts CI/CD artifacts describing Tekton tasks and Pipelines This use of Git as a source of truth follows the GitOps model for the configuration and management of an MQ deployment in a Kubernetes cluster. We'll learn more about this model throughout the guide. Notice the set of users who interact with these components: Integration Developers Integration Administrators DevOps specialists and Site Reliability Engineers Architects Business analysts ACE application users In this guide, we'll see how these users work within this environment.","title":"Target architecture"},{"location":"guides/cp4i/ace/overview/overview/","text":"App Connect Enterprise \u00b6 Overview \u00b6 IBM Cloud Pak for Integration includes a market-leading application integration capability called App Connect Enterprise (ACE). It enables the implementation of API and event-driven integrations and provides extensive adaptation to on-premises and cloud-based applications. It provides tooling that is optimized to the users' skillsets, so that they can be productive in a matter of hours and achieve real results in days. Powerful underlying capabilities facilitate the implementation of even the most complex integration patterns. As a result, data can be moved quickly, accurately and robustly. The purpose of this guide is to teach you how to deploy an ACE application on OpenShift Container Platform. We will be using Cloud Pak for Integration (CP4I) and other cloud native technologies such as containers, operators, microservices, immutable infrastructure and declarative APIs provided by CP4I to create a best-practice based production ready deploy of an ACE message flow. You will also be exposed to and learn how technologies such as OpenShift Pipelines (Tekton) and OpenShift Gitops (ArgoCD) integrate in a production environment.","title":"Overview"},{"location":"guides/cp4i/ace/overview/overview/#app-connect-enterprise","text":"","title":"App Connect Enterprise"},{"location":"guides/cp4i/ace/overview/overview/#overview","text":"IBM Cloud Pak for Integration includes a market-leading application integration capability called App Connect Enterprise (ACE). It enables the implementation of API and event-driven integrations and provides extensive adaptation to on-premises and cloud-based applications. It provides tooling that is optimized to the users' skillsets, so that they can be productive in a matter of hours and achieve real results in days. Powerful underlying capabilities facilitate the implementation of even the most complex integration patterns. As a result, data can be moved quickly, accurately and robustly. The purpose of this guide is to teach you how to deploy an ACE application on OpenShift Container Platform. We will be using Cloud Pak for Integration (CP4I) and other cloud native technologies such as containers, operators, microservices, immutable infrastructure and declarative APIs provided by CP4I to create a best-practice based production ready deploy of an ACE message flow. You will also be exposed to and learn how technologies such as OpenShift Pipelines (Tekton) and OpenShift Gitops (ArgoCD) integrate in a production environment.","title":"Overview"},{"location":"guides/cp4i/ace/performance/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/ace/performance/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/ace/scalability/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/ace/scalability/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/ace/security/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/ace/security/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/apic/architecture/ibm-cloud/","text":"IBM API Connect Architecture on IBM Cloud \u00b6 Abstract This document describes what a highly available production ready deployment of IBM API Connect looks like on the RedHat OpenShift Kubernetes Service on IBM Cloud , known as ROKS, on Virtual Private Cloud (VPC) Gen 2 infrastructure. Tip Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net As we can see in the topology above, the OpenShift cluster has been deployed on a MultiZone Region (MZR) data center where Virtual Private Cloud Gen 2 is available. For having IBM API Connect deployed in high availability, we need at least three worker nodes, each of these deployed on a different availability zone (an extra worker node is recommended when worker nodes need to be upgraded so that the extra node gets the load drained from the initial worker node being upgraded). We also need a minimum of 14 vCPUs and 48 Gigabytes of memory to install the n3xc14.m48 IBM API Connect profile, which is the recommended profile for larger systems and for production environments in the latest IBM API Connect v10.0.3.0 . You can search for more detailed IBM API Connect system requirements here Once the above requirements are met, we can tell the IBM API Connect Operator to deploy an IBM API Connect Cluster instance using its high availability profile ( n3xc14.m48 ). This profile, as you can see in the topology above, will deploy each of the IBM API Connect components (the Management System, made up of the Cloud Manager and the API Manager, the Gateway Service, the Developer Portal Service and the Analytics Service) in high availability. That is, each of the IBM API Connect components will be deployed with a replica factor of 3, using the Kubernetes affinity property to place each of these replicas in different nodes. Those set of replicas will, in turn, form a cluster object that the API Connect Operator understands, monitors and takes care of. As a result, our highly available IBM API Connect deployment is made up of a Management Cluster, Gateway Cluster, Portal Cluster and Analytics Cluster. Of course, some of the IBM API Components aforementioned need to store state and/or data. The technologies used for that like Postgresql or Etcd will be deployed in the same highly available fashion having three replicas one in each availability zone. As to the storage technology IBM API Connect requires, it is block storage that is claimed through the corresponding Persistent Volume Claims. The storage is provided by IBM Cloud Object Storage from your Virtual Private Cloud. Important The storageClass required by IBM API Connect must be of type metro . Info What metro means is that the volumeBindingMode of that storageClass will be set to WaitForFirstConsumer as opposed to the default Immediate . And what that means is that the Persistent Volume creation and allocation by the IBM Cloud Object Storage, as a result of its Persistent Volume Claim, will not happen until the pod linked to that Persistent Volume Claim is scheduled. This allows IBM Cloud Object Storage to know what Availability Zone of your MultiZone Region cluster the pod requesting block storage ended up on and, as a result, to be able to provision such storage in the appropriate place. Otherwise, if we used a storageClass whose volumeBindingMode was the default Immediate , IBM Cloud Object Storage would create and allocate the Persistent Volume in one of the Availability Zones which might not be the same Availability Zone the pod requiring such storage ends up on as a result of the OpenShift pod scheduler which would make the storage inaccessible to the pod. See Kubernetes official documentation here for further detail. Important The storageClass required by IBM API Connect must not have Retain Reclaim policy . Info If you retain the Persistent Volume, it might end up assigned to a pod in a different Availability Zone later, making that storage inaccessible to the pod allocated to. Summary The storageClassName you would need to provide when creating your IBM API Connect cluster, in the deployment section , will need to be of either ibmc-vpc-block-metro-10iops-tier , ibmc-vpc-block-metro-5iops-tier or ibmc-vpc-block-metro-custom types.","title":"IBM Cloud"},{"location":"guides/cp4i/apic/architecture/ibm-cloud/#ibm-api-connect-architecture-on-ibm-cloud","text":"Abstract This document describes what a highly available production ready deployment of IBM API Connect looks like on the RedHat OpenShift Kubernetes Service on IBM Cloud , known as ROKS, on Virtual Private Cloud (VPC) Gen 2 infrastructure. Tip Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net As we can see in the topology above, the OpenShift cluster has been deployed on a MultiZone Region (MZR) data center where Virtual Private Cloud Gen 2 is available. For having IBM API Connect deployed in high availability, we need at least three worker nodes, each of these deployed on a different availability zone (an extra worker node is recommended when worker nodes need to be upgraded so that the extra node gets the load drained from the initial worker node being upgraded). We also need a minimum of 14 vCPUs and 48 Gigabytes of memory to install the n3xc14.m48 IBM API Connect profile, which is the recommended profile for larger systems and for production environments in the latest IBM API Connect v10.0.3.0 . You can search for more detailed IBM API Connect system requirements here Once the above requirements are met, we can tell the IBM API Connect Operator to deploy an IBM API Connect Cluster instance using its high availability profile ( n3xc14.m48 ). This profile, as you can see in the topology above, will deploy each of the IBM API Connect components (the Management System, made up of the Cloud Manager and the API Manager, the Gateway Service, the Developer Portal Service and the Analytics Service) in high availability. That is, each of the IBM API Connect components will be deployed with a replica factor of 3, using the Kubernetes affinity property to place each of these replicas in different nodes. Those set of replicas will, in turn, form a cluster object that the API Connect Operator understands, monitors and takes care of. As a result, our highly available IBM API Connect deployment is made up of a Management Cluster, Gateway Cluster, Portal Cluster and Analytics Cluster. Of course, some of the IBM API Components aforementioned need to store state and/or data. The technologies used for that like Postgresql or Etcd will be deployed in the same highly available fashion having three replicas one in each availability zone. As to the storage technology IBM API Connect requires, it is block storage that is claimed through the corresponding Persistent Volume Claims. The storage is provided by IBM Cloud Object Storage from your Virtual Private Cloud. Important The storageClass required by IBM API Connect must be of type metro . Info What metro means is that the volumeBindingMode of that storageClass will be set to WaitForFirstConsumer as opposed to the default Immediate . And what that means is that the Persistent Volume creation and allocation by the IBM Cloud Object Storage, as a result of its Persistent Volume Claim, will not happen until the pod linked to that Persistent Volume Claim is scheduled. This allows IBM Cloud Object Storage to know what Availability Zone of your MultiZone Region cluster the pod requesting block storage ended up on and, as a result, to be able to provision such storage in the appropriate place. Otherwise, if we used a storageClass whose volumeBindingMode was the default Immediate , IBM Cloud Object Storage would create and allocate the Persistent Volume in one of the Availability Zones which might not be the same Availability Zone the pod requiring such storage ends up on as a result of the OpenShift pod scheduler which would make the storage inaccessible to the pod. See Kubernetes official documentation here for further detail. Important The storageClass required by IBM API Connect must not have Retain Reclaim policy . Info If you retain the Persistent Volume, it might end up assigned to a pod in a different Availability Zone later, making that storage inaccessible to the pod allocated to. Summary The storageClassName you would need to provide when creating your IBM API Connect cluster, in the deployment section , will need to be of either ibmc-vpc-block-metro-10iops-tier , ibmc-vpc-block-metro-5iops-tier or ibmc-vpc-block-metro-custom types.","title":"IBM API Connect Architecture on IBM Cloud"},{"location":"guides/cp4i/apic/architecture/multi-cloud/","text":"IBM API Connect Multi-cloud Architecture \u00b6 Abstract This document describes a multi-cloud deployment of IBM API Connect across different cloud providers, all using RedHat OpenSift Container Platform as the underlying unified platform. Tip Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net As we can see in the topology above for this use case, we have the brain of IBM API Connect, the Cloud Manager component, and the Developer Portal component on a RedHat OpenShift cluster hosted on IBM Cloud while the Gateway and Analytics components will be deployed on different RedHat OpenShift clusters hosted both in IBM Cloud as well as in Amazon AWS. This hybrid multi-cloud topologies are possible thanks to IBM API Connect components being completely independently deployable*. Also, thanks to RedHat OpenShift Container Platform being the underlying unified platform to run IBM Software in containers, we should be able to deploy any of the IBM API Connect components to any cloud provider or on premise infrastructure where the RedHat OpenShift Container Platform can be deployed to. Info We are deploying IBM API Connect and it's subsystems in standalone mode. That is, these are not being installed as part of the IBM Cloud Pak for Integration and, as a result, none of the Cloud Pak for Integration components, such as the Platform Navigator, Zen UI, etc is begin deployed alongside. The aim of this topology is to support hybrid multi-cloud client use cases where applications will live in a mix of clouds as well as on premises. For these scenarios, we want to use IBM API Connect not only to develop, manage and socialize APIs in an agile manner but also to secure these APIs using IBM API Connect Gateways, which get realized as IBM DataPower instances. For each of the IBM API Connect components, we can choose different deployment profiles in case we require that component to be highly available or not. Moreover, the IBM API Connect Gateway component (IBM DataPower) in IBM API Connect v10 comes with a Pod Autoscaling feature to ensure high availability of DataPower pods. As to the storage technology the different IBM API Connect components require, it is block storage that is claimed through the corresponding Persistent Volume Claims at deployment time. Important Block storage is provided differently depending on the cloud provider or on premises infrastructure your RedHat OpenShift clusters are hosted. Make sure you understand and review the different storage options you have available for each those RedHat OpenShift clusters you plan to deploy IBM API Connect components to. Also, some cloud providers have special storage classes for those RedHat OpenShift clusters that are hosted in Virtual Private Clouds (VPC). You can review in this guide the different storage options for the different cloud providers. (*) There are some restrictions as to how you deploy each of the IBM API Connect components. For instance, you must deploy the analytics component close to the Gateway component it is going to monitor.","title":"Multi-cloud"},{"location":"guides/cp4i/apic/architecture/multi-cloud/#ibm-api-connect-multi-cloud-architecture","text":"Abstract This document describes a multi-cloud deployment of IBM API Connect across different cloud providers, all using RedHat OpenSift Container Platform as the underlying unified platform. Tip Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net As we can see in the topology above for this use case, we have the brain of IBM API Connect, the Cloud Manager component, and the Developer Portal component on a RedHat OpenShift cluster hosted on IBM Cloud while the Gateway and Analytics components will be deployed on different RedHat OpenShift clusters hosted both in IBM Cloud as well as in Amazon AWS. This hybrid multi-cloud topologies are possible thanks to IBM API Connect components being completely independently deployable*. Also, thanks to RedHat OpenShift Container Platform being the underlying unified platform to run IBM Software in containers, we should be able to deploy any of the IBM API Connect components to any cloud provider or on premise infrastructure where the RedHat OpenShift Container Platform can be deployed to. Info We are deploying IBM API Connect and it's subsystems in standalone mode. That is, these are not being installed as part of the IBM Cloud Pak for Integration and, as a result, none of the Cloud Pak for Integration components, such as the Platform Navigator, Zen UI, etc is begin deployed alongside. The aim of this topology is to support hybrid multi-cloud client use cases where applications will live in a mix of clouds as well as on premises. For these scenarios, we want to use IBM API Connect not only to develop, manage and socialize APIs in an agile manner but also to secure these APIs using IBM API Connect Gateways, which get realized as IBM DataPower instances. For each of the IBM API Connect components, we can choose different deployment profiles in case we require that component to be highly available or not. Moreover, the IBM API Connect Gateway component (IBM DataPower) in IBM API Connect v10 comes with a Pod Autoscaling feature to ensure high availability of DataPower pods. As to the storage technology the different IBM API Connect components require, it is block storage that is claimed through the corresponding Persistent Volume Claims at deployment time. Important Block storage is provided differently depending on the cloud provider or on premises infrastructure your RedHat OpenShift clusters are hosted. Make sure you understand and review the different storage options you have available for each those RedHat OpenShift clusters you plan to deploy IBM API Connect components to. Also, some cloud providers have special storage classes for those RedHat OpenShift clusters that are hosted in Virtual Private Clouds (VPC). You can review in this guide the different storage options for the different cloud providers. (*) There are some restrictions as to how you deploy each of the IBM API Connect components. For instance, you must deploy the analytics component close to the Gateway component it is going to monitor.","title":"IBM API Connect Multi-cloud Architecture"},{"location":"guides/cp4i/apic/cluster-config/gitops-config/","text":"IBM API Connect GitOps Configuration \u00b6 In the cluster configuration of this chapter, we installed the fundamental components for continuous integration and continuous deployment into our cluster. These included a sample GitOps repository, and ArgoCD. In this section we're going to customize and enable the GitOps repository, so that we can install IBM API Connect and all of the components that go along with that. We will examine these components in more detail throughout this section of the tutorial as well. In this topic, we're going to: Explore the sample GitOps repository in a little more detail Customize the GitOps repository for our cluster Connect ArgoCD to the customized GitOps repository Bootstrap the cluster Explore how the ci namespace is created Try out some dynamic changes to the cluster with ArgoCD Explore how ArgoCD manages configuration drift By the end of this topic we'll have a cluster up and running, having used GitOps to do it. We'll fully understand how ArgoCD manages cluster change and configuration drift. Pre-requisites \u00b6 Before attempting this section, you must have completed the previous section of this tutorial where you created your Red Hat OpenShift cluster(s) and customized the GitOps repository and ArgoCD. Please see the previous sections of this guide for information on how to do these tasks. The sample GitOps repository \u00b6 Let's understand how the GitOps repository works by using it to install the components we've highlighted in the above diagram. In the first part of this topic, let's look at the high level structure of the GitOps repository. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Explore the high level folder structure Use the following command to display the folder structure: tree . -d -L 2 We can see the different folders in the GitOps repository: . \u251c\u2500\u2500 0-bootstrap \u2502 \u251c\u2500\u2500 others \u2502 \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 doc \u2502 \u251c\u2500\u2500 diagrams \u2502 \u251c\u2500\u2500 experimental \u2502 \u251c\u2500\u2500 images \u2502 \u2514\u2500\u2500 scenarios \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 bom \u2514\u2500\u2500 setup \u2514\u2500\u2500 ocp47 The 0-bootstrap folder is the key folder that contains all the available profiles. Within this folders are Cluster Profiles that control which resources are deployed to the cluster. There are other folders, but these contain utility scripts; and some document related information. Let us examine the 0-bootstrap folder structure: tree ./0-bootstrap/ -d -L 2 We can see the different folders: ./0-bootstrap/ \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u2514\u2500\u2500 3-apps If we consider single-cluster profile, you can see the three layers under that covering infrastructure, services, and applications. Use single-cluster profile to configure the cluster Use the following command to display the folder structure: tree ./0-bootstrap/single-cluster/ -L 3 We can see the different folders: ./0-bootstrap/single-cluster/ \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 1-infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 2-services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3-apps \u2502 \u251c\u2500\u2500 3-apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 cp4a \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Notice that each high level folder has an argocd folder. Within these argocd folders are ArgoCD applications that control which resources are deployed to the cluster. Sample ArgoCD application We'll see later how these ArgoCD applications work in detail. For now, notice how each layer of the architecture ( infrastructure , service , application ) has a set of ArgoCD applications. Type the following command: tree 0 -bootstrap/single-cluster/1-infra/ It shows a list of ArgoCD applications that are used to manage Kubernetes infrastructure resources: 0-bootstrap/single-cluster/1-infra/ \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Notice the namespace- YAMLs; we'll see in a moment how these each define an ArgoCD application dedicated to managing a Kubernetes namespace in our reference architecture. A word on terminology As we get started with ArgoCD, it can be easy to confuse the term ArgoCD application with your application. That's because ArgoCD uses the term application to refer to a Kubernetes custom resource that was initially designed to automate application management. However, an ArgoCD application can automate the deployment of any Kubernetes resource within a cluster, such as namespaces or queue managers, as we'll see a little later. Let's now customize this repository to deploy the highlighted Kubernetes resources to our cluster. Customizing the GitOps repositories \u00b6 The cloned GitOps configuration repositories (specifically, multi-tenancy-gitops and multi-tenancy-gitops-apps ) needs to be customized for your cluster. The ArgoCD Application and AppProject resources and kustomization.yaml resources contains environment variables which will need to be updated to your specific GitHub organization and branch. Update multi-tenancy-gitops repository \u00b6 Have a look at the following kustomization.yaml as an example of what will need to be updated. cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS_INFRA} - op: add path: /spec/source/targetRevision value: ${GIT_GITOPS_INFRA_BRANCH} Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Run the customization script Let's customize the cloned multi-tenancy-gitops repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. This script has also made some customizations that we'll use much later in the tutorial; we'll refer to those at the relevant time. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 199 files changed, 290 insertions ( + ) , 290 deletions ( - ) Set your GitOps repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (48/48), 4.70 KiB | 1.57 MiB/s, done. Total 48 (delta 32), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 23 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git d95eca5..a900c39 master -> master Update multi-tenancy-gitops-apps repository \u00b6 Similar to the previous section, there are corresponding ArgoCD Applications resources that will need to be updated in the multi-tenancy-gitops-apps repository. Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Locate the multi-tenancy-gitops-apps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops-apps Run the customization script Let's customize the cloned multi-tenancy-gitops-apps repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops-apps repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 21 files changed, 42 insertions ( + ) , 42 deletions ( - ) Set your repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops-apps repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (42/42), 4.70 KiB | 1.57 MiB/s, done. Total 42 (delta 42), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 21 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git d95eca5..a900c39 master -> master Connect ArgoCD to the GitOps repository \u00b6 Let's now connect your customized GitOps repository to the instance of ArgoCD running in the cluster. Once connected, ArgoCD will use the contents of the repository to create necessary resources. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Review ArgoCD infrastructure folder Let's examine the 0-bootstrap/single-cluster/1-infra/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml We can see the contents of the kustomization.yaml : resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Notice that the resources that needs to be applied to the cluster are all inactivate and commented out. Let us enable the resources that are needed by un-commenting them. Deploy Kubernetes resources with kustomization.yaml Open 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the below resources: argocd/consolelink.yaml argocd/consolenotification.yaml argocd/namespace-ibm-common-services.yaml argocd/namespace-ci.yaml argocd/namespace-dev.yaml argocd/namespace-staging.yaml argocd/namespace-prod.yaml argocd/namespace-sealed-secrets.yaml argocd/namespace-tools.yaml You will have the following resources un-commented for infrastructure: resources : - argocd/consolelink.yaml - argocd/consolenotification.yaml - argocd/namespace-ibm-common-services.yaml - argocd/namespace-ci.yaml - argocd/namespace-dev.yaml - argocd/namespace-staging.yaml - argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml - argocd/namespace-sealed-secrets.yaml - argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for infrastructure\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a900c39..e3f696d master -> master Examine bootstrap.yaml residing in 0-bootstrap/single-cluster/ . The bootstrap.yaml file is used to create our first ArgoCD application called bootstrap-single-cluster . This initial ArgoCD application will create all the other ArgoCD applications that control the application, service, and infrastructure resources (such as the ci and dev namespaces) deployed to the cluster. Examine the YAML that defines the ArgoCD bootstrap application: cat 0 -bootstrap/single-cluster/bootstrap.yaml Notice also how this ArgoCD application has been customized to use the GitOps repository repoURL: https://github.com/prod-ref-guide/multi-tenancy-gitops.git . apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-single-cluster namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/single-cluster repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Most importantly, see how path: 0-bootstrap/single-cluster refers to the 0-bootstrap/single-cluster folder within this repository. This will result in the creation of individual ArgoCD applications to manage our cluster resources. Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us for now only deploy infra resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and comment out the 2-services/2-services.yaml and 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml # - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Commit and push changes to your git repository: git add . git commit -s -m \"Using only infra\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 456 bytes | 456.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git e3f696d..ea3b43f master -> master Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. The repository contains a bootstrap-single-cluster ArgoCD application that is watching this repository and using its contents to manage the cluster. When the bootstrap-single-cluster ArgoCD application is applied to the cluster, it will continuously ensure that all the activated resources are applied to the cluster. Apply the bootstrap YAML to the cluster: oc apply -f 0 -bootstrap/single-cluster/bootstrap.yaml Kubernetes will confirm that the bootstrap ArgoCD application has been created: application.argoproj.io/bootstrap-single-cluster created The bootstrap ArgoCD application will watch the 0-bootstrap/single-cluster folder in our GitOps repository on GitHub. In this way, as resources are added to the infrastructure , service and application folders, they will be deployed to the cluster automatically. This is therefore the only direct cluster operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Verify the bootstrap deployment Verify that the bootstrap ArgoCD application is running with the following command: oc get app/bootstrap-single-cluster -n openshift-gitops You should see that the bootstrap application was recently updated: NAME SYNC STATUS HEALTH STATUS bootstrap-single-cluster Synced Healthy HEALTH_STATUS may temporarily show Missing ; simply re-issue the command to confirm it moves to Healthy . Using the UI to view the newly deployed ArgoCD applications In the previous section of this chapter you logged on to the ArgoCD web console. Switch back to that console, refresh the page and you should see the bootstrap-single-cluster ArgoCD application together with many other ArgoCD applications: (You may need to select List view rather than the Tiles view.) We can see that eleven ArgoCD applications have been deployed to the cluster as a result of applying bootstrap.yaml . In the next section of the tutorial, we'll examine these applications to see how and why they were created, but for now let's focus on one of them -- the namespace-ci ArgoCD application. Examining the namespace-ci ArgoCD application resources Let's examine the Kubernetes resources applied to the cluster by the namespace-ci ArgoCD application. In the ArgoCD application list, click on namespace-ci : (You may need to clear filters to see this screenshot.) The directed graph shows that the namespace-ci ArgoCD app has created 4 Kubernetes resources; our ci namespace and three role bindings. Verify the namespace using the oc CLI We've seen the new namespace definition in the GitOps repository and visually in the ArgoCD UI. Let's also verify it via the command line: Type the following command: oc get namespace ci -o yaml This will list the full details of the ci namespace: apiVersion : v1 kind : Namespace metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"namespace-ci\"},\"name\":\"ci\"},\"spec\":{}} openshift.io/sa.scc.mcs : s0:c27,c9 openshift.io/sa.scc.supplemental-groups : 1000720000/10000 openshift.io/sa.scc.uid-range : 1000720000/10000 creationTimestamp : \"2021-08-31T15:27:32Z\" labels : app.kubernetes.io/instance : namespace-ci managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:labels : . : {} f:app.kubernetes.io/instance : {} f:status : f:phase : {} manager : argocd-application-controller operation : Update time : \"2021-08-31T15:27:32Z\" - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : f:openshift.io/sa.scc.mcs : {} f:openshift.io/sa.scc.supplemental-groups : {} f:openshift.io/sa.scc.uid-range : {} manager : cluster-policy-controller operation : Update time : \"2021-08-31T15:27:32Z\" name : ci resourceVersion : \"2255607\" selfLink : /api/v1/namespaces/ci uid : fff6b82b-6318-4828-83bb-ade4e8e3c0cf spec : finalizers : - kubernetes status : phase : Active Notice how manager: argocd-application-controller identifies that this namespace was created by ArgoCD. It's important to understand the sequence of actions. We simply deployed the bootstrap-single-cluster ArgoCD application, and it ultimately resulted in the creation of the namespace-ci ArgoCD application which created the ci namespace. We don't directly apply resources to the cluster once the bootstrap-single-cluster ArgoCD application has been applied; the cluster state is determined by an ArgoCD application YAML in the corresponding application , service or infrastructure related folders. It's these ArgoCD applications that create and manage the underlying Kubernetes resources using the GitOps repository as the source of truth. Understanding the namespace-ci ArgoCD application Let's examine the ArgoCD application namespace-ci to see how it created the ci namespace and three role bindings in the cluster. Issue the following command to examine its YAML: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-ci.yaml Notice that apiVersion and kind identify this as an ArgoCD application: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ci labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" spec : destination : namespace : ci server : https://kubernetes.default.svc project : infra source : path : namespaces/ci syncPolicy : automated : prune : true selfHeal : true Most importantly, see how this namespace-ci ArgoCD application monitors the folder path: namespaces/ci in https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git ; it then applies the contents of this folder to the cluster whenever its content changes. Notice the SyncPolicy of automated ; any changes to this folder will automatically be applied to the cluster; we do not need to perform a manual Sync operation from the ArgoCD UI. Examine the ci namespace YAML To examine this, navigate to multi-tenancy-gitops-infra repository you cloned previously. cd $HOME /git cd multi-tenancy-gitops-infra Let's examine the ci namespace YAML in the namespaces/ci folder: cat namespaces/ci/namespace.yaml It's a very simple YAML: apiVersion : v1 kind : Namespace metadata : name : ci spec : {} This is a YAML that we would normally apply to the cluster manually or via a script. However, when we use GitOps, we push the ArgoCD application that uses this YAML to GitHub, and it applies the ci namespace YAML to the cluster. This is the essence of GitOps; we declare what we want to appear in the cluster using Git and ArgoCD synchronizes the cluster with this declaration. Examine the ci rolebinding YAML Now that we've seen how the namespace was created, let's see how the three other rolebindings were created by the namespace-ci ArgoCD application. In the same namespace/ci folder as the ci namespace YAML, there is a rolebinding.yaml file. This file will be also applied to the cluster by the namespace-ci ArgoCD application which is continuously watching this folder. Examine this file with the following command: cat namespaces/ci/rolebinding.yaml This YAML is slightly more complex than the namespace YAML: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-dev namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:dev --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-staging namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:staging --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-prod namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:prod --- However its structure is quite straightforward; look carefully and you'll see that there are indeed three rolebindings defined in this YAML. Each of these rolebindings binds different roles to different service accounts associated with components that will run within the ci namespace, such as Tekton pipelines for example. We'll see later how these rolebindings are important; they limit the operations that can be performed by the ci namespace, creating a well governed cluster. This confirms why we saw four resources created by the namespace-ci ArgoCD application in the ArgoCD UI: one namespace and three rolebindings. Again, notice the pattern: a single ArgoCD application manages one or more Kubernetes resources in the cluster -- using one or more YAML files in which those resources are defined. The bootstrap-single-cluster ArgoCD application in more detail In the ArgoCD UI Applications view, click on the bootstrap-single-cluster application: You can see the bootstrap application creates two Kubernetes resources, the infra ArgoCD application and the infra ArgoCD project. An ArgoCD project is a mechanism by which we can group related resources; we keep all our ArgoCD applications that manage infrastructure in the infra project. Later, we'll create a services project for the ArgoCD applications that manage the services we want deployed to the cluster such as the MQ application and queue manager pipelines. The infra ArgoCD application Let's examine the infra ArgoCD application in more detail to see how it works. In the ArgoCD UI Applications view, click on the open application icon for the infra application: We can see that the infra ArgoCD application creates 9 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It's the infra ArgoCD application that watches the 0-bootstrap/single-cluster/1-infra/argocd folder for ArgoCD applications that apply infrastructure resources to our cluster. It was the infra application that created the namespace-ci ArgoCD application which manages the ci namespace that we've been exploring in this section of the tutorial. We'll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it's not necessary, as you'll get lots of practice as we proceed. ArgoCD change management and governance \u00b6 In this final section about managing infrastructure resources, let's explore how ArgoCD provides some advanced resource management features: Dynamic Monitored Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Customize the web console banner Examine the banner in the OpenShift web console: We're going to use GitOps to modify this banner dynamically. The banner YAML This banner properties are defined by the YAML in /0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml . This YAML is currently being used by the cntk-consolenotification ArgoCD application that was deployed earlier. We can examine the YAML with the following command: cat 0 -bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml which shows the banner properties are part of the ConsoleNotification custom resource: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : cntk-consolenotification labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" finalizers : - resources-finalizer.argocd.argoproj.io spec : syncPolicy : automated : prune : true selfHeal : true destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : path : consolenotification helm : values : | ocp-console-notification: ## The name of the ConsoleNotification resource in the cluster name: \"banner-env\" ## The background color that should be used for the banner backgroundColor: teal ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text: \"Cluster Description\" See how the banner at the top of the screen: * contains the text Cluster Description * is located at top of the screen * has the color Teal Modify the YAML for this banner Let's now change this YAML In your editor, modify this YAML and change the below fields as follows: ocp-console-notification : ## The name of the ConsoleNotification resource in the cluster name : \"banner-env\" ## The background color that should be used for the banner backgroundColor : red ## The color of the text that will appear in the banner color : \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location : BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text : \"Production reference guide\" It's clear that our intention is to modify the banner's backgroundColor:red and text: Production reference guide to the newly specified values. If you look at the diff : git diff you should see the following diff --git a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml index 30adf1a..596e821 100644 --- a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml +++ b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml @@ -26,10 +26,10 @@ spec: name: \"banner-env\" ## The background color that should be used for the banner - backgroundColor: teal + backgroundColor: red ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created - text: \"Cluster Description\" + text: \"Production reference guide\" Make the web console YAML change active Let's make these changes visible to the cntk-consolenotification ArgoCD application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Modify console banner\" git push origin $GIT_BRANCH You'll see the changes being pushed to GitHub: Enumerating objects: 13, done. Counting objects: 100% (13/13), done. Delta compression using up to 8 threads Compressing objects: 100% (7/7), done. Writing objects: 100% (7/7), 670 bytes | 670.00 KiB/s, done. Total 7 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a1e8292..b49dff5 master -> master Let's see what effect they have on the web console. A dynamic change to the web console You can either wait for ArgoCD to automatically sync the cntk-consolenotification application or manually Sync it yourself: Returning to the OpenShift web console, you'll notice changes. Notice the dynamic nature of these changes; we updated the console YAML, pushed our changes to our GitOps repository and everything else happened automatically. As a result, our OpenShift console has a new banner color and text. Governing changes to the dev namespace Let's now look at how ArgoCD monitors Kubernetes resources for configuration drift, and what happens if it detects an unexpected change to a monitored resource. Don't worry about the following command; it might seem drastic and even reckless, but as you'll see, everything will be OK. Let's delete the dev namespace from the cluster: oc get namespace dev oc delete namespace dev See how the active namespace: NAME STATUS AGE dev Active 2d18h is deleted: namespace \"dev\" deleted We can see that the dev namespace has been manually deleted from the cluster. GitOps repository as a source of truth If you switch back to the ArgoCD UI, you may see that ArgoCD has detected a configuration drift: a resource is Missing (the dev namespace) namespace-dev therefore is OutOfSync namespace-dev is therefore Syncing with the GitOps repository After a while we'll see that namespace-dev is Healthy and Synced : ArgoCD has detected a configuration drift, and resynched with GitOps repository, re-applying the dev namespace to the cluster. Note You may miss seeing the first screenshot if ArgoCD detects and corrects the missing dev namespace before you get a chance to switch to the ArgoCD UI. Don't worry, you can try this operator again! The restored dev namespace Issue the following command to determine the status of the dev namespace: oc get namespace dev which confirms that the dev namespace has been re-instated: NAME STATUS AGE dev Active 115s Note that it is a different instance of the dev namespace; as indicated by its AGE value. Notice the governed nature of these changes; GitOps is our source of truth about the resources deployed to the cluster. ArgoCD restores any resources that suffer from configuration drift to their GitOps-defined configuration. Congratulations! You've used ArgoCD and the GitOps repository to set up ci , tools and dev namespaces. You've seen how to create ArgoCD applications that watch their respective GitOps namespace folders for details of the namespace resources they should apply to the cluster. You've seen how you can dynamically change deployed resources by updating the resource definition in the GitOps repository. Finally, you've experience how ArgoCD keeps the cluster synchronized with the GitOps repository as a source of truth; any unexpected configuration drift will be corrected without intervention.","title":"Getting started with GitOps"},{"location":"guides/cp4i/apic/cluster-config/gitops-config/#ibm-api-connect-gitops-configuration","text":"In the cluster configuration of this chapter, we installed the fundamental components for continuous integration and continuous deployment into our cluster. These included a sample GitOps repository, and ArgoCD. In this section we're going to customize and enable the GitOps repository, so that we can install IBM API Connect and all of the components that go along with that. We will examine these components in more detail throughout this section of the tutorial as well. In this topic, we're going to: Explore the sample GitOps repository in a little more detail Customize the GitOps repository for our cluster Connect ArgoCD to the customized GitOps repository Bootstrap the cluster Explore how the ci namespace is created Try out some dynamic changes to the cluster with ArgoCD Explore how ArgoCD manages configuration drift By the end of this topic we'll have a cluster up and running, having used GitOps to do it. We'll fully understand how ArgoCD manages cluster change and configuration drift.","title":"IBM API Connect GitOps Configuration"},{"location":"guides/cp4i/apic/cluster-config/gitops-config/#pre-requisites","text":"Before attempting this section, you must have completed the previous section of this tutorial where you created your Red Hat OpenShift cluster(s) and customized the GitOps repository and ArgoCD. Please see the previous sections of this guide for information on how to do these tasks.","title":"Pre-requisites"},{"location":"guides/cp4i/apic/cluster-config/gitops-config/#the-sample-gitops-repository","text":"Let's understand how the GitOps repository works by using it to install the components we've highlighted in the above diagram. In the first part of this topic, let's look at the high level structure of the GitOps repository. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Explore the high level folder structure Use the following command to display the folder structure: tree . -d -L 2 We can see the different folders in the GitOps repository: . \u251c\u2500\u2500 0-bootstrap \u2502 \u251c\u2500\u2500 others \u2502 \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 doc \u2502 \u251c\u2500\u2500 diagrams \u2502 \u251c\u2500\u2500 experimental \u2502 \u251c\u2500\u2500 images \u2502 \u2514\u2500\u2500 scenarios \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 bom \u2514\u2500\u2500 setup \u2514\u2500\u2500 ocp47 The 0-bootstrap folder is the key folder that contains all the available profiles. Within this folders are Cluster Profiles that control which resources are deployed to the cluster. There are other folders, but these contain utility scripts; and some document related information. Let us examine the 0-bootstrap folder structure: tree ./0-bootstrap/ -d -L 2 We can see the different folders: ./0-bootstrap/ \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u2514\u2500\u2500 3-apps If we consider single-cluster profile, you can see the three layers under that covering infrastructure, services, and applications. Use single-cluster profile to configure the cluster Use the following command to display the folder structure: tree ./0-bootstrap/single-cluster/ -L 3 We can see the different folders: ./0-bootstrap/single-cluster/ \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 1-infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 2-services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3-apps \u2502 \u251c\u2500\u2500 3-apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 cp4a \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Notice that each high level folder has an argocd folder. Within these argocd folders are ArgoCD applications that control which resources are deployed to the cluster. Sample ArgoCD application We'll see later how these ArgoCD applications work in detail. For now, notice how each layer of the architecture ( infrastructure , service , application ) has a set of ArgoCD applications. Type the following command: tree 0 -bootstrap/single-cluster/1-infra/ It shows a list of ArgoCD applications that are used to manage Kubernetes infrastructure resources: 0-bootstrap/single-cluster/1-infra/ \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Notice the namespace- YAMLs; we'll see in a moment how these each define an ArgoCD application dedicated to managing a Kubernetes namespace in our reference architecture. A word on terminology As we get started with ArgoCD, it can be easy to confuse the term ArgoCD application with your application. That's because ArgoCD uses the term application to refer to a Kubernetes custom resource that was initially designed to automate application management. However, an ArgoCD application can automate the deployment of any Kubernetes resource within a cluster, such as namespaces or queue managers, as we'll see a little later. Let's now customize this repository to deploy the highlighted Kubernetes resources to our cluster.","title":"The sample GitOps repository"},{"location":"guides/cp4i/apic/cluster-config/gitops-config/#customizing-the-gitops-repositories","text":"The cloned GitOps configuration repositories (specifically, multi-tenancy-gitops and multi-tenancy-gitops-apps ) needs to be customized for your cluster. The ArgoCD Application and AppProject resources and kustomization.yaml resources contains environment variables which will need to be updated to your specific GitHub organization and branch.","title":"Customizing the GitOps repositories"},{"location":"guides/cp4i/apic/cluster-config/gitops-config/#update-multi-tenancy-gitops-repository","text":"Have a look at the following kustomization.yaml as an example of what will need to be updated. cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS_INFRA} - op: add path: /spec/source/targetRevision value: ${GIT_GITOPS_INFRA_BRANCH} Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Run the customization script Let's customize the cloned multi-tenancy-gitops repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. This script has also made some customizations that we'll use much later in the tutorial; we'll refer to those at the relevant time. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 199 files changed, 290 insertions ( + ) , 290 deletions ( - ) Set your GitOps repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (48/48), 4.70 KiB | 1.57 MiB/s, done. Total 48 (delta 32), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 23 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git d95eca5..a900c39 master -> master","title":"Update multi-tenancy-gitops repository"},{"location":"guides/cp4i/apic/cluster-config/gitops-config/#update-multi-tenancy-gitops-apps-repository","text":"Similar to the previous section, there are corresponding ArgoCD Applications resources that will need to be updated in the multi-tenancy-gitops-apps repository. Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Locate the multi-tenancy-gitops-apps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops-apps Run the customization script Let's customize the cloned multi-tenancy-gitops-apps repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops-apps repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 21 files changed, 42 insertions ( + ) , 42 deletions ( - ) Set your repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops-apps repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (42/42), 4.70 KiB | 1.57 MiB/s, done. Total 42 (delta 42), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 21 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git d95eca5..a900c39 master -> master","title":"Update multi-tenancy-gitops-apps repository"},{"location":"guides/cp4i/apic/cluster-config/gitops-config/#connect-argocd-to-the-gitops-repository","text":"Let's now connect your customized GitOps repository to the instance of ArgoCD running in the cluster. Once connected, ArgoCD will use the contents of the repository to create necessary resources. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Review ArgoCD infrastructure folder Let's examine the 0-bootstrap/single-cluster/1-infra/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml We can see the contents of the kustomization.yaml : resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Notice that the resources that needs to be applied to the cluster are all inactivate and commented out. Let us enable the resources that are needed by un-commenting them. Deploy Kubernetes resources with kustomization.yaml Open 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the below resources: argocd/consolelink.yaml argocd/consolenotification.yaml argocd/namespace-ibm-common-services.yaml argocd/namespace-ci.yaml argocd/namespace-dev.yaml argocd/namespace-staging.yaml argocd/namespace-prod.yaml argocd/namespace-sealed-secrets.yaml argocd/namespace-tools.yaml You will have the following resources un-commented for infrastructure: resources : - argocd/consolelink.yaml - argocd/consolenotification.yaml - argocd/namespace-ibm-common-services.yaml - argocd/namespace-ci.yaml - argocd/namespace-dev.yaml - argocd/namespace-staging.yaml - argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml - argocd/namespace-sealed-secrets.yaml - argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for infrastructure\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a900c39..e3f696d master -> master Examine bootstrap.yaml residing in 0-bootstrap/single-cluster/ . The bootstrap.yaml file is used to create our first ArgoCD application called bootstrap-single-cluster . This initial ArgoCD application will create all the other ArgoCD applications that control the application, service, and infrastructure resources (such as the ci and dev namespaces) deployed to the cluster. Examine the YAML that defines the ArgoCD bootstrap application: cat 0 -bootstrap/single-cluster/bootstrap.yaml Notice also how this ArgoCD application has been customized to use the GitOps repository repoURL: https://github.com/prod-ref-guide/multi-tenancy-gitops.git . apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-single-cluster namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/single-cluster repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Most importantly, see how path: 0-bootstrap/single-cluster refers to the 0-bootstrap/single-cluster folder within this repository. This will result in the creation of individual ArgoCD applications to manage our cluster resources. Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us for now only deploy infra resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and comment out the 2-services/2-services.yaml and 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml # - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Commit and push changes to your git repository: git add . git commit -s -m \"Using only infra\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 456 bytes | 456.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git e3f696d..ea3b43f master -> master Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. The repository contains a bootstrap-single-cluster ArgoCD application that is watching this repository and using its contents to manage the cluster. When the bootstrap-single-cluster ArgoCD application is applied to the cluster, it will continuously ensure that all the activated resources are applied to the cluster. Apply the bootstrap YAML to the cluster: oc apply -f 0 -bootstrap/single-cluster/bootstrap.yaml Kubernetes will confirm that the bootstrap ArgoCD application has been created: application.argoproj.io/bootstrap-single-cluster created The bootstrap ArgoCD application will watch the 0-bootstrap/single-cluster folder in our GitOps repository on GitHub. In this way, as resources are added to the infrastructure , service and application folders, they will be deployed to the cluster automatically. This is therefore the only direct cluster operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Verify the bootstrap deployment Verify that the bootstrap ArgoCD application is running with the following command: oc get app/bootstrap-single-cluster -n openshift-gitops You should see that the bootstrap application was recently updated: NAME SYNC STATUS HEALTH STATUS bootstrap-single-cluster Synced Healthy HEALTH_STATUS may temporarily show Missing ; simply re-issue the command to confirm it moves to Healthy . Using the UI to view the newly deployed ArgoCD applications In the previous section of this chapter you logged on to the ArgoCD web console. Switch back to that console, refresh the page and you should see the bootstrap-single-cluster ArgoCD application together with many other ArgoCD applications: (You may need to select List view rather than the Tiles view.) We can see that eleven ArgoCD applications have been deployed to the cluster as a result of applying bootstrap.yaml . In the next section of the tutorial, we'll examine these applications to see how and why they were created, but for now let's focus on one of them -- the namespace-ci ArgoCD application. Examining the namespace-ci ArgoCD application resources Let's examine the Kubernetes resources applied to the cluster by the namespace-ci ArgoCD application. In the ArgoCD application list, click on namespace-ci : (You may need to clear filters to see this screenshot.) The directed graph shows that the namespace-ci ArgoCD app has created 4 Kubernetes resources; our ci namespace and three role bindings. Verify the namespace using the oc CLI We've seen the new namespace definition in the GitOps repository and visually in the ArgoCD UI. Let's also verify it via the command line: Type the following command: oc get namespace ci -o yaml This will list the full details of the ci namespace: apiVersion : v1 kind : Namespace metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"namespace-ci\"},\"name\":\"ci\"},\"spec\":{}} openshift.io/sa.scc.mcs : s0:c27,c9 openshift.io/sa.scc.supplemental-groups : 1000720000/10000 openshift.io/sa.scc.uid-range : 1000720000/10000 creationTimestamp : \"2021-08-31T15:27:32Z\" labels : app.kubernetes.io/instance : namespace-ci managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:labels : . : {} f:app.kubernetes.io/instance : {} f:status : f:phase : {} manager : argocd-application-controller operation : Update time : \"2021-08-31T15:27:32Z\" - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : f:openshift.io/sa.scc.mcs : {} f:openshift.io/sa.scc.supplemental-groups : {} f:openshift.io/sa.scc.uid-range : {} manager : cluster-policy-controller operation : Update time : \"2021-08-31T15:27:32Z\" name : ci resourceVersion : \"2255607\" selfLink : /api/v1/namespaces/ci uid : fff6b82b-6318-4828-83bb-ade4e8e3c0cf spec : finalizers : - kubernetes status : phase : Active Notice how manager: argocd-application-controller identifies that this namespace was created by ArgoCD. It's important to understand the sequence of actions. We simply deployed the bootstrap-single-cluster ArgoCD application, and it ultimately resulted in the creation of the namespace-ci ArgoCD application which created the ci namespace. We don't directly apply resources to the cluster once the bootstrap-single-cluster ArgoCD application has been applied; the cluster state is determined by an ArgoCD application YAML in the corresponding application , service or infrastructure related folders. It's these ArgoCD applications that create and manage the underlying Kubernetes resources using the GitOps repository as the source of truth. Understanding the namespace-ci ArgoCD application Let's examine the ArgoCD application namespace-ci to see how it created the ci namespace and three role bindings in the cluster. Issue the following command to examine its YAML: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-ci.yaml Notice that apiVersion and kind identify this as an ArgoCD application: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ci labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" spec : destination : namespace : ci server : https://kubernetes.default.svc project : infra source : path : namespaces/ci syncPolicy : automated : prune : true selfHeal : true Most importantly, see how this namespace-ci ArgoCD application monitors the folder path: namespaces/ci in https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git ; it then applies the contents of this folder to the cluster whenever its content changes. Notice the SyncPolicy of automated ; any changes to this folder will automatically be applied to the cluster; we do not need to perform a manual Sync operation from the ArgoCD UI. Examine the ci namespace YAML To examine this, navigate to multi-tenancy-gitops-infra repository you cloned previously. cd $HOME /git cd multi-tenancy-gitops-infra Let's examine the ci namespace YAML in the namespaces/ci folder: cat namespaces/ci/namespace.yaml It's a very simple YAML: apiVersion : v1 kind : Namespace metadata : name : ci spec : {} This is a YAML that we would normally apply to the cluster manually or via a script. However, when we use GitOps, we push the ArgoCD application that uses this YAML to GitHub, and it applies the ci namespace YAML to the cluster. This is the essence of GitOps; we declare what we want to appear in the cluster using Git and ArgoCD synchronizes the cluster with this declaration. Examine the ci rolebinding YAML Now that we've seen how the namespace was created, let's see how the three other rolebindings were created by the namespace-ci ArgoCD application. In the same namespace/ci folder as the ci namespace YAML, there is a rolebinding.yaml file. This file will be also applied to the cluster by the namespace-ci ArgoCD application which is continuously watching this folder. Examine this file with the following command: cat namespaces/ci/rolebinding.yaml This YAML is slightly more complex than the namespace YAML: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-dev namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:dev --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-staging namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:staging --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-prod namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:prod --- However its structure is quite straightforward; look carefully and you'll see that there are indeed three rolebindings defined in this YAML. Each of these rolebindings binds different roles to different service accounts associated with components that will run within the ci namespace, such as Tekton pipelines for example. We'll see later how these rolebindings are important; they limit the operations that can be performed by the ci namespace, creating a well governed cluster. This confirms why we saw four resources created by the namespace-ci ArgoCD application in the ArgoCD UI: one namespace and three rolebindings. Again, notice the pattern: a single ArgoCD application manages one or more Kubernetes resources in the cluster -- using one or more YAML files in which those resources are defined. The bootstrap-single-cluster ArgoCD application in more detail In the ArgoCD UI Applications view, click on the bootstrap-single-cluster application: You can see the bootstrap application creates two Kubernetes resources, the infra ArgoCD application and the infra ArgoCD project. An ArgoCD project is a mechanism by which we can group related resources; we keep all our ArgoCD applications that manage infrastructure in the infra project. Later, we'll create a services project for the ArgoCD applications that manage the services we want deployed to the cluster such as the MQ application and queue manager pipelines. The infra ArgoCD application Let's examine the infra ArgoCD application in more detail to see how it works. In the ArgoCD UI Applications view, click on the open application icon for the infra application: We can see that the infra ArgoCD application creates 9 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It's the infra ArgoCD application that watches the 0-bootstrap/single-cluster/1-infra/argocd folder for ArgoCD applications that apply infrastructure resources to our cluster. It was the infra application that created the namespace-ci ArgoCD application which manages the ci namespace that we've been exploring in this section of the tutorial. We'll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it's not necessary, as you'll get lots of practice as we proceed.","title":"Connect ArgoCD to the GitOps repository"},{"location":"guides/cp4i/apic/cluster-config/gitops-config/#argocd-change-management-and-governance","text":"In this final section about managing infrastructure resources, let's explore how ArgoCD provides some advanced resource management features: Dynamic Monitored Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Customize the web console banner Examine the banner in the OpenShift web console: We're going to use GitOps to modify this banner dynamically. The banner YAML This banner properties are defined by the YAML in /0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml . This YAML is currently being used by the cntk-consolenotification ArgoCD application that was deployed earlier. We can examine the YAML with the following command: cat 0 -bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml which shows the banner properties are part of the ConsoleNotification custom resource: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : cntk-consolenotification labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" finalizers : - resources-finalizer.argocd.argoproj.io spec : syncPolicy : automated : prune : true selfHeal : true destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : path : consolenotification helm : values : | ocp-console-notification: ## The name of the ConsoleNotification resource in the cluster name: \"banner-env\" ## The background color that should be used for the banner backgroundColor: teal ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text: \"Cluster Description\" See how the banner at the top of the screen: * contains the text Cluster Description * is located at top of the screen * has the color Teal Modify the YAML for this banner Let's now change this YAML In your editor, modify this YAML and change the below fields as follows: ocp-console-notification : ## The name of the ConsoleNotification resource in the cluster name : \"banner-env\" ## The background color that should be used for the banner backgroundColor : red ## The color of the text that will appear in the banner color : \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location : BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text : \"Production reference guide\" It's clear that our intention is to modify the banner's backgroundColor:red and text: Production reference guide to the newly specified values. If you look at the diff : git diff you should see the following diff --git a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml index 30adf1a..596e821 100644 --- a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml +++ b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml @@ -26,10 +26,10 @@ spec: name: \"banner-env\" ## The background color that should be used for the banner - backgroundColor: teal + backgroundColor: red ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created - text: \"Cluster Description\" + text: \"Production reference guide\" Make the web console YAML change active Let's make these changes visible to the cntk-consolenotification ArgoCD application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Modify console banner\" git push origin $GIT_BRANCH You'll see the changes being pushed to GitHub: Enumerating objects: 13, done. Counting objects: 100% (13/13), done. Delta compression using up to 8 threads Compressing objects: 100% (7/7), done. Writing objects: 100% (7/7), 670 bytes | 670.00 KiB/s, done. Total 7 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a1e8292..b49dff5 master -> master Let's see what effect they have on the web console. A dynamic change to the web console You can either wait for ArgoCD to automatically sync the cntk-consolenotification application or manually Sync it yourself: Returning to the OpenShift web console, you'll notice changes. Notice the dynamic nature of these changes; we updated the console YAML, pushed our changes to our GitOps repository and everything else happened automatically. As a result, our OpenShift console has a new banner color and text. Governing changes to the dev namespace Let's now look at how ArgoCD monitors Kubernetes resources for configuration drift, and what happens if it detects an unexpected change to a monitored resource. Don't worry about the following command; it might seem drastic and even reckless, but as you'll see, everything will be OK. Let's delete the dev namespace from the cluster: oc get namespace dev oc delete namespace dev See how the active namespace: NAME STATUS AGE dev Active 2d18h is deleted: namespace \"dev\" deleted We can see that the dev namespace has been manually deleted from the cluster. GitOps repository as a source of truth If you switch back to the ArgoCD UI, you may see that ArgoCD has detected a configuration drift: a resource is Missing (the dev namespace) namespace-dev therefore is OutOfSync namespace-dev is therefore Syncing with the GitOps repository After a while we'll see that namespace-dev is Healthy and Synced : ArgoCD has detected a configuration drift, and resynched with GitOps repository, re-applying the dev namespace to the cluster. Note You may miss seeing the first screenshot if ArgoCD detects and corrects the missing dev namespace before you get a chance to switch to the ArgoCD UI. Don't worry, you can try this operator again! The restored dev namespace Issue the following command to determine the status of the dev namespace: oc get namespace dev which confirms that the dev namespace has been re-instated: NAME STATUS AGE dev Active 115s Note that it is a different instance of the dev namespace; as indicated by its AGE value. Notice the governed nature of these changes; GitOps is our source of truth about the resources deployed to the cluster. ArgoCD restores any resources that suffer from configuration drift to their GitOps-defined configuration. Congratulations! You've used ArgoCD and the GitOps repository to set up ci , tools and dev namespaces. You've seen how to create ArgoCD applications that watch their respective GitOps namespace folders for details of the namespace resources they should apply to the cluster. You've seen how you can dynamically change deployed resources by updating the resource definition in the GitOps repository. Finally, you've experience how ArgoCD keeps the cluster synchronized with the GitOps repository as a source of truth; any unexpected configuration drift will be corrected without intervention.","title":"ArgoCD change management and governance"},{"location":"guides/cp4i/apic/cluster-config/gitops-tekton-argocd/","text":"IBM API Connect GitOps \u00b6 Overview \u00b6 A GitOps model must be at the core of the methodology or approach used to deploy and manage any IBM capability in production. It ensures that the installation, configuration and changes of these IBM capabilities, as well as their configuration and citizens (applications, integration flows, APIs, etc), is only done through code. GitOps puts git repositories and git commands such as git push (to request a change) and git merge (to approve a change) at the heart of configuration management. A GitOps approach helps an organization implement best practices in version control and release governance based on a widely used open standard -- git . The following diagram outlines the major components in an IBM API Connect GitOps model: Tip Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net Notice the clear separation of concerns: Tekton (OpenShift Pipelines) pipelines use IBM API Connect configuration as well as applications API definitions and products source repositories to apply those definitions to an existing IBM API Connect cluster. For the IBM API Connect configuration, this ensures that the exact same configuration of an IBM API Connect cluster can be applied to another IBM API Connect cluster, easing the activities in case of a disaster recovery situation for instance. For your IBM API Connect products and application API definitions for those products and catalogs, this OpenShift Pipelines ensure that any change on those artifacts gets immediately reflected on your IBM API Connect cluster. We need to use OpenShift pipelines for the IBM API Connect configuration and applications API definitions, products and catalogs since these are not yet Kubernetes resources that a GitOps tool such as ArgoCD can automatically watch for changes. ArgoCD (OpenShift GitOps) applications watch a Git config repository for changes. OpenShift GitOps applies the Kubernetes resources thus identified to the cluster, resulting in new or updated Kubernetes resources that represent the changed IBM API Connect cluster and its ecosystem, such as pods, routes etc. In contrast to pipeline runs, OpenShift GitOps changes are durable; they remain as defined unless and until they are explicitly changed or deleted in the GitOps repository. Moreover, if the cluster resources drift from their Git config values, ArgoCD will restore them to their desired values; only changes that are applied to the Git config repository affect the long term state of the cluster. In general, in application development, OpenShift Pipelines and OpenShift GitOps are used to separate Continuous Integration from Continuous Deployment . Often, the Tekton pipeline will perform its changes under a pull-request (PR) to provide an explicit approval mechanism for cluster changes. This is especially important in higher environments such as production which require a formal sign-off. Lower environments such as development often apply successful pipeline runs directly to the Git config repository that are immediately seen by ArgoCD and applied to the cluster. For IBM API connect, the Continuous Integration and Continuous Deployment concepts aren't as clear as in common application development scenarios since you are not building or deploying applications although the API definitions of these, and the product and catalogs they will belong to, could be. We strongly suggest to read over the MQ tutorial in this Cloud Pak Production Deployment Guides for a more traditional application development scenario. In this section of the tutorial, we're going to set up the GitOps repository, and install ArgoCD. More precisely, we are going to Create a Github Organization Download a sample GitOps repository and briefly review it Install ArgoCD Customize ArgoCD Login in to the ArgoCD UI By the end of this topic we'll have a all the basic components in place to perform GitOps in our cluster. Pre-requisites \u00b6 Before attempting this section, you must have completed the previous Create the Cluster section. Downloading the sample GitOps repository \u00b6 In this topic section, we're going to create the GitOps repository for our cluster. We'll use a sample GitOps repository as a starting point; we'll make our own copy and review its contents. Later in the tutorial, we'll customize it for our organization and cluster. ( If you'd like to re-familiarize yourself with git, GitHub or managing repositories, you'll find the following short video and short read helpful. ) Create a new GitHub organization Let's start by creating our own GitHub organization. In your browser, navigate to https://github.com/ . In the upper-right corner, click on the profile photo, and then click Settings . In user settings sidebar, click Organizations . In the \"Organizations\" section, click New organization. Create a new organization. Create the GitOps repositories in the new GitHub organization Let's start by making our own copy of the GitOps repository. In your browser, navigate to https://github.com/cloud-native-toolkit/multi-tenancy-gitops : Click on Use this template at the top RHS of the screen and you'll be asked where you'd like to use this repository: Select your organization, name the repository and click Create repository from template . After a few seconds the repository will be present in your chosen account: You now have a copy of the sample GitOps repository in your GitHub account. Create the infrastructure, services and applications GitOps repositories from the templates into the organization your GitHub organization As shown above, follow similar steps and copy the the repository templates into your organization. GitOps Infrastructure Repository GitOps Services Repository GitOps Application Repository Your organization now will have the below repositories in it. Set up environment variable for GitHub Organization This tutorial uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Replace <git-organization> in the following command: export GIT_ORG = <git-organization> You can verify your $GIT_ORG as follows: echo $GIT_ORG Change to your working git folder Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . We've used this default in the following command, but you may have a different location on your local machine. Change to your working git folder: cd $HOME /git You may need to create this folder if it's not already created. Clone the GitOps repository We're going to work on a local clone of the four repositories. Clone the GitOps repositories to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops'... remote: Enumerating objects: 132, done. remote: Counting objects: 100% (132/132), done. remote: Compressing objects: 100% (54/54), done. remote: Total 132 (delta 72), reused 118 (delta 67), pack-reused 0 Receiving objects: 100% (132/132), 310.92 KiB | 3.24 MiB/s, done. Resolving deltas: 100% (72/72), done. Clone the GitOps Infra repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-infra.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-infra'... remote: Enumerating objects: 62, done. remote: Counting objects: 100% (62/62), done. remote: Compressing objects: 100% (44/44), done. remote: Total 62 (delta 7), reused 53 (delta 3), pack-reused 0 Unpacking objects: 100% (62/62), done. Clone the GitOps Services repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-services.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-services'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Clone the GitOps Applications repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-apps'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Info Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the folder containing the local copy of your new repository: cd multi-tenancy-gitops Review the GitOps folder structure Show the folder structure with the following command: tree . -L 1 This will list all the top level folders and files: . \u251c\u2500\u2500 0-bootstrap \u251c\u2500\u2500 README.md \u251c\u2500\u2500 doc \u251c\u2500\u2500 scripts \u2514\u2500\u2500 setup The main folders perform the following roles: 0-bootstrap contains different profiles that can be used to initialize the cluster ready. We can choose one of the available profiles. scripts and setup contain useful scripts that we'll use during this tutorial. Feel free to explore this structure. We'll examine it in much more detail throughout this tutorial, and you'll become very familiar with it by trying it out. Installing ArgoCD for GitOps \u00b6 Now that we've had an initial view of the GitOps repository, let's install ArgoCD to make use of it. We will install ArgoCD using the Red Hat OpenShift GitOps operator. We will also configure it with the appropriate access rights to limit the resources it can create in the cluster. In so doing, we ensure that ArgoCD acts like a well-governed administrator of the cluster, only doing what it needs to do. Also, we will customize our ArgoCD instance by adding necessary custom checks. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". On successful login, you'll see: Logged into \"https://c100-e.jp-tok.containers.cloud.ibm.com:30749\" as \"IAM#hperabat@in.ibm.com\" using the token provided. You have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\" . This shows some basic information about your user within this cluster. (This user is different to your GitHub user.) Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. The following command creates the subscription: oc apply -f setup/ocp47/ The response confirms that the below resources has been created: clusterrole.rbac.authorization.k8s.io/custom-argocd-cluster-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-cntk-argocd-application-controller created subscription.operators.coreos.com/openshift-gitops-operator created Wait for the ArgoCD installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/gitops-operator-controller-manager -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/gitops-operator condition met ArgoCD is now installed and ready to use. If you see something like: Error from server (NotFound): deployments.apps \"gitops-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Alternatively, you can also validate as follows: while ! oc wait crd applications.argoproj.io --timeout = -1s --for = condition = Established 2 >/dev/null ; do sleep 30 ; done while ! oc wait pod --timeout = -1s --for = condition = Ready -l '!job-name' -n openshift-gitops > /dev/null ; do sleep 30 ; done ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are also setup along with the subscription. oc get clusterrole custom-argocd-cluster-argocd-application-controller oc get clusterrolebinding openshift-gitops-argocd-application-controller oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller You'll see the resources are created in the cluster: $ oc get clusterrole custom-argocd-cluster-argocd-application-controller NAME CREATED AT custom-argocd-cluster-argocd-application-controller 2021-08-27T13:35:13Z $ oc get clusterrolebinding openshift-gitops-argocd-application-controller NAME ROLE AGE openshift-gitops-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m43s $ oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller NAME ROLE AGE openshift-gitops-cntk-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m45s The clusterrole custom-argocd-cluster-argocd-application-controller defines a specific set of specific resources that are required by the ArgoCD service account and access rights over them. The cluster role bindings openshift-gitops-argocd-application-controller and openshift-gitops-cntk-argocd-application-controller binds the ArgoCD service account to the cluster role above. In combination, these definitions limit ArgoCD to perform the minimum set of operations required to manage the cluster. This is important; it means that ArgoCD behaves as a well governed administrator of the cluster. We'll also see later in the tutorial how ArgoCD creates resource in the cluster. Delete default ArgoCD instance Let us now delete the default ArgoCD instance that is created earlier. oc delete gitopsservice cluster -n openshift-gitops || true You will see the resources being deleted as follows: warning: deleting cluster-scoped resources, not scoped to the provided namespace gitopsservice.pipelines.openshift.io \"cluster\" deleted We are deleting the existing default instance and planning to make use of a customized ArgoCD instance with additional capabilities. Create a custom ArgoCD instance This customized ArgoCD instance implements custom checks. By default ArgoCD provides some built-in health checks for validating standard kubernetes resources. However, these checks will not be sufficient to validate the custom resources that belong to IBM Cloud Pak for Integration. Argo CD supports custom health checks that are defined in Lua . If you want explore more about the ArgoCD Resource Health, check this link out. Let us create a custom ArgoCD instance as follows: oc apply -f setup/ocp47/argocd-instance/ -n openshift-gitops The response confirms that the below resources has been created: argocd.argoproj.io/openshift-gitops-cntk created Wait till the ArgoCD instance pod is up and running. Issue the below command to verify the same: while ! oc wait pod --timeout = -1s --for = condition = ContainersReady -l app.kubernetes.io/name = openshift-gitops-cntk-server -n openshift-gitops > /dev/null ; do sleep 30 ; done Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops | grep openshift-gitops-cntk-server | awk '{print \"https://\"$2}' This will list the route to the ArgoCD we've just installed, for example: https://openshift-gitops-cntk-server-openshift-gitops.ibmcloud-roks-xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/openshift-gitops-cntk-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Once the UI launches, you'll see: See how there are no ArgoCD applications active at the moment. In the next section of the tutorial, we'll configure ArgoCD to create the ArgoCD applications that will in turn spin up infrastructure , service , and application resources to apply to the cluster. Congratulations! You have created the GitOps organization based on the sample GitOps repositories that will be used to manage the IBM capabilities and applications that you will install on your cluster. We have also briefly examined the structure of the main GitOps repository driving what will get installed. You have also installed ArgoCD. You have created a specific clusterrole and clusterrolebinding for the ArgoCD service account to ensure that it is able to manage the cluster in a well governed manner. Finally, you have launched the ArgoCD UI; you will make extensive use of it during this tutorial.","title":"GitOps, Tekton & ArgoCD"},{"location":"guides/cp4i/apic/cluster-config/gitops-tekton-argocd/#ibm-api-connect-gitops","text":"","title":"IBM API Connect GitOps"},{"location":"guides/cp4i/apic/cluster-config/gitops-tekton-argocd/#overview","text":"A GitOps model must be at the core of the methodology or approach used to deploy and manage any IBM capability in production. It ensures that the installation, configuration and changes of these IBM capabilities, as well as their configuration and citizens (applications, integration flows, APIs, etc), is only done through code. GitOps puts git repositories and git commands such as git push (to request a change) and git merge (to approve a change) at the heart of configuration management. A GitOps approach helps an organization implement best practices in version control and release governance based on a widely used open standard -- git . The following diagram outlines the major components in an IBM API Connect GitOps model: Tip Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net Notice the clear separation of concerns: Tekton (OpenShift Pipelines) pipelines use IBM API Connect configuration as well as applications API definitions and products source repositories to apply those definitions to an existing IBM API Connect cluster. For the IBM API Connect configuration, this ensures that the exact same configuration of an IBM API Connect cluster can be applied to another IBM API Connect cluster, easing the activities in case of a disaster recovery situation for instance. For your IBM API Connect products and application API definitions for those products and catalogs, this OpenShift Pipelines ensure that any change on those artifacts gets immediately reflected on your IBM API Connect cluster. We need to use OpenShift pipelines for the IBM API Connect configuration and applications API definitions, products and catalogs since these are not yet Kubernetes resources that a GitOps tool such as ArgoCD can automatically watch for changes. ArgoCD (OpenShift GitOps) applications watch a Git config repository for changes. OpenShift GitOps applies the Kubernetes resources thus identified to the cluster, resulting in new or updated Kubernetes resources that represent the changed IBM API Connect cluster and its ecosystem, such as pods, routes etc. In contrast to pipeline runs, OpenShift GitOps changes are durable; they remain as defined unless and until they are explicitly changed or deleted in the GitOps repository. Moreover, if the cluster resources drift from their Git config values, ArgoCD will restore them to their desired values; only changes that are applied to the Git config repository affect the long term state of the cluster. In general, in application development, OpenShift Pipelines and OpenShift GitOps are used to separate Continuous Integration from Continuous Deployment . Often, the Tekton pipeline will perform its changes under a pull-request (PR) to provide an explicit approval mechanism for cluster changes. This is especially important in higher environments such as production which require a formal sign-off. Lower environments such as development often apply successful pipeline runs directly to the Git config repository that are immediately seen by ArgoCD and applied to the cluster. For IBM API connect, the Continuous Integration and Continuous Deployment concepts aren't as clear as in common application development scenarios since you are not building or deploying applications although the API definitions of these, and the product and catalogs they will belong to, could be. We strongly suggest to read over the MQ tutorial in this Cloud Pak Production Deployment Guides for a more traditional application development scenario. In this section of the tutorial, we're going to set up the GitOps repository, and install ArgoCD. More precisely, we are going to Create a Github Organization Download a sample GitOps repository and briefly review it Install ArgoCD Customize ArgoCD Login in to the ArgoCD UI By the end of this topic we'll have a all the basic components in place to perform GitOps in our cluster.","title":"Overview"},{"location":"guides/cp4i/apic/cluster-config/gitops-tekton-argocd/#pre-requisites","text":"Before attempting this section, you must have completed the previous Create the Cluster section.","title":"Pre-requisites"},{"location":"guides/cp4i/apic/cluster-config/gitops-tekton-argocd/#downloading-the-sample-gitops-repository","text":"In this topic section, we're going to create the GitOps repository for our cluster. We'll use a sample GitOps repository as a starting point; we'll make our own copy and review its contents. Later in the tutorial, we'll customize it for our organization and cluster. ( If you'd like to re-familiarize yourself with git, GitHub or managing repositories, you'll find the following short video and short read helpful. ) Create a new GitHub organization Let's start by creating our own GitHub organization. In your browser, navigate to https://github.com/ . In the upper-right corner, click on the profile photo, and then click Settings . In user settings sidebar, click Organizations . In the \"Organizations\" section, click New organization. Create a new organization. Create the GitOps repositories in the new GitHub organization Let's start by making our own copy of the GitOps repository. In your browser, navigate to https://github.com/cloud-native-toolkit/multi-tenancy-gitops : Click on Use this template at the top RHS of the screen and you'll be asked where you'd like to use this repository: Select your organization, name the repository and click Create repository from template . After a few seconds the repository will be present in your chosen account: You now have a copy of the sample GitOps repository in your GitHub account. Create the infrastructure, services and applications GitOps repositories from the templates into the organization your GitHub organization As shown above, follow similar steps and copy the the repository templates into your organization. GitOps Infrastructure Repository GitOps Services Repository GitOps Application Repository Your organization now will have the below repositories in it. Set up environment variable for GitHub Organization This tutorial uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Replace <git-organization> in the following command: export GIT_ORG = <git-organization> You can verify your $GIT_ORG as follows: echo $GIT_ORG Change to your working git folder Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . We've used this default in the following command, but you may have a different location on your local machine. Change to your working git folder: cd $HOME /git You may need to create this folder if it's not already created. Clone the GitOps repository We're going to work on a local clone of the four repositories. Clone the GitOps repositories to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops'... remote: Enumerating objects: 132, done. remote: Counting objects: 100% (132/132), done. remote: Compressing objects: 100% (54/54), done. remote: Total 132 (delta 72), reused 118 (delta 67), pack-reused 0 Receiving objects: 100% (132/132), 310.92 KiB | 3.24 MiB/s, done. Resolving deltas: 100% (72/72), done. Clone the GitOps Infra repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-infra.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-infra'... remote: Enumerating objects: 62, done. remote: Counting objects: 100% (62/62), done. remote: Compressing objects: 100% (44/44), done. remote: Total 62 (delta 7), reused 53 (delta 3), pack-reused 0 Unpacking objects: 100% (62/62), done. Clone the GitOps Services repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-services.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-services'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Clone the GitOps Applications repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-apps'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Info Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the folder containing the local copy of your new repository: cd multi-tenancy-gitops Review the GitOps folder structure Show the folder structure with the following command: tree . -L 1 This will list all the top level folders and files: . \u251c\u2500\u2500 0-bootstrap \u251c\u2500\u2500 README.md \u251c\u2500\u2500 doc \u251c\u2500\u2500 scripts \u2514\u2500\u2500 setup The main folders perform the following roles: 0-bootstrap contains different profiles that can be used to initialize the cluster ready. We can choose one of the available profiles. scripts and setup contain useful scripts that we'll use during this tutorial. Feel free to explore this structure. We'll examine it in much more detail throughout this tutorial, and you'll become very familiar with it by trying it out.","title":"Downloading the sample GitOps repository"},{"location":"guides/cp4i/apic/cluster-config/gitops-tekton-argocd/#installing-argocd-for-gitops","text":"Now that we've had an initial view of the GitOps repository, let's install ArgoCD to make use of it. We will install ArgoCD using the Red Hat OpenShift GitOps operator. We will also configure it with the appropriate access rights to limit the resources it can create in the cluster. In so doing, we ensure that ArgoCD acts like a well-governed administrator of the cluster, only doing what it needs to do. Also, we will customize our ArgoCD instance by adding necessary custom checks. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". On successful login, you'll see: Logged into \"https://c100-e.jp-tok.containers.cloud.ibm.com:30749\" as \"IAM#hperabat@in.ibm.com\" using the token provided. You have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\" . This shows some basic information about your user within this cluster. (This user is different to your GitHub user.) Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. The following command creates the subscription: oc apply -f setup/ocp47/ The response confirms that the below resources has been created: clusterrole.rbac.authorization.k8s.io/custom-argocd-cluster-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-cntk-argocd-application-controller created subscription.operators.coreos.com/openshift-gitops-operator created Wait for the ArgoCD installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/gitops-operator-controller-manager -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/gitops-operator condition met ArgoCD is now installed and ready to use. If you see something like: Error from server (NotFound): deployments.apps \"gitops-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Alternatively, you can also validate as follows: while ! oc wait crd applications.argoproj.io --timeout = -1s --for = condition = Established 2 >/dev/null ; do sleep 30 ; done while ! oc wait pod --timeout = -1s --for = condition = Ready -l '!job-name' -n openshift-gitops > /dev/null ; do sleep 30 ; done ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are also setup along with the subscription. oc get clusterrole custom-argocd-cluster-argocd-application-controller oc get clusterrolebinding openshift-gitops-argocd-application-controller oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller You'll see the resources are created in the cluster: $ oc get clusterrole custom-argocd-cluster-argocd-application-controller NAME CREATED AT custom-argocd-cluster-argocd-application-controller 2021-08-27T13:35:13Z $ oc get clusterrolebinding openshift-gitops-argocd-application-controller NAME ROLE AGE openshift-gitops-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m43s $ oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller NAME ROLE AGE openshift-gitops-cntk-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m45s The clusterrole custom-argocd-cluster-argocd-application-controller defines a specific set of specific resources that are required by the ArgoCD service account and access rights over them. The cluster role bindings openshift-gitops-argocd-application-controller and openshift-gitops-cntk-argocd-application-controller binds the ArgoCD service account to the cluster role above. In combination, these definitions limit ArgoCD to perform the minimum set of operations required to manage the cluster. This is important; it means that ArgoCD behaves as a well governed administrator of the cluster. We'll also see later in the tutorial how ArgoCD creates resource in the cluster. Delete default ArgoCD instance Let us now delete the default ArgoCD instance that is created earlier. oc delete gitopsservice cluster -n openshift-gitops || true You will see the resources being deleted as follows: warning: deleting cluster-scoped resources, not scoped to the provided namespace gitopsservice.pipelines.openshift.io \"cluster\" deleted We are deleting the existing default instance and planning to make use of a customized ArgoCD instance with additional capabilities. Create a custom ArgoCD instance This customized ArgoCD instance implements custom checks. By default ArgoCD provides some built-in health checks for validating standard kubernetes resources. However, these checks will not be sufficient to validate the custom resources that belong to IBM Cloud Pak for Integration. Argo CD supports custom health checks that are defined in Lua . If you want explore more about the ArgoCD Resource Health, check this link out. Let us create a custom ArgoCD instance as follows: oc apply -f setup/ocp47/argocd-instance/ -n openshift-gitops The response confirms that the below resources has been created: argocd.argoproj.io/openshift-gitops-cntk created Wait till the ArgoCD instance pod is up and running. Issue the below command to verify the same: while ! oc wait pod --timeout = -1s --for = condition = ContainersReady -l app.kubernetes.io/name = openshift-gitops-cntk-server -n openshift-gitops > /dev/null ; do sleep 30 ; done Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops | grep openshift-gitops-cntk-server | awk '{print \"https://\"$2}' This will list the route to the ArgoCD we've just installed, for example: https://openshift-gitops-cntk-server-openshift-gitops.ibmcloud-roks-xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/openshift-gitops-cntk-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Once the UI launches, you'll see: See how there are no ArgoCD applications active at the moment. In the next section of the tutorial, we'll configure ArgoCD to create the ArgoCD applications that will in turn spin up infrastructure , service , and application resources to apply to the cluster. Congratulations! You have created the GitOps organization based on the sample GitOps repositories that will be used to manage the IBM capabilities and applications that you will install on your cluster. We have also briefly examined the structure of the main GitOps repository driving what will get installed. You have also installed ArgoCD. You have created a specific clusterrole and clusterrolebinding for the ArgoCD service account to ensure that it is able to manage the cluster in a well governed manner. Finally, you have launched the ArgoCD UI; you will make extensive use of it during this tutorial.","title":"Installing ArgoCD for GitOps"},{"location":"guides/cp4i/apic/cluster-create/ibm-cloud/","text":"Creating a cluster on IBM Cloud \u00b6 Abstract This document explains what are the options to create a Red Hat OpenShift cluster on IBM Cloud. Create a Red Hat OpenShift cluster instance. \u00b6 You can choose either IBM Technology Zone or your IBM Cloud account to create a Red Hat OpenShift cluster. You can use IBM Technology Zone to request a Red Hat OpenShift cluster on IBM Cloud. See the instructions here . The Red Hat OpenShift cluster on IBM Cloud you will be provided with will be hosted on classic infrastructure. You can use your IBM Cloud account to create a new Red Hat OpenShift cluster following these instructions here . You can choose to create your Red Hat OpenShift cluster on IBM Cloud either on classic infrastructure or on Virtual Private Cloud (VPC) infrastructure. Important Make sure you create your Red Hat OpenShift cluster on IBM Cloud with the following specs: OCP Version = 4.7 Worker Size (at least) = 14 CPU x 48 GB Worker Node Count = 3 Note It is important that you are aware and remember what kind of infrastructure, classic or vpc , your Red Hat OpenShift cluster on IBM Cloud has been deployed onto as the type of storage that IBM API Connect will require depends on this. Tools \u00b6 In order to interact with your Red Hat OpenShift cluster(s) and complete this tutorial successfully, we strongly recommend to install the following tools in your workstation. The oc command that matches the version of your cluster. Use these instructions to get the latest version of oc . Use oc version to confirm that you have Client Version: 4.6 or higher Server Version: 4.7 or higher The npm , git , tree and jq commands.","title":"IBM Cloud"},{"location":"guides/cp4i/apic/cluster-create/ibm-cloud/#creating-a-cluster-on-ibm-cloud","text":"Abstract This document explains what are the options to create a Red Hat OpenShift cluster on IBM Cloud.","title":"Creating a cluster on IBM Cloud"},{"location":"guides/cp4i/apic/cluster-create/ibm-cloud/#create-a-red-hat-openshift-cluster-instance","text":"You can choose either IBM Technology Zone or your IBM Cloud account to create a Red Hat OpenShift cluster. You can use IBM Technology Zone to request a Red Hat OpenShift cluster on IBM Cloud. See the instructions here . The Red Hat OpenShift cluster on IBM Cloud you will be provided with will be hosted on classic infrastructure. You can use your IBM Cloud account to create a new Red Hat OpenShift cluster following these instructions here . You can choose to create your Red Hat OpenShift cluster on IBM Cloud either on classic infrastructure or on Virtual Private Cloud (VPC) infrastructure. Important Make sure you create your Red Hat OpenShift cluster on IBM Cloud with the following specs: OCP Version = 4.7 Worker Size (at least) = 14 CPU x 48 GB Worker Node Count = 3 Note It is important that you are aware and remember what kind of infrastructure, classic or vpc , your Red Hat OpenShift cluster on IBM Cloud has been deployed onto as the type of storage that IBM API Connect will require depends on this.","title":"Create a Red Hat OpenShift cluster instance."},{"location":"guides/cp4i/apic/cluster-create/ibm-cloud/#tools","text":"In order to interact with your Red Hat OpenShift cluster(s) and complete this tutorial successfully, we strongly recommend to install the following tools in your workstation. The oc command that matches the version of your cluster. Use these instructions to get the latest version of oc . Use oc version to confirm that you have Client Version: 4.6 or higher Server Version: 4.7 or higher The npm , git , tree and jq commands.","title":"Tools"},{"location":"guides/cp4i/apic/configuration/ibm-cloud/","text":"IBM API Connect configuration on IBM Cloud \u00b6 In the previous chapter of this tutorial, you went through what needs to be done within your GitOps repository in order to get a highly available and production ready IBM API Connect Cluster deployed on a Red Hat OpenShift cluster on IBM Cloud. However, that deployment process, as the name indicates, only deploys the different subsystems that an IBM API Connect Cluster is made of. Overview \u00b6 An IBM API Connect cluster (defined through its APIConnectCluster Custom Resource Definition - CRD), deploys the following subsystems: Management Cluster (ManagementCluster CRD) Portal Cluster (PortalCluster CRD) Gateway Cluster (GatewayCluster CRD) Analytics Cluster (AnalyticsCluster CRD) Tip The reason for the IBM API Connect subsystems to be called clusters is because each of those can be easily deployed highly available by using the profile property. As a result, it is common to see and strongly recommended, specially on production environments, to deploy all of these IBM API Connect subsystems using their highly available profile, which will deploy 3 replicas of each. However, as already introduced, the deployment process will not configure all these IBM API Connect subsystems to work together straight away. Instead, you will need to complete the following IBM API Connect Cloud Manager configuration checklist that will drive you through things like Register a Gateway service, Register an Analytics service, Create a provider organization, etc . Automation \u00b6 Similarly to deploying any piece of software in production, you would like to be able to configure those pieces of software through a GitOps methodology as well. However, the configuration of some of those pieces of software might not yet be enabled be done following a GitOps methodology. Still, you would like to be able to configure any of those pieces of software or components through certain degree of automation. Automation will provide you with two major benefits: auditability and repetition (both of which are inherent to GitOps). Since the configuration of theses pieces of software is done through automation and automation is nothing but code describing actions to be done/taken, that pieces of code that is automating the configuration of a piece of software is auditable and repeatable. These two properties allow you to see how exactly a piece of software has been configured in case you need to adjust anything and also give you that repeatability that provides you with a kind of disaster recovery mechanism that will allow you to configure another instance of that piece of software in the exact same manner as its sibling. Of course, these capabilities are a must in production environments. The automation we are describing and looking for to automate the IBM API Connect Cloud Manager configuration checklist can be achieved thanks to Tekton (OpenShift Pipelines) and, in the particular case of IBM API Connect, the rich set of IBM API Connect administration APIs . With these two, we have managed to create an OpenShift pipeline that will automate the following actions for use to configure a new deployment of an IBM API Connect instance: Configure an email server for notifications. Register the default Gateway Service. Register the default Analytics Service. Associate the default Analytics Service with the default Gateway Service. Register the default Portal Service. Create a Provider Organization. Associate the default Gateway Service with the default Sandbox catalog. Important The IBM API Connect configuration automation provided in this Cloud Pak Production Guides should be use as reference and never as a production ready asset that will suit every client needs. Each client will have a particular set of requirements and architecture and, as a result, the automation presented here can be use as a template or starting point but it is not mean to provide a production ready solution by any means. OpenShift pipeline \u00b6 Let's see how can you create and execute the OpenShift pipeline that will configure your recent deployment of an IBM API Connect instance on IBM Cloud. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Open the 0-bootstrap/single-cluster/1-infra/kustomization.yaml file and un-comment the following resource, if this has not yet been done, in order to create the ci namespace where the OpenShift pipeline will get created and executed from: - argocd/namespace-ci.yaml Open the 0-bootstrap/single-cluster/2-services/kustomization.yaml file and un-comment the following resource in order to install the Red Hat OpenShift Pipelines operator: - argocd/operators/openshift-pipelines.yaml Open the 0-bootstrap/single-cluster/3-apps/kustomization.yaml file and un-comment the following resource in order to create the apps-apic-single-cluster ArgoCD Application that will take care of creating the IBM API Connect Configuration Pipeline (as well as the IBM API Connect Publish Products and APIs pipeline that you will work with in the next chapter of this tutorial): - argocd/apic/single-cluster.yaml Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Creating APIC pipelines\" git push origin $GIT_BRANCH If you check your ArgoCD UI, you should now see few new ArgoCD Applications as a result of the changes above: If you go to Pipelines --> Pipelines on the left hand side menu of your OpenShift web console, you will see that two new OpenShift pipelines have been created as a result of the previous changes to your GitOps repository under the ci Project: Info The pipeline you are interested in for getting your instance of IBM API Connect configured is called apic-post-install-config Before being able to execute the OpenShift pipeline that will configure your instance of IBM API Connect, you need to: Fork the OpenShift Pipeline repositories to your GitHub organization where you already forked the other GitOps repositories you have used to deploy your IBM API Connect Cluster instance. Create a GitHub Personal Access Token so that the IBM API Connect Configuration OpenShift Pipeline has permissions to access your IBM GitHub account (to the private repos you will fork the OpenShift Pipeline repositories to). Provide your email server configuration for notifications. Provide your IBM API Connect provider organization configuration 1. OpenShift Pipeline repositories \u00b6 You need to fork the OpenShift Pipeline repositories to your GitHub organization where you already forked the other GitOps repositories you have used to deploy your IBM API Connect Cluster instance. You can review the creation of that GitHub organization in the Configure the cluster - GitOps, Tekton & ArgoCD section. The OpenShift Pipeline repositories can be found at https://github.com/cloud-native-toolkit . Use the search bar to find the following three repositories: apic-publish-pipeline apic-config-pipeline apic-products-apis-yaml Important Fork ALL three repositories even though you will only use the apic-config-pipeline repository in this section. However, you will use the other two in the following section to publish application APIs in their respective products into IBM API Connect. You can review how to fork a repository in he Configure the cluster - GitOps, Tekton & ArgoCD section. However, make sure your forked repositories are private : Info The reason to make these repositories private is to simulate more closely a real scenario where your assets would be behind a firewall. As a result, the OpenShift Pipelines and the script these execute were developed to expect authentication and authorization mechanisms. By now you should have the following GitHub repositories in your GitHub organization: 2. GitHub Personal Access Token \u00b6 Create a GitHub Personal Access Token so that the IBM API Connect Configuration OpenShift Pipeline has permissions to access your IBM GitHub account (to the private OpenShift Pipeline repositories repos you forked). Provide just repo and admin:repo_hook permissions. You can find the instructions for creating that GitHub Personal Access Token here Once you have created your GitHub Personal Access Token, Ensure you are logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Warning Make sure you are logging into the IBM API Connect Management and Portal cluster Create the following secret containing your GitHub Username and Personal Access Token oc create secret generic apic-pipeline-git -n ci \\ --from-literal = GIT_USERNAME = <your_github_username> \\ --from-literal = GIT_PRIV_TOKEN = <your_github_personal_access_token> 3. Email Server \u00b6 You need to provide the configuration of your email servicer for notifications. You can read more about configuring an email server for notifications in the IBM API Connect documentation here . You can create your own dummy email server at https://mailtrap.io/ . Once you have your email server created and its configuration at hand, create the following secret that will hold your Email Server configuration for notifications: oc create secret generic apic-config-email-server -n ci \\ --from-literal = EMAIL_HOST = <your_email_server_host> \\ --from-literal = EMAIL_PORT = <your_email_server_port> \\ --from-literal = EMAIL_USERNAME = <your_email_server_username> \\ --from-literal = EMAIL_PASSWORD = <your_email_server_password> 4. Provider Organization \u00b6 You must provide the configuration for what your IBM API Connect Provider Organization will be. You can read more about creating a Provider Organization in the IBM API Connect documentation here . Once you have clear the above, create the following secret that will hold your Provider Organization configuration: oc create secret generic apic-pipeline-provider-org -n ci \\ --from-literal = PROV_ORG_OWNER_USERNAME = <provider_organization_owner_username> \\ --from-literal = PROV_ORG_OWNER_PASSWORD = <provider_organization_owner_password> \\ --from-literal = PROV_ORG_OWNER_EMAIL = <provider_organization_owner_email> \\ --from-literal = PROV_ORG_OWNER_FIRST_NAME = <provider_organization_owner_first_name> \\ --from-literal = PROV_ORG_OWNER_LAST_NAME = <provider_organization_owner_last_name> \\ --from-literal = PROV_ORG_NAME = <provider_organization_name> \\ --from-literal = PROV_ORG_TITLE = <provider_organization_title> \\ --from-literal = PROV_ORG_CATALOG_NAME = sandbox \\ --from-literal = PROV_ORG_REALM = provider/default-idp-2 An example of a dummy provider organization could be: PROV_ORG_CATALOG_NAME = sandbox PROV_ORG_NAME = test-org PROV_ORG_OWNER_EMAIL = test@test.com PROV_ORG_OWNER_FIRST_NAME = A_Name PROV_ORG_OWNER_LAST_NAME = A_Last_Name PROV_ORG_OWNER_PASSWORD = passw0rd PROV_ORG_OWNER_USERNAME = testorgadmin PROV_ORG_REALM = provider/default-idp-2 PROV_ORG_TITLE = Test Org Important The PROV_ORG_OWNER_USERNAME and PROV_ORG_OWNER_PASSWORD will be the credentials you will use later on to log into the IBM API Connect API Manager. Run the IBM API Connect Configuration Pipeline \u00b6 You are now ready to execute the OpenShift pipeline that will configure your IBM API Connect instance. Go again to Pipelines --> Pipelines on the left hand side menu of your OpenShift web console and click on the apic-post-install-config pipeline. Then, click on Start on the Actions drop down menu at the top right corner. On the Start Pipeline OpenShift Pipeline configuration menu that pops up, make sure you provide the appropriate values for the following pipeline parameters: git-apic-pipeline-git-url which is the GitHub url where the pipeline scripts are stored. These are the scripts the pipeline will execute. This is the GitHub repository your forked into your GitHub organization earlier in this section. Click Start . This will create a Pipeline Run object that represents this specific execution of the pipeline. You can follow along the execution of this OpenShift Pipeline Run by clicking the Logs tab at the top bar. If this execution of the IBM API Connect Configuration Pipeline finishes successfully you should see an END message at the bottom of the logs as well as a green check mark on the task the Pipeline Run has executed on the left hand side. If you switch back to the Details tab of the Pipeline Run , you should also see green check marks both at the Pipeline Run name at the top and below it in the task that has been executed by this Pipeline Run that is displayed on the Pipeline Run Details section. And in the main Pipelines section of your OpenShift web console you should also see the last Pipeline Run for the IBM API Connect config Pipeline in green Finally, if you open your IBM API Connect Cloud Manager again, you will see that the IBM API Connect Configuration Pipeline has configured a Topology for you with the default IBM API Connect subsystems that get installed so that you now have a working IBM API Connect environment. And in the IBM API Connect API Manager, you can see that the default Gateway service that has been configured in the IBM API Connect Cloud Manager Topology has been registered with the default Sandbox catalog, that gets created for the Provider Organization you specified in the IBM API Connect Configuration Pipeline, so that you can securely publish, expose and access your application APIs. Tip You can open your IBM API Connect API Manager through your Red Hat OpenShift web console. Go to Networking --> Routes under the tools project and click on the Location value for the apic-clust-xxxxx-api-manager route. Then, use the credentials you specified in your provider organization configuration to log in. Important Do not execute the IBM API Connect Configuration Pipeline twice, even if the first attempt failed, as what the first run created must be manually removed for the second attempt to succeed. We do understand this is a limitation of the IBM API Connect Configuration Pipeline and something that could be fixed by implementing a previous cleanup task within the IBM API Connect Configuration Pipeline. However, this is out of the scope of this tutorial and, once again, the assets provided within this tutorial are meant to be a jump-start or a template to start from that should not be treated as production-ready assets. These assets can then be leveraged for a particular client, leaving the specific tailoring of these for the client to the team making use of the assets. Congratulations! You have successfully created and executed the IBM API Connect Configuration Pipeline that configures all of the IBM API Connect subsystems that the IBM API Connect Operator creates for your IBM API Connect instance so that you now have your API Connect instance ready to start working with.","title":"IBM Cloud"},{"location":"guides/cp4i/apic/configuration/ibm-cloud/#ibm-api-connect-configuration-on-ibm-cloud","text":"In the previous chapter of this tutorial, you went through what needs to be done within your GitOps repository in order to get a highly available and production ready IBM API Connect Cluster deployed on a Red Hat OpenShift cluster on IBM Cloud. However, that deployment process, as the name indicates, only deploys the different subsystems that an IBM API Connect Cluster is made of.","title":"IBM API Connect configuration on IBM Cloud"},{"location":"guides/cp4i/apic/configuration/ibm-cloud/#overview","text":"An IBM API Connect cluster (defined through its APIConnectCluster Custom Resource Definition - CRD), deploys the following subsystems: Management Cluster (ManagementCluster CRD) Portal Cluster (PortalCluster CRD) Gateway Cluster (GatewayCluster CRD) Analytics Cluster (AnalyticsCluster CRD) Tip The reason for the IBM API Connect subsystems to be called clusters is because each of those can be easily deployed highly available by using the profile property. As a result, it is common to see and strongly recommended, specially on production environments, to deploy all of these IBM API Connect subsystems using their highly available profile, which will deploy 3 replicas of each. However, as already introduced, the deployment process will not configure all these IBM API Connect subsystems to work together straight away. Instead, you will need to complete the following IBM API Connect Cloud Manager configuration checklist that will drive you through things like Register a Gateway service, Register an Analytics service, Create a provider organization, etc .","title":"Overview"},{"location":"guides/cp4i/apic/configuration/ibm-cloud/#automation","text":"Similarly to deploying any piece of software in production, you would like to be able to configure those pieces of software through a GitOps methodology as well. However, the configuration of some of those pieces of software might not yet be enabled be done following a GitOps methodology. Still, you would like to be able to configure any of those pieces of software or components through certain degree of automation. Automation will provide you with two major benefits: auditability and repetition (both of which are inherent to GitOps). Since the configuration of theses pieces of software is done through automation and automation is nothing but code describing actions to be done/taken, that pieces of code that is automating the configuration of a piece of software is auditable and repeatable. These two properties allow you to see how exactly a piece of software has been configured in case you need to adjust anything and also give you that repeatability that provides you with a kind of disaster recovery mechanism that will allow you to configure another instance of that piece of software in the exact same manner as its sibling. Of course, these capabilities are a must in production environments. The automation we are describing and looking for to automate the IBM API Connect Cloud Manager configuration checklist can be achieved thanks to Tekton (OpenShift Pipelines) and, in the particular case of IBM API Connect, the rich set of IBM API Connect administration APIs . With these two, we have managed to create an OpenShift pipeline that will automate the following actions for use to configure a new deployment of an IBM API Connect instance: Configure an email server for notifications. Register the default Gateway Service. Register the default Analytics Service. Associate the default Analytics Service with the default Gateway Service. Register the default Portal Service. Create a Provider Organization. Associate the default Gateway Service with the default Sandbox catalog. Important The IBM API Connect configuration automation provided in this Cloud Pak Production Guides should be use as reference and never as a production ready asset that will suit every client needs. Each client will have a particular set of requirements and architecture and, as a result, the automation presented here can be use as a template or starting point but it is not mean to provide a production ready solution by any means.","title":"Automation"},{"location":"guides/cp4i/apic/configuration/ibm-cloud/#openshift-pipeline","text":"Let's see how can you create and execute the OpenShift pipeline that will configure your recent deployment of an IBM API Connect instance on IBM Cloud. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Open the 0-bootstrap/single-cluster/1-infra/kustomization.yaml file and un-comment the following resource, if this has not yet been done, in order to create the ci namespace where the OpenShift pipeline will get created and executed from: - argocd/namespace-ci.yaml Open the 0-bootstrap/single-cluster/2-services/kustomization.yaml file and un-comment the following resource in order to install the Red Hat OpenShift Pipelines operator: - argocd/operators/openshift-pipelines.yaml Open the 0-bootstrap/single-cluster/3-apps/kustomization.yaml file and un-comment the following resource in order to create the apps-apic-single-cluster ArgoCD Application that will take care of creating the IBM API Connect Configuration Pipeline (as well as the IBM API Connect Publish Products and APIs pipeline that you will work with in the next chapter of this tutorial): - argocd/apic/single-cluster.yaml Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Creating APIC pipelines\" git push origin $GIT_BRANCH If you check your ArgoCD UI, you should now see few new ArgoCD Applications as a result of the changes above: If you go to Pipelines --> Pipelines on the left hand side menu of your OpenShift web console, you will see that two new OpenShift pipelines have been created as a result of the previous changes to your GitOps repository under the ci Project: Info The pipeline you are interested in for getting your instance of IBM API Connect configured is called apic-post-install-config Before being able to execute the OpenShift pipeline that will configure your instance of IBM API Connect, you need to: Fork the OpenShift Pipeline repositories to your GitHub organization where you already forked the other GitOps repositories you have used to deploy your IBM API Connect Cluster instance. Create a GitHub Personal Access Token so that the IBM API Connect Configuration OpenShift Pipeline has permissions to access your IBM GitHub account (to the private repos you will fork the OpenShift Pipeline repositories to). Provide your email server configuration for notifications. Provide your IBM API Connect provider organization configuration","title":"OpenShift pipeline"},{"location":"guides/cp4i/apic/configuration/ibm-cloud/#1-openshift-pipeline-repositories","text":"You need to fork the OpenShift Pipeline repositories to your GitHub organization where you already forked the other GitOps repositories you have used to deploy your IBM API Connect Cluster instance. You can review the creation of that GitHub organization in the Configure the cluster - GitOps, Tekton & ArgoCD section. The OpenShift Pipeline repositories can be found at https://github.com/cloud-native-toolkit . Use the search bar to find the following three repositories: apic-publish-pipeline apic-config-pipeline apic-products-apis-yaml Important Fork ALL three repositories even though you will only use the apic-config-pipeline repository in this section. However, you will use the other two in the following section to publish application APIs in their respective products into IBM API Connect. You can review how to fork a repository in he Configure the cluster - GitOps, Tekton & ArgoCD section. However, make sure your forked repositories are private : Info The reason to make these repositories private is to simulate more closely a real scenario where your assets would be behind a firewall. As a result, the OpenShift Pipelines and the script these execute were developed to expect authentication and authorization mechanisms. By now you should have the following GitHub repositories in your GitHub organization:","title":"1. OpenShift Pipeline repositories"},{"location":"guides/cp4i/apic/configuration/ibm-cloud/#2-github-personal-access-token","text":"Create a GitHub Personal Access Token so that the IBM API Connect Configuration OpenShift Pipeline has permissions to access your IBM GitHub account (to the private OpenShift Pipeline repositories repos you forked). Provide just repo and admin:repo_hook permissions. You can find the instructions for creating that GitHub Personal Access Token here Once you have created your GitHub Personal Access Token, Ensure you are logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Warning Make sure you are logging into the IBM API Connect Management and Portal cluster Create the following secret containing your GitHub Username and Personal Access Token oc create secret generic apic-pipeline-git -n ci \\ --from-literal = GIT_USERNAME = <your_github_username> \\ --from-literal = GIT_PRIV_TOKEN = <your_github_personal_access_token>","title":"2. GitHub Personal Access Token"},{"location":"guides/cp4i/apic/configuration/ibm-cloud/#3-email-server","text":"You need to provide the configuration of your email servicer for notifications. You can read more about configuring an email server for notifications in the IBM API Connect documentation here . You can create your own dummy email server at https://mailtrap.io/ . Once you have your email server created and its configuration at hand, create the following secret that will hold your Email Server configuration for notifications: oc create secret generic apic-config-email-server -n ci \\ --from-literal = EMAIL_HOST = <your_email_server_host> \\ --from-literal = EMAIL_PORT = <your_email_server_port> \\ --from-literal = EMAIL_USERNAME = <your_email_server_username> \\ --from-literal = EMAIL_PASSWORD = <your_email_server_password>","title":"3. Email Server"},{"location":"guides/cp4i/apic/configuration/ibm-cloud/#4-provider-organization","text":"You must provide the configuration for what your IBM API Connect Provider Organization will be. You can read more about creating a Provider Organization in the IBM API Connect documentation here . Once you have clear the above, create the following secret that will hold your Provider Organization configuration: oc create secret generic apic-pipeline-provider-org -n ci \\ --from-literal = PROV_ORG_OWNER_USERNAME = <provider_organization_owner_username> \\ --from-literal = PROV_ORG_OWNER_PASSWORD = <provider_organization_owner_password> \\ --from-literal = PROV_ORG_OWNER_EMAIL = <provider_organization_owner_email> \\ --from-literal = PROV_ORG_OWNER_FIRST_NAME = <provider_organization_owner_first_name> \\ --from-literal = PROV_ORG_OWNER_LAST_NAME = <provider_organization_owner_last_name> \\ --from-literal = PROV_ORG_NAME = <provider_organization_name> \\ --from-literal = PROV_ORG_TITLE = <provider_organization_title> \\ --from-literal = PROV_ORG_CATALOG_NAME = sandbox \\ --from-literal = PROV_ORG_REALM = provider/default-idp-2 An example of a dummy provider organization could be: PROV_ORG_CATALOG_NAME = sandbox PROV_ORG_NAME = test-org PROV_ORG_OWNER_EMAIL = test@test.com PROV_ORG_OWNER_FIRST_NAME = A_Name PROV_ORG_OWNER_LAST_NAME = A_Last_Name PROV_ORG_OWNER_PASSWORD = passw0rd PROV_ORG_OWNER_USERNAME = testorgadmin PROV_ORG_REALM = provider/default-idp-2 PROV_ORG_TITLE = Test Org Important The PROV_ORG_OWNER_USERNAME and PROV_ORG_OWNER_PASSWORD will be the credentials you will use later on to log into the IBM API Connect API Manager.","title":"4. Provider Organization"},{"location":"guides/cp4i/apic/configuration/ibm-cloud/#run-the-ibm-api-connect-configuration-pipeline","text":"You are now ready to execute the OpenShift pipeline that will configure your IBM API Connect instance. Go again to Pipelines --> Pipelines on the left hand side menu of your OpenShift web console and click on the apic-post-install-config pipeline. Then, click on Start on the Actions drop down menu at the top right corner. On the Start Pipeline OpenShift Pipeline configuration menu that pops up, make sure you provide the appropriate values for the following pipeline parameters: git-apic-pipeline-git-url which is the GitHub url where the pipeline scripts are stored. These are the scripts the pipeline will execute. This is the GitHub repository your forked into your GitHub organization earlier in this section. Click Start . This will create a Pipeline Run object that represents this specific execution of the pipeline. You can follow along the execution of this OpenShift Pipeline Run by clicking the Logs tab at the top bar. If this execution of the IBM API Connect Configuration Pipeline finishes successfully you should see an END message at the bottom of the logs as well as a green check mark on the task the Pipeline Run has executed on the left hand side. If you switch back to the Details tab of the Pipeline Run , you should also see green check marks both at the Pipeline Run name at the top and below it in the task that has been executed by this Pipeline Run that is displayed on the Pipeline Run Details section. And in the main Pipelines section of your OpenShift web console you should also see the last Pipeline Run for the IBM API Connect config Pipeline in green Finally, if you open your IBM API Connect Cloud Manager again, you will see that the IBM API Connect Configuration Pipeline has configured a Topology for you with the default IBM API Connect subsystems that get installed so that you now have a working IBM API Connect environment. And in the IBM API Connect API Manager, you can see that the default Gateway service that has been configured in the IBM API Connect Cloud Manager Topology has been registered with the default Sandbox catalog, that gets created for the Provider Organization you specified in the IBM API Connect Configuration Pipeline, so that you can securely publish, expose and access your application APIs. Tip You can open your IBM API Connect API Manager through your Red Hat OpenShift web console. Go to Networking --> Routes under the tools project and click on the Location value for the apic-clust-xxxxx-api-manager route. Then, use the credentials you specified in your provider organization configuration to log in. Important Do not execute the IBM API Connect Configuration Pipeline twice, even if the first attempt failed, as what the first run created must be manually removed for the second attempt to succeed. We do understand this is a limitation of the IBM API Connect Configuration Pipeline and something that could be fixed by implementing a previous cleanup task within the IBM API Connect Configuration Pipeline. However, this is out of the scope of this tutorial and, once again, the assets provided within this tutorial are meant to be a jump-start or a template to start from that should not be treated as production-ready assets. These assets can then be leveraged for a particular client, leaving the specific tailoring of these for the client to the team making use of the assets. Congratulations! You have successfully created and executed the IBM API Connect Configuration Pipeline that configures all of the IBM API Connect subsystems that the IBM API Connect Operator creates for your IBM API Connect instance so that you now have your API Connect instance ready to start working with.","title":"Run the IBM API Connect Configuration Pipeline"},{"location":"guides/cp4i/apic/deployment/ibm-cloud-old/","text":"Deployment on IBM Cloud \u00b6 Abstract This document describes a highly available production ready deployment of IBM API Connect on a RedHat OpenShift Kubernetes Service (ROKS) cluster on a Virtual Private Cloud (VPC) infrastructure on IBM Cloud using a GitOps approach Overview \u00b6 The official IBM API Connect deployment instructions can be found here . However, we strongly recommend to use a GitOps approach for managing your production environments . That is, any interaction with your production environment will be done through committing changes to that Infrastructure, Configuration, etc as Code that is stored in a SCM repository such as GitHub that describes the desired state of your cluster. We will then leave the task to apply any needed change to our production environment to the GitOps tools, such as the RedHat OpenShift GitOps operator that uses ArgoCD for the mentioned task. To deploy the API Connect production reference architecture on an OpenShift cluster, we are going to use the IBM Cloud Native Toolkit GitOps Framework . There are only five steps you need to take: Prereqs - Make sure you have a RedHat OpenShift cluster and you are able to use the RedHat OpenShift CLI against it. Sealed Secrets - Provide the private key used to seal the secrets provided with the API Connect GitOps repository. RedHat OpenShift GitOps Operator - Install the RedHat OpenShift GitOps operator which provides the GitOps tools needed for installing and managing IBM API Connect instances through the GitOps approach already explained. IBM API Connect - Deploy an instance of IBM API Connect on your cluster. IBM API Connect Cloud Manager - Validate the installation of your IBM API Connect instance by making sure you are able to log into the IBM API Connect Cloud Manager. 1 - Prereqs \u00b6 Get a clean RedHat OpenShift cluster deployed through the RedHat OpenShift Kubernetes Service on IBM Cloud where nothing else has been installed on top afterwards. Log into your RedHat OpenShift cluster through the RedHat OpenShift CLI in order to execute commands to interact with it through your terminal. 2 - Sealed Secrets \u00b6 Create the sealed-secrets project. This project will host the Sealed Secrets operator that will allow us to decrypt sealed secrets stored in GitHub. oc new-project sealed-secrets Download the private key sealed-secrets-ibm-demo-key.yaml used to seal any secret contained in this demonstration and apply it to the cluster. In our case, we have included a demo IBM Entitlement Key within the API Connect GitOps GitHub repository so that we are able to pull down IBM Software. oc apply -f sealed-secrets-ibm-demo-key.yaml Remove the private key from your workstation. rm sealed-secrets-ibm-demo-key.yaml Important Do not check the file into git . The private key MUST NOT be checked into GitHub under any circumstances. Please, remove the private key from your workstation to avoid any issues. 3 - RedHat OpenShift GitOps Operator \u00b6 Clone the following GitHub repository that contains the GitOps structure that the Cloud Native Toolkit GitOps Framework understands. git clone https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-apic.git Change directory into multi-tenancy-gitops-apic . cd multi-tenancy-gitops-apic Warning Checkout the appropriate GitHub branch based on your RedHat OpenShift cluster version, either 4.6 or 4.7 . and the infrastructure it has been deployed onto, either classic or vpc . Choose from: ocp4.6-vpc ocp4.6-classic ocp4.7-vpc ocp4.7-classic Then checkout the appropriate branch by executing git checkout ocp4.6-classic The difference between RedHat OpenShift version 4.6 and 4.7 is that the 4.6 version of RedHat OpenShift installs the RedHat OpenShift Pipelines operator when you install the RedHat OpenShift GitOps operator whereas for the 4.7 version of RedHat OpenShift we must declare explicitly the installation of that RedHat OpenShift pipelines. See here . The difference between vpc and classic infrastructures, where your RedHat OpenShift cluster goes on top of, is that we need to use different storage for our IBM API Connect Cluster. See here and here for a difference of these storageClassName between vpc and classic respectively. For vpc infrastructures we not only need to use block storage but a specific block storage type that allows IBM API Connect to be deployed highly available across multiple availability zones as explained in the beginning of this section. Install the RedHat OpenShift GitOps operator on your RedHat OpenShift cluster and wait for it to be available: If your RedHat OpenShift cluster version is 4.6 oc apply -f setup/ocp46/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done If your RedHat OpenShift cluster version is 4.7 oc apply -f setup/ocp47/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done Once the above command returns, you can open your RedHat OpenShift Web Console and check out that the RedHat OpenShift GitOps operator has been successfully installed in the openshift-gitops project. As you can see in the image, the RedHat OpenShift GitOps operator also installs the RedHat OpenShift Pipelines operator and ArgoCD (which will be that GitOps tool that synchronizes the Infrastructure/Configuration as Code we have stored in GitHub with the state of the RedHat OpenShift cluster). Note The RedHat OpenShift Pipelines operator gets installed by the RedHat OpenShift GitOps Subscription only for RedHat OpenShift version 4.6 . If your RedHat OpenShift cluster is version 4.7, you will need to install the RedHat OpenShift Pipelines operator as part of the GitOps process explained in this section. For getting such RedHat OpenShift Pipelines operator installed, you would need to specify that in the kustomize.yaml file for the services layer you can find in multi-tenancy-gitops-apic/0-bootstrap/argocd/single-cluster/2-services/kustomization.yaml . Open the ArgoCD web console by clicking on the ArgoCD console link you can see at the top of your RedHat OpenShift web console and log in. You can find your ArgoCD login password by executing: If your RedHat OpenShift cluster version is 4.6 oc extract secrets/argocd-cluster-cluster --keys=admin.password -n openshift-gitops --to=- If your RedHat OpenShift cluster version is 4.7 oc extract secrets/openshift-gitops-cluster --keys=admin.password -n openshift-gitops --to=- Once you login, you should see that your ArgoCD web console is empty as we have not deployed any Argo Application yet. 4 - IBM API Connect \u00b6 Install the ArgoCD Bootstrap Application oc apply -n openshift-gitops -f 0-bootstrap/argocd/bootstrap.yaml This ArgoCD Bootstrap Application will bootstrap the deployment of you IBM API Connect cluster based on the configuration you have defined in the GitOps GitHub repository we cloned earlier. You can see that in the GitOps approach used we integrate Kustomize for configuration management. As soon as you create this ArgoCD Bootstrap Application, the rest of the ArgoCD Applications and the respective RedHat Openshift resources these manage start to get created as a result of the synchronization process the GitOps approach is based on. You can see these ArgoCD Applications being created in the ArgoCD web console. After 5-10 minutes, you should see the IBM API Connect operator installed in the openshift-operators project in your RedHat OpenShift web console. You will also see the IBM Cloud Pak foundational services and IBM DataPower Gateway operators that the IBM API Connect operator depends on. If you go back to your ArgoCD web console, you will see that all the ArgoCD Applications are green except from the ArgoCD Applications that are in charge of the actual IBM API Connect cluster instance and its pipelines (will explain later) that are OutOfSync in yellow. This is expected since the resources these ArgoCD Applications manage depend on the IBM API Connect operator that had not been successfully installed at the time when these two ArgoCD Application were created. Synchronize the ArgoCD Applications that are still OutOfSync by clicking on the SYNC APPS button on the top right corner of the ArgoCD web console. Then, select those applications that are OutOfSync and click on Sync at the top. After few seconds you should now see all of the ArgoCD Applications in green. If you go into the ArgoCD Application called apic-prod-instance (or apic-prod-instance-classic if you are installing on a RedHat OpenShift cluster on IBM Cloud classic infrastructure) within the applications ArgoCD Project, you can monitor the IBM API Connect cluster instance installation. You will see how RedHat OpenShift resources are being created as a result of having the apic-prod-instance ArgoCD Application created the initial APIConnectCluster resource, which was then picked up by the IBM API Connect operator that. If you go to Installed Operators --> IBM API Connect --> All Instances under the prod RedHat OpenShift Project, you should see the apic-single-cluster-production (or apic-single-cluster-production if you are installing on a RedHat OpenShift cluster on IBM Cloud classic infrastructure) APIConnectCluster object. As explained in the previous step, this object gets created by the IBM API Connect operator as a result of having the corresponding ArgoCD Application created a APIConnectCluster RedHat OpenShift Resource. If you click on that APIConnectCluster object, you will be presented with the details about this object. In this page, you can see the name of the object, the namespace where it was created, the different attributes of the object but more importantly, you can follow along the installation/deployment of your IBM API Connect Cluster that this object represents by looking at its Phase and State attributes. You can also open a terminal and execute the following command to follow the installation/deployment of your IBM API Connect Cluster along as far as the pods that get created watch -n 10 oc get pods -n prod After 30-40 minutes, the installation/deployment of your IBM API Connect Cluster should be done. If you go back to Installed Operators --> IBM API Connect --> All Instances under the prod RedHat OpenShift Project, you should now be able to match the Custom Resource objects that the IBM API Connect operator has created, as a result of the installation/deployment of your IBM API Connect Cluster, with those IBM API Connect Cluster objects we presented at the top of this section as far as the different components of the IBM API Connect Production Reference Architecture. These objects are the APIConnectCluster , AnalyticsCluster , GatewayCluster , ManagementCluster and PortalCluster (amongst others). 5 - IBM API Connect Cloud Manager \u00b6 Now, let's make sure that our API Connect Cluster is up and running. For doing that we are going to make sure that we can open and access the IBM API Connect Cloud Manager web console. Go to Routes under the prod RedHat OpenShift Project and click on the Location value for the apic-singl-xxxx-admin route. That should bring you to the IBM API Connect Cloud Manager web console login page. Log into the IBM API Connect Cloud Manager web console by using admin as the username and getting it's password with the following command oc get secret `oc get secrets -n prod | grep mgmt-admin-pass | awk '{print $1}'` -n prod -o jsonpath='{.data.password}' | base64 -D Finally, click on the Configure Topology option presented in the IBM API Connect Cloud Manager web console. You should see that there is no topology configured at all. Summary \u00b6 In this section, we have reviewed the IBM API Connect Production Reference Architecture on IBM Cloud and how we can use the IBM Cloud Native Toolkit GitOps Framework to install/deploy and manage our IBM API Connect Cluster instances using a GitOps approach whereby we define/declare the desired status of our infrastructure/clusters/components as code in a SCM tool such as GitHub and then offload the burden of making sure our infrastructure/clusters/components are as specified to GitOps tools such as the RedHat OpenShift GitOps operator which makes use of ArgoCD behind the scenes for that synchronization process. Finally, we have seen how we can verify that our IBM API Connect Cluster instance is up and running and we have seen that such IBM API Connect Cluster instance comes without any configuration at all, not even for the IBM API Connect Cluster components that have been deployed as part of the creation of an APIConnectCluster Custom Resource (i.e. the AnalyticsCluster , GatewayCluster , ManagementCluster and PortalCluster ). Success You have successfully deployed IBM API Connect on the RedHat OpenShift Kubernetes Service (ROKS) on a Virtual Private Cloud (VPC) infrastructure on IBM Cloud using a GitOps approach","title":"Deployment on IBM Cloud"},{"location":"guides/cp4i/apic/deployment/ibm-cloud-old/#deployment-on-ibm-cloud","text":"Abstract This document describes a highly available production ready deployment of IBM API Connect on a RedHat OpenShift Kubernetes Service (ROKS) cluster on a Virtual Private Cloud (VPC) infrastructure on IBM Cloud using a GitOps approach","title":"Deployment on IBM Cloud"},{"location":"guides/cp4i/apic/deployment/ibm-cloud-old/#overview","text":"The official IBM API Connect deployment instructions can be found here . However, we strongly recommend to use a GitOps approach for managing your production environments . That is, any interaction with your production environment will be done through committing changes to that Infrastructure, Configuration, etc as Code that is stored in a SCM repository such as GitHub that describes the desired state of your cluster. We will then leave the task to apply any needed change to our production environment to the GitOps tools, such as the RedHat OpenShift GitOps operator that uses ArgoCD for the mentioned task. To deploy the API Connect production reference architecture on an OpenShift cluster, we are going to use the IBM Cloud Native Toolkit GitOps Framework . There are only five steps you need to take: Prereqs - Make sure you have a RedHat OpenShift cluster and you are able to use the RedHat OpenShift CLI against it. Sealed Secrets - Provide the private key used to seal the secrets provided with the API Connect GitOps repository. RedHat OpenShift GitOps Operator - Install the RedHat OpenShift GitOps operator which provides the GitOps tools needed for installing and managing IBM API Connect instances through the GitOps approach already explained. IBM API Connect - Deploy an instance of IBM API Connect on your cluster. IBM API Connect Cloud Manager - Validate the installation of your IBM API Connect instance by making sure you are able to log into the IBM API Connect Cloud Manager.","title":"Overview"},{"location":"guides/cp4i/apic/deployment/ibm-cloud-old/#1-prereqs","text":"Get a clean RedHat OpenShift cluster deployed through the RedHat OpenShift Kubernetes Service on IBM Cloud where nothing else has been installed on top afterwards. Log into your RedHat OpenShift cluster through the RedHat OpenShift CLI in order to execute commands to interact with it through your terminal.","title":"1 - Prereqs"},{"location":"guides/cp4i/apic/deployment/ibm-cloud-old/#2-sealed-secrets","text":"Create the sealed-secrets project. This project will host the Sealed Secrets operator that will allow us to decrypt sealed secrets stored in GitHub. oc new-project sealed-secrets Download the private key sealed-secrets-ibm-demo-key.yaml used to seal any secret contained in this demonstration and apply it to the cluster. In our case, we have included a demo IBM Entitlement Key within the API Connect GitOps GitHub repository so that we are able to pull down IBM Software. oc apply -f sealed-secrets-ibm-demo-key.yaml Remove the private key from your workstation. rm sealed-secrets-ibm-demo-key.yaml Important Do not check the file into git . The private key MUST NOT be checked into GitHub under any circumstances. Please, remove the private key from your workstation to avoid any issues.","title":"2 - Sealed Secrets"},{"location":"guides/cp4i/apic/deployment/ibm-cloud-old/#3-redhat-openshift-gitops-operator","text":"Clone the following GitHub repository that contains the GitOps structure that the Cloud Native Toolkit GitOps Framework understands. git clone https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-apic.git Change directory into multi-tenancy-gitops-apic . cd multi-tenancy-gitops-apic Warning Checkout the appropriate GitHub branch based on your RedHat OpenShift cluster version, either 4.6 or 4.7 . and the infrastructure it has been deployed onto, either classic or vpc . Choose from: ocp4.6-vpc ocp4.6-classic ocp4.7-vpc ocp4.7-classic Then checkout the appropriate branch by executing git checkout ocp4.6-classic The difference between RedHat OpenShift version 4.6 and 4.7 is that the 4.6 version of RedHat OpenShift installs the RedHat OpenShift Pipelines operator when you install the RedHat OpenShift GitOps operator whereas for the 4.7 version of RedHat OpenShift we must declare explicitly the installation of that RedHat OpenShift pipelines. See here . The difference between vpc and classic infrastructures, where your RedHat OpenShift cluster goes on top of, is that we need to use different storage for our IBM API Connect Cluster. See here and here for a difference of these storageClassName between vpc and classic respectively. For vpc infrastructures we not only need to use block storage but a specific block storage type that allows IBM API Connect to be deployed highly available across multiple availability zones as explained in the beginning of this section. Install the RedHat OpenShift GitOps operator on your RedHat OpenShift cluster and wait for it to be available: If your RedHat OpenShift cluster version is 4.6 oc apply -f setup/ocp46/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done If your RedHat OpenShift cluster version is 4.7 oc apply -f setup/ocp47/ while ! kubectl wait --for=condition=Established crd applications.argoproj.io; do sleep 30; done Once the above command returns, you can open your RedHat OpenShift Web Console and check out that the RedHat OpenShift GitOps operator has been successfully installed in the openshift-gitops project. As you can see in the image, the RedHat OpenShift GitOps operator also installs the RedHat OpenShift Pipelines operator and ArgoCD (which will be that GitOps tool that synchronizes the Infrastructure/Configuration as Code we have stored in GitHub with the state of the RedHat OpenShift cluster). Note The RedHat OpenShift Pipelines operator gets installed by the RedHat OpenShift GitOps Subscription only for RedHat OpenShift version 4.6 . If your RedHat OpenShift cluster is version 4.7, you will need to install the RedHat OpenShift Pipelines operator as part of the GitOps process explained in this section. For getting such RedHat OpenShift Pipelines operator installed, you would need to specify that in the kustomize.yaml file for the services layer you can find in multi-tenancy-gitops-apic/0-bootstrap/argocd/single-cluster/2-services/kustomization.yaml . Open the ArgoCD web console by clicking on the ArgoCD console link you can see at the top of your RedHat OpenShift web console and log in. You can find your ArgoCD login password by executing: If your RedHat OpenShift cluster version is 4.6 oc extract secrets/argocd-cluster-cluster --keys=admin.password -n openshift-gitops --to=- If your RedHat OpenShift cluster version is 4.7 oc extract secrets/openshift-gitops-cluster --keys=admin.password -n openshift-gitops --to=- Once you login, you should see that your ArgoCD web console is empty as we have not deployed any Argo Application yet.","title":"3 - RedHat OpenShift GitOps Operator"},{"location":"guides/cp4i/apic/deployment/ibm-cloud-old/#4-ibm-api-connect","text":"Install the ArgoCD Bootstrap Application oc apply -n openshift-gitops -f 0-bootstrap/argocd/bootstrap.yaml This ArgoCD Bootstrap Application will bootstrap the deployment of you IBM API Connect cluster based on the configuration you have defined in the GitOps GitHub repository we cloned earlier. You can see that in the GitOps approach used we integrate Kustomize for configuration management. As soon as you create this ArgoCD Bootstrap Application, the rest of the ArgoCD Applications and the respective RedHat Openshift resources these manage start to get created as a result of the synchronization process the GitOps approach is based on. You can see these ArgoCD Applications being created in the ArgoCD web console. After 5-10 minutes, you should see the IBM API Connect operator installed in the openshift-operators project in your RedHat OpenShift web console. You will also see the IBM Cloud Pak foundational services and IBM DataPower Gateway operators that the IBM API Connect operator depends on. If you go back to your ArgoCD web console, you will see that all the ArgoCD Applications are green except from the ArgoCD Applications that are in charge of the actual IBM API Connect cluster instance and its pipelines (will explain later) that are OutOfSync in yellow. This is expected since the resources these ArgoCD Applications manage depend on the IBM API Connect operator that had not been successfully installed at the time when these two ArgoCD Application were created. Synchronize the ArgoCD Applications that are still OutOfSync by clicking on the SYNC APPS button on the top right corner of the ArgoCD web console. Then, select those applications that are OutOfSync and click on Sync at the top. After few seconds you should now see all of the ArgoCD Applications in green. If you go into the ArgoCD Application called apic-prod-instance (or apic-prod-instance-classic if you are installing on a RedHat OpenShift cluster on IBM Cloud classic infrastructure) within the applications ArgoCD Project, you can monitor the IBM API Connect cluster instance installation. You will see how RedHat OpenShift resources are being created as a result of having the apic-prod-instance ArgoCD Application created the initial APIConnectCluster resource, which was then picked up by the IBM API Connect operator that. If you go to Installed Operators --> IBM API Connect --> All Instances under the prod RedHat OpenShift Project, you should see the apic-single-cluster-production (or apic-single-cluster-production if you are installing on a RedHat OpenShift cluster on IBM Cloud classic infrastructure) APIConnectCluster object. As explained in the previous step, this object gets created by the IBM API Connect operator as a result of having the corresponding ArgoCD Application created a APIConnectCluster RedHat OpenShift Resource. If you click on that APIConnectCluster object, you will be presented with the details about this object. In this page, you can see the name of the object, the namespace where it was created, the different attributes of the object but more importantly, you can follow along the installation/deployment of your IBM API Connect Cluster that this object represents by looking at its Phase and State attributes. You can also open a terminal and execute the following command to follow the installation/deployment of your IBM API Connect Cluster along as far as the pods that get created watch -n 10 oc get pods -n prod After 30-40 minutes, the installation/deployment of your IBM API Connect Cluster should be done. If you go back to Installed Operators --> IBM API Connect --> All Instances under the prod RedHat OpenShift Project, you should now be able to match the Custom Resource objects that the IBM API Connect operator has created, as a result of the installation/deployment of your IBM API Connect Cluster, with those IBM API Connect Cluster objects we presented at the top of this section as far as the different components of the IBM API Connect Production Reference Architecture. These objects are the APIConnectCluster , AnalyticsCluster , GatewayCluster , ManagementCluster and PortalCluster (amongst others).","title":"4 - IBM API Connect"},{"location":"guides/cp4i/apic/deployment/ibm-cloud-old/#5-ibm-api-connect-cloud-manager","text":"Now, let's make sure that our API Connect Cluster is up and running. For doing that we are going to make sure that we can open and access the IBM API Connect Cloud Manager web console. Go to Routes under the prod RedHat OpenShift Project and click on the Location value for the apic-singl-xxxx-admin route. That should bring you to the IBM API Connect Cloud Manager web console login page. Log into the IBM API Connect Cloud Manager web console by using admin as the username and getting it's password with the following command oc get secret `oc get secrets -n prod | grep mgmt-admin-pass | awk '{print $1}'` -n prod -o jsonpath='{.data.password}' | base64 -D Finally, click on the Configure Topology option presented in the IBM API Connect Cloud Manager web console. You should see that there is no topology configured at all.","title":"5 - IBM API Connect Cloud Manager"},{"location":"guides/cp4i/apic/deployment/ibm-cloud-old/#summary","text":"In this section, we have reviewed the IBM API Connect Production Reference Architecture on IBM Cloud and how we can use the IBM Cloud Native Toolkit GitOps Framework to install/deploy and manage our IBM API Connect Cluster instances using a GitOps approach whereby we define/declare the desired status of our infrastructure/clusters/components as code in a SCM tool such as GitHub and then offload the burden of making sure our infrastructure/clusters/components are as specified to GitOps tools such as the RedHat OpenShift GitOps operator which makes use of ArgoCD behind the scenes for that synchronization process. Finally, we have seen how we can verify that our IBM API Connect Cluster instance is up and running and we have seen that such IBM API Connect Cluster instance comes without any configuration at all, not even for the IBM API Connect Cluster components that have been deployed as part of the creation of an APIConnectCluster Custom Resource (i.e. the AnalyticsCluster , GatewayCluster , ManagementCluster and PortalCluster ). Success You have successfully deployed IBM API Connect on the RedHat OpenShift Kubernetes Service (ROKS) on a Virtual Private Cloud (VPC) infrastructure on IBM Cloud using a GitOps approach","title":"Summary"},{"location":"guides/cp4i/apic/deployment/ibm-cloud/","text":"Deployment on IBM Cloud \u00b6 In the previous chapter of this tutorial, we have worked with ArgoCD and the GitOps repository to understand what these are and how these work together and interact. We have seen how to create ArgoCD applications that watch their respective GitOps repository folders for details of the resources they should apply to the cluster. We have seen how we can dynamically change deployed resources by updating the resource definition in the GitOps repository. Finally, you've experience how ArgoCD keeps the cluster synchronized with the GitOps repository as a source of truth; any unexpected configuration drift will be corrected without intervention. Now, in this section, we are going to look at what changes we need to do to that GitOps repository so that we get IBM API Connect and all the components it needs/depends on deployed in our cluster for having a highly available production ready deployment of IBM API Connect. Services \u00b6 We have seen in the previous chapter of this tutorial that in the 0-bootstrap/single-cluster/kustomization.yaml file we have defined what layers out of infra , services and apps we want the main bootstrap-single-cluster ArgoCD application to watch. Before, we had it set up to watch only the infra layer. However, we now need it to watch the services and apps layers too in order to deploy IBM API Connect and all the components it needs/depends on. Ensure you are logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Make sure the infra , services and apps layers are un-commented, and therefore active, for the main bootstrap-single-cluster ArgoCD application to watch them in the file 0-bootstrap/single-cluster/kustomization.yaml resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml - 3-apps/3-apps.yaml Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Adding services and applications layers\" git push origin $GIT_BRANCH If you go to your ArgoCD UI, you will see two new ArgoCD applications, one that will watch for resources on the services layer and another that will watch for resources on the apps layer. Tip You might want to manually sync your ArgoCD applications, instead of waiting for changes to get automatically picked up. For manually synchronize ArgoCD applications, you can click on the SYNC APPS button at the top. Then, select all the ArgoCD applications you want to get synched (or all for easiness) and click SYNC We are now all set to start deploying IBM API Connect and all the components it needs/depends on. However, we will do it in two steps. In the first one, we will deploy all of the components that IBM API Connect needs/depends on. These are things like an instance of Sealed Secrets to be able to unseal certain secrets needed like the IBM Entitlement Key to pull IBM software down from IBM's software registry, the IBM Operators catalog to be able to install IBM Operators and, finally, the IBM Foundations, IBM DataPower and IBM API Connect operators where the first two are a dependency of the IBM API Connect operator. To get all of this installed, all we need to do, in the same fashion we did for the components we wanted to get installed on the infra layer, is to un-comment these from the 0-bootstrap/single-cluster/2-services/kustomization.yaml file. Open the 0-bootstrap/single-cluster/2-services/kustomization.yaml file and un-comment the following resources: - argocd/operators/ibm-apic-operator.yaml - argocd/operators/ibm-datapower-operator.yaml - argocd/operators/ibm-foundations.yaml - argocd/operators/ibm-catalogs.yaml - argocd/instances/sealed-secrets.yaml Your 0-bootstrap/single-cluster/2-services/kustomization.yaml should look like resources : # IBM Software #- argocd/operators/ibm-ace-operator.yaml - argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml - argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml #- argocd/operators/ibm-cp4a-operator.yaml # IBM Foundations / Common Services - argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml # IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount #- argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Installing services\" git push origin $GIT_BRANCH If you go to your ArgoCD UI, you will now see new ArgoCD applications. If you click on the services main ArgoCD application, you will see that it has created five new ArgoCD applications that correspond to each of the components we have un-commented and therefore bring to active state in the 0-bootstrap/single-cluster/2-services/kustomization.yaml file. After 5-10 mins, if you go to your Red Hat OpenShift web console and click on Operators --> Installed Operators on the right hand side menu and select the the tools project on the pull down menu at the top bar, you will see that the IBM Foundations, IBM DataPower and IBM API Connect operators are being installed or have successfully installed already (apart from the OpenShift GitOps operator that was installed previously). IBM API Connect \u00b6 Last step, is to get an IBM API Connect cluster created through the IBM API Connect operator. However, before being able to deploy any IBM capability we must have an IBM Entitlement Key to be able to pull IBM software down from IBM's software registry available in the Red Hat OpenShift project where we are deploying such IBM capability. Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. Create a secret containing your IBM Entitlement Key in the tools namespace (which is where the IBM API Connect operator has been installed/deployed into and there the IBM API Connect instance it will create afterwards will end up into as a result). oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io Link that docker-registry secret containing your IBM Entitlement Key with the default secret for pulling Docker images within your Red Hat OpenShift project oc secrets link default ibm-entitlement-key --for = pull Finally, to get an IBM API Connect cluster instance created, all we need to do is to make sure that the definition of the IBM API Connect instance we want to deploy is correct on our GitOps repository and then, once again, tell the ArgoCD application that is watching over the services layer to activate such resource. This resource will, in turn, create another ArgoCD application that will watch over the resources specified in our definition of the IBM API Connect instance. Info The IBM API Connect Cluster name might have changed from what the screenshots below show. Therefore, expect that apic-single-cluster-production-classic might not be the name for the IBM API Connect Cluster being deployed in your cluster. Instead, it might be something like apic-cluster . Open the 0-bootstrap/single-cluster/2-services/kustomization.yaml file and un-comment the following resource: - argocd/instances/ibm-apic-instance.yaml Make sure the storage settings for the IBM API Connect instance we are about to deploy are correct based on the IBM Cloud infrastructure your Red Hat OpenShift cluster is deployed on (review the Create the cluster section if needed). You can adjust storage settings in in 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-apic-instance.yaml Storage Make sure the storageClassName value, which defaults to ibmc-block-gold , corresponds to an available block storage class in your cluster. Make sure you understand block storage for Red Hat OpenShift clusters on IBM Cloud by reading IBM Cloud documentation here specially for clusters hosted on Virtual Private Clouds (VPC). Also, make sure you understand IBM API Connect production reference architecture on IBM Cloud here . Make sure the high availability settings for the IBM API Connect instance we are about to deploy are correct based on your requirements (and cluster sizing. Review the Create the cluster section if needed). You can adjust storage settings in in 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-apic-instance.yaml High availability Make sure the profile value, which defaults to n3xc14.m48 , corresponds to the desired profile: development vs production. n1xc10.m48 - Deploys 1 replica of each pod, so this profile is most suitable for a small, non-HA system. Recommended use of this profile is for development and testing. n3xc14.m48 - Deploys 3 or more replicas of each pod, so this profile is most suitable for larger systems and for production environments. This profile is supported for installation on a cluster with three or more nodes. It is not supported on a cluster with fewer than three nodes. Important: Make sure the Red Hat OpenShift cluster you are deploying this IBM API Connect recipe to has been sized appropriately based on the profiles above where: n stands for the number of worker nodes. c stands for the amount of CPU per worker node. m stands for the amount of RAM per worker node. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Installing an IBM API Connect instance\" git push origin $GIT_BRANCH If you go to your ArgoCD UI, you will now see the new ibm-apic-instance ArgoCD application. If you go into that ArgoCD application, you can monitor the IBM API Connect cluster instance installation. You will see how RedHat OpenShift resources are being created as a result of having the ibm-apic-instance ArgoCD application created the initial APIConnectCluster resource, which was then picked up by the IBM API Connect operator. If you go to Operators --> Installed Operators under the tools project, click on the IBM API Connect operator and then on the All Instances tab, you should see the apic-cluster APIConnectCluster object. If you click on that APIConnectCluster object, you will be presented with the details about this object. In this page, you can see the name of the object, the namespace where it was created, the different attributes of the object but more importantly, you can follow along the installation/deployment of your IBM API Connect Cluster that this object represents by looking at its Phase and State attributes. You can also open a terminal and execute the following command to follow the installation/deployment of your IBM API Connect Cluster along as far as the pods that get created watch -n 10 oc get pods -n tools After 30-40 minutes, the installation/deployment of your IBM API Connect Cluster should be done. If you go back to Operators --> Installed Operators under the tools project, click on the IBM API Connect operator and then on the All Instances tab, you should now be able to match the Custom Resource objects that the IBM API Connect operator has created, as a result of the installation/deployment of your IBM API Connect Cluster, with those IBM API Connect Cluster components presented at the IBM API Connect Production Reference Architecture . These objects are the APIConnectCluster , AnalyticsCluster , GatewayCluster , ManagementCluster and PortalCluster (amongst others). IBM API Connect Cloud Manager \u00b6 Now, let's make sure that our API Connect Cluster is up and running. For doing that, we are going to make sure that we can open and access the IBM API Connect Cloud Manager web console. In your Red Hat OpenShift we console, go to Networking --> Routes under the tools project and click on the Location value for the apic-clust-xxxx-admin route. That should bring you to the IBM API Connect Cloud Manager web console login page. Log into the IBM API Connect Cloud Manager web console by using admin as the username and getting it's password with the following command oc get secret `oc get secrets -n tools | grep mgmt-admin-pass | awk '{print $1}'` -n tools -o jsonpath='{.data.password}' | base64 -D Finally, click on the Configure Topology option presented in the IBM API Connect Cloud Manager web console. You should see that there is no topology configured at all. You can follow the IBM API Connect Cloud Manager configuration checklist documentation to manually proceed with the tasks you need to accomplish to get your IBM API Connect cluster ready to be used or you can go to the next section in this chapter where you will create an OpenShift Pipeline that gets your recently deployed IBM API Connect instance configured so that you can start working with it right away. Congratulations! You have successfully deployed a highly available production ready IBM API Connect cluster on your RedHat OpenShift cluster on IBM Cloud using a GitOps methodology","title":"IBM Cloud"},{"location":"guides/cp4i/apic/deployment/ibm-cloud/#deployment-on-ibm-cloud","text":"In the previous chapter of this tutorial, we have worked with ArgoCD and the GitOps repository to understand what these are and how these work together and interact. We have seen how to create ArgoCD applications that watch their respective GitOps repository folders for details of the resources they should apply to the cluster. We have seen how we can dynamically change deployed resources by updating the resource definition in the GitOps repository. Finally, you've experience how ArgoCD keeps the cluster synchronized with the GitOps repository as a source of truth; any unexpected configuration drift will be corrected without intervention. Now, in this section, we are going to look at what changes we need to do to that GitOps repository so that we get IBM API Connect and all the components it needs/depends on deployed in our cluster for having a highly available production ready deployment of IBM API Connect.","title":"Deployment on IBM Cloud"},{"location":"guides/cp4i/apic/deployment/ibm-cloud/#services","text":"We have seen in the previous chapter of this tutorial that in the 0-bootstrap/single-cluster/kustomization.yaml file we have defined what layers out of infra , services and apps we want the main bootstrap-single-cluster ArgoCD application to watch. Before, we had it set up to watch only the infra layer. However, we now need it to watch the services and apps layers too in order to deploy IBM API Connect and all the components it needs/depends on. Ensure you are logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Make sure the infra , services and apps layers are un-commented, and therefore active, for the main bootstrap-single-cluster ArgoCD application to watch them in the file 0-bootstrap/single-cluster/kustomization.yaml resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml - 3-apps/3-apps.yaml Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Adding services and applications layers\" git push origin $GIT_BRANCH If you go to your ArgoCD UI, you will see two new ArgoCD applications, one that will watch for resources on the services layer and another that will watch for resources on the apps layer. Tip You might want to manually sync your ArgoCD applications, instead of waiting for changes to get automatically picked up. For manually synchronize ArgoCD applications, you can click on the SYNC APPS button at the top. Then, select all the ArgoCD applications you want to get synched (or all for easiness) and click SYNC We are now all set to start deploying IBM API Connect and all the components it needs/depends on. However, we will do it in two steps. In the first one, we will deploy all of the components that IBM API Connect needs/depends on. These are things like an instance of Sealed Secrets to be able to unseal certain secrets needed like the IBM Entitlement Key to pull IBM software down from IBM's software registry, the IBM Operators catalog to be able to install IBM Operators and, finally, the IBM Foundations, IBM DataPower and IBM API Connect operators where the first two are a dependency of the IBM API Connect operator. To get all of this installed, all we need to do, in the same fashion we did for the components we wanted to get installed on the infra layer, is to un-comment these from the 0-bootstrap/single-cluster/2-services/kustomization.yaml file. Open the 0-bootstrap/single-cluster/2-services/kustomization.yaml file and un-comment the following resources: - argocd/operators/ibm-apic-operator.yaml - argocd/operators/ibm-datapower-operator.yaml - argocd/operators/ibm-foundations.yaml - argocd/operators/ibm-catalogs.yaml - argocd/instances/sealed-secrets.yaml Your 0-bootstrap/single-cluster/2-services/kustomization.yaml should look like resources : # IBM Software #- argocd/operators/ibm-ace-operator.yaml - argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml - argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml #- argocd/operators/ibm-cp4a-operator.yaml # IBM Foundations / Common Services - argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml # IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount #- argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Installing services\" git push origin $GIT_BRANCH If you go to your ArgoCD UI, you will now see new ArgoCD applications. If you click on the services main ArgoCD application, you will see that it has created five new ArgoCD applications that correspond to each of the components we have un-commented and therefore bring to active state in the 0-bootstrap/single-cluster/2-services/kustomization.yaml file. After 5-10 mins, if you go to your Red Hat OpenShift web console and click on Operators --> Installed Operators on the right hand side menu and select the the tools project on the pull down menu at the top bar, you will see that the IBM Foundations, IBM DataPower and IBM API Connect operators are being installed or have successfully installed already (apart from the OpenShift GitOps operator that was installed previously).","title":"Services"},{"location":"guides/cp4i/apic/deployment/ibm-cloud/#ibm-api-connect","text":"Last step, is to get an IBM API Connect cluster created through the IBM API Connect operator. However, before being able to deploy any IBM capability we must have an IBM Entitlement Key to be able to pull IBM software down from IBM's software registry available in the Red Hat OpenShift project where we are deploying such IBM capability. Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. Create a secret containing your IBM Entitlement Key in the tools namespace (which is where the IBM API Connect operator has been installed/deployed into and there the IBM API Connect instance it will create afterwards will end up into as a result). oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io Link that docker-registry secret containing your IBM Entitlement Key with the default secret for pulling Docker images within your Red Hat OpenShift project oc secrets link default ibm-entitlement-key --for = pull Finally, to get an IBM API Connect cluster instance created, all we need to do is to make sure that the definition of the IBM API Connect instance we want to deploy is correct on our GitOps repository and then, once again, tell the ArgoCD application that is watching over the services layer to activate such resource. This resource will, in turn, create another ArgoCD application that will watch over the resources specified in our definition of the IBM API Connect instance. Info The IBM API Connect Cluster name might have changed from what the screenshots below show. Therefore, expect that apic-single-cluster-production-classic might not be the name for the IBM API Connect Cluster being deployed in your cluster. Instead, it might be something like apic-cluster . Open the 0-bootstrap/single-cluster/2-services/kustomization.yaml file and un-comment the following resource: - argocd/instances/ibm-apic-instance.yaml Make sure the storage settings for the IBM API Connect instance we are about to deploy are correct based on the IBM Cloud infrastructure your Red Hat OpenShift cluster is deployed on (review the Create the cluster section if needed). You can adjust storage settings in in 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-apic-instance.yaml Storage Make sure the storageClassName value, which defaults to ibmc-block-gold , corresponds to an available block storage class in your cluster. Make sure you understand block storage for Red Hat OpenShift clusters on IBM Cloud by reading IBM Cloud documentation here specially for clusters hosted on Virtual Private Clouds (VPC). Also, make sure you understand IBM API Connect production reference architecture on IBM Cloud here . Make sure the high availability settings for the IBM API Connect instance we are about to deploy are correct based on your requirements (and cluster sizing. Review the Create the cluster section if needed). You can adjust storage settings in in 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-apic-instance.yaml High availability Make sure the profile value, which defaults to n3xc14.m48 , corresponds to the desired profile: development vs production. n1xc10.m48 - Deploys 1 replica of each pod, so this profile is most suitable for a small, non-HA system. Recommended use of this profile is for development and testing. n3xc14.m48 - Deploys 3 or more replicas of each pod, so this profile is most suitable for larger systems and for production environments. This profile is supported for installation on a cluster with three or more nodes. It is not supported on a cluster with fewer than three nodes. Important: Make sure the Red Hat OpenShift cluster you are deploying this IBM API Connect recipe to has been sized appropriately based on the profiles above where: n stands for the number of worker nodes. c stands for the amount of CPU per worker node. m stands for the amount of RAM per worker node. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Installing an IBM API Connect instance\" git push origin $GIT_BRANCH If you go to your ArgoCD UI, you will now see the new ibm-apic-instance ArgoCD application. If you go into that ArgoCD application, you can monitor the IBM API Connect cluster instance installation. You will see how RedHat OpenShift resources are being created as a result of having the ibm-apic-instance ArgoCD application created the initial APIConnectCluster resource, which was then picked up by the IBM API Connect operator. If you go to Operators --> Installed Operators under the tools project, click on the IBM API Connect operator and then on the All Instances tab, you should see the apic-cluster APIConnectCluster object. If you click on that APIConnectCluster object, you will be presented with the details about this object. In this page, you can see the name of the object, the namespace where it was created, the different attributes of the object but more importantly, you can follow along the installation/deployment of your IBM API Connect Cluster that this object represents by looking at its Phase and State attributes. You can also open a terminal and execute the following command to follow the installation/deployment of your IBM API Connect Cluster along as far as the pods that get created watch -n 10 oc get pods -n tools After 30-40 minutes, the installation/deployment of your IBM API Connect Cluster should be done. If you go back to Operators --> Installed Operators under the tools project, click on the IBM API Connect operator and then on the All Instances tab, you should now be able to match the Custom Resource objects that the IBM API Connect operator has created, as a result of the installation/deployment of your IBM API Connect Cluster, with those IBM API Connect Cluster components presented at the IBM API Connect Production Reference Architecture . These objects are the APIConnectCluster , AnalyticsCluster , GatewayCluster , ManagementCluster and PortalCluster (amongst others).","title":"IBM API Connect"},{"location":"guides/cp4i/apic/deployment/ibm-cloud/#ibm-api-connect-cloud-manager","text":"Now, let's make sure that our API Connect Cluster is up and running. For doing that, we are going to make sure that we can open and access the IBM API Connect Cloud Manager web console. In your Red Hat OpenShift we console, go to Networking --> Routes under the tools project and click on the Location value for the apic-clust-xxxx-admin route. That should bring you to the IBM API Connect Cloud Manager web console login page. Log into the IBM API Connect Cloud Manager web console by using admin as the username and getting it's password with the following command oc get secret `oc get secrets -n tools | grep mgmt-admin-pass | awk '{print $1}'` -n tools -o jsonpath='{.data.password}' | base64 -D Finally, click on the Configure Topology option presented in the IBM API Connect Cloud Manager web console. You should see that there is no topology configured at all. You can follow the IBM API Connect Cloud Manager configuration checklist documentation to manually proceed with the tasks you need to accomplish to get your IBM API Connect cluster ready to be used or you can go to the next section in this chapter where you will create an OpenShift Pipeline that gets your recently deployed IBM API Connect instance configured so that you can start working with it right away. Congratulations! You have successfully deployed a highly available production ready IBM API Connect cluster on your RedHat OpenShift cluster on IBM Cloud using a GitOps methodology","title":"IBM API Connect Cloud Manager"},{"location":"guides/cp4i/apic/disaster-recovery/overview/","text":"IBM API Connect Disaster Recovery on OpenShift \u00b6 Abstract Disaster Recovery is one of the major concerns when implementing a Production instance of any software solution and API Connect on OpenShift is no different. It is important that you correctly prepare for any disaster in order to ensure that you are able to recover configuration and data if disaster does strike. High availability versus disaster recovery \u00b6 High availability is concerned with configuring a system so that it continues to operate, and the users and any using systems don't experience any loss of service, in the event of hardware or software failures. Whereas, disaster recovery (DR) is concerned with the configurations and procedures that allow a system to be recovered following a catastrophic hardware, software, or operator failure that's led to a loss of service. The following two important metrics should be considered for DR solutions: Recovery Time Objective (RTO) - The RTO is the time that it is acceptable for a system to be unavailable for during a disaster. Recovery Point Objective (RPO) - As DR solutions are usually based on some sort of data copy or backup, it is possible that a system might be recovered to a state prior to the disaster occurring, rather than the state it was in at the actual instant of the disaster. The RPO measures how far back in time the recovery point will be, and therefore how much data might be lost. An RPO of zero would assert that no data will be lost, but such a solution is often a compromise against the cost and performance of the system. To achieve high availability in your API Connect deployment, a minimum of three data centers are required (see the IBM API Connect High Availability section here ). This configuration creates a quorum of data centers, allowing automated failover in any direction, and enabling all three data centers to be active. The quorum majority voting algorithm, allows for a single data center to be offline, and yet still maintain data consistency and availability across the remaining two data centers as they continue to represent a majority in the deployment (avoiding split-brain syndrome). However, when having three data centers is not possible, a two data center deployment provides a disaster recovery solution that has both a low Recovery Time Objective (RTO), and a low Recovery Point Objective (RPO): The tables below describe the worst-case scenario for availability of each of the IBM API Connect components: You can follow the IBM API Connect documentation on the two data center disaster recovery deployment strategy on OpenShift here Warning Make sure you read and understand the the requirements for deploying a two data center disaster recovery (DR) solution. For instance, for this scenario to work, the latency between the two data center must be less than 80ms . From the above website you can find links to the instructions for, How to install a two data center deployment . Maintain a two data center deployment which includes information about normal operation data flows for the warm components of the passive cluster. There are other IBM API Connect HA/DR topologies that can be achieved when the requirements for the two data center disaster recovery (DR) solution explained here can not be met. These are explained in further detail in the IBM API Connect v10.x Deployment WhitePaper written by Chris Phillips. For some of those IBM API Connect HA/DR solutions as well as for non-kubernetes deployments, manual backup and synchronization processes might be involved. You can read more detailed information about those manual processes for how to prepare for a disaster and how to recover from a disaster in the IBM API Connect official documentation here . Remember that if your IBM API Connect deployment is hosted on a cloud provider, you might be able to make use of some of the data replication mechanisms for storage.","title":"Disaster Recovery"},{"location":"guides/cp4i/apic/disaster-recovery/overview/#ibm-api-connect-disaster-recovery-on-openshift","text":"Abstract Disaster Recovery is one of the major concerns when implementing a Production instance of any software solution and API Connect on OpenShift is no different. It is important that you correctly prepare for any disaster in order to ensure that you are able to recover configuration and data if disaster does strike.","title":"IBM API Connect Disaster Recovery on OpenShift"},{"location":"guides/cp4i/apic/disaster-recovery/overview/#high-availability-versus-disaster-recovery","text":"High availability is concerned with configuring a system so that it continues to operate, and the users and any using systems don't experience any loss of service, in the event of hardware or software failures. Whereas, disaster recovery (DR) is concerned with the configurations and procedures that allow a system to be recovered following a catastrophic hardware, software, or operator failure that's led to a loss of service. The following two important metrics should be considered for DR solutions: Recovery Time Objective (RTO) - The RTO is the time that it is acceptable for a system to be unavailable for during a disaster. Recovery Point Objective (RPO) - As DR solutions are usually based on some sort of data copy or backup, it is possible that a system might be recovered to a state prior to the disaster occurring, rather than the state it was in at the actual instant of the disaster. The RPO measures how far back in time the recovery point will be, and therefore how much data might be lost. An RPO of zero would assert that no data will be lost, but such a solution is often a compromise against the cost and performance of the system. To achieve high availability in your API Connect deployment, a minimum of three data centers are required (see the IBM API Connect High Availability section here ). This configuration creates a quorum of data centers, allowing automated failover in any direction, and enabling all three data centers to be active. The quorum majority voting algorithm, allows for a single data center to be offline, and yet still maintain data consistency and availability across the remaining two data centers as they continue to represent a majority in the deployment (avoiding split-brain syndrome). However, when having three data centers is not possible, a two data center deployment provides a disaster recovery solution that has both a low Recovery Time Objective (RTO), and a low Recovery Point Objective (RPO): The tables below describe the worst-case scenario for availability of each of the IBM API Connect components: You can follow the IBM API Connect documentation on the two data center disaster recovery deployment strategy on OpenShift here Warning Make sure you read and understand the the requirements for deploying a two data center disaster recovery (DR) solution. For instance, for this scenario to work, the latency between the two data center must be less than 80ms . From the above website you can find links to the instructions for, How to install a two data center deployment . Maintain a two data center deployment which includes information about normal operation data flows for the warm components of the passive cluster. There are other IBM API Connect HA/DR topologies that can be achieved when the requirements for the two data center disaster recovery (DR) solution explained here can not be met. These are explained in further detail in the IBM API Connect v10.x Deployment WhitePaper written by Chris Phillips. For some of those IBM API Connect HA/DR solutions as well as for non-kubernetes deployments, manual backup and synchronization processes might be involved. You can read more detailed information about those manual processes for how to prepare for a disaster and how to recover from a disaster in the IBM API Connect official documentation here . Remember that if your IBM API Connect deployment is hosted on a cloud provider, you might be able to make use of some of the data replication mechanisms for storage.","title":"High availability versus disaster recovery"},{"location":"guides/cp4i/apic/high-availability/overview/","text":"IBM API Connect High Availability on OpenShift \u00b6 Abstract This document describes the best practices for achieving highly available deployments of IBM API Connect on OpenShift, a requirement to be met for any production environment. High availability is concerned with configuring a system so that it continues to operate, and the users and any using systems don't experience any loss of service, in the event of hardware or software failures. To achieve high availability in your API Connect deployment, a minimum of three data centers are required. This configuration creates a quorum of data centers, allowing automated failover in any direction, and enabling all three data centers to be active. The quorum majority voting algorithm, allows for a single data center to be offline, and yet still maintain data consistency and availability across the remaining two data centers as they continue to represent a majority in the deployment (avoiding split-brain syndrome). The recommended topology for deploying IBM API Connect on OpenShift is an Active-Active-Active topology achieved in a single OpenShift cluster that spreads across three data centers, or availability zones (AZs) in the case of public cloud providers. Important There must be a very low latency network available between data centers or availability zones. The latency between sites must be less than 6ms . The tables below describe the worst-case scenario for availability of each of the IBM API Connect components: Info For more information about this and other IBM API Connect high availability topologies, please refer to the IBM API Connect v10.x Deployment WhitePaper written by Chris Phillips.","title":"High Availability"},{"location":"guides/cp4i/apic/high-availability/overview/#ibm-api-connect-high-availability-on-openshift","text":"Abstract This document describes the best practices for achieving highly available deployments of IBM API Connect on OpenShift, a requirement to be met for any production environment. High availability is concerned with configuring a system so that it continues to operate, and the users and any using systems don't experience any loss of service, in the event of hardware or software failures. To achieve high availability in your API Connect deployment, a minimum of three data centers are required. This configuration creates a quorum of data centers, allowing automated failover in any direction, and enabling all three data centers to be active. The quorum majority voting algorithm, allows for a single data center to be offline, and yet still maintain data consistency and availability across the remaining two data centers as they continue to represent a majority in the deployment (avoiding split-brain syndrome). The recommended topology for deploying IBM API Connect on OpenShift is an Active-Active-Active topology achieved in a single OpenShift cluster that spreads across three data centers, or availability zones (AZs) in the case of public cloud providers. Important There must be a very low latency network available between data centers or availability zones. The latency between sites must be less than 6ms . The tables below describe the worst-case scenario for availability of each of the IBM API Connect components: Info For more information about this and other IBM API Connect high availability topologies, please refer to the IBM API Connect v10.x Deployment WhitePaper written by Chris Phillips.","title":"IBM API Connect High Availability on OpenShift"},{"location":"guides/cp4i/apic/monitoring/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/apic/monitoring/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/apic/multi-cluster/config-pipeline/","text":"IBM API Connect Multi-Cluster - Configure APIC \u00b6 Overview \u00b6 In the previous chapters of this tutorial, you went through what needs to be done within your GitOps repositories in order to get a highly available and production ready IBM API Connect cluster deployed across multiple Red Hat OpenShift clusters to suit your multi-cluster use case. However, that deployment process only deploys the different IBM API Connect subsystems (Management, Portal, Gateway and Analytics) on their correspondent Red Hat OpenShift cluster. It does not configure these components in the IBM API Connect Cloud Manager, which can be seen as the brain or control plane of IBM API Connect, to work together. Instead, you would need to complete the following IBM API Connect Cloud Manager configuration checklist that will drive you through things like Register a Gateway service, Register an Analytics service, Create a provider organization, etc . Automation \u00b6 Similarly to deploying any piece of software in production, you would like to be able to configure those pieces of software through a GitOps methodology as well. However, the configuration of some of those pieces of software might not yet be enabled be done following a GitOps methodology. Still, you would like to be able to configure any of those pieces of software or components through certain degree of automation. Automation will provide you with two major benefits: auditability and repetition (both of which are inherent to GitOps). Since the configuration of theses pieces of software is done through automation and automation is nothing but code describing actions to be done/taken, that pieces of code that is automating the configuration of a piece of software is auditable and repeatable. These two properties allow you to see how exactly a piece of software has been configured in case you need to adjust anything and also give you that repeatability that provides you with a kind of disaster recovery mechanism that will allow you to configure another instance of that piece of software in the exact same manner as its sibling. Of course, these capabilities are a must in production environments. The automation we are describing and looking for to automate the IBM API Connect Cloud Manager configuration checklist can be achieved thanks to Tekton (OpenShift Pipelines) and, in the particular case of IBM API Connect, the rich set of IBM API Connect administration APIs . With these two, we have managed to create an OpenShift pipeline that will automate the following actions for use to configure a new deployment of an IBM API Connect instance: Configure an email server for notifications. For each IBM API Connect Gateway and Analytics cluster: Create an Availability Zone. Register the Gateway service. Register the Analytics service. Associate the Analytics service with the Gateway service. Register the Portal service. Create a Provider Organization. Associate the Gateway services with the default Sandbox catalog. Important The IBM API Connect configuration automation provided in this Cloud Pak Production Guides should be use as reference and never as a production ready asset that will suit every client needs. Each client will have a particular set of requirements and architecture and, as a result, the automation presented here can be use as a template or starting point but it is not meant to provide a production ready solution by any means. OpenShift pipeline \u00b6 The OpenShift Pipeline just introduced above has been created in your IBM API Connect Management and Portal Red Hat OpenShift custer. If you go to Pipelines --> Pipelines on the left hand side menu of your Red Hat OpenShift web console under the ci Project, you will see two OpenShift pipelines have been created as a result of what you had specified in your GitOps repositories: Info The pipeline you are interested in for getting your instance of IBM API Connect configured is called apic-post-install-config If you go check the kustomization.yaml under 0-bootstrap/others/apic-multi-cluster/management-portal-cluster/3-apps in your multi-tenancy-gitops repository, you will see that for your IBM API Connect Management and Portal cluster you have specified you want to get installed the following resource: - argocd/apic/multi-cluster-ops.yaml by having such resource un-commented. If you go to check that file you will see the following: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : apps-apic-multi-cluster-ops annotations : argocd.argoproj.io/sync-wave : \"300\" labels : gitops.tier.layer : applications finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : applications source : path : apic/config/argocd/multi-cluster/ops syncPolicy : automated : prune : true selfHeal : true That is, an ArgoCD Application that points to the path apic/config/argocd/multi-cluster/ops in your multi-tenancy-gitops-apps . If you keep following the paths of the chained ArgoCD Applications, you will end up realising that what the original ArgoCD Application above would get deployed in your IBM API Connect Management and Portal cluster is located in apic/environments/multi-cluster/ops of your multi-tenancy-gitops-apps repository. If you execute tree multi-tenancy-gitops-apps/apic/environments/multi-cluster/ops -L 2 you should see the following: multi-tenancy-gitops-apps/apic/environments/multi-cluster/ops \u251c\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 pipelines \u2502 \u251c\u2500\u2500 apic-config-pipeline.yaml \u2502 \u2514\u2500\u2500 apic-publish-pipeline.yaml \u251c\u2500\u2500 roles \u2502 \u251c\u2500\u2500 role.yaml \u2502 \u2514\u2500\u2500 rolebinding.yaml \u2514\u2500\u2500 tasks \u251c\u2500\u2500 apic-config-task.yaml \u2514\u2500\u2500 apic-publish-task.yaml where you can find the two pipelines that are getting created in your IBM API Connect Management and Portal cluster. apic-config-pipeline.yaml will create the OpenShift Pipeline that will automate the configuration of you IBM API Connect multi-cluster architecture you have just deployed whereas the apic-publish-pipeline.yaml will create an OpenShift Pipeline that will automate the publishing of IBM API Connect products and their respective APIs into your IBM API Connect instance. Before being able to execute the OpenShift pipeline that will configure your instance of IBM API Connect, you need to: Fork the OpenShift Pipeline repositories to your GitHub organization where you already forked the other GitOps repositories you have used to deploy your IBM API Connect multi-cluster instance. Create a GitHub Personal Access Token so that the IBM API Connect configuration OpenShift pipeline has permissions to access your IBM GitHub account (to the private repos you will fork the OpenShift Pipeline repositories to). Provide your email server configuration for notifications. Provide your IBM API Connect provider organization configuration 1. OpenShift Pipeline repositories \u00b6 You need to fork the OpenShift Pipeline repositories to your GitHub organization where you already forked the other GitOps repositories you have used to deploy your IBM API Connect multi-cluster instance. You can review the creation of that GitHub organization in the Configure the clusters - GitOps repos & ArgoCD section. The OpenShift Pipeline repositories can be found at https://github.com/cloud-native-toolkit . Use the search bar to find the following three repositories: apic-publish-pipeline apic-config-pipeline apic-products-apis-yaml Important Fork ALL three repositories even though you will only use the apic-config-pipeline repository in this section. However, you will use the other two in the following section to publish an API of the dummy application that has been deployed into the IBM API Connect Gateway and Analytics clusters into an IBM API Connect product so that it is exposed and available through the gateways deployed in such clusters. You can review how to fork a repository in he Configure the clusters - GitOps repos & ArgoCD section. However, make sure your forked repositories are private and that you also Include all branches : Info The reason to make these repositories private is to simulate more closely a real scenario where your assets would be behind a firewall. As a result, the OpenShift Pipelines and the script these execute were developed to expect authentication and authorization mechanisms. The reason to include all branches is because you will be using the apic-multicloud branch. By now you should have the following GitHub repositories in your GitHub organization: 2. GitHub Personal Access Token \u00b6 Create a GitHub Personal Access Token so that the IBM API Connect configuration OpenShift pipeline has permissions to access your IBM GitHub Enterprise account. Provide just repo and admin:repo_hook permissions. You can find the instructions for creating that GitHub Personal Access Token here Once you have created your GitHub Personal Access Token, Ensure you are logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Warning Make sure you are logging into the IBM API Connect Management and Portal cluster Create the following secret containing your GitHub Username and Personal Access Token oc create secret generic apic-pipeline-git -n ci \\ --from-literal = GIT_USERNAME = <your_github_username> \\ --from-literal = GIT_PRIV_TOKEN = <your_github_personal_access_token> 3. Email Server \u00b6 You need to provide the configuration of your email servicer for notifications. You can read more about configuring an email server for notifications in the IBM API Connect documentation here . You can create your own dummy email server at https://mailtrap.io/ . Once you have your email server created and its configuration at hand, create the following secret that will hold your Email Server configuration for notifications: oc create secret generic apic-config-email-server -n ci \\ --from-literal = EMAIL_HOST = <your_email_server_host> \\ --from-literal = EMAIL_PORT = <your_email_server_port> \\ --from-literal = EMAIL_USERNAME = <your_email_server_username> \\ --from-literal = EMAIL_PASSWORD = <your_email_server_password> 4. Provider Organization \u00b6 You must provide the configuration for what your IBM API Connect Provider Organization will be. You can read more about creating a Provider Organization in the IBM API Connect documentation here . Once you have clear the above, create the following secret that will hold your Provider Organization configuration: oc create secret generic apic-pipeline-provider-org -n ci \\ --from-literal = PROV_ORG_OWNER_USERNAME = <provider_organization_owner_username> \\ --from-literal = PROV_ORG_OWNER_PASSWORD = <provider_organization_owner_password> \\ --from-literal = PROV_ORG_OWNER_EMAIL = <provider_organization_owner_email> \\ --from-literal = PROV_ORG_OWNER_FIRST_NAME = <provider_organization_owner_first_name> \\ --from-literal = PROV_ORG_OWNER_LAST_NAME = <provider_organization_owner_last_name> \\ --from-literal = PROV_ORG_NAME = <provider_organization_name> \\ --from-literal = PROV_ORG_TITLE = <provider_organization_title> \\ --from-literal = PROV_ORG_CATALOG_NAME = sandbox \\ --from-literal = PROV_ORG_REALM = provider/default-idp-2 An example of a dummy provider organization could be: PROV_ORG_CATALOG_NAME = sandbox PROV_ORG_NAME = test-org PROV_ORG_OWNER_EMAIL = test@test.com PROV_ORG_OWNER_FIRST_NAME = A_Name PROV_ORG_OWNER_LAST_NAME = A_Last_Name PROV_ORG_OWNER_PASSWORD = passw0rd PROV_ORG_OWNER_USERNAME = testorgadmin PROV_ORG_REALM = provider/default-idp-2 PROV_ORG_TITLE = Test Org Important The PROV_ORG_OWNER_USERNAME and PROV_ORG_OWNER_PASSWORD will be the credentials you will use later on to log into the IBM API Connect API Manager. Run the IBM API Connect Configuration Pipeline \u00b6 You are now ready to execute the OpenShift pipeline that will configure your IBM API Connect instance. Go again to Pipelines --> Pipelines on the left hand side menu of your OpenShift web console and click on the apic-post-install-config pipeline. Then, click on Start on the Actions drop down menu at the top right corner. On the Start Pipeline OpenShift Pipeline configuration menu that pops up, make sure you provide the appropriate values for the following pipeline parameters: git-apic-pipeline-git-url which is the GitHub url where the pipeline scripts are stored. These are the scripts the pipeline will execute. This is the GitHub repository your forked into your GitHub organization earlier in this section. gtw-a7s-azs which is a comma-separated list of IBM API Connect availability zones names. This parameter expects an availability zone per IBM API Connect Gateway and Analytics cluster you set up your apic-multi-cluster profile in your multi-tenancy-gitops repository. That is, each set of IBM API Connect Gateway and Analytics will get configured in their own availability zone The length of this list must match the gtw-a7s-domains list below. gtw-a7s-domains which is a comma-separated list of the OpenShift domains where the IBM API Connect Gateway and Analytics components have been installed to. Once again, the Red Hat OpenShift cluster domains of each of the IBM API Connect Gateway and Analytics clusters you set up your apic-multi-cluster profile in your multi-tenancy-gitops repository. The length of this list must match the gtw-a7s-azs list above. Click Start . This will create a Pipeline Run object that represents this specific execution of the pipeline. You can follow along the execution of this OpenShift Pipeline Run by clicking the Logs tab at the top bar. If this execution of the IBM API Connect Configuration Pipeline finishes successfully you should see an END message at the bottom of the logs as well as a green check mark on the task the Pipeline Run has executed on the left hand side. If you switch back to the Details tab of the Pipeline Run , you should also see green check marks both at the Pipeline Run name at the top and below it in the task that has been executed by this Pipeline Run that is displayed on the Pipeline Run Details section. And in the main Pipelines section of your OpenShift web console you should also see the last Pipeline Run for the IBM API Connect config Pipeline in green Finally, if you open your IBM API Connect Cloud Manager again, you will see that the IBM API Connect Configuration Pipeline has configured your IBM API Connect Topology for you with the IBM API Connect multi-cluster architecture you defined in your apic-multi-cluster profile in the multi-tenancy-gitops repository. And in the IBM API Connect API Manager, you can see that the Gateway services that have been configured in the IBM API Connect Cloud Manager Topology for each of the availability zones have been registered with the default Sandbox catalog, that gets created out of the box for the Provider Organization you specified in the IBM API Connect Configuration Pipeline, so that you can securely publish, expose and access your application APIs. Tip You can open your IBM API Connect API Manager through your Red Hat OpenShift web console. Go to Networking --> Routes under the tools project and click on the Location value for the management-api-manager route. Then, use the credentials you specified in your provider organization configuration to log in. Important Do not execute the IBM API Connect Configuration Pipeline twice, even if the first attempt failed, as what the first run created must be manually removed for the second attempt to succeed. We do understand this is a limitation of the IBM API Connect Configuration Pipeline and something that could be fixed by implementing a previous cleanup task within the IBM API Connect Configuration Pipeline. However, this is out of the scope of this tutorial and, once again, the assets provided within this tutorial are meant to be a jump-start or a template to start from that should not be treated as production-ready assets. These assets can then be leveraged for a particular client, leaving the specific tailoring of these for the client to the team making use of the assets. Congratulations! You have successfully created and executed the IBM API Connect Configuration Pipeline that configures your IBM API Connect multi-cluster instance. Your IBM API Connect multi-cluster instance is now ready to start working with. Go to the next section to execute another OpenShift Pipeline that will publish the dummy application's API, that is running on your IBM API Connect Gateway and Analytics clusters, into a product in IBM API Connect so that you can securely access the application through that API exposed on the IBM API Connect Gateway.","title":"Configure APIC"},{"location":"guides/cp4i/apic/multi-cluster/config-pipeline/#ibm-api-connect-multi-cluster-configure-apic","text":"","title":"IBM API Connect Multi-Cluster - Configure APIC"},{"location":"guides/cp4i/apic/multi-cluster/config-pipeline/#overview","text":"In the previous chapters of this tutorial, you went through what needs to be done within your GitOps repositories in order to get a highly available and production ready IBM API Connect cluster deployed across multiple Red Hat OpenShift clusters to suit your multi-cluster use case. However, that deployment process only deploys the different IBM API Connect subsystems (Management, Portal, Gateway and Analytics) on their correspondent Red Hat OpenShift cluster. It does not configure these components in the IBM API Connect Cloud Manager, which can be seen as the brain or control plane of IBM API Connect, to work together. Instead, you would need to complete the following IBM API Connect Cloud Manager configuration checklist that will drive you through things like Register a Gateway service, Register an Analytics service, Create a provider organization, etc .","title":"Overview"},{"location":"guides/cp4i/apic/multi-cluster/config-pipeline/#automation","text":"Similarly to deploying any piece of software in production, you would like to be able to configure those pieces of software through a GitOps methodology as well. However, the configuration of some of those pieces of software might not yet be enabled be done following a GitOps methodology. Still, you would like to be able to configure any of those pieces of software or components through certain degree of automation. Automation will provide you with two major benefits: auditability and repetition (both of which are inherent to GitOps). Since the configuration of theses pieces of software is done through automation and automation is nothing but code describing actions to be done/taken, that pieces of code that is automating the configuration of a piece of software is auditable and repeatable. These two properties allow you to see how exactly a piece of software has been configured in case you need to adjust anything and also give you that repeatability that provides you with a kind of disaster recovery mechanism that will allow you to configure another instance of that piece of software in the exact same manner as its sibling. Of course, these capabilities are a must in production environments. The automation we are describing and looking for to automate the IBM API Connect Cloud Manager configuration checklist can be achieved thanks to Tekton (OpenShift Pipelines) and, in the particular case of IBM API Connect, the rich set of IBM API Connect administration APIs . With these two, we have managed to create an OpenShift pipeline that will automate the following actions for use to configure a new deployment of an IBM API Connect instance: Configure an email server for notifications. For each IBM API Connect Gateway and Analytics cluster: Create an Availability Zone. Register the Gateway service. Register the Analytics service. Associate the Analytics service with the Gateway service. Register the Portal service. Create a Provider Organization. Associate the Gateway services with the default Sandbox catalog. Important The IBM API Connect configuration automation provided in this Cloud Pak Production Guides should be use as reference and never as a production ready asset that will suit every client needs. Each client will have a particular set of requirements and architecture and, as a result, the automation presented here can be use as a template or starting point but it is not meant to provide a production ready solution by any means.","title":"Automation"},{"location":"guides/cp4i/apic/multi-cluster/config-pipeline/#openshift-pipeline","text":"The OpenShift Pipeline just introduced above has been created in your IBM API Connect Management and Portal Red Hat OpenShift custer. If you go to Pipelines --> Pipelines on the left hand side menu of your Red Hat OpenShift web console under the ci Project, you will see two OpenShift pipelines have been created as a result of what you had specified in your GitOps repositories: Info The pipeline you are interested in for getting your instance of IBM API Connect configured is called apic-post-install-config If you go check the kustomization.yaml under 0-bootstrap/others/apic-multi-cluster/management-portal-cluster/3-apps in your multi-tenancy-gitops repository, you will see that for your IBM API Connect Management and Portal cluster you have specified you want to get installed the following resource: - argocd/apic/multi-cluster-ops.yaml by having such resource un-commented. If you go to check that file you will see the following: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : apps-apic-multi-cluster-ops annotations : argocd.argoproj.io/sync-wave : \"300\" labels : gitops.tier.layer : applications finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : applications source : path : apic/config/argocd/multi-cluster/ops syncPolicy : automated : prune : true selfHeal : true That is, an ArgoCD Application that points to the path apic/config/argocd/multi-cluster/ops in your multi-tenancy-gitops-apps . If you keep following the paths of the chained ArgoCD Applications, you will end up realising that what the original ArgoCD Application above would get deployed in your IBM API Connect Management and Portal cluster is located in apic/environments/multi-cluster/ops of your multi-tenancy-gitops-apps repository. If you execute tree multi-tenancy-gitops-apps/apic/environments/multi-cluster/ops -L 2 you should see the following: multi-tenancy-gitops-apps/apic/environments/multi-cluster/ops \u251c\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 pipelines \u2502 \u251c\u2500\u2500 apic-config-pipeline.yaml \u2502 \u2514\u2500\u2500 apic-publish-pipeline.yaml \u251c\u2500\u2500 roles \u2502 \u251c\u2500\u2500 role.yaml \u2502 \u2514\u2500\u2500 rolebinding.yaml \u2514\u2500\u2500 tasks \u251c\u2500\u2500 apic-config-task.yaml \u2514\u2500\u2500 apic-publish-task.yaml where you can find the two pipelines that are getting created in your IBM API Connect Management and Portal cluster. apic-config-pipeline.yaml will create the OpenShift Pipeline that will automate the configuration of you IBM API Connect multi-cluster architecture you have just deployed whereas the apic-publish-pipeline.yaml will create an OpenShift Pipeline that will automate the publishing of IBM API Connect products and their respective APIs into your IBM API Connect instance. Before being able to execute the OpenShift pipeline that will configure your instance of IBM API Connect, you need to: Fork the OpenShift Pipeline repositories to your GitHub organization where you already forked the other GitOps repositories you have used to deploy your IBM API Connect multi-cluster instance. Create a GitHub Personal Access Token so that the IBM API Connect configuration OpenShift pipeline has permissions to access your IBM GitHub account (to the private repos you will fork the OpenShift Pipeline repositories to). Provide your email server configuration for notifications. Provide your IBM API Connect provider organization configuration","title":"OpenShift pipeline"},{"location":"guides/cp4i/apic/multi-cluster/config-pipeline/#1-openshift-pipeline-repositories","text":"You need to fork the OpenShift Pipeline repositories to your GitHub organization where you already forked the other GitOps repositories you have used to deploy your IBM API Connect multi-cluster instance. You can review the creation of that GitHub organization in the Configure the clusters - GitOps repos & ArgoCD section. The OpenShift Pipeline repositories can be found at https://github.com/cloud-native-toolkit . Use the search bar to find the following three repositories: apic-publish-pipeline apic-config-pipeline apic-products-apis-yaml Important Fork ALL three repositories even though you will only use the apic-config-pipeline repository in this section. However, you will use the other two in the following section to publish an API of the dummy application that has been deployed into the IBM API Connect Gateway and Analytics clusters into an IBM API Connect product so that it is exposed and available through the gateways deployed in such clusters. You can review how to fork a repository in he Configure the clusters - GitOps repos & ArgoCD section. However, make sure your forked repositories are private and that you also Include all branches : Info The reason to make these repositories private is to simulate more closely a real scenario where your assets would be behind a firewall. As a result, the OpenShift Pipelines and the script these execute were developed to expect authentication and authorization mechanisms. The reason to include all branches is because you will be using the apic-multicloud branch. By now you should have the following GitHub repositories in your GitHub organization:","title":"1. OpenShift Pipeline repositories"},{"location":"guides/cp4i/apic/multi-cluster/config-pipeline/#2-github-personal-access-token","text":"Create a GitHub Personal Access Token so that the IBM API Connect configuration OpenShift pipeline has permissions to access your IBM GitHub Enterprise account. Provide just repo and admin:repo_hook permissions. You can find the instructions for creating that GitHub Personal Access Token here Once you have created your GitHub Personal Access Token, Ensure you are logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Warning Make sure you are logging into the IBM API Connect Management and Portal cluster Create the following secret containing your GitHub Username and Personal Access Token oc create secret generic apic-pipeline-git -n ci \\ --from-literal = GIT_USERNAME = <your_github_username> \\ --from-literal = GIT_PRIV_TOKEN = <your_github_personal_access_token>","title":"2. GitHub Personal Access Token"},{"location":"guides/cp4i/apic/multi-cluster/config-pipeline/#3-email-server","text":"You need to provide the configuration of your email servicer for notifications. You can read more about configuring an email server for notifications in the IBM API Connect documentation here . You can create your own dummy email server at https://mailtrap.io/ . Once you have your email server created and its configuration at hand, create the following secret that will hold your Email Server configuration for notifications: oc create secret generic apic-config-email-server -n ci \\ --from-literal = EMAIL_HOST = <your_email_server_host> \\ --from-literal = EMAIL_PORT = <your_email_server_port> \\ --from-literal = EMAIL_USERNAME = <your_email_server_username> \\ --from-literal = EMAIL_PASSWORD = <your_email_server_password>","title":"3. Email Server"},{"location":"guides/cp4i/apic/multi-cluster/config-pipeline/#4-provider-organization","text":"You must provide the configuration for what your IBM API Connect Provider Organization will be. You can read more about creating a Provider Organization in the IBM API Connect documentation here . Once you have clear the above, create the following secret that will hold your Provider Organization configuration: oc create secret generic apic-pipeline-provider-org -n ci \\ --from-literal = PROV_ORG_OWNER_USERNAME = <provider_organization_owner_username> \\ --from-literal = PROV_ORG_OWNER_PASSWORD = <provider_organization_owner_password> \\ --from-literal = PROV_ORG_OWNER_EMAIL = <provider_organization_owner_email> \\ --from-literal = PROV_ORG_OWNER_FIRST_NAME = <provider_organization_owner_first_name> \\ --from-literal = PROV_ORG_OWNER_LAST_NAME = <provider_organization_owner_last_name> \\ --from-literal = PROV_ORG_NAME = <provider_organization_name> \\ --from-literal = PROV_ORG_TITLE = <provider_organization_title> \\ --from-literal = PROV_ORG_CATALOG_NAME = sandbox \\ --from-literal = PROV_ORG_REALM = provider/default-idp-2 An example of a dummy provider organization could be: PROV_ORG_CATALOG_NAME = sandbox PROV_ORG_NAME = test-org PROV_ORG_OWNER_EMAIL = test@test.com PROV_ORG_OWNER_FIRST_NAME = A_Name PROV_ORG_OWNER_LAST_NAME = A_Last_Name PROV_ORG_OWNER_PASSWORD = passw0rd PROV_ORG_OWNER_USERNAME = testorgadmin PROV_ORG_REALM = provider/default-idp-2 PROV_ORG_TITLE = Test Org Important The PROV_ORG_OWNER_USERNAME and PROV_ORG_OWNER_PASSWORD will be the credentials you will use later on to log into the IBM API Connect API Manager.","title":"4. Provider Organization"},{"location":"guides/cp4i/apic/multi-cluster/config-pipeline/#run-the-ibm-api-connect-configuration-pipeline","text":"You are now ready to execute the OpenShift pipeline that will configure your IBM API Connect instance. Go again to Pipelines --> Pipelines on the left hand side menu of your OpenShift web console and click on the apic-post-install-config pipeline. Then, click on Start on the Actions drop down menu at the top right corner. On the Start Pipeline OpenShift Pipeline configuration menu that pops up, make sure you provide the appropriate values for the following pipeline parameters: git-apic-pipeline-git-url which is the GitHub url where the pipeline scripts are stored. These are the scripts the pipeline will execute. This is the GitHub repository your forked into your GitHub organization earlier in this section. gtw-a7s-azs which is a comma-separated list of IBM API Connect availability zones names. This parameter expects an availability zone per IBM API Connect Gateway and Analytics cluster you set up your apic-multi-cluster profile in your multi-tenancy-gitops repository. That is, each set of IBM API Connect Gateway and Analytics will get configured in their own availability zone The length of this list must match the gtw-a7s-domains list below. gtw-a7s-domains which is a comma-separated list of the OpenShift domains where the IBM API Connect Gateway and Analytics components have been installed to. Once again, the Red Hat OpenShift cluster domains of each of the IBM API Connect Gateway and Analytics clusters you set up your apic-multi-cluster profile in your multi-tenancy-gitops repository. The length of this list must match the gtw-a7s-azs list above. Click Start . This will create a Pipeline Run object that represents this specific execution of the pipeline. You can follow along the execution of this OpenShift Pipeline Run by clicking the Logs tab at the top bar. If this execution of the IBM API Connect Configuration Pipeline finishes successfully you should see an END message at the bottom of the logs as well as a green check mark on the task the Pipeline Run has executed on the left hand side. If you switch back to the Details tab of the Pipeline Run , you should also see green check marks both at the Pipeline Run name at the top and below it in the task that has been executed by this Pipeline Run that is displayed on the Pipeline Run Details section. And in the main Pipelines section of your OpenShift web console you should also see the last Pipeline Run for the IBM API Connect config Pipeline in green Finally, if you open your IBM API Connect Cloud Manager again, you will see that the IBM API Connect Configuration Pipeline has configured your IBM API Connect Topology for you with the IBM API Connect multi-cluster architecture you defined in your apic-multi-cluster profile in the multi-tenancy-gitops repository. And in the IBM API Connect API Manager, you can see that the Gateway services that have been configured in the IBM API Connect Cloud Manager Topology for each of the availability zones have been registered with the default Sandbox catalog, that gets created out of the box for the Provider Organization you specified in the IBM API Connect Configuration Pipeline, so that you can securely publish, expose and access your application APIs. Tip You can open your IBM API Connect API Manager through your Red Hat OpenShift web console. Go to Networking --> Routes under the tools project and click on the Location value for the management-api-manager route. Then, use the credentials you specified in your provider organization configuration to log in. Important Do not execute the IBM API Connect Configuration Pipeline twice, even if the first attempt failed, as what the first run created must be manually removed for the second attempt to succeed. We do understand this is a limitation of the IBM API Connect Configuration Pipeline and something that could be fixed by implementing a previous cleanup task within the IBM API Connect Configuration Pipeline. However, this is out of the scope of this tutorial and, once again, the assets provided within this tutorial are meant to be a jump-start or a template to start from that should not be treated as production-ready assets. These assets can then be leveraged for a particular client, leaving the specific tailoring of these for the client to the team making use of the assets. Congratulations! You have successfully created and executed the IBM API Connect Configuration Pipeline that configures your IBM API Connect multi-cluster instance. Your IBM API Connect multi-cluster instance is now ready to start working with. Go to the next section to execute another OpenShift Pipeline that will publish the dummy application's API, that is running on your IBM API Connect Gateway and Analytics clusters, into a product in IBM API Connect so that you can securely access the application through that API exposed on the IBM API Connect Gateway.","title":"Run the IBM API Connect Configuration Pipeline"},{"location":"guides/cp4i/apic/multi-cluster/configure-clusters-gitops-config/","text":"IBM API Connect Multi-Cluster - Configure the clusters - Configure GitOps repos \u00b6 Overview \u00b6 In the previous chapter of this guide, we installed the Red Hat GitOps Operator, which relies on ArgoCD behind the scenes, as the fundamental component for deploying and managing any component on our clusters. We also went through creating a GitHub organization where the different GitOps repositories that will drive the GitOps processes for our clusters will be forked into. These GitOps repositories that will store the definitions and configurations of all the components we want to have in our clusters, that ArgoCD will use as the source of truth to what needs to be in our clusters or not, are: The multi-tenancy-gitops repository The multi-tenancy-gitops-infra repository The multi-tenancy-gitops-services repository The multi-tenancy-gitops-apps repository By now, you should have a copy/fork of each of these repositories in the GitHub organization you must have created in the previous section too as well as a clone of those GitOps repositories on your local workstation. In fact, in the previous section we already used the local clone of the multi-tenancy-gitops repository to install ArgoCD in each of your Red Hat OpenShift clusters where we are going to install IBM API Connect components. Goal In this section, we are going to see how to create the appropriate structure in the multi-tenancy-gitops and multi-tenancy-gitops-apps repositories that will allow us to install the right specific components of IBM API Connect to successfully deploy IBM API Connect in a multi-cluster landscape. Pre-requisites \u00b6 For a better understanding of the following sections, we strongly recommend to review and understand the Getting started with GitOps section for the IBM API Connect single cluster scenario tutorial where a more in-depth explanation of the multi-tenancy-gitops GitOps repo structure can be found. Customizing the GitOps repositories \u00b6 The cloned GitOps repositories (specifically, multi-tenancy-gitops and multi-tenancy-gitops-apps repositories) need to be customized for a correct deployment of IBM API Connect in a multi-cluster landscape. We will see in the following sections how to do the appropriate customizations to the aforementioned GitOps repositories. Update multi-tenancy-gitops repository \u00b6 The multi-tenancy-gitops repository contains different profiles for different potential infrastructure landscapes. If you execute tree multi-tenancy-gitops/0-bootstrap -L 2 , multi-tenancy-gitops/0-bootstrap \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml you should see the single-cluster and others folder whose aim is to represent those different potential infrastructure landscapes. The single-cluster profile represents the layered GitOps structure ( 1-infra , 2-services , 3-apps ) for a single cluster. On the other hand, others folder contains other more complex profiles for multi-cluster infrastructure landscapes. If you execute tree multi-tenancy-gitops/0-bootstrap/others -L 3 multi-tenancy-gitops/0-bootstrap/others \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 bootstrap-cluster-1-cicd-dev-stage-prod.yaml \u2502 \u251c\u2500\u2500 bootstrap-cluster-n-prod.yaml \u2502 \u251c\u2500\u2500 cluster-1-cicd-dev-stage-prod \u2502 \u2502 \u251c\u2500\u2500 1-infra \u2502 \u2502 \u251c\u2500\u2500 2-services \u2502 \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 cluster-n-prod \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2-isolated-cluster \u2502 \u251c\u2500\u2500 bootstrap-cluster-1-cicd-dev-stage.yaml \u2502 \u251c\u2500\u2500 bootstrap-cluster-n-prod.yaml \u2502 \u251c\u2500\u2500 cluster-1-cicd-dev-stage \u2502 \u2502 \u251c\u2500\u2500 1-infra \u2502 \u2502 \u251c\u2500\u2500 2-services \u2502 \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 cluster-n-prod \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 3-multi-cluster \u251c\u2500\u2500 bootstrap-cluster-1-cicd.yaml \u251c\u2500\u2500 bootstrap-cluster-1-dev.yaml \u251c\u2500\u2500 bootstrap-cluster-1-stage.yaml \u251c\u2500\u2500 bootstrap-cluster-n-prod.yaml \u251c\u2500\u2500 cluster-1-cicd \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 cluster-2-dev \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 cluster-3-stage \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 cluster-n-prod \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u2514\u2500\u2500 kustomization.yaml you will see that each of those more complex multi-cluster infrastructure landscape profiles are the same as the single-cluster profile as far as the layered GitOps structure that represents a clusters but with the difference that in these multi-cluster profiles we will be driving the GitOps processes for several clusters. Tip Once again, we strongly recommend to review and understand the Getting started with GitOps section for the IBM API Connect single cluster scenario tutorial, where a more in-depth explanation of the multi-tenancy-gitops GitOps approach and repo structure can be found. It is clear then that the profile for our IBM API Connect multi-cluster use case will need to live under others . However, since this is a very specific use case for an IBM capability such as IBM API Connect, we have not included such profile on the GitOps repo by default out of the box. Instead, we have placed the structure for what will be your IBM API Connect Management and Portal cluster and IBM API Connect Gateway and Analytics cluster profiles under docs/scenarios/apic-multi-cluster . If you execute tree multi-tenancy-gitops/doc/scenarios/apic-multi-cluster -L 2 , multi-tenancy-gitops/doc/scenarios/apic-multi-cluster \u251c\u2500\u2500 bootstrap-gateway-analytics-cluster.yaml \u251c\u2500\u2500 bootstrap-management-portal-cluster.yaml \u251c\u2500\u2500 gateway-analytics-cluster \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 management-portal-cluster \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 scripts \u251c\u2500\u2500 create-apic-multi-cluster-profile.sh \u2514\u2500\u2500 create-gateway-analytics-cluster.sh you should see, as in with any of the other profiles, that your IBM API Connect Management and Portal cluster and IBM API Connect Gateway and Analytics cluster profiles look exactly the same as far as the layered GitOps structure is concerned. The content under multi-tenancy-gitops/doc/scenarios/apic-multi-cluster should be treated as the templates which you will build your apic-multi-cluster profile under multi-tenancy-gitops/0-bootstrap/others from. For doing that, we provide two scripts under the scripts folder you can see above. The create-apic-multi-cluster-profile.sh script will be responsible for creating that apic-multi-cluster profile under multi-tenancy-gitops/0-bootstrap/others . The create-gateway-analytics-cluster.sh script will be responsible for creating extra IBM API Connect Gateway and Analytics clusters under the apic-multi-cluster profile you have just created with the other script. This will allow you to create those IBM API Connect multi-cluster implementations where you have multiple clusters hosting applications that you would want to front with the IBM API Connect Gateway and Analytics components for securely exposing their APIs to outside world as well as using IBM API Connect capabilities for publishing and socializing such APIs as it is explained in the Overview section. Let's have a look at what you need to do in this multi-tenancy-gitops GitOps repository to create the apic-multi-cluster profile that will drive your GitOps process to implement the IBM API Connect multi-cluster use case introduced in the Overview section which will be made up of one IBM API Connect Management and Portal cluster and two IBM API Connect Gateway and Analytics clusters. The instructions below will simulate, with the names used for the IBM API Connect Gateway and Analytics clusters, that one of those clusters will be in IBM Cloud and the other in AWS. However, all the clusters used in this set of instructions are hosted in IBM Cloud and have been requested through IBM Technology Zone as explained in the Create clusters section. Go to multi-tenancy-gitops/doc/scenarios/apic-multi-cluster/scripts cd $HOME /git cd multi-tenancy-gitops/doc/scenarios/apic-multi-cluster/scripts Execute the create-apic-multi-cluster-profile.sh script: NAME = IBM ./create-apic-multi-cluster-profile.sh You should see the following output: Creating an IBM API Connect multi-cluster profile under 0-bootstrap/others The IBM Gateway and Analytics cluster will be called IBM Done You can find the new apic-multi-cluster profile under 0-bootsrap/others/apic-multi-cluster Execute the create-gateway-analytics-cluster.sh script to create the IBM API Connect Gateway and Analytics cluster that will be hosted in AWS under your apic-multi-cluster profile created in the previous step: NAME = AWS ./create-gateway-analytics-cluster.sh You should see the following output: Creating a new IBM Gateway and Analytics cluster called AWS Your new cluster can be found in the AWS-gateway-analytics-cluster folder The bootstrap ArgoCD application for you new cluster is bootstrap-AWS-gateway-analytics-cluster.yaml Change directory and go back to multi-tenancy-gitops : cd $HOME /git cd multi-tenancy-gitops Execute tree 0-bootstrap/others/apic-multi-cluster -L 2 , you should see the following output: 0-bootstrap/others/apic-multi-cluster \u251c\u2500\u2500 AWS-gateway-analytics-cluster \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 IBM-gateway-analytics-cluster \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap-AWS-gateway-analytics-cluster.yaml \u251c\u2500\u2500 bootstrap-IBM-gateway-analytics-cluster.yaml \u251c\u2500\u2500 bootstrap-management-portal-cluster.yaml \u2514\u2500\u2500 management-portal-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u2514\u2500\u2500 kustomization.yaml which should correspond to your desired IBM API Connect multi-cluster GitOps profile which we will use to drive the implementation of the IBM API Connect multi-cluster use case. Last step would be to configure all of the ArgoCD applications and ArgoCD projects that will drive the GitOps processes to point to the appropriate GitHub organization and repositories where you are delivering these changes to. That is the GitHub organization and repositories your created and forked respectively in the Configure Clusters - GitOps section. As you will see in the following section of this tutorial, you will be kicking off those GitOps processes that will get your RedHat OpenShift clusters with the components and configuration that you want and that you specified through these GitOps repositories just now through the bootstrap files above. If you inspect the content of one of those bootstrap files above: cat 0 -bootstrap/others/apic-multi-cluster/bootstrap-management-portal-cluster.yaml you should see the following: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-management-portal namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/others/apic-multi-cluster/management-portal-cluster repoURL : ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS} targetRevision : ${GIT_GITOPS_BRANCH} syncPolicy : automated : prune : true selfHeal : true where you can see highlighted that the source for this ArgoCD Application resides in a GitHub repository at repoURL , on the branch targetRevision and on the path path and that the repoURL and targetRevision are parametrized. The reason for this is to allow the multi-tenancy-gitops repo to be a template you can fork and work with without stepping into anyone else's work. As a result, we will need to configure all of these ArgoCD yaml files to point to your GitHub organization and repositories you are working with for the IBM API Connect multi-cluster use case. For this, we have provided a util script that is under multi-tenancy-gitops/scripts . Execute the set-git-source.sh script: GIT_ORG = <your_git_org> ./scripts/set-git-source.sh where <your_git_org> is your GitHub organization you created in the Configure Clusters - GitOps section and forked the GitOps repositories you are using to drive the GitOps processes for the IBM API Connect multi-cluster use case into. Info The set-git-source.sh allows for fine-grained customization of each of the GitOps repositories and their respective branches for greater flexibility. However, we assume you are working off the master branch and your GitOps repositories are called as the original GitOps repositories your forked from. Warning This script might take a minute or so to execute. You should see the following output: Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now (*) where we have used apic-multi-cluster as our GitHub organization. If you inspect the bootstrap file again, you should see it will now point to the GitHub organization and repository you are storing your GitOps configuration into. apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-management-portal namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/others/apic-multi-cluster/management-portal-cluster repoURL : https://github.com/apic-multi-cluster/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Finally, you can commit and deliver the changes to your multi-tenancy-gitops repository: git add . git commit -s -m \"Created APIC multi-cluster profile\" git push origin $GIT_BRANCH What is getting installed? \u00b6 You have seen in the previous section how to create the apic-multi-cluster profile that will represent your IBM API Connect multi-cluster scenario as well as how to configure all the ArgoCD applications and projects that the OpenShift GitOps operator installed on your Red Hat OpenShift clusters will use to drive the GitOps process on them. However, you have not looked at what is actually being installed/deployed on each of these Red Hat OpenShift clusters by your GitOps repositories. To do that, you need to look at the kustomization.yaml files in each of the layers ( 1-infra , 2-services , 3-apps ) for each of the Red Hat OpenShift clusters you have represented within your apic-multi-cluster profile. These kustomization.yaml files are nothing but an index of the resources, that in turn are represented as ArgoCD Applications, that you want to get created, and as a result installed/deployed, which are those that are not commented out. You can use cat or any visual text editor to inspect any of those kustomization.yaml files that you should be able to locate under your apic-multi-cluster profile. For example, if you have a quick look at the 0-bootstrap/others/apic-multi-cluster/management-portal-cluster/2-services/kustomization.yaml : cat 0 -bootstrap/others/apic-multi-cluster/management-portal-cluster/2-services/kustomization.yaml | grep '^- argo' You should see a similar output as: - argocd/operators/ibm-apic-operator.yaml - argocd/instances/ibm-apic-management-portal-instance.yaml - argocd/operators/ibm-datapower-operator.yaml - argocd/operators/ibm-foundations.yaml - argocd/operators/ibm-catalogs.yaml - argocd/operators/openshift-pipelines.yaml where you can figure out what will get installed/deployed on your IBM API Connect Management and Portal cluster: argocd/operators/ibm-catalogs.yaml - will install the IBM Operator Catalogs so that you can install the different IBM Operators we need for your use case. argocd/operators/ibm-foundations.yaml and argocd/operators/ibm-datapower-operator.yaml - will install the IBM Foundations and IBM DataPower operators that are a dependency for the IBM API Connect operator. argocd/operators/ibm-apic-operator.yaml - will install the IBM API Connect operator. argocd/instances/ibm-apic-management-portal-instance.yaml - will install an instance of the IBM API Connect Management and Portal components. argocd/operators/openshift-pipelines.yaml - will install the Red Hat OpenShift Pipelines operator that will allow you to execute Red Hat OpenShift Pipelines in the following sections to configure your IBM API Connect instance as well as to publish your IBM API Connect products and APIs. Update multi-tenancy-gitops-apps repository \u00b6 The multi-tenancy-gitops-apps repository contains the actual IBM API Connect artifacts that will get installed on your Red Hat OpenShift clusters. For example, if you have a closer look at the 0-bootstrap/others/apic-multi-cluster/management-portal-cluster/2-services/argocd/instances/ibm-apic-management-portal-instance.yaml from your multi-tenancy-gitops repo you will see the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : ibm-apic-management-portal-instance annotations : argocd.argoproj.io/sync-wave : \"240\" finalizers : - resources-finalizer.argocd.argoproj.io labels : gitops.tier.group : cloudpak gitops.tier.layer : applications spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : services source : # repoURL and targetRevision populated by kustomize patches in 2-services/kustomization.yaml path : apic/config/argocd/multi-cluster/instances/management-portal-instance syncPolicy : automated : prune : true selfHeal : true The ibm-apic-management-portal-instance.yaml file is nothing but an ArgoCD Application that will point to another place for the resources to be deployed/installed as a result. In line number 11, this ArgoCD Application is labelled with gitops.tier.layer: applications so that, as you can read in line number 17, it will take the appropriate value for repoURL and targetRevision . In line number 17, this ArgoCD Application will get its repoURL and targetRevision attributes of the source property patched so that these point to the appropriate repository, where the source for the resources to be created is. In line number 18, you can see the path where the aforementioned resources are within the repoURL repository. To see what that repoURL is you need to inspect, once again, the 0-bootstrap/others/apic-multi-cluster/management-portal-cluster/2-services/kustomization.yaml file. In here you should find the Kustomize patches at the very end: patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/apic-multi-cluster/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/apic-multi-cluster/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 and you should be able to identify the patches that will get applied to repoURL and targetRevision for each of the ArgoCD Applications that are defined to be installed/deployed within that very same kustomization.yaml file. In summary, you have seen that certain resources we have configured in the multi-tenancy-gitops repository to be installed/deployed actually point to the multi-tenancy-gitops-apps repository. As a result of that, you need to also update the multi-tenancy-gitops-apps repository to your desired configuration for the IBM API Connect multi-cluster use case. If you inspect the IBM API Connect subdirectory ( apic ) within the multi-tenancy-gitops-apps repository with tree multi-tenancy-gitops-apps/apic -L 3 you should see the following output: multi-tenancy-gitops-apps/apic \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 multi-cluster \u2502 \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 environments \u2502 \u251c\u2500\u2500 multi-cluster \u2502 \u2502 \u251c\u2500\u2500 app \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 ops \u2502 \u2514\u2500\u2500 single-cluster \u2502 \u251c\u2500\u2500 instances \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 pipelines \u2502 \u251c\u2500\u2500 roles \u2502 \u2514\u2500\u2500 tasks \u251c\u2500\u2500 scripts \u2502 \u251c\u2500\u2500 create-apic-multi-cluster-instances.sh \u2502 \u2514\u2500\u2500 create-gateway-analytics-instance.sh \u2514\u2500\u2500 templates \u2514\u2500\u2500 apic-multi-cluster \u251c\u2500\u2500 gateway-analytics-instance \u251c\u2500\u2500 gateway-analytics-instance.yaml \u251c\u2500\u2500 management-portal-instance \u2514\u2500\u2500 management-portal-instance.yaml where config stores the ArgoCD Applications that the multi-tenancy-gitops repository you worked with in this section points their ArgoCD Applications to. environments stores all the Kubernetes resources that will get installed/deployed on your Red Hat OpenShift clusters. scripts stores a couple of utility scripts that will help you creating your instances within your environments matching the same apic-multi-cluster profile structure you created earlier in multi-tenancy-gitops/0-bootstrap/others . templates stores those apic-multi-cluster instances templates. It seems you somehow need to replicate the structure for your IBM API Connect multi-cluster profile you created within the multi-tenancy-gitops repository in the previous subsection but now within your multi-tenancy-gitops-apps repository. And this makes sense since you saw earlier that the apic-multi-cluster profile will vary based on your specific requirements and naming and therefore you can only re-create that structure in the multi-tenancy-gitops-apps repository once you have created the apic-multi-cluster profile structure within the multi-tenancy-gitops repository. Moreover, you know that the IBM API Connect Management and Portal cluster plus the IBM API Connect Gateway and Analytics clusters created within the apic-multi-cluster profile in the multi-tenancy-gitops repository will, in turn, enable in their respective 2-services/kustomization.yaml files either the ibm-apic-management-portal-instance.yaml or ibm-apic-gateway-analytics-instance.yaml and that those instances are nothing but ArgoCD Applications pointing to this multi-tenancy-gitops-apps repository. So let's have a look at what you need to recreate the structure of your apic-multi-cluster profile. Go to multi-tenancy-gitops-apps/apic/scripts : cd $HOME /git cd multi-tenancy-gitops-apps/apic/scripts Execute the create-apic-multi-cluster-instances.sh script: NAME = IBM ./create-apic-multi-cluster-instances.sh where NAME should be set to the same name you used in the multi-tenancy-gitops repository for the IBM API Connect Gateway and Analytics cluster you created. You should see similar output as: Creating the IBM API Connect component instances under multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances Creating the IBM API Connect component instances bootstrap applications under --> multi-tenancy-gitops-apps/apic/config/argocd/multi-cluster/instances The IBM Gateway and Analytics instance name is IBM Done Execute the create-gateway-analytics-instance.sh script for any extra IBM API Connect Gateway and Analytics cluster you might have created in the apic-multi-cluster profile in the multi-tenancy-gitops repository. NAME = AWS ./create-gateway-analytics-instance.sh where NAME should be set to the same name you used in the multi-tenancy-gitops repository for your extra IBM API Connect Gateway and Analytics cluster you created. You should see similar output as: Creating a new IBM Gateway and Analytics instance called AWS The IBM API Gateway and Analytics instance will be under --> multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances The IBM API Gateway and Analytics instance bootstrap application will be under --> multi-tenancy-gitops-apps/apic/config/argocd/multi-cluster/instances Done Let's look at the new structure for your multi-tenancy-gitops-apps repository. Let's first look at the multi-tenancy-gitops-apps/apic/config/argocd/multi-cluster folder which is where the ArgoCD applications from multi-tenancy-gitops repository point to. Execute tree multi-tenancy-gitops-apps/apic/config/argocd/multi-cluster -L 3 : multi-tenancy-gitops-apps/apic/config/argocd/multi-cluster \u251c\u2500\u2500 app \u2502 \u2514\u2500\u2500 apic-app-multi-cluster.yaml \u251c\u2500\u2500 instances \u2502 \u251c\u2500\u2500 AWS-gateway-analytics-instance \u2502 \u2502 \u2514\u2500\u2500 AWS-gateway-analytics-instance.yaml \u2502 \u251c\u2500\u2500 IBM-gateway-analytics-instance \u2502 \u2502 \u2514\u2500\u2500 IBM-gateway-analytics-instance.yaml \u2502 \u2514\u2500\u2500 management-portal-instance \u2502 \u2514\u2500\u2500 management-portal-instance.yaml \u2514\u2500\u2500 ops \u2514\u2500\u2500 apic-pipelines-multi-cluster.yaml You can see now that under instances you have the same structure in terms of IBM API Connect Management and Portal and IBM API Connect Gateway and Analytics clusters as you have in your multi-tenancy-gitops repository. There is also an app and ops folders which will be responsible for deploying a dummy application that returns a message about the cloud where it is running (for later testing of the scenario) and the OpenShift pipelines that will configure your IBM API Connect multi-cluster instance and publish your IBM API Connect product and APIs respectively. These folders, which in turn contain ArgoCD applications, will get deployed in the IBM API Connect Gateway and Analytics and IBM API Connect Management and Portal clusters respectively. You can figure this out by checking the 3-services/kustomization.yaml files for your clusters within the apic-multi-cluster profile in the multi-tenancy-gitops repository. You will work with the application and the OpenShift Pipelines in the following sections. Now, let's have a look at the multi-tenancy-gitops-apps/apic/environments/multi-cluster folder which is where the above ArgoCD Applications will be watching Kubernetes resources for. Execute tree multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances -L 3 (the app and ops folders will get examined in the following sections): multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances \u251c\u2500\u2500 AWS-gateway-analytics-instance \u2502 \u251c\u2500\u2500 certificates \u2502 \u2502 \u251c\u2500\u2500 gateway-peering.yaml \u2502 \u2502 \u251c\u2500\u2500 gateway-service.yaml \u2502 \u2502 \u2514\u2500\u2500 ingress-ca.yaml \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-analytics-instance.yaml \u2502 \u2502 \u2514\u2500\u2500 ibm-apic-gateway-instance.yaml \u2502 \u251c\u2500\u2500 issuers \u2502 \u2502 \u251c\u2500\u2500 ingress-issuer.yaml \u2502 \u2502 \u2514\u2500\u2500 selfsigning-issuer.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 scripts \u2502 \u2502 \u2514\u2500\u2500 set-storage-ocp-domain.sh \u2502 \u2514\u2500\u2500 secrets \u2502 \u251c\u2500\u2500 datapower-admin-credentials.yaml \u2502 \u2514\u2500\u2500 ingress-ca.yaml \u251c\u2500\u2500 IBM-gateway-analytics-instance \u2502 \u251c\u2500\u2500 certificates \u2502 \u2502 \u251c\u2500\u2500 gateway-peering.yaml \u2502 \u2502 \u251c\u2500\u2500 gateway-service.yaml \u2502 \u2502 \u2514\u2500\u2500 ingress-ca.yaml \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-analytics-instance.yaml \u2502 \u2502 \u2514\u2500\u2500 ibm-apic-gateway-instance.yaml \u2502 \u251c\u2500\u2500 issuers \u2502 \u2502 \u251c\u2500\u2500 ingress-issuer.yaml \u2502 \u2502 \u2514\u2500\u2500 selfsigning-issuer.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 scripts \u2502 \u2502 \u2514\u2500\u2500 set-storage-ocp-domain.sh \u2502 \u2514\u2500\u2500 secrets \u2502 \u251c\u2500\u2500 datapower-admin-credentials.yaml \u2502 \u2514\u2500\u2500 ingress-ca.yaml \u2514\u2500\u2500 management-portal-instance \u251c\u2500\u2500 certificates \u2502 \u251c\u2500\u2500 analytics-client-client.yaml \u2502 \u251c\u2500\u2500 analytics-ingestion-client.yaml \u2502 \u251c\u2500\u2500 gateway-client-client.yaml \u2502 \u251c\u2500\u2500 ingress-ca.yaml \u2502 \u2514\u2500\u2500 portal-admin-client.yaml \u251c\u2500\u2500 instances \u2502 \u251c\u2500\u2500 ibm-apic-management-instance.yaml \u2502 \u2514\u2500\u2500 ibm-apic-portal-instance.yaml \u251c\u2500\u2500 issuers \u2502 \u251c\u2500\u2500 ingress-issuer.yaml \u2502 \u2514\u2500\u2500 selfsigning-issuer.yaml \u251c\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 set-storage-ocp-domain.sh \u2514\u2500\u2500 secrets \u251c\u2500\u2500 ingress-ca.yaml \u2514\u2500\u2500 management-admin-credentials.yaml where you can see A folder per IBM API Connect Management and Portal cluster and IBM API Connect Gateway and Analytics clusters that should resemble, as already said, the structure you defined for the apic-multi-cluster profile in the multi-tenancy-gitops repository. A certificates , issuers and secrets folders that will store the certificates, issuers and secrets that need to get created/deployed in order for the specific IBM API Connect components to work fine. An instances folder that will store the specific IBM API Connect component instances that will get deployed/created in that cluster. A scripts folder that will store a utility script to help you set the specific Red Hat OpenShift domain and block storage class for that Red Hat OpenShift cluster. A kustomization.yaml file that, once again, will contain the index of resources the ArgoCD Application taking care of this location within your multi-tenancy-gitops-apps repository will create/deploy. Certificates and Issuers IBM API Connect requires certificates to stablish secure mutual TLS (mTLS) communication between its Management component and the rest of the components. The certificates can be provided or you can let the IBM API Connect operator, with the help of the IBM Cert Manager operator installed as part of the IBM Foundations operator, create them. It will use the Issuers to create these which in turn require of a Certificate Authority to sign these with. Since this is a reference asset, certificates will get created by the IBM API Connect operator. However, a Certificate Authority called ingress-ca.yaml has been provided within the secrets folder so that it reflects a more real world scenario. For more information about certificates, issuers and certificate authorities, please contact and IBM API Connect SME as this is out of the scope of this asset. There are just another two steps you need to take in order to have the multi-tenancy-gitops-apps repository totally configured for you specific IBM API Connect multi-cluster use case. First, you need to configure each of the management, portal, gateway and analytics instances within your Management and Portal and Gateway and Analytics instances above in multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances with the specific Red Hat OpenShift domain and block storage class name for the Red Hat OpenShift cluster these will be installed/deployed into. For example, if you inspect the multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances/management-portal-instance/instances/ibm-apic-management-instance.yaml instance: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : ibm-apic-management-instance annotations : argocd.argoproj.io/sync-wave : \"250\" finalizers : - resources-finalizer.argocd.argoproj.io labels : gitops.tier.group : cloudpak gitops.tier.layer : services spec : syncPolicy : automated : prune : true selfHeal : true destination : namespace : tools server : https://kubernetes.default.svc project : services source : path : instances/ibm-apic-mgmt-instance helm : values : | ibm-apic-mgmt-instance: ibmapicmgmtinstance: name: management spec: version: 10.0.3.0-ifix1 profile: n3xc4.m16 portal: admin: secretName: portal-admin-client analytics: client: secretName: analytics-client-client ingestion: secretName: analytics-ingestion-client gateway: client: secretName: gateway-client-client cloudManagerEndpoint: annotations: certmanager.k8s.io/issuer: ingress-issuer hosts: - name: admin.<your-openshift-domain> secretName: management-cm-endpoint-secret apiManagerEndpoint: annotations: certmanager.k8s.io/issuer: ingress-issuer hosts: - name: manager.<your-openshift-domain> secretName: management-apim-endpoint-secret platformAPIEndpoint: annotations: certmanager.k8s.io/issuer: ingress-issuer hosts: - name: api.<your-openshift-domain> secretName: management-api-endpoint-secret consumerAPIEndpoint: annotations: certmanager.k8s.io/issuer: ingress-issuer hosts: - name: consumer.<your-openshift-domain> secretName: management-consumer-endpoint-secret databaseVolumeClaimTemplate: storageClassName: <your-block-storage-class> microServiceSecurity: certManager certManagerIssuer: name: selfsigning-issuer kind: Issuer adminUser: secretName: management-admin-credentials license: accept: true license: L-RJON-BZEP9N use: production you will see in lines 46 , 52 , 58 and 64 that there is a placeholder for your Red Hat OpenShift cluster domain and in line 67 there is a placeholder for you Red Hat OpenShift cluster block storage class name. For replacing these placeholders at once for a particular IBM API Connect Management and Portal or IBM API Connect Gateway and Analytics instance, you have the set-storage-ocp-domain.sh script under the scripts folder of that specific instance. All you need to do is execute it: Change directory to the specific instance within multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances and into the scripts folder. Execute the set-storage-ocp-domain.sh script: OCP_DOMAIN = <your_ocp_domain> STORAGE = <your_block_storage_class_name> ./set-storage-ocp-domain.sh where <your_ocp_domain> is the domain for your Red Hat OpenShift cluster. Tip You can find your Red Hat OpenShift cluster domain by executing oc get route console -n openshift-console -o jsonpath='{.spec.host}' . Your Red Hat OpenShift cluster domain will be what is after console-openshift-console. . That is, what the command would return you is console-openshift-console.<your_ocp_domain> . Another way to find out your Red Hat OpenShift cluster domain would be with the following command oc get route console -n openshift-console -o jsonpath='{.status.ingress[].routerCanonicalHostname}' <your_block_storage_class_name> is the name of a block storage class in your Red Hat OpenShift cluster. Tip You can find your Storage Classes with oc get storageclass After executing the set-storage-ocp-domain.sh script, you should see similar output as: Setting the storage class name for the IBM API Connect Management and Portal components to ibmc-block-gold Setting the OpenShift domain for the IBM API Connect Management and Portal components to itzroks-270005wyhs-0y6bxj-6ccd7f378ae819553d37d5f2ee142bd6-0000.eu-gb.containers.appdomain.cloud Done If you now inspect again the ibm-apic-management-instance.yaml or ibm-apic-portal-instance.yaml under multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances/management-portal-instance/instances/ you should now see that the placeholders have been properly replaced. Again, we have chosen the IBM API Connect Management and Portal instance as the example. Important Remember you need to set the Red Hat OpenShift cluster domain and block storage class name for the IBM API Connect Management and Portal instance and all the IBM API Connect Gateway and Analytics instances you have configured your IBM API Connect multi-cluster use case with. That is, all the instances you will find within multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances Second, you also need to set the git sources for all of the ArgoCD Applications and kustomization.yaml files within the multi-tenancy-gitops-apps repository, in the same fashion you did earlier in the multi-tenancy-gitops repository, so that the repoURL and targetRevision get patched with the appropriate GitHub repository URLs. As we do in the multi-tenancy-gitops repository, we also provide a utility script called set-git-source.sh under the multi-tenancy-gitops-apps/scripts folder to help you with replacing those git source placeholders. Change directory to multi-tenancy-gitops-apps/scripts . Execute the set-git-source.sh script: GIT_ORG = <your_git_org> ./scripts/set-git-source.sh where <your_git_org> is your GitHub organization you created in the Configure Clusters - GitOps section and forked the GitOps repositories you are using to drive the GitOps processes for the IBM API Connect multi-cluster use case into. Info The set-git-source.sh allows for fine-grained customization of each of the GitOps repositories and their respective branches for greater flexibility. However, we assume you are working off the master branch and your GitOps repositories are called as the original GitOps repositories your forked from. Warning This script might take a minute or so to execute. You should see similar output as: Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops-apps.git on branch master done replacing variables in ArgoCD Application files git commit and push changes now Finally, you can commit and deliver the changes to your multi-tenancy-gitops-apps repository: git add . git commit -s -m \"Created APIC multi-cluster instances\" git push origin $GIT_BRANCH Congratulations! You have created an IBM API Connect multi-cluster profile in your multi-tenancy-gitops repository that should suit your IBM API Connect multi-cluster use case needs. You have then replicated that profile structure in your multi-tenancy-gitops-apps repository where the actual resources to be installed/deployed by the different ArgoCD applications reside after tailoring these to your Red Hat OpenShift cluster specifics. As a result, you have successfully configured both your multi-tenancy-gitops and multi-tenancy-gitops-apps repositories that will be driving the GitOps process for deploying your IBM API Connect multi-cluster scenario. Go to the next section to start deploying it!","title":"Configure GitOps repos"},{"location":"guides/cp4i/apic/multi-cluster/configure-clusters-gitops-config/#ibm-api-connect-multi-cluster-configure-the-clusters-configure-gitops-repos","text":"","title":"IBM API Connect Multi-Cluster - Configure the clusters - Configure GitOps repos"},{"location":"guides/cp4i/apic/multi-cluster/configure-clusters-gitops-config/#overview","text":"In the previous chapter of this guide, we installed the Red Hat GitOps Operator, which relies on ArgoCD behind the scenes, as the fundamental component for deploying and managing any component on our clusters. We also went through creating a GitHub organization where the different GitOps repositories that will drive the GitOps processes for our clusters will be forked into. These GitOps repositories that will store the definitions and configurations of all the components we want to have in our clusters, that ArgoCD will use as the source of truth to what needs to be in our clusters or not, are: The multi-tenancy-gitops repository The multi-tenancy-gitops-infra repository The multi-tenancy-gitops-services repository The multi-tenancy-gitops-apps repository By now, you should have a copy/fork of each of these repositories in the GitHub organization you must have created in the previous section too as well as a clone of those GitOps repositories on your local workstation. In fact, in the previous section we already used the local clone of the multi-tenancy-gitops repository to install ArgoCD in each of your Red Hat OpenShift clusters where we are going to install IBM API Connect components. Goal In this section, we are going to see how to create the appropriate structure in the multi-tenancy-gitops and multi-tenancy-gitops-apps repositories that will allow us to install the right specific components of IBM API Connect to successfully deploy IBM API Connect in a multi-cluster landscape.","title":"Overview"},{"location":"guides/cp4i/apic/multi-cluster/configure-clusters-gitops-config/#pre-requisites","text":"For a better understanding of the following sections, we strongly recommend to review and understand the Getting started with GitOps section for the IBM API Connect single cluster scenario tutorial where a more in-depth explanation of the multi-tenancy-gitops GitOps repo structure can be found.","title":"Pre-requisites"},{"location":"guides/cp4i/apic/multi-cluster/configure-clusters-gitops-config/#customizing-the-gitops-repositories","text":"The cloned GitOps repositories (specifically, multi-tenancy-gitops and multi-tenancy-gitops-apps repositories) need to be customized for a correct deployment of IBM API Connect in a multi-cluster landscape. We will see in the following sections how to do the appropriate customizations to the aforementioned GitOps repositories.","title":"Customizing the GitOps repositories"},{"location":"guides/cp4i/apic/multi-cluster/configure-clusters-gitops-config/#update-multi-tenancy-gitops-repository","text":"The multi-tenancy-gitops repository contains different profiles for different potential infrastructure landscapes. If you execute tree multi-tenancy-gitops/0-bootstrap -L 2 , multi-tenancy-gitops/0-bootstrap \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml you should see the single-cluster and others folder whose aim is to represent those different potential infrastructure landscapes. The single-cluster profile represents the layered GitOps structure ( 1-infra , 2-services , 3-apps ) for a single cluster. On the other hand, others folder contains other more complex profiles for multi-cluster infrastructure landscapes. If you execute tree multi-tenancy-gitops/0-bootstrap/others -L 3 multi-tenancy-gitops/0-bootstrap/others \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 bootstrap-cluster-1-cicd-dev-stage-prod.yaml \u2502 \u251c\u2500\u2500 bootstrap-cluster-n-prod.yaml \u2502 \u251c\u2500\u2500 cluster-1-cicd-dev-stage-prod \u2502 \u2502 \u251c\u2500\u2500 1-infra \u2502 \u2502 \u251c\u2500\u2500 2-services \u2502 \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 cluster-n-prod \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2-isolated-cluster \u2502 \u251c\u2500\u2500 bootstrap-cluster-1-cicd-dev-stage.yaml \u2502 \u251c\u2500\u2500 bootstrap-cluster-n-prod.yaml \u2502 \u251c\u2500\u2500 cluster-1-cicd-dev-stage \u2502 \u2502 \u251c\u2500\u2500 1-infra \u2502 \u2502 \u251c\u2500\u2500 2-services \u2502 \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 cluster-n-prod \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 3-multi-cluster \u251c\u2500\u2500 bootstrap-cluster-1-cicd.yaml \u251c\u2500\u2500 bootstrap-cluster-1-dev.yaml \u251c\u2500\u2500 bootstrap-cluster-1-stage.yaml \u251c\u2500\u2500 bootstrap-cluster-n-prod.yaml \u251c\u2500\u2500 cluster-1-cicd \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 cluster-2-dev \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 cluster-3-stage \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 cluster-n-prod \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u2514\u2500\u2500 kustomization.yaml you will see that each of those more complex multi-cluster infrastructure landscape profiles are the same as the single-cluster profile as far as the layered GitOps structure that represents a clusters but with the difference that in these multi-cluster profiles we will be driving the GitOps processes for several clusters. Tip Once again, we strongly recommend to review and understand the Getting started with GitOps section for the IBM API Connect single cluster scenario tutorial, where a more in-depth explanation of the multi-tenancy-gitops GitOps approach and repo structure can be found. It is clear then that the profile for our IBM API Connect multi-cluster use case will need to live under others . However, since this is a very specific use case for an IBM capability such as IBM API Connect, we have not included such profile on the GitOps repo by default out of the box. Instead, we have placed the structure for what will be your IBM API Connect Management and Portal cluster and IBM API Connect Gateway and Analytics cluster profiles under docs/scenarios/apic-multi-cluster . If you execute tree multi-tenancy-gitops/doc/scenarios/apic-multi-cluster -L 2 , multi-tenancy-gitops/doc/scenarios/apic-multi-cluster \u251c\u2500\u2500 bootstrap-gateway-analytics-cluster.yaml \u251c\u2500\u2500 bootstrap-management-portal-cluster.yaml \u251c\u2500\u2500 gateway-analytics-cluster \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 management-portal-cluster \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 scripts \u251c\u2500\u2500 create-apic-multi-cluster-profile.sh \u2514\u2500\u2500 create-gateway-analytics-cluster.sh you should see, as in with any of the other profiles, that your IBM API Connect Management and Portal cluster and IBM API Connect Gateway and Analytics cluster profiles look exactly the same as far as the layered GitOps structure is concerned. The content under multi-tenancy-gitops/doc/scenarios/apic-multi-cluster should be treated as the templates which you will build your apic-multi-cluster profile under multi-tenancy-gitops/0-bootstrap/others from. For doing that, we provide two scripts under the scripts folder you can see above. The create-apic-multi-cluster-profile.sh script will be responsible for creating that apic-multi-cluster profile under multi-tenancy-gitops/0-bootstrap/others . The create-gateway-analytics-cluster.sh script will be responsible for creating extra IBM API Connect Gateway and Analytics clusters under the apic-multi-cluster profile you have just created with the other script. This will allow you to create those IBM API Connect multi-cluster implementations where you have multiple clusters hosting applications that you would want to front with the IBM API Connect Gateway and Analytics components for securely exposing their APIs to outside world as well as using IBM API Connect capabilities for publishing and socializing such APIs as it is explained in the Overview section. Let's have a look at what you need to do in this multi-tenancy-gitops GitOps repository to create the apic-multi-cluster profile that will drive your GitOps process to implement the IBM API Connect multi-cluster use case introduced in the Overview section which will be made up of one IBM API Connect Management and Portal cluster and two IBM API Connect Gateway and Analytics clusters. The instructions below will simulate, with the names used for the IBM API Connect Gateway and Analytics clusters, that one of those clusters will be in IBM Cloud and the other in AWS. However, all the clusters used in this set of instructions are hosted in IBM Cloud and have been requested through IBM Technology Zone as explained in the Create clusters section. Go to multi-tenancy-gitops/doc/scenarios/apic-multi-cluster/scripts cd $HOME /git cd multi-tenancy-gitops/doc/scenarios/apic-multi-cluster/scripts Execute the create-apic-multi-cluster-profile.sh script: NAME = IBM ./create-apic-multi-cluster-profile.sh You should see the following output: Creating an IBM API Connect multi-cluster profile under 0-bootstrap/others The IBM Gateway and Analytics cluster will be called IBM Done You can find the new apic-multi-cluster profile under 0-bootsrap/others/apic-multi-cluster Execute the create-gateway-analytics-cluster.sh script to create the IBM API Connect Gateway and Analytics cluster that will be hosted in AWS under your apic-multi-cluster profile created in the previous step: NAME = AWS ./create-gateway-analytics-cluster.sh You should see the following output: Creating a new IBM Gateway and Analytics cluster called AWS Your new cluster can be found in the AWS-gateway-analytics-cluster folder The bootstrap ArgoCD application for you new cluster is bootstrap-AWS-gateway-analytics-cluster.yaml Change directory and go back to multi-tenancy-gitops : cd $HOME /git cd multi-tenancy-gitops Execute tree 0-bootstrap/others/apic-multi-cluster -L 2 , you should see the following output: 0-bootstrap/others/apic-multi-cluster \u251c\u2500\u2500 AWS-gateway-analytics-cluster \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 IBM-gateway-analytics-cluster \u2502 \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 3-apps \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap-AWS-gateway-analytics-cluster.yaml \u251c\u2500\u2500 bootstrap-IBM-gateway-analytics-cluster.yaml \u251c\u2500\u2500 bootstrap-management-portal-cluster.yaml \u2514\u2500\u2500 management-portal-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u2514\u2500\u2500 kustomization.yaml which should correspond to your desired IBM API Connect multi-cluster GitOps profile which we will use to drive the implementation of the IBM API Connect multi-cluster use case. Last step would be to configure all of the ArgoCD applications and ArgoCD projects that will drive the GitOps processes to point to the appropriate GitHub organization and repositories where you are delivering these changes to. That is the GitHub organization and repositories your created and forked respectively in the Configure Clusters - GitOps section. As you will see in the following section of this tutorial, you will be kicking off those GitOps processes that will get your RedHat OpenShift clusters with the components and configuration that you want and that you specified through these GitOps repositories just now through the bootstrap files above. If you inspect the content of one of those bootstrap files above: cat 0 -bootstrap/others/apic-multi-cluster/bootstrap-management-portal-cluster.yaml you should see the following: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-management-portal namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/others/apic-multi-cluster/management-portal-cluster repoURL : ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS} targetRevision : ${GIT_GITOPS_BRANCH} syncPolicy : automated : prune : true selfHeal : true where you can see highlighted that the source for this ArgoCD Application resides in a GitHub repository at repoURL , on the branch targetRevision and on the path path and that the repoURL and targetRevision are parametrized. The reason for this is to allow the multi-tenancy-gitops repo to be a template you can fork and work with without stepping into anyone else's work. As a result, we will need to configure all of these ArgoCD yaml files to point to your GitHub organization and repositories you are working with for the IBM API Connect multi-cluster use case. For this, we have provided a util script that is under multi-tenancy-gitops/scripts . Execute the set-git-source.sh script: GIT_ORG = <your_git_org> ./scripts/set-git-source.sh where <your_git_org> is your GitHub organization you created in the Configure Clusters - GitOps section and forked the GitOps repositories you are using to drive the GitOps processes for the IBM API Connect multi-cluster use case into. Info The set-git-source.sh allows for fine-grained customization of each of the GitOps repositories and their respective branches for greater flexibility. However, we assume you are working off the master branch and your GitOps repositories are called as the original GitOps repositories your forked from. Warning This script might take a minute or so to execute. You should see the following output: Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now (*) where we have used apic-multi-cluster as our GitHub organization. If you inspect the bootstrap file again, you should see it will now point to the GitHub organization and repository you are storing your GitOps configuration into. apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-management-portal namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/others/apic-multi-cluster/management-portal-cluster repoURL : https://github.com/apic-multi-cluster/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Finally, you can commit and deliver the changes to your multi-tenancy-gitops repository: git add . git commit -s -m \"Created APIC multi-cluster profile\" git push origin $GIT_BRANCH","title":"Update multi-tenancy-gitops repository"},{"location":"guides/cp4i/apic/multi-cluster/configure-clusters-gitops-config/#update-multi-tenancy-gitops-apps-repository","text":"The multi-tenancy-gitops-apps repository contains the actual IBM API Connect artifacts that will get installed on your Red Hat OpenShift clusters. For example, if you have a closer look at the 0-bootstrap/others/apic-multi-cluster/management-portal-cluster/2-services/argocd/instances/ibm-apic-management-portal-instance.yaml from your multi-tenancy-gitops repo you will see the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : ibm-apic-management-portal-instance annotations : argocd.argoproj.io/sync-wave : \"240\" finalizers : - resources-finalizer.argocd.argoproj.io labels : gitops.tier.group : cloudpak gitops.tier.layer : applications spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : services source : # repoURL and targetRevision populated by kustomize patches in 2-services/kustomization.yaml path : apic/config/argocd/multi-cluster/instances/management-portal-instance syncPolicy : automated : prune : true selfHeal : true The ibm-apic-management-portal-instance.yaml file is nothing but an ArgoCD Application that will point to another place for the resources to be deployed/installed as a result. In line number 11, this ArgoCD Application is labelled with gitops.tier.layer: applications so that, as you can read in line number 17, it will take the appropriate value for repoURL and targetRevision . In line number 17, this ArgoCD Application will get its repoURL and targetRevision attributes of the source property patched so that these point to the appropriate repository, where the source for the resources to be created is. In line number 18, you can see the path where the aforementioned resources are within the repoURL repository. To see what that repoURL is you need to inspect, once again, the 0-bootstrap/others/apic-multi-cluster/management-portal-cluster/2-services/kustomization.yaml file. In here you should find the Kustomize patches at the very end: patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/apic-multi-cluster/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/apic-multi-cluster/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 and you should be able to identify the patches that will get applied to repoURL and targetRevision for each of the ArgoCD Applications that are defined to be installed/deployed within that very same kustomization.yaml file. In summary, you have seen that certain resources we have configured in the multi-tenancy-gitops repository to be installed/deployed actually point to the multi-tenancy-gitops-apps repository. As a result of that, you need to also update the multi-tenancy-gitops-apps repository to your desired configuration for the IBM API Connect multi-cluster use case. If you inspect the IBM API Connect subdirectory ( apic ) within the multi-tenancy-gitops-apps repository with tree multi-tenancy-gitops-apps/apic -L 3 you should see the following output: multi-tenancy-gitops-apps/apic \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 multi-cluster \u2502 \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 environments \u2502 \u251c\u2500\u2500 multi-cluster \u2502 \u2502 \u251c\u2500\u2500 app \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 ops \u2502 \u2514\u2500\u2500 single-cluster \u2502 \u251c\u2500\u2500 instances \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 pipelines \u2502 \u251c\u2500\u2500 roles \u2502 \u2514\u2500\u2500 tasks \u251c\u2500\u2500 scripts \u2502 \u251c\u2500\u2500 create-apic-multi-cluster-instances.sh \u2502 \u2514\u2500\u2500 create-gateway-analytics-instance.sh \u2514\u2500\u2500 templates \u2514\u2500\u2500 apic-multi-cluster \u251c\u2500\u2500 gateway-analytics-instance \u251c\u2500\u2500 gateway-analytics-instance.yaml \u251c\u2500\u2500 management-portal-instance \u2514\u2500\u2500 management-portal-instance.yaml where config stores the ArgoCD Applications that the multi-tenancy-gitops repository you worked with in this section points their ArgoCD Applications to. environments stores all the Kubernetes resources that will get installed/deployed on your Red Hat OpenShift clusters. scripts stores a couple of utility scripts that will help you creating your instances within your environments matching the same apic-multi-cluster profile structure you created earlier in multi-tenancy-gitops/0-bootstrap/others . templates stores those apic-multi-cluster instances templates. It seems you somehow need to replicate the structure for your IBM API Connect multi-cluster profile you created within the multi-tenancy-gitops repository in the previous subsection but now within your multi-tenancy-gitops-apps repository. And this makes sense since you saw earlier that the apic-multi-cluster profile will vary based on your specific requirements and naming and therefore you can only re-create that structure in the multi-tenancy-gitops-apps repository once you have created the apic-multi-cluster profile structure within the multi-tenancy-gitops repository. Moreover, you know that the IBM API Connect Management and Portal cluster plus the IBM API Connect Gateway and Analytics clusters created within the apic-multi-cluster profile in the multi-tenancy-gitops repository will, in turn, enable in their respective 2-services/kustomization.yaml files either the ibm-apic-management-portal-instance.yaml or ibm-apic-gateway-analytics-instance.yaml and that those instances are nothing but ArgoCD Applications pointing to this multi-tenancy-gitops-apps repository. So let's have a look at what you need to recreate the structure of your apic-multi-cluster profile. Go to multi-tenancy-gitops-apps/apic/scripts : cd $HOME /git cd multi-tenancy-gitops-apps/apic/scripts Execute the create-apic-multi-cluster-instances.sh script: NAME = IBM ./create-apic-multi-cluster-instances.sh where NAME should be set to the same name you used in the multi-tenancy-gitops repository for the IBM API Connect Gateway and Analytics cluster you created. You should see similar output as: Creating the IBM API Connect component instances under multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances Creating the IBM API Connect component instances bootstrap applications under --> multi-tenancy-gitops-apps/apic/config/argocd/multi-cluster/instances The IBM Gateway and Analytics instance name is IBM Done Execute the create-gateway-analytics-instance.sh script for any extra IBM API Connect Gateway and Analytics cluster you might have created in the apic-multi-cluster profile in the multi-tenancy-gitops repository. NAME = AWS ./create-gateway-analytics-instance.sh where NAME should be set to the same name you used in the multi-tenancy-gitops repository for your extra IBM API Connect Gateway and Analytics cluster you created. You should see similar output as: Creating a new IBM Gateway and Analytics instance called AWS The IBM API Gateway and Analytics instance will be under --> multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances The IBM API Gateway and Analytics instance bootstrap application will be under --> multi-tenancy-gitops-apps/apic/config/argocd/multi-cluster/instances Done Let's look at the new structure for your multi-tenancy-gitops-apps repository. Let's first look at the multi-tenancy-gitops-apps/apic/config/argocd/multi-cluster folder which is where the ArgoCD applications from multi-tenancy-gitops repository point to. Execute tree multi-tenancy-gitops-apps/apic/config/argocd/multi-cluster -L 3 : multi-tenancy-gitops-apps/apic/config/argocd/multi-cluster \u251c\u2500\u2500 app \u2502 \u2514\u2500\u2500 apic-app-multi-cluster.yaml \u251c\u2500\u2500 instances \u2502 \u251c\u2500\u2500 AWS-gateway-analytics-instance \u2502 \u2502 \u2514\u2500\u2500 AWS-gateway-analytics-instance.yaml \u2502 \u251c\u2500\u2500 IBM-gateway-analytics-instance \u2502 \u2502 \u2514\u2500\u2500 IBM-gateway-analytics-instance.yaml \u2502 \u2514\u2500\u2500 management-portal-instance \u2502 \u2514\u2500\u2500 management-portal-instance.yaml \u2514\u2500\u2500 ops \u2514\u2500\u2500 apic-pipelines-multi-cluster.yaml You can see now that under instances you have the same structure in terms of IBM API Connect Management and Portal and IBM API Connect Gateway and Analytics clusters as you have in your multi-tenancy-gitops repository. There is also an app and ops folders which will be responsible for deploying a dummy application that returns a message about the cloud where it is running (for later testing of the scenario) and the OpenShift pipelines that will configure your IBM API Connect multi-cluster instance and publish your IBM API Connect product and APIs respectively. These folders, which in turn contain ArgoCD applications, will get deployed in the IBM API Connect Gateway and Analytics and IBM API Connect Management and Portal clusters respectively. You can figure this out by checking the 3-services/kustomization.yaml files for your clusters within the apic-multi-cluster profile in the multi-tenancy-gitops repository. You will work with the application and the OpenShift Pipelines in the following sections. Now, let's have a look at the multi-tenancy-gitops-apps/apic/environments/multi-cluster folder which is where the above ArgoCD Applications will be watching Kubernetes resources for. Execute tree multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances -L 3 (the app and ops folders will get examined in the following sections): multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances \u251c\u2500\u2500 AWS-gateway-analytics-instance \u2502 \u251c\u2500\u2500 certificates \u2502 \u2502 \u251c\u2500\u2500 gateway-peering.yaml \u2502 \u2502 \u251c\u2500\u2500 gateway-service.yaml \u2502 \u2502 \u2514\u2500\u2500 ingress-ca.yaml \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-analytics-instance.yaml \u2502 \u2502 \u2514\u2500\u2500 ibm-apic-gateway-instance.yaml \u2502 \u251c\u2500\u2500 issuers \u2502 \u2502 \u251c\u2500\u2500 ingress-issuer.yaml \u2502 \u2502 \u2514\u2500\u2500 selfsigning-issuer.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 scripts \u2502 \u2502 \u2514\u2500\u2500 set-storage-ocp-domain.sh \u2502 \u2514\u2500\u2500 secrets \u2502 \u251c\u2500\u2500 datapower-admin-credentials.yaml \u2502 \u2514\u2500\u2500 ingress-ca.yaml \u251c\u2500\u2500 IBM-gateway-analytics-instance \u2502 \u251c\u2500\u2500 certificates \u2502 \u2502 \u251c\u2500\u2500 gateway-peering.yaml \u2502 \u2502 \u251c\u2500\u2500 gateway-service.yaml \u2502 \u2502 \u2514\u2500\u2500 ingress-ca.yaml \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-analytics-instance.yaml \u2502 \u2502 \u2514\u2500\u2500 ibm-apic-gateway-instance.yaml \u2502 \u251c\u2500\u2500 issuers \u2502 \u2502 \u251c\u2500\u2500 ingress-issuer.yaml \u2502 \u2502 \u2514\u2500\u2500 selfsigning-issuer.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 scripts \u2502 \u2502 \u2514\u2500\u2500 set-storage-ocp-domain.sh \u2502 \u2514\u2500\u2500 secrets \u2502 \u251c\u2500\u2500 datapower-admin-credentials.yaml \u2502 \u2514\u2500\u2500 ingress-ca.yaml \u2514\u2500\u2500 management-portal-instance \u251c\u2500\u2500 certificates \u2502 \u251c\u2500\u2500 analytics-client-client.yaml \u2502 \u251c\u2500\u2500 analytics-ingestion-client.yaml \u2502 \u251c\u2500\u2500 gateway-client-client.yaml \u2502 \u251c\u2500\u2500 ingress-ca.yaml \u2502 \u2514\u2500\u2500 portal-admin-client.yaml \u251c\u2500\u2500 instances \u2502 \u251c\u2500\u2500 ibm-apic-management-instance.yaml \u2502 \u2514\u2500\u2500 ibm-apic-portal-instance.yaml \u251c\u2500\u2500 issuers \u2502 \u251c\u2500\u2500 ingress-issuer.yaml \u2502 \u2514\u2500\u2500 selfsigning-issuer.yaml \u251c\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 set-storage-ocp-domain.sh \u2514\u2500\u2500 secrets \u251c\u2500\u2500 ingress-ca.yaml \u2514\u2500\u2500 management-admin-credentials.yaml where you can see A folder per IBM API Connect Management and Portal cluster and IBM API Connect Gateway and Analytics clusters that should resemble, as already said, the structure you defined for the apic-multi-cluster profile in the multi-tenancy-gitops repository. A certificates , issuers and secrets folders that will store the certificates, issuers and secrets that need to get created/deployed in order for the specific IBM API Connect components to work fine. An instances folder that will store the specific IBM API Connect component instances that will get deployed/created in that cluster. A scripts folder that will store a utility script to help you set the specific Red Hat OpenShift domain and block storage class for that Red Hat OpenShift cluster. A kustomization.yaml file that, once again, will contain the index of resources the ArgoCD Application taking care of this location within your multi-tenancy-gitops-apps repository will create/deploy. Certificates and Issuers IBM API Connect requires certificates to stablish secure mutual TLS (mTLS) communication between its Management component and the rest of the components. The certificates can be provided or you can let the IBM API Connect operator, with the help of the IBM Cert Manager operator installed as part of the IBM Foundations operator, create them. It will use the Issuers to create these which in turn require of a Certificate Authority to sign these with. Since this is a reference asset, certificates will get created by the IBM API Connect operator. However, a Certificate Authority called ingress-ca.yaml has been provided within the secrets folder so that it reflects a more real world scenario. For more information about certificates, issuers and certificate authorities, please contact and IBM API Connect SME as this is out of the scope of this asset. There are just another two steps you need to take in order to have the multi-tenancy-gitops-apps repository totally configured for you specific IBM API Connect multi-cluster use case. First, you need to configure each of the management, portal, gateway and analytics instances within your Management and Portal and Gateway and Analytics instances above in multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances with the specific Red Hat OpenShift domain and block storage class name for the Red Hat OpenShift cluster these will be installed/deployed into. For example, if you inspect the multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances/management-portal-instance/instances/ibm-apic-management-instance.yaml instance: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : ibm-apic-management-instance annotations : argocd.argoproj.io/sync-wave : \"250\" finalizers : - resources-finalizer.argocd.argoproj.io labels : gitops.tier.group : cloudpak gitops.tier.layer : services spec : syncPolicy : automated : prune : true selfHeal : true destination : namespace : tools server : https://kubernetes.default.svc project : services source : path : instances/ibm-apic-mgmt-instance helm : values : | ibm-apic-mgmt-instance: ibmapicmgmtinstance: name: management spec: version: 10.0.3.0-ifix1 profile: n3xc4.m16 portal: admin: secretName: portal-admin-client analytics: client: secretName: analytics-client-client ingestion: secretName: analytics-ingestion-client gateway: client: secretName: gateway-client-client cloudManagerEndpoint: annotations: certmanager.k8s.io/issuer: ingress-issuer hosts: - name: admin.<your-openshift-domain> secretName: management-cm-endpoint-secret apiManagerEndpoint: annotations: certmanager.k8s.io/issuer: ingress-issuer hosts: - name: manager.<your-openshift-domain> secretName: management-apim-endpoint-secret platformAPIEndpoint: annotations: certmanager.k8s.io/issuer: ingress-issuer hosts: - name: api.<your-openshift-domain> secretName: management-api-endpoint-secret consumerAPIEndpoint: annotations: certmanager.k8s.io/issuer: ingress-issuer hosts: - name: consumer.<your-openshift-domain> secretName: management-consumer-endpoint-secret databaseVolumeClaimTemplate: storageClassName: <your-block-storage-class> microServiceSecurity: certManager certManagerIssuer: name: selfsigning-issuer kind: Issuer adminUser: secretName: management-admin-credentials license: accept: true license: L-RJON-BZEP9N use: production you will see in lines 46 , 52 , 58 and 64 that there is a placeholder for your Red Hat OpenShift cluster domain and in line 67 there is a placeholder for you Red Hat OpenShift cluster block storage class name. For replacing these placeholders at once for a particular IBM API Connect Management and Portal or IBM API Connect Gateway and Analytics instance, you have the set-storage-ocp-domain.sh script under the scripts folder of that specific instance. All you need to do is execute it: Change directory to the specific instance within multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances and into the scripts folder. Execute the set-storage-ocp-domain.sh script: OCP_DOMAIN = <your_ocp_domain> STORAGE = <your_block_storage_class_name> ./set-storage-ocp-domain.sh where <your_ocp_domain> is the domain for your Red Hat OpenShift cluster. Tip You can find your Red Hat OpenShift cluster domain by executing oc get route console -n openshift-console -o jsonpath='{.spec.host}' . Your Red Hat OpenShift cluster domain will be what is after console-openshift-console. . That is, what the command would return you is console-openshift-console.<your_ocp_domain> . Another way to find out your Red Hat OpenShift cluster domain would be with the following command oc get route console -n openshift-console -o jsonpath='{.status.ingress[].routerCanonicalHostname}' <your_block_storage_class_name> is the name of a block storage class in your Red Hat OpenShift cluster. Tip You can find your Storage Classes with oc get storageclass After executing the set-storage-ocp-domain.sh script, you should see similar output as: Setting the storage class name for the IBM API Connect Management and Portal components to ibmc-block-gold Setting the OpenShift domain for the IBM API Connect Management and Portal components to itzroks-270005wyhs-0y6bxj-6ccd7f378ae819553d37d5f2ee142bd6-0000.eu-gb.containers.appdomain.cloud Done If you now inspect again the ibm-apic-management-instance.yaml or ibm-apic-portal-instance.yaml under multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances/management-portal-instance/instances/ you should now see that the placeholders have been properly replaced. Again, we have chosen the IBM API Connect Management and Portal instance as the example. Important Remember you need to set the Red Hat OpenShift cluster domain and block storage class name for the IBM API Connect Management and Portal instance and all the IBM API Connect Gateway and Analytics instances you have configured your IBM API Connect multi-cluster use case with. That is, all the instances you will find within multi-tenancy-gitops-apps/apic/environments/multi-cluster/instances Second, you also need to set the git sources for all of the ArgoCD Applications and kustomization.yaml files within the multi-tenancy-gitops-apps repository, in the same fashion you did earlier in the multi-tenancy-gitops repository, so that the repoURL and targetRevision get patched with the appropriate GitHub repository URLs. As we do in the multi-tenancy-gitops repository, we also provide a utility script called set-git-source.sh under the multi-tenancy-gitops-apps/scripts folder to help you with replacing those git source placeholders. Change directory to multi-tenancy-gitops-apps/scripts . Execute the set-git-source.sh script: GIT_ORG = <your_git_org> ./scripts/set-git-source.sh where <your_git_org> is your GitHub organization you created in the Configure Clusters - GitOps section and forked the GitOps repositories you are using to drive the GitOps processes for the IBM API Connect multi-cluster use case into. Info The set-git-source.sh allows for fine-grained customization of each of the GitOps repositories and their respective branches for greater flexibility. However, we assume you are working off the master branch and your GitOps repositories are called as the original GitOps repositories your forked from. Warning This script might take a minute or so to execute. You should see similar output as: Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/apic-multi-cluster/multi-tenancy-gitops-apps.git on branch master done replacing variables in ArgoCD Application files git commit and push changes now Finally, you can commit and deliver the changes to your multi-tenancy-gitops-apps repository: git add . git commit -s -m \"Created APIC multi-cluster instances\" git push origin $GIT_BRANCH Congratulations! You have created an IBM API Connect multi-cluster profile in your multi-tenancy-gitops repository that should suit your IBM API Connect multi-cluster use case needs. You have then replicated that profile structure in your multi-tenancy-gitops-apps repository where the actual resources to be installed/deployed by the different ArgoCD applications reside after tailoring these to your Red Hat OpenShift cluster specifics. As a result, you have successfully configured both your multi-tenancy-gitops and multi-tenancy-gitops-apps repositories that will be driving the GitOps process for deploying your IBM API Connect multi-cluster scenario. Go to the next section to start deploying it!","title":"Update multi-tenancy-gitops-apps repository"},{"location":"guides/cp4i/apic/multi-cluster/configure-clusters-gitops/","text":"IBM API Connect Multi-Cluster - Configure the clusters - GitOps repos & ArgoCD \u00b6 Overview \u00b6 A GitOps model must be at the core of the methodology or approach used to deploy and manage any IBM capability in production. It ensures that the installation, configuration and changes of these IBM capabilities, as well as their configuration and citizens (applications, integration flows, APIs, etc), is only done through code. GitOps puts git repositories and git commands such as git push (to request a change) and git merge (to approve a change) at the heart of configuration management. A GitOps approach helps an organization implement best practices in version control and release governance based on a widely used open standard -- git . The following diagram outlines the major components in an IBM API Connect GitOps model: Tip Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net Notice the clear separation of concerns: Tekton (OpenShift Pipelines) pipelines use IBM API Connect configuration as well as applications API definitions and products source repositories to apply those definitions to an existing IBM API Connect cluster. For the IBM API Connect configuration, this ensures that the exact same configuration of an IBM API Connect cluster can be applied to another IBM API Connect cluster, easing the activities in case of a disaster recovery situation for instance. For your IBM API Connect products and application API definitions for those products and catalogs, this OpenShift Pipelines ensure that any change on those artifacts gets immediately reflected on your IBM API Connect cluster. We need to use OpenShift pipelines for the IBM API Connect configuration and applications API definitions, products and catalogs since these are not yet Kubernetes resources that a GitOps tool such as ArgoCD can automatically watch for changes. ArgoCD (OpenShift GitOps) applications watch a Git config repository for changes. OpenShift GitOps applies the Kubernetes resources thus identified to the cluster, resulting in new or updated Kubernetes resources that represent the changed IBM API Connect cluster and its ecosystem, such as pods, routes etc. In contrast to pipeline runs, OpenShift GitOps changes are durable; they remain as defined unless and until they are explicitly changed or deleted in the GitOps repository. Moreover, if the cluster resources drift from their Git config values, ArgoCD will restore them to their desired values; only changes that are applied to the Git config repository affect the long term state of the cluster. In general, in application development, OpenShift Pipelines and OpenShift GitOps are used to separate Continuous Integration from Continuous Deployment . Often, the Tekton pipeline will perform its changes under a pull-request (PR) to provide an explicit approval mechanism for cluster changes. This is especially important in higher environments such as production which require a formal sign-off. Lower environments such as development often apply successful pipeline runs directly to the Git config repository that are immediately seen by ArgoCD and applied to the cluster. For IBM API connect, the Continuous Integration and Continuous Deployment concepts aren't as clear as in common application development scenarios since you are not building or deploying applications although the API definitions of these, and the product and catalogs they will belong to, could be. We strongly suggest to read over the MQ tutorial in this Cloud Pak Production Deployment Guides for a more traditional application development scenario. In this section of the tutorial, we're going to set up the GitOps repository, and install ArgoCD. More precisely, we are going to Create a Github Organization Download a sample GitOps repository and briefly review it Install ArgoCD Customize ArgoCD Login in to the ArgoCD UI By the end of this topic we'll have a all the basic components in place to perform GitOps in our cluster. Pre-requisites \u00b6 Before attempting this section, you must have completed the previous Create the Clusters section. Downloading the sample GitOps repository \u00b6 In this topic section, we're going to create the GitOps repository for our cluster. We'll use a sample GitOps repository as a starting point; we'll make our own copy and review its contents. Later in the tutorial, we'll customize it for our organization and cluster. ( If you'd like to re-familiarize yourself with git, GitHub or managing repositories, you'll find the following short video and short read helpful. ) Create a new GitHub organization Let's start by creating our own GitHub organization. In your browser, navigate to https://github.com/ . In the upper-right corner, click on the profile photo, and then click Settings . In user settings sidebar, click Organizations . In the \"Organizations\" section, click New organization. Create a new organization. Create the GitOps repositories in the new GitHub organization Let's start by making our own copy of the GitOps repository. In your browser, navigate to https://github.com/cloud-native-toolkit/multi-tenancy-gitops : Click on Use this template at the top RHS of the screen and you'll be asked where you'd like to use this repository: Select your organization, name the repository and click Create repository from template . After a few seconds the repository will be present in your chosen account: You now have a copy of the sample GitOps repository in your GitHub account. Create the infrastructure, services and applications GitOps repositories from the templates into the organization your GitHub organization As shown above, follow similar steps and copy the the repository templates into your organization. GitOps Infrastructure Repository GitOps Services Repository GitOps Application Repository Your organization now will have the below repositories in it. Set up environment variable for GitHub Organization This tutorial uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Replace <git-organization> in the following command: export GIT_ORG = <git-organization> You can verify your $GIT_ORG as follows: echo $GIT_ORG Change to your working git folder Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . We've used this default in the following command, but you may have a different location on your local machine. Change to your working git folder: cd $HOME /git You may need to create this folder if it's not already created. Clone the GitOps repository We're going to work on a local clone of the four repositories. Clone the GitOps repositories to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops'... remote: Enumerating objects: 132, done. remote: Counting objects: 100% (132/132), done. remote: Compressing objects: 100% (54/54), done. remote: Total 132 (delta 72), reused 118 (delta 67), pack-reused 0 Receiving objects: 100% (132/132), 310.92 KiB | 3.24 MiB/s, done. Resolving deltas: 100% (72/72), done. Clone the GitOps Infra repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-infra.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-infra'... remote: Enumerating objects: 62, done. remote: Counting objects: 100% (62/62), done. remote: Compressing objects: 100% (44/44), done. remote: Total 62 (delta 7), reused 53 (delta 3), pack-reused 0 Unpacking objects: 100% (62/62), done. Clone the GitOps Services repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-services.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-services'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Clone the GitOps Applications repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-apps'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Info Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the folder containing the local copy of your new repository: cd multi-tenancy-gitops Review the GitOps folder structure Show the folder structure with the following command: tree . -L 1 This will list all the top level folders and files: . \u251c\u2500\u2500 0-bootstrap \u251c\u2500\u2500 README.md \u251c\u2500\u2500 doc \u251c\u2500\u2500 scripts \u2514\u2500\u2500 setup The main folders perform the following roles: 0-bootstrap contains different profiles that can be used to initialize the cluster ready. We can choose one of the available profiles. scripts and setup contain useful scripts that we'll use during this tutorial. Feel free to explore this structure. We'll examine it in much more detail throughout this tutorial, and you'll become very familiar with it by trying it out. Important You must repeat the following Installing ArgoCD for GitOps section for each of your clusters where you plan to install any component of IBM API Connect. Installing ArgoCD for GitOps \u00b6 Now that we've had an initial view of the GitOps repository, let's install ArgoCD to make use of it. We will install ArgoCD using the Red Hat OpenShift GitOps operator. We will also configure it with the appropriate access rights to limit the resources it can create in the cluster. In so doing, we ensure that ArgoCD acts like a well-governed administrator of the cluster, only doing what it needs to do. Also, we will customize our ArgoCD instance by adding necessary custom checks. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". On successful login, you'll see: Logged into \"https://c100-e.jp-tok.containers.cloud.ibm.com:30749\" as \"IAM#hperabat@in.ibm.com\" using the token provided. You have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\" . This shows some basic information about your user within this cluster. (This user is different to your GitHub user.) Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. The following command creates the subscription: oc apply -f setup/ocp47/ The response confirms that the below resources has been created: clusterrole.rbac.authorization.k8s.io/custom-argocd-cluster-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-cntk-argocd-application-controller created subscription.operators.coreos.com/openshift-gitops-operator created Wait for the ArgoCD installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/gitops-operator-controller-manager -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/gitops-operator condition met ArgoCD is now installed and ready to use. If you see something like: Error from server (NotFound): deployments.apps \"gitops-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Alternatively, you can also validate as follows: while ! oc wait crd applications.argoproj.io --timeout = -1s --for = condition = Established 2 >/dev/null ; do sleep 30 ; done while ! oc wait pod --timeout = -1s --for = condition = Ready -l '!job-name' -n openshift-gitops > /dev/null ; do sleep 30 ; done ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are also setup along with the subscription. oc get clusterrole custom-argocd-cluster-argocd-application-controller oc get clusterrolebinding openshift-gitops-argocd-application-controller oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller You'll see the resources are created in the cluster: $ oc get clusterrole custom-argocd-cluster-argocd-application-controller NAME CREATED AT custom-argocd-cluster-argocd-application-controller 2021-08-27T13:35:13Z $ oc get clusterrolebinding openshift-gitops-argocd-application-controller NAME ROLE AGE openshift-gitops-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m43s $ oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller NAME ROLE AGE openshift-gitops-cntk-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m45s The clusterrole custom-argocd-cluster-argocd-application-controller defines a specific set of specific resources that are required by the ArgoCD service account and access rights over them. The cluster role bindings openshift-gitops-argocd-application-controller and openshift-gitops-cntk-argocd-application-controller binds the ArgoCD service account to the cluster role above. In combination, these definitions limit ArgoCD to perform the minimum set of operations required to manage the cluster. This is important; it means that ArgoCD behaves as a well governed administrator of the cluster. We'll also see later in the tutorial how ArgoCD creates resource in the cluster. Delete default ArgoCD instance Let us now delete the default ArgoCD instance that is created earlier. oc delete gitopsservice cluster -n openshift-gitops || true You will see the resources being deleted as follows: warning: deleting cluster-scoped resources, not scoped to the provided namespace gitopsservice.pipelines.openshift.io \"cluster\" deleted We are deleting the existing default instance and planning to make use of a customized ArgoCD instance with additional capabilities. Create a custom ArgoCD instance This customized ArgoCD instance implements custom checks. By default ArgoCD provides some built-in health checks for validating standard kubernetes resources. However, these checks will not be sufficient to validate the custom resources that belong to IBM Cloud Pak for Integration. Argo CD supports custom health checks that are defined in Lua . If you want explore more about the ArgoCD Resource Health, check this link out. Let us create a custom ArgoCD instance as follows: oc apply -f setup/ocp47/argocd-instance/ -n openshift-gitops The response confirms that the below resources has been created: argocd.argoproj.io/openshift-gitops-cntk created Wait till the ArgoCD instance pod is up and running. Issue the below command to verify the same: while ! oc wait pod --timeout = -1s --for = condition = ContainersReady -l app.kubernetes.io/name = openshift-gitops-cntk-server -n openshift-gitops > /dev/null ; do sleep 30 ; done Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops | grep openshift-gitops-cntk-server | awk '{print \"https://\"$2}' This will list the route to the ArgoCD we've just installed, for example: https://openshift-gitops-cntk-server-openshift-gitops.ibmcloud-roks-xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/openshift-gitops-cntk-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Once the UI launches, you'll see: See how there are no ArgoCD applications active at the moment. In the next section of the tutorial, we'll configure ArgoCD to create the ArgoCD applications that will in turn spin up infrastructure , service , and application resources to apply to the cluster. Important You must repeat this Installing ArgoCD for GitOps section for each of your clusters where you plan to install any component of IBM API Connect. Congratulations! You have created the GitOps organization based on the sample GitOps repositories that will be used to manage the IBM capabilities and applications that you will install on your cluster. We have also briefly examined the structure of the main GitOps repository driving what will get installed. You have also installed ArgoCD. You have created a specific clusterrole and clusterrolebinding for the ArgoCD service account to ensure that it is able to manage the cluster in a well governed manner. Finally, you have launched the ArgoCD UI; you will make extensive use of it during this tutorial.","title":"GitOps repos & ArgoCD"},{"location":"guides/cp4i/apic/multi-cluster/configure-clusters-gitops/#ibm-api-connect-multi-cluster-configure-the-clusters-gitops-repos-argocd","text":"","title":"IBM API Connect Multi-Cluster - Configure the clusters - GitOps repos &amp; ArgoCD"},{"location":"guides/cp4i/apic/multi-cluster/configure-clusters-gitops/#overview","text":"A GitOps model must be at the core of the methodology or approach used to deploy and manage any IBM capability in production. It ensures that the installation, configuration and changes of these IBM capabilities, as well as their configuration and citizens (applications, integration flows, APIs, etc), is only done through code. GitOps puts git repositories and git commands such as git push (to request a change) and git merge (to approve a change) at the heart of configuration management. A GitOps approach helps an organization implement best practices in version control and release governance based on a widely used open standard -- git . The following diagram outlines the major components in an IBM API Connect GitOps model: Tip Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net Notice the clear separation of concerns: Tekton (OpenShift Pipelines) pipelines use IBM API Connect configuration as well as applications API definitions and products source repositories to apply those definitions to an existing IBM API Connect cluster. For the IBM API Connect configuration, this ensures that the exact same configuration of an IBM API Connect cluster can be applied to another IBM API Connect cluster, easing the activities in case of a disaster recovery situation for instance. For your IBM API Connect products and application API definitions for those products and catalogs, this OpenShift Pipelines ensure that any change on those artifacts gets immediately reflected on your IBM API Connect cluster. We need to use OpenShift pipelines for the IBM API Connect configuration and applications API definitions, products and catalogs since these are not yet Kubernetes resources that a GitOps tool such as ArgoCD can automatically watch for changes. ArgoCD (OpenShift GitOps) applications watch a Git config repository for changes. OpenShift GitOps applies the Kubernetes resources thus identified to the cluster, resulting in new or updated Kubernetes resources that represent the changed IBM API Connect cluster and its ecosystem, such as pods, routes etc. In contrast to pipeline runs, OpenShift GitOps changes are durable; they remain as defined unless and until they are explicitly changed or deleted in the GitOps repository. Moreover, if the cluster resources drift from their Git config values, ArgoCD will restore them to their desired values; only changes that are applied to the Git config repository affect the long term state of the cluster. In general, in application development, OpenShift Pipelines and OpenShift GitOps are used to separate Continuous Integration from Continuous Deployment . Often, the Tekton pipeline will perform its changes under a pull-request (PR) to provide an explicit approval mechanism for cluster changes. This is especially important in higher environments such as production which require a formal sign-off. Lower environments such as development often apply successful pipeline runs directly to the Git config repository that are immediately seen by ArgoCD and applied to the cluster. For IBM API connect, the Continuous Integration and Continuous Deployment concepts aren't as clear as in common application development scenarios since you are not building or deploying applications although the API definitions of these, and the product and catalogs they will belong to, could be. We strongly suggest to read over the MQ tutorial in this Cloud Pak Production Deployment Guides for a more traditional application development scenario. In this section of the tutorial, we're going to set up the GitOps repository, and install ArgoCD. More precisely, we are going to Create a Github Organization Download a sample GitOps repository and briefly review it Install ArgoCD Customize ArgoCD Login in to the ArgoCD UI By the end of this topic we'll have a all the basic components in place to perform GitOps in our cluster.","title":"Overview"},{"location":"guides/cp4i/apic/multi-cluster/configure-clusters-gitops/#pre-requisites","text":"Before attempting this section, you must have completed the previous Create the Clusters section.","title":"Pre-requisites"},{"location":"guides/cp4i/apic/multi-cluster/configure-clusters-gitops/#downloading-the-sample-gitops-repository","text":"In this topic section, we're going to create the GitOps repository for our cluster. We'll use a sample GitOps repository as a starting point; we'll make our own copy and review its contents. Later in the tutorial, we'll customize it for our organization and cluster. ( If you'd like to re-familiarize yourself with git, GitHub or managing repositories, you'll find the following short video and short read helpful. ) Create a new GitHub organization Let's start by creating our own GitHub organization. In your browser, navigate to https://github.com/ . In the upper-right corner, click on the profile photo, and then click Settings . In user settings sidebar, click Organizations . In the \"Organizations\" section, click New organization. Create a new organization. Create the GitOps repositories in the new GitHub organization Let's start by making our own copy of the GitOps repository. In your browser, navigate to https://github.com/cloud-native-toolkit/multi-tenancy-gitops : Click on Use this template at the top RHS of the screen and you'll be asked where you'd like to use this repository: Select your organization, name the repository and click Create repository from template . After a few seconds the repository will be present in your chosen account: You now have a copy of the sample GitOps repository in your GitHub account. Create the infrastructure, services and applications GitOps repositories from the templates into the organization your GitHub organization As shown above, follow similar steps and copy the the repository templates into your organization. GitOps Infrastructure Repository GitOps Services Repository GitOps Application Repository Your organization now will have the below repositories in it. Set up environment variable for GitHub Organization This tutorial uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Replace <git-organization> in the following command: export GIT_ORG = <git-organization> You can verify your $GIT_ORG as follows: echo $GIT_ORG Change to your working git folder Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . We've used this default in the following command, but you may have a different location on your local machine. Change to your working git folder: cd $HOME /git You may need to create this folder if it's not already created. Clone the GitOps repository We're going to work on a local clone of the four repositories. Clone the GitOps repositories to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops'... remote: Enumerating objects: 132, done. remote: Counting objects: 100% (132/132), done. remote: Compressing objects: 100% (54/54), done. remote: Total 132 (delta 72), reused 118 (delta 67), pack-reused 0 Receiving objects: 100% (132/132), 310.92 KiB | 3.24 MiB/s, done. Resolving deltas: 100% (72/72), done. Clone the GitOps Infra repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-infra.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-infra'... remote: Enumerating objects: 62, done. remote: Counting objects: 100% (62/62), done. remote: Compressing objects: 100% (44/44), done. remote: Total 62 (delta 7), reused 53 (delta 3), pack-reused 0 Unpacking objects: 100% (62/62), done. Clone the GitOps Services repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-services.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-services'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Clone the GitOps Applications repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-apps'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Info Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the folder containing the local copy of your new repository: cd multi-tenancy-gitops Review the GitOps folder structure Show the folder structure with the following command: tree . -L 1 This will list all the top level folders and files: . \u251c\u2500\u2500 0-bootstrap \u251c\u2500\u2500 README.md \u251c\u2500\u2500 doc \u251c\u2500\u2500 scripts \u2514\u2500\u2500 setup The main folders perform the following roles: 0-bootstrap contains different profiles that can be used to initialize the cluster ready. We can choose one of the available profiles. scripts and setup contain useful scripts that we'll use during this tutorial. Feel free to explore this structure. We'll examine it in much more detail throughout this tutorial, and you'll become very familiar with it by trying it out. Important You must repeat the following Installing ArgoCD for GitOps section for each of your clusters where you plan to install any component of IBM API Connect.","title":"Downloading the sample GitOps repository"},{"location":"guides/cp4i/apic/multi-cluster/configure-clusters-gitops/#installing-argocd-for-gitops","text":"Now that we've had an initial view of the GitOps repository, let's install ArgoCD to make use of it. We will install ArgoCD using the Red Hat OpenShift GitOps operator. We will also configure it with the appropriate access rights to limit the resources it can create in the cluster. In so doing, we ensure that ArgoCD acts like a well-governed administrator of the cluster, only doing what it needs to do. Also, we will customize our ArgoCD instance by adding necessary custom checks. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". On successful login, you'll see: Logged into \"https://c100-e.jp-tok.containers.cloud.ibm.com:30749\" as \"IAM#hperabat@in.ibm.com\" using the token provided. You have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\" . This shows some basic information about your user within this cluster. (This user is different to your GitHub user.) Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. The following command creates the subscription: oc apply -f setup/ocp47/ The response confirms that the below resources has been created: clusterrole.rbac.authorization.k8s.io/custom-argocd-cluster-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-cntk-argocd-application-controller created subscription.operators.coreos.com/openshift-gitops-operator created Wait for the ArgoCD installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/gitops-operator-controller-manager -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/gitops-operator condition met ArgoCD is now installed and ready to use. If you see something like: Error from server (NotFound): deployments.apps \"gitops-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Alternatively, you can also validate as follows: while ! oc wait crd applications.argoproj.io --timeout = -1s --for = condition = Established 2 >/dev/null ; do sleep 30 ; done while ! oc wait pod --timeout = -1s --for = condition = Ready -l '!job-name' -n openshift-gitops > /dev/null ; do sleep 30 ; done ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are also setup along with the subscription. oc get clusterrole custom-argocd-cluster-argocd-application-controller oc get clusterrolebinding openshift-gitops-argocd-application-controller oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller You'll see the resources are created in the cluster: $ oc get clusterrole custom-argocd-cluster-argocd-application-controller NAME CREATED AT custom-argocd-cluster-argocd-application-controller 2021-08-27T13:35:13Z $ oc get clusterrolebinding openshift-gitops-argocd-application-controller NAME ROLE AGE openshift-gitops-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m43s $ oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller NAME ROLE AGE openshift-gitops-cntk-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m45s The clusterrole custom-argocd-cluster-argocd-application-controller defines a specific set of specific resources that are required by the ArgoCD service account and access rights over them. The cluster role bindings openshift-gitops-argocd-application-controller and openshift-gitops-cntk-argocd-application-controller binds the ArgoCD service account to the cluster role above. In combination, these definitions limit ArgoCD to perform the minimum set of operations required to manage the cluster. This is important; it means that ArgoCD behaves as a well governed administrator of the cluster. We'll also see later in the tutorial how ArgoCD creates resource in the cluster. Delete default ArgoCD instance Let us now delete the default ArgoCD instance that is created earlier. oc delete gitopsservice cluster -n openshift-gitops || true You will see the resources being deleted as follows: warning: deleting cluster-scoped resources, not scoped to the provided namespace gitopsservice.pipelines.openshift.io \"cluster\" deleted We are deleting the existing default instance and planning to make use of a customized ArgoCD instance with additional capabilities. Create a custom ArgoCD instance This customized ArgoCD instance implements custom checks. By default ArgoCD provides some built-in health checks for validating standard kubernetes resources. However, these checks will not be sufficient to validate the custom resources that belong to IBM Cloud Pak for Integration. Argo CD supports custom health checks that are defined in Lua . If you want explore more about the ArgoCD Resource Health, check this link out. Let us create a custom ArgoCD instance as follows: oc apply -f setup/ocp47/argocd-instance/ -n openshift-gitops The response confirms that the below resources has been created: argocd.argoproj.io/openshift-gitops-cntk created Wait till the ArgoCD instance pod is up and running. Issue the below command to verify the same: while ! oc wait pod --timeout = -1s --for = condition = ContainersReady -l app.kubernetes.io/name = openshift-gitops-cntk-server -n openshift-gitops > /dev/null ; do sleep 30 ; done Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops | grep openshift-gitops-cntk-server | awk '{print \"https://\"$2}' This will list the route to the ArgoCD we've just installed, for example: https://openshift-gitops-cntk-server-openshift-gitops.ibmcloud-roks-xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/openshift-gitops-cntk-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Once the UI launches, you'll see: See how there are no ArgoCD applications active at the moment. In the next section of the tutorial, we'll configure ArgoCD to create the ArgoCD applications that will in turn spin up infrastructure , service , and application resources to apply to the cluster. Important You must repeat this Installing ArgoCD for GitOps section for each of your clusters where you plan to install any component of IBM API Connect. Congratulations! You have created the GitOps organization based on the sample GitOps repositories that will be used to manage the IBM capabilities and applications that you will install on your cluster. We have also briefly examined the structure of the main GitOps repository driving what will get installed. You have also installed ArgoCD. You have created a specific clusterrole and clusterrolebinding for the ArgoCD service account to ensure that it is able to manage the cluster in a well governed manner. Finally, you have launched the ArgoCD UI; you will make extensive use of it during this tutorial.","title":"Installing ArgoCD for GitOps"},{"location":"guides/cp4i/apic/multi-cluster/create-clusters/","text":"IBM API Connect Multi-Cluster - Create the clusters \u00b6 As explained in the previous overview section, we will need at least 2 Red Hat OpenShift clusters . It should not matter where these clusters reside (IBM Cloud, Amazon, Azure, on-premises, etc). You can bring your own Red Hat OpenShift cluster or you can leverage the Create the cluster section on this IBM API Connect in-depth tutorial to request Red Hat OpenShift clusters for the different providers available. Remember you should have at least 2 Red Hat OpenShift clusters. Important Make sure you create your Red Hat OpenShift clusters with the following specs: OCP Version = 4.7 Worker Size (at least) = 14 CPU x 48 GB Worker Node Count = 3","title":"Create the cluster"},{"location":"guides/cp4i/apic/multi-cluster/create-clusters/#ibm-api-connect-multi-cluster-create-the-clusters","text":"As explained in the previous overview section, we will need at least 2 Red Hat OpenShift clusters . It should not matter where these clusters reside (IBM Cloud, Amazon, Azure, on-premises, etc). You can bring your own Red Hat OpenShift cluster or you can leverage the Create the cluster section on this IBM API Connect in-depth tutorial to request Red Hat OpenShift clusters for the different providers available. Remember you should have at least 2 Red Hat OpenShift clusters. Important Make sure you create your Red Hat OpenShift clusters with the following specs: OCP Version = 4.7 Worker Size (at least) = 14 CPU x 48 GB Worker Node Count = 3","title":"IBM API Connect Multi-Cluster - Create the clusters"},{"location":"guides/cp4i/apic/multi-cluster/deploy-old-manual/","text":"IBM API Connect Multi-cloud Deployment \u00b6 Abstract This document describes the steps to carry out a multi-cloud deployment of IBM API Connect across different cloud providers, IBM Cloud and Amazon AWS, all using RedHat OpenSift Container Platform as the underlying unified platform. Overview \u00b6 In this document you will find the steps followed in order to deploy the a multi-cloud topology for IBM API Connect. The Management and Developer components have been installed on a RedHat OpenShift Kubernetes Service cluster in IBM Cloud. The Gateway and Analytics components have been installed on two different RedHat OpenShift clusters, one being a RedHat OpenShift Kubernetes Service cluster on IBM Cloud and the other a RedHat OpenShift cluster on Amazon AWS. The aim is to simulate the use case where you might have applications running on RedHat OpenShift clusters that are hosted on different clouds but you want to unify the way you develop, manage, socialize and secure the APIs of your applications using IBM API Connect. Note How to create RedHat OpenShift Kubernetes Service clusters on IBM Cloud is not the goal of this document. You can find the official documentation here . How to deploy a RedHat OpenShift cluster on Amazon AWS isn't the goal of this document either. You can find the official documentation here . Once you have these RedHat OpenShift clusters (we will use three different RedHat OpenShift clusters in this use case), the deployment process for the different IBM API Connect components should be similar regardless of the cloud provider (or on-premises infrastructure) where those RedHat OpenShift clusters have been deployed onto. The only caveat you will need to pay attention to would be the host parameter for the different endpoints the IBM API Connect components require and the storageClass the different IBM API Connect components will make use of to claim storage. The use case described in this document is based on: The IBM API Connect official documentation here - This documentation refers to plain kubernetes IBM API Connect installations but provide good information of the required process for this use case The work carried out by Cesar Cavazos on IBM API Connect highly available installations on 2 data centers here . Tip Make sure you are comfortable with both the RedHat OpenShift web console and CLI to be able to execute all the steps in this document. IBM API Connect Operator \u00b6 In order to install any IBM API Connect component in any RedHat OpenShift cluster, the first thing we need to do is to install the IBM API Connect Operator. We have used the official IBM API Connect documentation here as the guideline to install the IBM API Connect Operator in the three RedHat OpenShift clusters. Here are the exact steps we have followed: Create a namespace called apic where the IBM API Connect components will be created into: oc new-project apic Create the IBM Entitlement Key secret so that you are allowed to pull down IBM Software. oc create secret docker-registry ibm-entitlement-key --docker-server = cp.icr.io --docker-username = cp --docker-password = <YOUR_IBM_ENTITLEMENT_KEY> --docker-email = <YOUR_EMAIL> where YOUR_IBM_ENTITLEMENT_KEY is your IBM Entitlement Key that you can retrieve from the IBM Container Software Library . YOUR_EMAIL is you email address. Link your IBM Entitlement Key secret to the default Service Account for pulling images. oc secrets link default ibm-entitlement-key --for = pull From the RedHat OpenShift web console, click + and create the following CatalogSource resources: The IBM Operator Catalog source. Add the following CatalogSource definition and click save. apiVersion : operators.coreos.com/v1alpha1 kind : CatalogSource metadata : name : ibm-operator-catalog namespace : openshift-marketplace spec : displayName : \"IBM Operator Catalog\" publisher : IBM sourceType : grpc image : docker.io/ibmcom/ibm-operator-catalog updateStrategy : registryPoll : interval : 45m The IBM Cloud Pak foundational services operator catalog. Add the following CatalogSource definition and click save. apiVersion : operators.coreos.com/v1alpha1 kind : CatalogSource metadata : name : opencloud-operators namespace : openshift-marketplace spec : displayName : IBMCS Operators image : docker.io/ibmcom/ibm-common-service-catalog:latest publisher : IBM sourceType : grpc updateStrategy : registryPoll : interval : 45m Install the IBM API Connect Operator. Go to Operators --> OperatorHub on the left hand side menu of your RedHat OpenShift web console. Search for API Connect and click on the tile. Click on Install on the IBM API Connect menu that hovers from the right hand side. Click on Install on the Install Operator page leaving the defaults. After few minutes, you should see the following IBM Operators installed on your apic namespace if you go to Operators --> Installed Operators on the left hand side menu of your RedHat OpenShift web console. Congratulations You should now have the IBM API Connect Operator installed on your RedHat OpenShift cluster. Warning Remember to execute this section in all RedHat OpenShift clusters where you want to deploy any IBM API Connect component. Management and Developer Portal \u00b6 In this section, we are going to deploy the IBM API Connect Management and Developer Portal components on the same RedHat OpenShift cluster. This cluster will be a separate cluster from the other two RedHat OpenShift clusters that will host the IBM API Connect Gateway component to securely expose our application APIs as well as the IBM API Connect Analytics component to gather the IBM API Connect Gateway analytics. However, before installing any IBM API Connect component in our RedHat OpenShift clusters, we need to create some certificates that will be used to ensure secure connections between these different IBM API Connect components. Refer to the official IBM API Connect documentation for more detail about the different IBM API Connect component endpoints as well as certificates . Info You can find how to obtain the IBM API Connect official product files that will be used throughout the rest of this document here . Create the ingress-issuer-v1-alpha1.yaml file that contains the Issuer and Certificate object definitions with the content below: # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : selfsigning-issuer labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"selfsigning-issuer\" } spec : selfSigned : {} --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : ingress-ca labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"ingress-ca\" } spec : duration : 87600h # 10 years renewBefore : 720h # 30 days secretName : ingress-ca commonName : \"ingress-ca\" usages : - digital signature - key encipherment - cert sign isCA : true issuerRef : name : selfsigning-issuer kind : Issuer --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : ingress-issuer labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"ingress-issuer\" } spec : ca : secretName : ingress-ca --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : portal-admin-client labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"portal-admin-client\" } spec : commonName : portal-admin-client secretName : portal-admin-client issuerRef : name : ingress-issuer usages : - \"client auth\" - \"signing\" - \"key encipherment\" duration : 17520h # 2 years renewBefore : 720h # 30 days --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : gateway-client-client labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"gateway-client-client\" } spec : commonName : gateway-client-client secretName : gateway-client-client issuerRef : name : ingress-issuer usages : - \"client auth\" - \"signing\" - \"key encipherment\" duration : 17520h # 2 years renewBefore : 720h # 30 days --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : analytics-client-client labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"analytics-client-client\" } spec : commonName : analytics-client-client secretName : analytics-client-client issuerRef : name : ingress-issuer usages : - \"client auth\" - \"signing\" - \"key encipherment\" duration : 17520h # 2 years renewBefore : 720h # 30 days --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : analytics-ingestion-client labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"analytics-ingestion-client\" } spec : commonName : analytics-ingestion-client secretName : analytics-ingestion-client issuerRef : name : ingress-issuer usages : - \"client auth\" - \"signing\" - \"key encipherment\" duration : 17520h # 2 years renewBefore : 720h # 30 days Apply that ingress-issuer-v1-alpha1.yaml file to get the Issuer and Certificate objects created in the apic namespace of your RedHat OpenShift cluster. oc apply -f ingress-issuer-v1-alpha1.yaml -n apic You should see the following output. issuer.certmanager.k8s.io/selfsigning-issuer created certificate.certmanager.k8s.io/ingress-ca created issuer.certmanager.k8s.io/ingress-issuer created certificate.certmanager.k8s.io/portal-admin-client created certificate.certmanager.k8s.io/gateway-client-client created certificate.certmanager.k8s.io/analytics-client-client created certificate.certmanager.k8s.io/analytics-ingestion-client created If you list the certificates on your system, you should see the following: oc get certificates -n apic NAME READY SECRET AGE EXPIRATION analytics-client-client True analytics-client-client 48s 2023-09-07T11:22:03Z analytics-ingestion-client True analytics-ingestion-client 47s 2023-09-07T11:22:00Z gateway-client-client True gateway-client-client 48s 2023-09-07T11:22:15Z ingress-ca True ingress-ca 49s 2031-09-05T11:21:44Z portal-admin-client True portal-admin-client 49s 2023-09-07T11:22:00Z Important You must extract the ingress-ca certificates and key that have been used to create the certificate authority to sign all the certificates you just created above. These certificates and key must be provided to create the same certificate authority in the other RedHat OpenShift clusters where you want to deploy any other IBM API Connect component to be part of the same IBM API Connect cluster. Extract the ingress-ca certificates and key that have been used to create the certificate authority to sign all the certificates you created in the previous step. oc extract secret/ingress-ca --to = . --confirm You should now have the following certificates and key. ls -all total 24 drwxr-xr-x 5 user staff 160B 7 Sep 13:25 ./ drwxr-xr-x 9 user staff 288B 7 Sep 13:24 ../ -rw------- 1 user staff 1.1K 7 Sep 13:25 ca.crt -rw------- 1 user staff 1.1K 7 Sep 13:25 tls.crt -rw------- 1 user staff 1.6K 7 Sep 13:25 tls.key Management \u00b6 In this section we are going to install the IBM API Connect Management component. Refer to the official documentation here for more detail. Create the IBM API Connect Management component administrator credentials oc create secret generic management-admin-credentials --from-literal = email = <MGMT_USER_EMAIL> --from-literal = password = <MGMT_USER_PASSWORD> -n apic where MGMT_USER_EMAIL is the email of the IBM API Connect Management component administrator. MGMT_USER_PASSWORD is the password for the IBM API Connect Management component administrator credentials. Create the management_cr.yaml file that contains the ManagementCluster object definition with the content below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 apiVersion : management.apiconnect.ibm.com/v1beta1 kind : ManagementCluster metadata : name : management labels : app.kubernetes.io/instance : management app.kubernetes.io/managed-by : ibm-apiconnect app.kubernetes.io/name : management namespace : apic spec : version : 10.0.3.0 imagePullSecrets : - ibm-entitlement-key profile : $PROFILE portal : admin : secretName : portal-admin-client analytics : client : secretName : analytics-client-client ingestion : secretName : analytics-ingestion-client gateway : client : secretName : gateway-client-client cloudManagerEndpoint : annotations : certmanager.k8s.io/issuer : ingress-issuer hosts : - name : admin.$STACK_HOST secretName : management-cm-endpoint-secret apiManagerEndpoint : annotations : certmanager.k8s.io/issuer : ingress-issuer hosts : - name : manager.$STACK_HOST secretName : management-apim-endpoint-secret platformAPIEndpoint : annotations : certmanager.k8s.io/issuer : ingress-issuer hosts : - name : api.$STACK_HOST secretName : management-api-endpoint-secret consumerAPIEndpoint : annotations : certmanager.k8s.io/issuer : ingress-issuer hosts : - name : consumer.$STACK_HOST secretName : management-consumer-endpoint-secret databaseVolumeClaimTemplate : storageClassName : $STORAGE_CLASS microServiceSecurity : certManager certManagerIssuer : name : selfsigning-issuer kind : Issuer adminUser : secretName : management-admin-credentials license : accept : true license : $LICENSE use : $USE where $PROFILE is either set to n1xc4.m16 for development environments or to n3xc4.m16 for production environments. If you need high availability use the production profile. $STACK_HOST is the desired ingress subdomain for the API Connect stack. You can find the ingress subdomain of you RedHat OpenShift cluster by executing oc get routes . Your subdomain usually is apps.xxx.yyy.zzz . $STORAGE_CLASS is the Kubernetes storage class to be used for Persistent Volume Claims. Remember this must be RWO block storage. $LICENSE is the License ID for the version of API Connect that you purchased. See API Connect licenses . $USE is either production or nonproduction , to match the license you purchased. Apply that management_cr.yaml file to get the ManagementCluster object created in the apic namespace of your RedHat OpenShift cluster. oc apply -f management_cr.yaml -n apic Verify that the IBM API Connect Management component is fully installed. oc get ManagementCluster -n apic You should see a similar output as the following. NAME READY STATUS VERSION RECONCILED VERSION AGE management 16/16 Running 10.0.3.0 10.0.3.0-1320 8d The installation has completed when the READY attribute reports that all services are online (e.g. 16/16) STATUS attribute reports Running . Developer Portal \u00b6 In this section we are going to install the IBM API Connect Developer Portal component. Refer to the official documentation here for more detail. Create the portal_cr.yaml file that contains the PortalCluster object definition with the content below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 apiVersion : portal.apiconnect.ibm.com/v1beta1 kind : PortalCluster metadata : namespace : apic name : portal labels : app.kubernetes.io/instance : portal app.kubernetes.io/managed-by : ibm-apiconnect app.kubernetes.io/name : portal spec : version : 10.0.3.0 profile : $PROFILE imagePullSecrets : - ibm-entitlement-key portalAdminEndpoint : annotations : certmanager.k8s.io/issuer : ingress-issuer hosts : - name : padmin.$STACK_HOST secretName : portal-admin-endpoint-secret portalUIEndpoint : annotations : certmanager.k8s.io/issuer : ingress-issuer hosts : - name : portal.$STACK_HOST secretName : portal-web-endpoint-secret databaseLogsVolumeClaimTemplate : storageClassName : $STORAGE_CLASS volumeSize : 12Gi databaseVolumeClaimTemplate : storageClassName : $STORAGE_CLASS volumeSize : 300Gi backupVolumeClaimTemplate : storageClassName : $STORAGE_CLASS volumeSize : 300Gi webVolumeClaimTemplate : storageClassName : $STORAGE_CLASS volumeSize : 200Gi adminVolumeClaimTemplate : storageClassName : $STORAGE_CLASS volumeSize : 20Gi adminClientSubjectDN : 'CN=portal-admin-client' # adminClientSubjectDN: 'CN=portal-admin-client,O=cert-manager' microServiceSecurity : certManager certManagerIssuer : name : selfsigning-issuer kind : Issuer license : accept : true license : $LICENSE use : $USE where $PROFILE is either set to n1xc2.m8 for development environments or to n3xc4.m8 for production environments. If you need high availability use the production profile. $STACK_HOST is the desired ingress subdomain for the API Connect stack. You can find the ingress subdomain of you RedHat OpenShift cluster by executing oc get routes . Your subdomain usually is apps.xxx.yyy.zzz . $STORAGE_CLASS is the Kubernetes storage class to be used for Persistent Volume Claims. Remember this must be RWO block storage. $LICENSE is the License ID for the version of API Connect that you purchased. See API Connect licenses . $USE is either production or nonproduction , to match the license you purchased. Apply that portal_cr.yaml file to get the PortalCluster object created in the apic namespace of your RedHat OpenShift cluster. oc apply -f portal_cr.yaml -n apic Verify that the IBM API Connect Portal component is fully installed. oc get PortalCluster -n apic You should see a similar output as the following. NAME READY STATUS VERSION RECONCILED VERSION AGE portal 3/3 Running 10.0.3.0 10.0.3.0-1320 8d The installation has completed when the READY attribute reports that all services are online (e.g. 3/3) STATUS attribute reports Running . Congratulations You have deployed the IBM API Connect Management and Portal components as well as created all the certificates and issuers needed to get all IBM API Connect components to communicate with each other in a secure manner. Gateway and Analytics \u00b6 In this section, we are going to deploy show how to deploy the IBM API Connect Gateway component to securely expose our application APIs as well as the IBM API Connect Analytics component to gather the IBM API Connect Gateway analytics on any RedHat OpenShift cluster where we might want to host our applications. However, before installing the IBM API Connect Gateway or Analytics components in our RedHat OpenShift clusters, we need to create some certificates that will be used to ensure secure connections not only between these IBM API Connect components but also between these IBM API Connect components and the IBM API Connect Management and Developer Portal components that should have been already deployed on a separate RedHat OpenShift cluster. Refer to the official IBM API Connect documentation for more detail about the different IBM API Connect component endpoints as well as certificates . Info You can find how to obtain the IBM API Connect official product files that will be used throughout the rest of this document here . The main difference between creating the appropriate certificates for secure communication between all IBM API Connect components that we want to belong to the same IBM API Connect Cluster in these RedHat OpenShift clusters hosting the IBM API Connect Gateway and Analytics components is that we must use the same certificate authority that has been used earlier to sign all the certificates created in the RedHat OpenShift cluster that hosts the IBM API Connect Management and Developer Portal components. Warning You must have extracted the ingress-ca certificates and key that have been used to create the certificate authority to sign all the certificates in the RedHat OpenShift cluster that hosts the IBM API Connect Management and Developer Portal components. Make sure you have the ingress-ca certificates and key that have been used to create the certificate authority to sign all the certificates in the RedHat OpenShift cluster that hosts the IBM API Connect Management and Developer Portal components. ls -all total 24 drwxr-xr-x 5 user staff 160B 7 Sep 13:25 ./ drwxr-xr-x 9 user staff 288B 7 Sep 13:24 ../ -rw------- 1 user staff 1.1K 7 Sep 13:25 ca.crt -rw------- 1 user staff 1.1K 7 Sep 13:25 tls.crt -rw------- 1 user staff 1.6K 7 Sep 13:25 tls.key Create the ingress-ca secret that holds the certificates and key to be used to create the certificate authority to sign all the certificates needed to secure communications of all IBM API Connect components belonging to the same IBM API Connect Cluster. oc create secret generic ingress-ca --from-file = tls.crt = ./tls.crt --from-file = tls.key = ./tls.key --from-file = ca.crt = ./ca.crt -n apic Create the ingress-issuer-v1-alpha1-dc2.yaml file that contains the Issuer and Certificate object definitions with the content below: # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : selfsigning-issuer labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"selfsigning-issuer\" } spec : selfSigned : {} --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : ingress-issuer labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"ingress-issuer\" } spec : ca : secretName : ingress-ca --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : gateway-service labels : { app.kubernetes.io/instance : \"gatewaycluster\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"gateway-service\" } spec : commonName : gateway-service secretName : gateway-service issuerRef : name : ingress-issuer usages : - \"client auth\" - \"signing\" - \"key encipherment\" duration : 17520h # 2 years renewBefore : 720h # 30 days --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : gateway-peering labels : { app.kubernetes.io/instance : \"gatewaycluster\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"gateway-peering\" } spec : commonName : gateway-peering secretName : gateway-peering issuerRef : name : ingress-issuer usages : - \"server auth\" - \"client auth\" - \"signing\" - \"key encipherment\" duration : 17520h # 2 years renewBefore : 720h # 30 days Tip You can check that this ingress-issuer-v1-alpha1-dc2.yaml file does not contain the ingress-ca certificate definition that will be used to sign all the certificates needed for secure communications between the IBM API Connect Gateway and Analytics component and between these and the IBM API Connect Management and Developer Portal components, all belonging to the same IBM API Connect cluster eventually, as it has been manually created before. Apply that ingress-issuer-v1-alpha1-dc2.yaml file to get the Issuer and Certificate objects created in the apic namespace of your RedHat OpenShift cluster. oc apply -f ingress-issuer-v1-alpha1-dc2.yaml -n apic You should see the following output. issuer.certmanager.k8s.io/selfsigning-issuer created issuer.certmanager.k8s.io/ingress-issuer created certificate.certmanager.k8s.io/gateway-service created certificate.certmanager.k8s.io/gateway-peering created Analytics \u00b6 In this section we are going to install the IBM API Connect Analytics component. Refer to the official documentation here for more detail. Create the analytics_cr.yaml file that contains the AnalyticsCluster object definition with the content below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 apiVersion : analytics.apiconnect.ibm.com/v1beta1 kind : AnalyticsCluster metadata : name : analytics labels : app.kubernetes.io/instance : analytics app.kubernetes.io/managed-by : ibm-apiconnect app.kubernetes.io/name : analytics namespace : apic spec : version : 10.0.3.0 license : accept : true license : $LICENSE use : $USE profile : $PROFILE imagePullSecrets : - ibm-entitlement-key microServiceSecurity : certManager certManagerIssuer : name : selfsigning-issuer kind : Issuer client : clientAuthEnabled : true clientSubjectDN : 'CN=analytics-client-client' # clientSubjectDN: 'CN=analytics-client-client,O=cert-manager' enabled : true endpoint : annotations : certmanager.k8s.io/issuer : ingress-issuer hosts : - name : ac.$STACK_HOST secretName : analytics-endpoint-secret ingestion : enabled : true clientSubjectDN : 'CN=analytics-ingestion-client' # clientSubjectDN: 'CN=analytics-ingestion-client,O=cert-manager' clientAuthEnabled : true configReloadAutomatic : true endpoint : annotations : certmanager.k8s.io/issuer : ingress-issuer hosts : - name : ai.$STACK_HOST secretName : analytics-ai-endpoint-secret storage : clientAuthEnabled : true data : volumeClaimTemplate : storageClassName : $STORAGE_CLASS volumeSize : 200Gi enabled : true master : volumeClaimTemplate : storageClassName : $STORAGE_CLASS volumeSize : 10Gi type : unique cronjobs : enabled : true where $PROFILE is either set to n1xc2.m16 for development environments or to n3xc4.m16 for production environments. If you need high availability use the production profile. $STACK_HOST is the desired ingress subdomain for the API Connect stack. You can find the ingress subdomain of you RedHat OpenShift cluster by executing oc get routes . Your subdomain usually is apps.xxx.yyy.zzz . $STORAGE_CLASS is the Kubernetes storage class to be used for Persistent Volume Claims. Remember this must be RWO block storage. $LICENSE is the License ID for the version of API Connect that you purchased. See API Connect licenses . $USE is either production or nonproduction , to match the license you purchased. Apply that analytics_cr.yaml file to get the AnalyticsCluster object created in the apic namespace of your RedHat OpenShift cluster. oc apply -f analytics_cr.yaml -n apic Verify that the IBM API Connect Analytics component is fully installed. oc get AnalyticsCluster -n apic You should see a similar output as the following. NAME READY STATUS VERSION RECONCILED VERSION AGE analytics 8/8 Running 10.0.3.0 10.0.3.0-1320 8d The installation has completed when the READY attribute reports that all services are online (e.g. 8/8) STATUS attribute reports Running . Gateway \u00b6 In this section we are going to install the IBM API Connect Gateway component. Refer to the official documentation here for more detail. Create the IBM API Connect Gateway component administrator credentials oc create secret generic datapower-admin-credentials --from-literal = password = <PASSWORD> -n apic where PASSWORD is the password for the IBM API Connect Gateway component administrator credentials. Create the apigateway_cr.yaml file that contains the GatewayCluster object definition with the content below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 apiVersion : gateway.apiconnect.ibm.com/v1beta1 kind : GatewayCluster metadata : name : gateway labels : app.kubernetes.io/instance : gateway app.kubernetes.io/managed-by : ibm-apiconnect app.kubernetes.io/name : gateway namespace : apic spec : version : 10.0.3.0 profile : $PROFILE imagePullSecrets : - ibm-entitlement-key apicGatewayServiceV5CompatibilityMode : false gatewayEndpoint : annotations : certmanager.k8s.io/issuer : ingress-issuer hosts : - name : gw.$STACK_HOST secretName : gateway-endpoint-secret gatewayManagerEndpoint : annotations : certmanager.k8s.io/issuer : ingress-issuer hosts : - name : gwmanager.$STACK_HOST secretName : gateway-manager-endpoint-secret apicGatewayServiceTLS : secretName : gateway-service apicGatewayPeeringTLS : secretName : gateway-peering datapowerLogLevel : 3 license : accept : true license : $LICENSE use : $USE tokenManagementService : enabled : true storage : storageClassName : $STORAGE_CLASS volumeSize : 30Gi adminUser : secretName : datapower-admin-credentials where $PROFILE is either set to n1xc4.m8 for development environments or to n3xc4.m8 for production environments. If you need high availability use the production profile. $STACK_HOST is the desired ingress subdomain for the API Connect stack. You can find the ingress subdomain of you RedHat OpenShift cluster by executing oc get routes . Your subdomain usually is apps.xxx.yyy.zzz . $STORAGE_CLASS is the Kubernetes storage class to be used for Persistent Volume Claims. Remember this must be RWO block storage. $LICENSE is the License ID for the version of API Connect that you purchased. See API Connect licenses . $USE is either production or nonproduction , to match the license you purchased. Apply that apigateway_cr.yaml file to get the GatewayCluster object created in the apic namespace of your RedHat OpenShift cluster. oc apply -f apigateway_cr.yaml -n apic Verify that the IBM API Connect Gateway component is fully installed. oc get GatewayCluster -n apic You should see a similar output as the following. NAME READY STATUS VERSION RECONCILED VERSION AGE gateway 2/2 Running 10.0.3.0 10.0.3.0-1320 8d The installation has completed when the READY attribute reports that all services are online (e.g. 2/2) STATUS attribute reports Running . Congratulations You have deployed the IBM API Connect Gateway and Analytics components as well as created all the certificates and issuers needed to get all IBM API Connect components to communicate with each other in a secure manner. Info Remember to execute this section in all RedHat OpenShift clusters where you want to host your applications as well as the IBM API Connect Gateway component to securely expose these and the IBM API Connect Analytics component to gather analytics from the IBM API Gateway component. Post Install Configuration \u00b6 Now that we have all IBM API Connect components deployed in the different RedHat OpenShift clusters to suit our IBM API Connect hybrid multi-cloud topology we must configure that topology in the IBM API Connect Cloud Manager, the brain of your IBM API Connect Cluster. Refer to the official IBM API Connect documentation for the Cloud Manager configuration checklist , a summary, with links, of the key initial configuration tasks that you must complete in the Cloud Manager user interface after installing and deploying IBM\u00ae API Connect, and further supplementary tasks.","title":"IBM API Connect Multi-cloud Deployment"},{"location":"guides/cp4i/apic/multi-cluster/deploy-old-manual/#ibm-api-connect-multi-cloud-deployment","text":"Abstract This document describes the steps to carry out a multi-cloud deployment of IBM API Connect across different cloud providers, IBM Cloud and Amazon AWS, all using RedHat OpenSift Container Platform as the underlying unified platform.","title":"IBM API Connect Multi-cloud Deployment"},{"location":"guides/cp4i/apic/multi-cluster/deploy-old-manual/#overview","text":"In this document you will find the steps followed in order to deploy the a multi-cloud topology for IBM API Connect. The Management and Developer components have been installed on a RedHat OpenShift Kubernetes Service cluster in IBM Cloud. The Gateway and Analytics components have been installed on two different RedHat OpenShift clusters, one being a RedHat OpenShift Kubernetes Service cluster on IBM Cloud and the other a RedHat OpenShift cluster on Amazon AWS. The aim is to simulate the use case where you might have applications running on RedHat OpenShift clusters that are hosted on different clouds but you want to unify the way you develop, manage, socialize and secure the APIs of your applications using IBM API Connect. Note How to create RedHat OpenShift Kubernetes Service clusters on IBM Cloud is not the goal of this document. You can find the official documentation here . How to deploy a RedHat OpenShift cluster on Amazon AWS isn't the goal of this document either. You can find the official documentation here . Once you have these RedHat OpenShift clusters (we will use three different RedHat OpenShift clusters in this use case), the deployment process for the different IBM API Connect components should be similar regardless of the cloud provider (or on-premises infrastructure) where those RedHat OpenShift clusters have been deployed onto. The only caveat you will need to pay attention to would be the host parameter for the different endpoints the IBM API Connect components require and the storageClass the different IBM API Connect components will make use of to claim storage. The use case described in this document is based on: The IBM API Connect official documentation here - This documentation refers to plain kubernetes IBM API Connect installations but provide good information of the required process for this use case The work carried out by Cesar Cavazos on IBM API Connect highly available installations on 2 data centers here . Tip Make sure you are comfortable with both the RedHat OpenShift web console and CLI to be able to execute all the steps in this document.","title":"Overview"},{"location":"guides/cp4i/apic/multi-cluster/deploy-old-manual/#ibm-api-connect-operator","text":"In order to install any IBM API Connect component in any RedHat OpenShift cluster, the first thing we need to do is to install the IBM API Connect Operator. We have used the official IBM API Connect documentation here as the guideline to install the IBM API Connect Operator in the three RedHat OpenShift clusters. Here are the exact steps we have followed: Create a namespace called apic where the IBM API Connect components will be created into: oc new-project apic Create the IBM Entitlement Key secret so that you are allowed to pull down IBM Software. oc create secret docker-registry ibm-entitlement-key --docker-server = cp.icr.io --docker-username = cp --docker-password = <YOUR_IBM_ENTITLEMENT_KEY> --docker-email = <YOUR_EMAIL> where YOUR_IBM_ENTITLEMENT_KEY is your IBM Entitlement Key that you can retrieve from the IBM Container Software Library . YOUR_EMAIL is you email address. Link your IBM Entitlement Key secret to the default Service Account for pulling images. oc secrets link default ibm-entitlement-key --for = pull From the RedHat OpenShift web console, click + and create the following CatalogSource resources: The IBM Operator Catalog source. Add the following CatalogSource definition and click save. apiVersion : operators.coreos.com/v1alpha1 kind : CatalogSource metadata : name : ibm-operator-catalog namespace : openshift-marketplace spec : displayName : \"IBM Operator Catalog\" publisher : IBM sourceType : grpc image : docker.io/ibmcom/ibm-operator-catalog updateStrategy : registryPoll : interval : 45m The IBM Cloud Pak foundational services operator catalog. Add the following CatalogSource definition and click save. apiVersion : operators.coreos.com/v1alpha1 kind : CatalogSource metadata : name : opencloud-operators namespace : openshift-marketplace spec : displayName : IBMCS Operators image : docker.io/ibmcom/ibm-common-service-catalog:latest publisher : IBM sourceType : grpc updateStrategy : registryPoll : interval : 45m Install the IBM API Connect Operator. Go to Operators --> OperatorHub on the left hand side menu of your RedHat OpenShift web console. Search for API Connect and click on the tile. Click on Install on the IBM API Connect menu that hovers from the right hand side. Click on Install on the Install Operator page leaving the defaults. After few minutes, you should see the following IBM Operators installed on your apic namespace if you go to Operators --> Installed Operators on the left hand side menu of your RedHat OpenShift web console. Congratulations You should now have the IBM API Connect Operator installed on your RedHat OpenShift cluster. Warning Remember to execute this section in all RedHat OpenShift clusters where you want to deploy any IBM API Connect component.","title":"IBM API Connect Operator"},{"location":"guides/cp4i/apic/multi-cluster/deploy-old-manual/#management-and-developer-portal","text":"In this section, we are going to deploy the IBM API Connect Management and Developer Portal components on the same RedHat OpenShift cluster. This cluster will be a separate cluster from the other two RedHat OpenShift clusters that will host the IBM API Connect Gateway component to securely expose our application APIs as well as the IBM API Connect Analytics component to gather the IBM API Connect Gateway analytics. However, before installing any IBM API Connect component in our RedHat OpenShift clusters, we need to create some certificates that will be used to ensure secure connections between these different IBM API Connect components. Refer to the official IBM API Connect documentation for more detail about the different IBM API Connect component endpoints as well as certificates . Info You can find how to obtain the IBM API Connect official product files that will be used throughout the rest of this document here . Create the ingress-issuer-v1-alpha1.yaml file that contains the Issuer and Certificate object definitions with the content below: # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : selfsigning-issuer labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"selfsigning-issuer\" } spec : selfSigned : {} --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : ingress-ca labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"ingress-ca\" } spec : duration : 87600h # 10 years renewBefore : 720h # 30 days secretName : ingress-ca commonName : \"ingress-ca\" usages : - digital signature - key encipherment - cert sign isCA : true issuerRef : name : selfsigning-issuer kind : Issuer --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : ingress-issuer labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"ingress-issuer\" } spec : ca : secretName : ingress-ca --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : portal-admin-client labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"portal-admin-client\" } spec : commonName : portal-admin-client secretName : portal-admin-client issuerRef : name : ingress-issuer usages : - \"client auth\" - \"signing\" - \"key encipherment\" duration : 17520h # 2 years renewBefore : 720h # 30 days --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : gateway-client-client labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"gateway-client-client\" } spec : commonName : gateway-client-client secretName : gateway-client-client issuerRef : name : ingress-issuer usages : - \"client auth\" - \"signing\" - \"key encipherment\" duration : 17520h # 2 years renewBefore : 720h # 30 days --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : analytics-client-client labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"analytics-client-client\" } spec : commonName : analytics-client-client secretName : analytics-client-client issuerRef : name : ingress-issuer usages : - \"client auth\" - \"signing\" - \"key encipherment\" duration : 17520h # 2 years renewBefore : 720h # 30 days --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : analytics-ingestion-client labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"analytics-ingestion-client\" } spec : commonName : analytics-ingestion-client secretName : analytics-ingestion-client issuerRef : name : ingress-issuer usages : - \"client auth\" - \"signing\" - \"key encipherment\" duration : 17520h # 2 years renewBefore : 720h # 30 days Apply that ingress-issuer-v1-alpha1.yaml file to get the Issuer and Certificate objects created in the apic namespace of your RedHat OpenShift cluster. oc apply -f ingress-issuer-v1-alpha1.yaml -n apic You should see the following output. issuer.certmanager.k8s.io/selfsigning-issuer created certificate.certmanager.k8s.io/ingress-ca created issuer.certmanager.k8s.io/ingress-issuer created certificate.certmanager.k8s.io/portal-admin-client created certificate.certmanager.k8s.io/gateway-client-client created certificate.certmanager.k8s.io/analytics-client-client created certificate.certmanager.k8s.io/analytics-ingestion-client created If you list the certificates on your system, you should see the following: oc get certificates -n apic NAME READY SECRET AGE EXPIRATION analytics-client-client True analytics-client-client 48s 2023-09-07T11:22:03Z analytics-ingestion-client True analytics-ingestion-client 47s 2023-09-07T11:22:00Z gateway-client-client True gateway-client-client 48s 2023-09-07T11:22:15Z ingress-ca True ingress-ca 49s 2031-09-05T11:21:44Z portal-admin-client True portal-admin-client 49s 2023-09-07T11:22:00Z Important You must extract the ingress-ca certificates and key that have been used to create the certificate authority to sign all the certificates you just created above. These certificates and key must be provided to create the same certificate authority in the other RedHat OpenShift clusters where you want to deploy any other IBM API Connect component to be part of the same IBM API Connect cluster. Extract the ingress-ca certificates and key that have been used to create the certificate authority to sign all the certificates you created in the previous step. oc extract secret/ingress-ca --to = . --confirm You should now have the following certificates and key. ls -all total 24 drwxr-xr-x 5 user staff 160B 7 Sep 13:25 ./ drwxr-xr-x 9 user staff 288B 7 Sep 13:24 ../ -rw------- 1 user staff 1.1K 7 Sep 13:25 ca.crt -rw------- 1 user staff 1.1K 7 Sep 13:25 tls.crt -rw------- 1 user staff 1.6K 7 Sep 13:25 tls.key","title":"Management and Developer Portal"},{"location":"guides/cp4i/apic/multi-cluster/deploy-old-manual/#gateway-and-analytics","text":"In this section, we are going to deploy show how to deploy the IBM API Connect Gateway component to securely expose our application APIs as well as the IBM API Connect Analytics component to gather the IBM API Connect Gateway analytics on any RedHat OpenShift cluster where we might want to host our applications. However, before installing the IBM API Connect Gateway or Analytics components in our RedHat OpenShift clusters, we need to create some certificates that will be used to ensure secure connections not only between these IBM API Connect components but also between these IBM API Connect components and the IBM API Connect Management and Developer Portal components that should have been already deployed on a separate RedHat OpenShift cluster. Refer to the official IBM API Connect documentation for more detail about the different IBM API Connect component endpoints as well as certificates . Info You can find how to obtain the IBM API Connect official product files that will be used throughout the rest of this document here . The main difference between creating the appropriate certificates for secure communication between all IBM API Connect components that we want to belong to the same IBM API Connect Cluster in these RedHat OpenShift clusters hosting the IBM API Connect Gateway and Analytics components is that we must use the same certificate authority that has been used earlier to sign all the certificates created in the RedHat OpenShift cluster that hosts the IBM API Connect Management and Developer Portal components. Warning You must have extracted the ingress-ca certificates and key that have been used to create the certificate authority to sign all the certificates in the RedHat OpenShift cluster that hosts the IBM API Connect Management and Developer Portal components. Make sure you have the ingress-ca certificates and key that have been used to create the certificate authority to sign all the certificates in the RedHat OpenShift cluster that hosts the IBM API Connect Management and Developer Portal components. ls -all total 24 drwxr-xr-x 5 user staff 160B 7 Sep 13:25 ./ drwxr-xr-x 9 user staff 288B 7 Sep 13:24 ../ -rw------- 1 user staff 1.1K 7 Sep 13:25 ca.crt -rw------- 1 user staff 1.1K 7 Sep 13:25 tls.crt -rw------- 1 user staff 1.6K 7 Sep 13:25 tls.key Create the ingress-ca secret that holds the certificates and key to be used to create the certificate authority to sign all the certificates needed to secure communications of all IBM API Connect components belonging to the same IBM API Connect Cluster. oc create secret generic ingress-ca --from-file = tls.crt = ./tls.crt --from-file = tls.key = ./tls.key --from-file = ca.crt = ./ca.crt -n apic Create the ingress-issuer-v1-alpha1-dc2.yaml file that contains the Issuer and Certificate object definitions with the content below: # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : selfsigning-issuer labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"selfsigning-issuer\" } spec : selfSigned : {} --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : ingress-issuer labels : { app.kubernetes.io/instance : \"management\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"ingress-issuer\" } spec : ca : secretName : ingress-ca --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : gateway-service labels : { app.kubernetes.io/instance : \"gatewaycluster\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"gateway-service\" } spec : commonName : gateway-service secretName : gateway-service issuerRef : name : ingress-issuer usages : - \"client auth\" - \"signing\" - \"key encipherment\" duration : 17520h # 2 years renewBefore : 720h # 30 days --- apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : gateway-peering labels : { app.kubernetes.io/instance : \"gatewaycluster\" , app.kubernetes.io/managed-by : \"ibm-apiconnect\" , app.kubernetes.io/name : \"gateway-peering\" } spec : commonName : gateway-peering secretName : gateway-peering issuerRef : name : ingress-issuer usages : - \"server auth\" - \"client auth\" - \"signing\" - \"key encipherment\" duration : 17520h # 2 years renewBefore : 720h # 30 days Tip You can check that this ingress-issuer-v1-alpha1-dc2.yaml file does not contain the ingress-ca certificate definition that will be used to sign all the certificates needed for secure communications between the IBM API Connect Gateway and Analytics component and between these and the IBM API Connect Management and Developer Portal components, all belonging to the same IBM API Connect cluster eventually, as it has been manually created before. Apply that ingress-issuer-v1-alpha1-dc2.yaml file to get the Issuer and Certificate objects created in the apic namespace of your RedHat OpenShift cluster. oc apply -f ingress-issuer-v1-alpha1-dc2.yaml -n apic You should see the following output. issuer.certmanager.k8s.io/selfsigning-issuer created issuer.certmanager.k8s.io/ingress-issuer created certificate.certmanager.k8s.io/gateway-service created certificate.certmanager.k8s.io/gateway-peering created","title":"Gateway and Analytics"},{"location":"guides/cp4i/apic/multi-cluster/deploy-old-manual/#post-install-configuration","text":"Now that we have all IBM API Connect components deployed in the different RedHat OpenShift clusters to suit our IBM API Connect hybrid multi-cloud topology we must configure that topology in the IBM API Connect Cloud Manager, the brain of your IBM API Connect Cluster. Refer to the official IBM API Connect documentation for the Cloud Manager configuration checklist , a summary, with links, of the key initial configuration tasks that you must complete in the Cloud Manager user interface after installing and deploying IBM\u00ae API Connect, and further supplementary tasks.","title":"Post Install Configuration"},{"location":"guides/cp4i/apic/multi-cluster/deploy/","text":"IBM API Connect Multi-Cluster - Deploy APIC \u00b6 Abstract This document describes the steps to carry out a multi-cluster deployment of IBM API Connect across, potentially, different cloud providers (such as IBM Cloud and Amazon AWS), all using RedHat OpenSift Container Platform as the underlying unified platform. Overview \u00b6 In previous sections of this IBM API Connect Multi-Cluster tutorial you have seen how to create the Red Hat OpenShift clusters that will support your IBM API Connect multi-cluster use case, forked and cloned the GitHub GitOps repositories that will drive the GitOps processes to realize the implementation of your IBM API Connect multi-cluster use case and finally, in the last section, you have configured these GitOps repositories to reflect the specific architecture of your IBM API multi-cluster use case. Next step is to get the installation of your IBM API Connect multi-cluster use case done and for that you would just need to kickoff the GitOps processes you have already prepared by creating those bootstrap ArgoCD Applications you should be able to find in your multi-tenancy-gitops repository and apic-multi-cluster profile under 0-bootstrap/others . Deploy the IBM API Connect Management and Portal components \u00b6 You are ready to create the bootstrap ArgoCD Application that will kick off the GitOps process that will end up having the IBM API Connect Management and Portal components installed/deployed in your IBM API Connect Management and Portal cluster. However, you still need to complete a small task before being able to deploy any IBM Software in your Red Hat OpenShift cluster. And that is providing your IBM Entitlement Key to be able to pull IBM Software down from the IBM's Software registry. The GitOps processes you have set up in the multi-tenancy-gitops repository would install/deploy any IBM Software in the tools Red Hat OpenShift project. Therefore, you must provide your IBM Entitlement Key in that Red Hat OpenShift project. Ensure you are logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Warning Make sure you are logging into the IBM API Connect Management and Portal cluster Create the tools Red Hat OpenShift project: oc new-project tools Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. Create a secret containing your IBM Entitlement Key in the tools namespace. oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io Link that docker-registry secret containing your IBM Entitlement Key with the default secret for pulling Docker images within your Red Hat OpenShift project oc secrets link default ibm-entitlement-key --for = pull Finally, you are all set to bootstrap the IBM API Connect Management and Portal cluster with the initial ArgoCD Application that will connect the ArgoCD instance that is running in the cluster that you deployed in the Configure the clusters - GitOps repos & ArgoCD section and your multi-tenancy-gitops GitOps repository where you have defined, as code, what you want to get deployed in your Red Hat OpenShift cluster. This is what will bootstrap your GitOps processes. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Change directory to 0-bootstrap/others/apic-multi-cluster Apply the bootstrap-management-portal-cluster.yaml file: oc apply -f bootstrap-management-portal-cluster.yaml If you go to your ArgoCD web console, you would now see ArgoCD Applications being created. These applications will be the responsible for watching over the different code in the GitOps repositories (mainly the multi-tenancy-gitops and multi-tenancy-gitops-apps repositories) and make sure that what is specified there gets created in your Red Hat OpenShift cluster. If you go to your OpenShift web console under the Operators --> Installed Operators section on the left hand side of the menu, you should be able to see how the different operators that we have specified in the multi-tenancy-gitops GitOps repository to be deployed are starting to get deployed. Tip While the IBM API Connect Management and Portal components are getting deployed in this cluster, you can jump to the next section to start deploying the IBM API Connect Gateway and Portal components in their respective cluster(s) After around 45 minutes, you can check that the ArgoCD web console now displays all the ArgoCD Applications in green the OpenShift web console shows that all of the operators have been successfully installed and, finally, if you go under the tools namespace for the Operators --> Installed Operators , click on the IBM API Connect operator and click on the All instances tab, you should see that your management and portal are Running Deploy the IBM API Connect Gateway and Analytics components \u00b6 Again, you are ready to create the bootstrap ArgoCD Application that will kick off the GitOps process that will end up having the IBM API Connect Gateway and Analytics components installed/deployed in your IBM API Connect Management and Portal cluster(s). However, again, you still need to complete a small task before being able to deploy any IBM Software in your Red Hat OpenShift cluster. And that is providing your IBM Entitlement Key to be able to pull IBM Software down from the IBM's Software registry. The GitOps processes you have set up in the multi-tenancy-gitops repository would install/deploy any IBM Software in the tools Red Hat OpenShift project. Therefore, you must provide your IBM Entitlement Key in that Red Hat OpenShift project. Ensure you are logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Warning Make sure you are logging into the IBM API Connect Gateway and Analytics cluster Create the tools Red Hat OpenShift project: oc new-project tools Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. Create a secret containing your IBM Entitlement Key in the tools namespace. oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io Link that docker-registry secret containing your IBM Entitlement Key with the default secret for pulling Docker images within your Red Hat OpenShift project ```bash oc secrets link default ibm-entitlement-key --for=pull Finally, you are all set to bootstrap the IBM API Connect Gateway and Analytics cluster with the initial ArgoCD Application that will connect the ArgoCD instance that is running in the cluster that you deployed in the Configure the clusters - GitOps repos & ArgoCD section and your multi-tenancy-gitops GitOps repository where you have defined, as code, what you want to get deployed in your Red Hat OpenShift cluster. This is what will bootstrap your GitOps processes. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Change directory to 0-bootstrap/others/apic-multi-cluster Apply the bootstrap-<NAME>-gateway-analytics-cluster.yaml file: oc apply -f bootstrap-<NAME>-gateway-analytics-cluster.yaml where <NAME> is the unique name for your IBM API Connect Gateway and Analytics cluster you created in the previous Configure the clusters - Configure GitOps repos section. If you go to your ArgoCD web console, you would now see ArgoCD Applications being created. These applications will be the responsible for watching over the different code in the GitOps repositories (mainly the multi-tenancy-gitops and multi-tenancy-gitops-apps repositories) and make sure that what is specified there gets created in your Red Hat OpenShift cluster. If you go to your OpenShift web console under the Operators --> Installed Operators section on the left hand side of the menu, you should be able to see how the different operators that we have specified in the GitOps repository to be deployed are starting to get deployed. After around 45 minutes, you can check that the ArgoCD web console now displays all the ArgoCD Applications in green (except one, the apic-app-multicloud . See below at the end of this subsection why) the OpenShift web console shows that all of the operators have been successfully installed and, finally, if you go under the tools namespace for the Operators --> Installed Operators , click on the IBM API Connect operator and click on the All instances tab, you should see that your gateway and analytics are Running Warning You must repeat this Deploy the IBM API Connect Gateway and Analytics components for each of the IBM API Connect Gateway and Analytics clusters you created in your apic-multi-cluster profile in your multi-tenancy-gitops repository. Why an ArgoCD Application is out of sync \u00b6 The reason why the apic-app-multicloud ArgoCD Application is in OutOfSync state at first is because this is intended. If you check that ArgoCD Application definition which is in the multi-tenancy-gitops-apps repository under apic/config/argocd/multi-cluster/app you will see the following: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : apic-app-multicloud annotations : argocd.argoproj.io/sync-wave : \"300\" finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : dev server : https://kubernetes.default.svc project : applications source : path : apic/environments/multi-cluster/app repoURL : ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS_APPLICATIONS} targetRevision : ${GIT_GITOPS_APPLICATIONS_BRANCH} syncPolicy : {} ignoreDifferences : - group : apps kind : DeploymentConfig jsonPointers : - /spec/containers/image - /spec/containers/terminationMessagePath - /spec/containers/terminationMessagePolicy You can see in line number 18 that this ArgoCD Application has not been configured to automatically get synchronised. So if you go into that ArgoCD Application from the ArgoCD web console, you will see all the resources this ArgoCD is meant to watch are also in OutOfSync state. And the reason for this is that we are using BuildConfig and DeploymentConfig resources which will collide with ArgoCD in the sense that these resource would auto-generate certain tags and values for the resulting yaml files which ArgoCD does not expect. This is not a bug. It is just that these two type of resources are not made to be watched and managed by GitOps application such as ArgoCD since these will generate attributes, properties and even other resources that ArgoCD will not expect. As a result, the auto-sync has been disabled for this ArgoCD Application as you could see in its definition above. However, you want to have whatever resources this ArgoCD Application is meant to watch and managed deployed in your cluster. The resources that this ArgoCD Application is meant to deploy is the dummy application you will use at the end of the tutorial to verify that the requests to a certain IBM API Connect cluster do, in fact, reach the dummy application running on that same Red Hat OpenShift cluster. In your ArgoCD web console, for each of the IBM API Connect Gateway and Analytics clusters , go into the apic-app-multicloud ArgoCD Application and click on the SYNC button at the top: You should see, as already said, all the resources this ArgoCD Application is meant to watch and manage and deploy in the first place as a result in OutOfSync state. On the sync menu that will hover from the right, make sure all the resources are checked and then click on SYNCHRONIZE at the top. After some time, you should see all the resources created and in green except from the DeploymentConfig because of what has already been explained above: You can click on that resource and then on the DIFF option for the manifest to see what is not matching. IBM API Connect Cloud Manager \u00b6 Now, let's make sure that our API Connect Cluster is up and running. For doing that, we are going to make sure that we can open and access the IBM API Connect Cloud Manager web console. In your Red Hat OpenShift we console, go to Networking --> Routes under the tools project and click on the Location value for the management-admin route. That should bring you to the IBM API Connect Cloud Manager web console login page. Log into the IBM API Connect Cloud Manager web console by using admin as the username and getting it's password with the following command oc get secret `oc get secrets -n prod | grep management-admin-credentials | awk '{print $1}'` -n prod -o jsonpath='{.data.password}' | base64 -D Finally, click on the Configure Topology option presented in the IBM API Connect Cloud Manager web console. You should see that there is no topology configured at all. You can follow the IBM API Connect Cloud Manager configuration checklist documentation to manually proceed with the tasks you need to accomplish to get your IBM API Connect cluster ready to be used or you can go to the next section in this chapter where you will create an OpenShift Pipeline that gets your recently deployed IBM API Connect instance configured so that you can start working with it right away. Congratulations! You have successfully installed the IBM API Connect Management and Portal components and the IBM API Connect Gateway and Analytics components in their respective clusters achieving an IBM API Connect Multi-cluster scenario.","title":"Deploy APIC"},{"location":"guides/cp4i/apic/multi-cluster/deploy/#ibm-api-connect-multi-cluster-deploy-apic","text":"Abstract This document describes the steps to carry out a multi-cluster deployment of IBM API Connect across, potentially, different cloud providers (such as IBM Cloud and Amazon AWS), all using RedHat OpenSift Container Platform as the underlying unified platform.","title":"IBM API Connect Multi-Cluster - Deploy APIC"},{"location":"guides/cp4i/apic/multi-cluster/deploy/#overview","text":"In previous sections of this IBM API Connect Multi-Cluster tutorial you have seen how to create the Red Hat OpenShift clusters that will support your IBM API Connect multi-cluster use case, forked and cloned the GitHub GitOps repositories that will drive the GitOps processes to realize the implementation of your IBM API Connect multi-cluster use case and finally, in the last section, you have configured these GitOps repositories to reflect the specific architecture of your IBM API multi-cluster use case. Next step is to get the installation of your IBM API Connect multi-cluster use case done and for that you would just need to kickoff the GitOps processes you have already prepared by creating those bootstrap ArgoCD Applications you should be able to find in your multi-tenancy-gitops repository and apic-multi-cluster profile under 0-bootstrap/others .","title":"Overview"},{"location":"guides/cp4i/apic/multi-cluster/deploy/#deploy-the-ibm-api-connect-management-and-portal-components","text":"You are ready to create the bootstrap ArgoCD Application that will kick off the GitOps process that will end up having the IBM API Connect Management and Portal components installed/deployed in your IBM API Connect Management and Portal cluster. However, you still need to complete a small task before being able to deploy any IBM Software in your Red Hat OpenShift cluster. And that is providing your IBM Entitlement Key to be able to pull IBM Software down from the IBM's Software registry. The GitOps processes you have set up in the multi-tenancy-gitops repository would install/deploy any IBM Software in the tools Red Hat OpenShift project. Therefore, you must provide your IBM Entitlement Key in that Red Hat OpenShift project. Ensure you are logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Warning Make sure you are logging into the IBM API Connect Management and Portal cluster Create the tools Red Hat OpenShift project: oc new-project tools Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. Create a secret containing your IBM Entitlement Key in the tools namespace. oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io Link that docker-registry secret containing your IBM Entitlement Key with the default secret for pulling Docker images within your Red Hat OpenShift project oc secrets link default ibm-entitlement-key --for = pull Finally, you are all set to bootstrap the IBM API Connect Management and Portal cluster with the initial ArgoCD Application that will connect the ArgoCD instance that is running in the cluster that you deployed in the Configure the clusters - GitOps repos & ArgoCD section and your multi-tenancy-gitops GitOps repository where you have defined, as code, what you want to get deployed in your Red Hat OpenShift cluster. This is what will bootstrap your GitOps processes. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Change directory to 0-bootstrap/others/apic-multi-cluster Apply the bootstrap-management-portal-cluster.yaml file: oc apply -f bootstrap-management-portal-cluster.yaml If you go to your ArgoCD web console, you would now see ArgoCD Applications being created. These applications will be the responsible for watching over the different code in the GitOps repositories (mainly the multi-tenancy-gitops and multi-tenancy-gitops-apps repositories) and make sure that what is specified there gets created in your Red Hat OpenShift cluster. If you go to your OpenShift web console under the Operators --> Installed Operators section on the left hand side of the menu, you should be able to see how the different operators that we have specified in the multi-tenancy-gitops GitOps repository to be deployed are starting to get deployed. Tip While the IBM API Connect Management and Portal components are getting deployed in this cluster, you can jump to the next section to start deploying the IBM API Connect Gateway and Portal components in their respective cluster(s) After around 45 minutes, you can check that the ArgoCD web console now displays all the ArgoCD Applications in green the OpenShift web console shows that all of the operators have been successfully installed and, finally, if you go under the tools namespace for the Operators --> Installed Operators , click on the IBM API Connect operator and click on the All instances tab, you should see that your management and portal are Running","title":"Deploy the IBM API Connect Management and Portal components"},{"location":"guides/cp4i/apic/multi-cluster/deploy/#deploy-the-ibm-api-connect-gateway-and-analytics-components","text":"Again, you are ready to create the bootstrap ArgoCD Application that will kick off the GitOps process that will end up having the IBM API Connect Gateway and Analytics components installed/deployed in your IBM API Connect Management and Portal cluster(s). However, again, you still need to complete a small task before being able to deploy any IBM Software in your Red Hat OpenShift cluster. And that is providing your IBM Entitlement Key to be able to pull IBM Software down from the IBM's Software registry. The GitOps processes you have set up in the multi-tenancy-gitops repository would install/deploy any IBM Software in the tools Red Hat OpenShift project. Therefore, you must provide your IBM Entitlement Key in that Red Hat OpenShift project. Ensure you are logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Warning Make sure you are logging into the IBM API Connect Gateway and Analytics cluster Create the tools Red Hat OpenShift project: oc new-project tools Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. Create a secret containing your IBM Entitlement Key in the tools namespace. oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io Link that docker-registry secret containing your IBM Entitlement Key with the default secret for pulling Docker images within your Red Hat OpenShift project ```bash oc secrets link default ibm-entitlement-key --for=pull Finally, you are all set to bootstrap the IBM API Connect Gateway and Analytics cluster with the initial ArgoCD Application that will connect the ArgoCD instance that is running in the cluster that you deployed in the Configure the clusters - GitOps repos & ArgoCD section and your multi-tenancy-gitops GitOps repository where you have defined, as code, what you want to get deployed in your Red Hat OpenShift cluster. This is what will bootstrap your GitOps processes. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Change directory to 0-bootstrap/others/apic-multi-cluster Apply the bootstrap-<NAME>-gateway-analytics-cluster.yaml file: oc apply -f bootstrap-<NAME>-gateway-analytics-cluster.yaml where <NAME> is the unique name for your IBM API Connect Gateway and Analytics cluster you created in the previous Configure the clusters - Configure GitOps repos section. If you go to your ArgoCD web console, you would now see ArgoCD Applications being created. These applications will be the responsible for watching over the different code in the GitOps repositories (mainly the multi-tenancy-gitops and multi-tenancy-gitops-apps repositories) and make sure that what is specified there gets created in your Red Hat OpenShift cluster. If you go to your OpenShift web console under the Operators --> Installed Operators section on the left hand side of the menu, you should be able to see how the different operators that we have specified in the GitOps repository to be deployed are starting to get deployed. After around 45 minutes, you can check that the ArgoCD web console now displays all the ArgoCD Applications in green (except one, the apic-app-multicloud . See below at the end of this subsection why) the OpenShift web console shows that all of the operators have been successfully installed and, finally, if you go under the tools namespace for the Operators --> Installed Operators , click on the IBM API Connect operator and click on the All instances tab, you should see that your gateway and analytics are Running Warning You must repeat this Deploy the IBM API Connect Gateway and Analytics components for each of the IBM API Connect Gateway and Analytics clusters you created in your apic-multi-cluster profile in your multi-tenancy-gitops repository.","title":"Deploy the IBM API Connect Gateway and Analytics components"},{"location":"guides/cp4i/apic/multi-cluster/deploy/#why-an-argocd-application-is-out-of-sync","text":"The reason why the apic-app-multicloud ArgoCD Application is in OutOfSync state at first is because this is intended. If you check that ArgoCD Application definition which is in the multi-tenancy-gitops-apps repository under apic/config/argocd/multi-cluster/app you will see the following: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : apic-app-multicloud annotations : argocd.argoproj.io/sync-wave : \"300\" finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : dev server : https://kubernetes.default.svc project : applications source : path : apic/environments/multi-cluster/app repoURL : ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS_APPLICATIONS} targetRevision : ${GIT_GITOPS_APPLICATIONS_BRANCH} syncPolicy : {} ignoreDifferences : - group : apps kind : DeploymentConfig jsonPointers : - /spec/containers/image - /spec/containers/terminationMessagePath - /spec/containers/terminationMessagePolicy You can see in line number 18 that this ArgoCD Application has not been configured to automatically get synchronised. So if you go into that ArgoCD Application from the ArgoCD web console, you will see all the resources this ArgoCD is meant to watch are also in OutOfSync state. And the reason for this is that we are using BuildConfig and DeploymentConfig resources which will collide with ArgoCD in the sense that these resource would auto-generate certain tags and values for the resulting yaml files which ArgoCD does not expect. This is not a bug. It is just that these two type of resources are not made to be watched and managed by GitOps application such as ArgoCD since these will generate attributes, properties and even other resources that ArgoCD will not expect. As a result, the auto-sync has been disabled for this ArgoCD Application as you could see in its definition above. However, you want to have whatever resources this ArgoCD Application is meant to watch and managed deployed in your cluster. The resources that this ArgoCD Application is meant to deploy is the dummy application you will use at the end of the tutorial to verify that the requests to a certain IBM API Connect cluster do, in fact, reach the dummy application running on that same Red Hat OpenShift cluster. In your ArgoCD web console, for each of the IBM API Connect Gateway and Analytics clusters , go into the apic-app-multicloud ArgoCD Application and click on the SYNC button at the top: You should see, as already said, all the resources this ArgoCD Application is meant to watch and manage and deploy in the first place as a result in OutOfSync state. On the sync menu that will hover from the right, make sure all the resources are checked and then click on SYNCHRONIZE at the top. After some time, you should see all the resources created and in green except from the DeploymentConfig because of what has already been explained above: You can click on that resource and then on the DIFF option for the manifest to see what is not matching.","title":"Why an ArgoCD Application is out of sync"},{"location":"guides/cp4i/apic/multi-cluster/deploy/#ibm-api-connect-cloud-manager","text":"Now, let's make sure that our API Connect Cluster is up and running. For doing that, we are going to make sure that we can open and access the IBM API Connect Cloud Manager web console. In your Red Hat OpenShift we console, go to Networking --> Routes under the tools project and click on the Location value for the management-admin route. That should bring you to the IBM API Connect Cloud Manager web console login page. Log into the IBM API Connect Cloud Manager web console by using admin as the username and getting it's password with the following command oc get secret `oc get secrets -n prod | grep management-admin-credentials | awk '{print $1}'` -n prod -o jsonpath='{.data.password}' | base64 -D Finally, click on the Configure Topology option presented in the IBM API Connect Cloud Manager web console. You should see that there is no topology configured at all. You can follow the IBM API Connect Cloud Manager configuration checklist documentation to manually proceed with the tasks you need to accomplish to get your IBM API Connect cluster ready to be used or you can go to the next section in this chapter where you will create an OpenShift Pipeline that gets your recently deployed IBM API Connect instance configured so that you can start working with it right away. Congratulations! You have successfully installed the IBM API Connect Management and Portal components and the IBM API Connect Gateway and Analytics components in their respective clusters achieving an IBM API Connect Multi-cluster scenario.","title":"IBM API Connect Cloud Manager"},{"location":"guides/cp4i/apic/multi-cluster/overview/","text":"IBM API Connect Multi-Cluster - Overview \u00b6 Abstract This document introduces the IBM API Connect Multi-Cluster use case. Tip Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net Video Description and Demonstration \u00b6 The implementation details in this video may have changed from when it was created in early November 2021. The architectural description and design remain the same. Please follow the instructions in the Tutorial section when implementing this solution. Overview \u00b6 The IBM API Connect Multi-Cluster scenario tries to provide a more real world client scenario where the client's IT landscape looks more like a hybrid multi-cloud landscape rather than a single cloud single cluster landscape. As a result of this, we would like to test the ability of API Connect to provide a central API management and security solution to the disperse scenario where client's applications are spread across multiple clusters on that hybrid multi-cloud scenario. We assume that Red Hat OpenShift Container Platform is the standard platform across that hybrid multi-cloud scenario. As you can see in the picture, the IBM API Connect Multi-Cluster architecture that we propose is that where the different IBM API Connect subsystems will be deployed in several and separate clusters according to the needs of these clusters. That is, we will have the IBM API Connect Management and Portal subsystems running together on a cluster, which will be considered the control plane cluster since the Management and Portal components of IBM API Connect could be considered as the brain of IBM API Connect. Then, the IBM API Connect Gateway and Analytics components will be deployed to all of those other clusters where applications run, so that their APIs are exposed and secured through the IBM API Connect Gateway component while the IBM API Connect Analytics component, which is deployed alongside, gathers all the analytics related to that IBM API Connect Gateway component for monitoring and further analysis. Both the IBM API Connect Gateway and Analytics components will be managed from the control plane cluster. That is, from the IBM API Connect Management and Portal components. From there, we will be able to publish new APIs to the appropriate gateways as well as receive and consolidate the analytics data sent back from the associated IBM API Connect Analytics component (the data that the IBM API Connect Analytics component gathers from the IBM API Connect Gateway component it is associated with can be forwarded/sent to a third party system. You can read more here but it is out of the scope of this document). Recommendation We strongly recommend to first go through the single cluster IBM API Connect deployment tutorial before doing the multi-cluster use case. For that, go to the single cluster IBM API Connect deployment Overview section .","title":"Overview"},{"location":"guides/cp4i/apic/multi-cluster/overview/#ibm-api-connect-multi-cluster-overview","text":"Abstract This document introduces the IBM API Connect Multi-Cluster use case. Tip Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net","title":"IBM API Connect Multi-Cluster - Overview"},{"location":"guides/cp4i/apic/multi-cluster/overview/#video-description-and-demonstration","text":"The implementation details in this video may have changed from when it was created in early November 2021. The architectural description and design remain the same. Please follow the instructions in the Tutorial section when implementing this solution.","title":"Video Description and Demonstration"},{"location":"guides/cp4i/apic/multi-cluster/overview/#overview","text":"The IBM API Connect Multi-Cluster scenario tries to provide a more real world client scenario where the client's IT landscape looks more like a hybrid multi-cloud landscape rather than a single cloud single cluster landscape. As a result of this, we would like to test the ability of API Connect to provide a central API management and security solution to the disperse scenario where client's applications are spread across multiple clusters on that hybrid multi-cloud scenario. We assume that Red Hat OpenShift Container Platform is the standard platform across that hybrid multi-cloud scenario. As you can see in the picture, the IBM API Connect Multi-Cluster architecture that we propose is that where the different IBM API Connect subsystems will be deployed in several and separate clusters according to the needs of these clusters. That is, we will have the IBM API Connect Management and Portal subsystems running together on a cluster, which will be considered the control plane cluster since the Management and Portal components of IBM API Connect could be considered as the brain of IBM API Connect. Then, the IBM API Connect Gateway and Analytics components will be deployed to all of those other clusters where applications run, so that their APIs are exposed and secured through the IBM API Connect Gateway component while the IBM API Connect Analytics component, which is deployed alongside, gathers all the analytics related to that IBM API Connect Gateway component for monitoring and further analysis. Both the IBM API Connect Gateway and Analytics components will be managed from the control plane cluster. That is, from the IBM API Connect Management and Portal components. From there, we will be able to publish new APIs to the appropriate gateways as well as receive and consolidate the analytics data sent back from the associated IBM API Connect Analytics component (the data that the IBM API Connect Analytics component gathers from the IBM API Connect Gateway component it is associated with can be forwarded/sent to a third party system. You can read more here but it is out of the scope of this document). Recommendation We strongly recommend to first go through the single cluster IBM API Connect deployment tutorial before doing the multi-cluster use case. For that, go to the single cluster IBM API Connect deployment Overview section .","title":"Overview"},{"location":"guides/cp4i/apic/multi-cluster/publish-pipeline/","text":"IBM API Connect Multi-Cluster - Publish APIs \u00b6 Overview \u00b6 You have reached the last section of this IBM API Connect Multi-Cluster in-depth tutorial. By now, you should have a working IBM API Connect multi-cluster instance that spans across multiple cluster, each of which could be either on premise or in a cloud provider or multiple cloud providers as long as it sits on Red Hat OpenShift, which is the main piece that allow you to really be hybrid multi-cloud. Hence, by going through this tutorial you should have learnt how to deploy IBM API Connect in a hybrid multi-cloud landscape using GitOps!. However, there is one last step to be taken. If you remember in the first place, the goal of deploying IBM API Connect was to be able to securely expose your application APIs through the IBM API Connect Gateway component, be able to gather analytics on those API requests through the IBM API Connect Analytics component and have all your application APIs published, socialized and managed from a central solution that is IBM API Connect in itself. And, of course, you would want to have this process as much automated as possible which is what the recommendation for a production environment is. And remember that this tutorial, and the Cloud Pak Production Deployment Guides in fact, is all about best practices and recommendations as far as how to deploy and manage IBM Software in production on Red Hat OpenShift. The IBM API Connect citizens you would want to work with in this section are products and APIs. Products are a logical grouping of APIs and Plans. The Catalog is where API Publishing and Consumption is handled and a Catalog has a one to one relationship with a Portal Site. An API Product is staged to a Catalog, published to the Portal site and then API consumers can begin subscribing and making API calls. Catalogs provide isolated API run-time environments within a Provider Organization. If you remember from the previous Configure APIC section where you ran IBM API Connect Configuration OpenShift Pipeline to get your recently deployed IBM API Connect multi-cluster instance configured, you got a Provider Organization created and a default Sandbox catalog created in it. As a result, you should be ready to publish APIs into products and publish those products into your catalog, which would make those application APIs accessible through your IBM API Connect Gateway components. For more information about IBM API Connect terminology and logical architecture, we strongly recommend you read the IBM API Connect v10.x Deployment WhitePaper written by Chris Phillips. In this section then, you will be looking at the second of the OpenShift Pipelines your multi-tenancy-gitops repository created for you. This IBM API Connect Publish Products and APIs OpenShift Pipeline automates the process for getting products and APIs, which are stored as code in GitHub, and publish those to your IBM API Connect multi-cluster instance. Hence, if any change is delivered to those product and APIs definitions in GitHub, the IBM API Connect Publish Products and APIs OpenShift Pipeline will make sure those changes are applied to your IBM API Connect multi-cluster instance providing a sort of GitOps approach to managing your products and APIs. More specifically, your IBM API Connect Publish Products and APIs OpenShift Pipeline will: Download and process product definitions. Download corresponding API definitions for the products. Publish products and APIs to catalog. OpenShift pipeline \u00b6 The OpenShift Pipeline just introduced above has been created in your IBM API Connect Management and Portal Red Hat OpenShift custer. If you go to Pipelines --> Pipelines on the left hand side menu of your Red Hat OpenShift web console under the ci Project, you will see two OpenShift pipelines have been created as a result of what you had specified in your GitOps repositories: Info The pipeline you are interested in for getting your instance of IBM API Connect configured is called apic-publish-products-apis-pipeline Important Make sure you have read that previous Configure APIC section to better understand where this pipeline comes from within your GitOps repositories, how it is created and the GitHub repositories behind it. More importantly, make sure you have forked the pipelines repositories into your own GitHub organization and created your GitHub Personal Access Token as explained in the OpenShift pipeline subsection of that previous section. Products and APIs definitions \u00b6 The GitHub repository that will store those products and APIs you want your IBM API Connect Publish Products and APIs OpenShift Pipeline to work with is https://github.com/cloud-native-toolkit/apic-products-apis-yaml , which you should have already forked into your GitHub organization. If you inspect that repository under the multicloud folder, you will see the following API that exposes your dummy application running on your IBM API Connect Gateway and Analytics clusters and the product it will be part of, both of which will get published through your IBM API Connect Publish Products and APIs OpenShift Pipeline: apic-products-apis-yaml/multicloud \u251c\u2500\u2500 apis \u2502 \u2514\u2500\u2500 cloud-provider_1.0.0.yaml \u2514\u2500\u2500 products \u2514\u2500\u2500 cloud-provider-product_1.0.0.yaml You can inspect both files for better understanding if you would like to. Run the IBM API Connect Configuration pipeline \u00b6 Before you execute your IBM API Connect Publish Products and APIs OpenShift Pipeline, let's first have a look at the Sandbox default catalog that was created when the IBM API Connect Configuration OpenShift Pipeline from the previous section created your Provider Organization is actually empty. You can check your catalog from your IBM API Connect API Manager: Tip You can open your IBM API Connect API Manager through your Red Hat OpenShift web console. Go to Networking --> Routes under the tools project and click on the Location value for the management-api-manager route. Then, use the credentials you specified in your provider organization configuration to log in. Click on the apic-publish-products-apis-pipeline . Then, click on Start on the Actions drop down menu at the top right corner. On the Start Pipeline OpenShift Pipeline configuration menu that pops up, make sure you provide the appropriate values for the following pipeline parameters: git-apic-pipeline-git-url which is the GitHub url where the pipeline scripts are stored. These are the scripts the pipeline will execute. This is the GitHub repository your forked into your GitHub organization in the previous Configure APIC section. git-apic-yaml-url which is the GitHub url where the products and APIs definitions you want the pipeline to work with are stored. Once again, you should have forked this repository into your GitHub organization in the previous Configure APIC section. Click Start . This will create a Pipeline Run object that represents this specific execution of the pipeline. You can follow along the execution of this OpenShift Pipeline Run by clicking the Logs tab at the top bar. If this execution of the IBM API Connect configuration pipeline finishes successfully you should see SUCCESSFUL messages at the bottom of the logs as well as a green check mark on the task the Pipeline Run has executed on the left hand side: If you switch back to the Details tab of the Pipeline Run , you should also see green check marks both at the Pipeline Run name at the top and below it in the task that has been executed by this Pipeline Run that is displayed on the Pipeline Run Details section: And in the main Pipelines section of your OpenShift web console you should also see the last Pipeline Run for the IBM API Connect config Pipeline in green: If you go to your IBM API Connect API Manager and check your default Sandbox catalog, you should now see your just published Cloud Provider product. If you click on the menu icon at the right hand side of your Cloud Provider product, a drop down menu should display. On that menu, click on Manage APIs : You should see the Cloud Provider API that the dummy application running on your IBM API Connect Gateway and Analytics clusters exposes: Check your API \u00b6 You have finally set up all the pieces you needed to: Deploy IBM API Connect in a multi-cluster scenario where you will use the IBM API Connect Gateway components to securely expose your applications APIs, as well as some other automation tools (such as the Red Hat OpenShift Pipeline operator) and an application for testing the whole scenario. You have done so following GitOps methodology as you would want to have for production environments. You have leveraged IBM API Connect Cloud Manager Administration APIs and Red Hat OpenShift Pipelines to build automation around configuring your IBM API Connect multi-cluster instance with all the components you have created such profile with. You have leveraged IBM API Connect API Manager APIs and Red Hat OpenShift Pipelines to build automation to manage your applications APIs and the products these will belong to from their definitions stored in GitHub. As a result, you are now all set to take that last step which is to test if you can access your application's API through the IBM API Connect Gateway. To do so, go to your IBM API Connect API Manager, click on Manage catalogs , then click on the default Sandbox catalog and click on the Catalog settings tab. Then, click on the Gateway services section on the left hand side to find out your IBM API Connect Gateway services and their URLs: Use the URL of your Gateway services to point your browser to http://<your_gateway_url>/cloud-provider/cloud and see if you can access the dummy application running on those IBM API Connect Gateway and Analytics clusters whose API should give you back which IBM API Connect Gateway your request has been made from: You have tested that your dummy application's API can be accessed through the IBM API Connect Gateway service that fronts it thanks to having successfully published and exposed such API on your successfully deployed and configured IBM API Connect multi-cluster instance. Congratulations! You have successfully completed the in-depth tutorial for IBM API Connect Multi-Cluster use case. We strongly recommend you check out the other in-depth tutorials from the left hand side menu of this Cloud Pak Production Deployment Guides. Thank you.","title":"Publish APIs"},{"location":"guides/cp4i/apic/multi-cluster/publish-pipeline/#ibm-api-connect-multi-cluster-publish-apis","text":"","title":"IBM API Connect Multi-Cluster - Publish APIs"},{"location":"guides/cp4i/apic/multi-cluster/publish-pipeline/#overview","text":"You have reached the last section of this IBM API Connect Multi-Cluster in-depth tutorial. By now, you should have a working IBM API Connect multi-cluster instance that spans across multiple cluster, each of which could be either on premise or in a cloud provider or multiple cloud providers as long as it sits on Red Hat OpenShift, which is the main piece that allow you to really be hybrid multi-cloud. Hence, by going through this tutorial you should have learnt how to deploy IBM API Connect in a hybrid multi-cloud landscape using GitOps!. However, there is one last step to be taken. If you remember in the first place, the goal of deploying IBM API Connect was to be able to securely expose your application APIs through the IBM API Connect Gateway component, be able to gather analytics on those API requests through the IBM API Connect Analytics component and have all your application APIs published, socialized and managed from a central solution that is IBM API Connect in itself. And, of course, you would want to have this process as much automated as possible which is what the recommendation for a production environment is. And remember that this tutorial, and the Cloud Pak Production Deployment Guides in fact, is all about best practices and recommendations as far as how to deploy and manage IBM Software in production on Red Hat OpenShift. The IBM API Connect citizens you would want to work with in this section are products and APIs. Products are a logical grouping of APIs and Plans. The Catalog is where API Publishing and Consumption is handled and a Catalog has a one to one relationship with a Portal Site. An API Product is staged to a Catalog, published to the Portal site and then API consumers can begin subscribing and making API calls. Catalogs provide isolated API run-time environments within a Provider Organization. If you remember from the previous Configure APIC section where you ran IBM API Connect Configuration OpenShift Pipeline to get your recently deployed IBM API Connect multi-cluster instance configured, you got a Provider Organization created and a default Sandbox catalog created in it. As a result, you should be ready to publish APIs into products and publish those products into your catalog, which would make those application APIs accessible through your IBM API Connect Gateway components. For more information about IBM API Connect terminology and logical architecture, we strongly recommend you read the IBM API Connect v10.x Deployment WhitePaper written by Chris Phillips. In this section then, you will be looking at the second of the OpenShift Pipelines your multi-tenancy-gitops repository created for you. This IBM API Connect Publish Products and APIs OpenShift Pipeline automates the process for getting products and APIs, which are stored as code in GitHub, and publish those to your IBM API Connect multi-cluster instance. Hence, if any change is delivered to those product and APIs definitions in GitHub, the IBM API Connect Publish Products and APIs OpenShift Pipeline will make sure those changes are applied to your IBM API Connect multi-cluster instance providing a sort of GitOps approach to managing your products and APIs. More specifically, your IBM API Connect Publish Products and APIs OpenShift Pipeline will: Download and process product definitions. Download corresponding API definitions for the products. Publish products and APIs to catalog.","title":"Overview"},{"location":"guides/cp4i/apic/multi-cluster/publish-pipeline/#openshift-pipeline","text":"The OpenShift Pipeline just introduced above has been created in your IBM API Connect Management and Portal Red Hat OpenShift custer. If you go to Pipelines --> Pipelines on the left hand side menu of your Red Hat OpenShift web console under the ci Project, you will see two OpenShift pipelines have been created as a result of what you had specified in your GitOps repositories: Info The pipeline you are interested in for getting your instance of IBM API Connect configured is called apic-publish-products-apis-pipeline Important Make sure you have read that previous Configure APIC section to better understand where this pipeline comes from within your GitOps repositories, how it is created and the GitHub repositories behind it. More importantly, make sure you have forked the pipelines repositories into your own GitHub organization and created your GitHub Personal Access Token as explained in the OpenShift pipeline subsection of that previous section.","title":"OpenShift pipeline"},{"location":"guides/cp4i/apic/multi-cluster/publish-pipeline/#products-and-apis-definitions","text":"The GitHub repository that will store those products and APIs you want your IBM API Connect Publish Products and APIs OpenShift Pipeline to work with is https://github.com/cloud-native-toolkit/apic-products-apis-yaml , which you should have already forked into your GitHub organization. If you inspect that repository under the multicloud folder, you will see the following API that exposes your dummy application running on your IBM API Connect Gateway and Analytics clusters and the product it will be part of, both of which will get published through your IBM API Connect Publish Products and APIs OpenShift Pipeline: apic-products-apis-yaml/multicloud \u251c\u2500\u2500 apis \u2502 \u2514\u2500\u2500 cloud-provider_1.0.0.yaml \u2514\u2500\u2500 products \u2514\u2500\u2500 cloud-provider-product_1.0.0.yaml You can inspect both files for better understanding if you would like to.","title":"Products and APIs definitions"},{"location":"guides/cp4i/apic/multi-cluster/publish-pipeline/#run-the-ibm-api-connect-configuration-pipeline","text":"Before you execute your IBM API Connect Publish Products and APIs OpenShift Pipeline, let's first have a look at the Sandbox default catalog that was created when the IBM API Connect Configuration OpenShift Pipeline from the previous section created your Provider Organization is actually empty. You can check your catalog from your IBM API Connect API Manager: Tip You can open your IBM API Connect API Manager through your Red Hat OpenShift web console. Go to Networking --> Routes under the tools project and click on the Location value for the management-api-manager route. Then, use the credentials you specified in your provider organization configuration to log in. Click on the apic-publish-products-apis-pipeline . Then, click on Start on the Actions drop down menu at the top right corner. On the Start Pipeline OpenShift Pipeline configuration menu that pops up, make sure you provide the appropriate values for the following pipeline parameters: git-apic-pipeline-git-url which is the GitHub url where the pipeline scripts are stored. These are the scripts the pipeline will execute. This is the GitHub repository your forked into your GitHub organization in the previous Configure APIC section. git-apic-yaml-url which is the GitHub url where the products and APIs definitions you want the pipeline to work with are stored. Once again, you should have forked this repository into your GitHub organization in the previous Configure APIC section. Click Start . This will create a Pipeline Run object that represents this specific execution of the pipeline. You can follow along the execution of this OpenShift Pipeline Run by clicking the Logs tab at the top bar. If this execution of the IBM API Connect configuration pipeline finishes successfully you should see SUCCESSFUL messages at the bottom of the logs as well as a green check mark on the task the Pipeline Run has executed on the left hand side: If you switch back to the Details tab of the Pipeline Run , you should also see green check marks both at the Pipeline Run name at the top and below it in the task that has been executed by this Pipeline Run that is displayed on the Pipeline Run Details section: And in the main Pipelines section of your OpenShift web console you should also see the last Pipeline Run for the IBM API Connect config Pipeline in green: If you go to your IBM API Connect API Manager and check your default Sandbox catalog, you should now see your just published Cloud Provider product. If you click on the menu icon at the right hand side of your Cloud Provider product, a drop down menu should display. On that menu, click on Manage APIs : You should see the Cloud Provider API that the dummy application running on your IBM API Connect Gateway and Analytics clusters exposes:","title":"Run the IBM API Connect Configuration pipeline"},{"location":"guides/cp4i/apic/multi-cluster/publish-pipeline/#check-your-api","text":"You have finally set up all the pieces you needed to: Deploy IBM API Connect in a multi-cluster scenario where you will use the IBM API Connect Gateway components to securely expose your applications APIs, as well as some other automation tools (such as the Red Hat OpenShift Pipeline operator) and an application for testing the whole scenario. You have done so following GitOps methodology as you would want to have for production environments. You have leveraged IBM API Connect Cloud Manager Administration APIs and Red Hat OpenShift Pipelines to build automation around configuring your IBM API Connect multi-cluster instance with all the components you have created such profile with. You have leveraged IBM API Connect API Manager APIs and Red Hat OpenShift Pipelines to build automation to manage your applications APIs and the products these will belong to from their definitions stored in GitHub. As a result, you are now all set to take that last step which is to test if you can access your application's API through the IBM API Connect Gateway. To do so, go to your IBM API Connect API Manager, click on Manage catalogs , then click on the default Sandbox catalog and click on the Catalog settings tab. Then, click on the Gateway services section on the left hand side to find out your IBM API Connect Gateway services and their URLs: Use the URL of your Gateway services to point your browser to http://<your_gateway_url>/cloud-provider/cloud and see if you can access the dummy application running on those IBM API Connect Gateway and Analytics clusters whose API should give you back which IBM API Connect Gateway your request has been made from: You have tested that your dummy application's API can be accessed through the IBM API Connect Gateway service that fronts it thanks to having successfully published and exposed such API on your successfully deployed and configured IBM API Connect multi-cluster instance. Congratulations! You have successfully completed the in-depth tutorial for IBM API Connect Multi-Cluster use case. We strongly recommend you check out the other in-depth tutorials from the left hand side menu of this Cloud Pak Production Deployment Guides. Thank you.","title":"Check your API"},{"location":"guides/cp4i/apic/overview/overview/","text":"API Connect \u00b6 Overview \u00b6 IBM Cloud Pak for Integration (CP4I) deploys and manages instances of Integration Services running on OpenShift Kubernetes infrastructure. CP4I high level architecture is shown below Individual CP4I capabilities such as API Connect are deployed individually as needed to satisfy each integration use-case. All components of CP4I can be combined in any way as required and are deployed 100% as containers. Each capability is deployed and managed by a corresponding operator. CP4I also provides an optional Platform Navigator, an over-arching Management UI layer that provides a common UI experience for various installed integration capabilities. Platform Navigator is not required to deploy individual CP4I capabilities, as each integration capability can be deployed independently leveraging its cloud native deployment operators. In addition, CP4I includes a set of foundational services, previously known as Common Services. Foundational services scope has been rapidly evolving. For example, it used to be a vehicle to provide out-of-the-box monitoring and logging capabilities for Cloud Pak components. However, as as OpenShift itself has started to provide facilities for workload monitoring, these functions of Foundational services have been deprecated. Currently, foundational services mostly are focused on identity and access management for Platform Navigator UI, single sign-on experience for UI, license metering CP4I integration capabilities are shown as side by side as there is little direct dependency of CP4I integration capabilities on foundational services, but in typical CP4I installation foundational services are automatically installed if CP4I license is used, to fully support Cloud Pak experience. In this guide, we will provide IBM point of view, best practices and recommendations for how a production ready IBM API Connect deployment should be, focusing on the use of GitOps as the core methodology for deploying and managing IBM capabilities such as IBM API Connect in production. The guide is structured as a tutorial, and it is recommended that you follow the topics in order by following the table of contents from top to bottom. Whether you're a beginner or expert in IBM API Connect and Kubernetes, you are recommended to complete the guide in order. There is an intuitive Table of Contents, and Next and Previous links at the bottom of each page to help you navigate. For other more theoretical and complex IBM API Connect production readiness topics such as high availability, disaster recovery, security, etc you can head over to the Production Reference section of this Cloud Pak Production Deployment Guides. At the end of this tutorial, there is a section on how to carry out an IBM API Connect multi-cluster deployment. However, we do encourage to first complete the in-depth IBM API Connect single-cluster deployment.","title":"Overview"},{"location":"guides/cp4i/apic/overview/overview/#api-connect","text":"","title":"API Connect"},{"location":"guides/cp4i/apic/overview/overview/#overview","text":"IBM Cloud Pak for Integration (CP4I) deploys and manages instances of Integration Services running on OpenShift Kubernetes infrastructure. CP4I high level architecture is shown below Individual CP4I capabilities such as API Connect are deployed individually as needed to satisfy each integration use-case. All components of CP4I can be combined in any way as required and are deployed 100% as containers. Each capability is deployed and managed by a corresponding operator. CP4I also provides an optional Platform Navigator, an over-arching Management UI layer that provides a common UI experience for various installed integration capabilities. Platform Navigator is not required to deploy individual CP4I capabilities, as each integration capability can be deployed independently leveraging its cloud native deployment operators. In addition, CP4I includes a set of foundational services, previously known as Common Services. Foundational services scope has been rapidly evolving. For example, it used to be a vehicle to provide out-of-the-box monitoring and logging capabilities for Cloud Pak components. However, as as OpenShift itself has started to provide facilities for workload monitoring, these functions of Foundational services have been deprecated. Currently, foundational services mostly are focused on identity and access management for Platform Navigator UI, single sign-on experience for UI, license metering CP4I integration capabilities are shown as side by side as there is little direct dependency of CP4I integration capabilities on foundational services, but in typical CP4I installation foundational services are automatically installed if CP4I license is used, to fully support Cloud Pak experience. In this guide, we will provide IBM point of view, best practices and recommendations for how a production ready IBM API Connect deployment should be, focusing on the use of GitOps as the core methodology for deploying and managing IBM capabilities such as IBM API Connect in production. The guide is structured as a tutorial, and it is recommended that you follow the topics in order by following the table of contents from top to bottom. Whether you're a beginner or expert in IBM API Connect and Kubernetes, you are recommended to complete the guide in order. There is an intuitive Table of Contents, and Next and Previous links at the bottom of each page to help you navigate. For other more theoretical and complex IBM API Connect production readiness topics such as high availability, disaster recovery, security, etc you can head over to the Production Reference section of this Cloud Pak Production Deployment Guides. At the end of this tutorial, there is a section on how to carry out an IBM API Connect multi-cluster deployment. However, we do encourage to first complete the in-depth IBM API Connect single-cluster deployment.","title":"Overview"},{"location":"guides/cp4i/apic/performance/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/apic/performance/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/apic/publish/ibm-cloud/","text":"IBM API Connect publish APIs pipeline on IBM Cloud \u00b6 Overview \u00b6 You have reached the last section of this IBM API Connect Cluster in-depth tutorial. By now, you should have a working IBM API Connect Cluster instance on IBM Cloud that sits on the Red Hat OpenShift Kubernetes Service. However, there is one last step to be taken. If you remember in the first place, the goal of deploying IBM API Connect was to be able to securely expose your application APIs through the IBM API Connect Gateway component, be able to gather analytics on those API requests through the IBM API Connect Analytics component and have all your application APIs published, socialized and managed from a central solution that is IBM API Connect in itself. And, of course, you would want to have this process as much automated as possible which is what the recommendation for a production environment is. And remember that this tutorial, and the Cloud Pak Production Deployment Guides in fact, is all about best practices and recommendations as far as how to deploy and manage IBM Software in production on Red Hat OpenShift. The IBM API Connect citizens you would want to work with in this section are products and APIs. Products are a logical grouping of APIs and Plans. The Catalog is where API Publishing and Consumption is handled and a Catalog has a one to one relationship with a Portal Site. An API Product is staged to a Catalog, published to the Portal site and then API consumers can begin subscribing and making API calls. Catalogs provide isolated API run-time environments within a Provider Organization. If you remember from the previous Configure APIC section where you ran the IBM API Connect Configuration OpenShift Pipeline to get your recently deployed IBM API Connect Cluster instance configured, you got a Provider Organization created and a default Sandbox catalog created in it. As a result, you should be ready to publish APIs into products and publish those products into your catalog, which would make those application APIs accessible through your IBM API Connect Gateway component. For more information about IBM API Connect terminology and logical architecture, we strongly recommend you read the IBM API Connect v10.x Deployment WhitePaper written by Chris Phillips. In this section then, you will be looking at the second of the OpenShift Pipelines your multi-tenancy-gitops repository created for you. This IBM API Connect Publish Products and APIs OpenShift Pipeline automates the process for getting products and APIs, which are stored as code in GitHub, and publish those to your IBM API Connect Cluster instance. Hence, if any change is delivered to those product and APIs definitions in GitHub, the IBM API Connect Publish Products and APIs OpenShift Pipeline will make sure those changes are applied to your IBM API Connect Cluster instance providing a sort of GitOps approach to managing your products and APIs. More specifically, your IBM API Connect Publish Products and APIs OpenShift Pipeline will: Download and process product definitions. Download corresponding API definitions for the products. Publish products and APIs to catalog. OpenShift pipeline \u00b6 The OpenShift Pipeline just introduced above has been created in your Red Hat OpenShift custer. If you go to Pipelines --> Pipelines on the left hand side menu of your Red Hat OpenShift web console under the ci Project, you will see two OpenShift pipelines have been created as a result of what you had specified in your GitOps repositories: Info The pipeline you are interested in for getting your instance of IBM API Connect configured is called apic-publish-products-apis-pipeline Important Make sure you have read that previous Configure APIC section to better understand where this pipeline comes from within your GitOps repositories, how it is created and the GitHub repositories behind it. More importantly, make sure you have forked the pipelines repositories into your own GitHub organization and created your GitHub Personal Access Token as explained in the OpenShift pipeline subsection of that previous section. Products and APIs definitions \u00b6 The GitHub repository that will store those products and APIs you want your IBM API Connect Publish Products and APIs OpenShift Pipeline to work with is https://github.com/cloud-native-toolkit/apic-products-apis-yaml , which you should have already forked into your GitHub organization. If you inspect that repository under the Demo folder, you will see the following APIs and products you would want to get published through your IBM API Connect Publish Products and APIs OpenShift Pipeline: apic-products-apis-yaml/Demo \u251c\u2500\u2500 APIs \u2502 \u251c\u2500\u2500 account-consent-open-banking-standard_1.0.0.yaml \u2502 \u251c\u2500\u2500 corporate-multi-payment-online_1.0.0.yaml \u2502 \u251c\u2500\u2500 retail-single-payment-open-banking_1.0.0.yaml \u2502 \u251c\u2500\u2500 retrieve-account-details-open-banking-standard_1.0.0.yaml \u2502 \u251c\u2500\u2500 retrieve-balance-details-open-banking-standard_1.0.0.yaml \u2502 \u2514\u2500\u2500 retrieve-transaction-details-open-banking-standard_1.0.0.yaml \u2514\u2500\u2500 Products \u251c\u2500\u2500 accounts-open-banking-standard_1.0.0.yaml \u251c\u2500\u2500 corp-multi-payment-online-open-banking-standard_1.0.0.yaml \u2514\u2500\u2500 retail-single-payment-open-banking-standard_1.0.0.yaml You can inspect those files for better understanding if you would like to. Run the IBM API Connect Configuration pipeline \u00b6 Before you execute your IBM API Connect Publish Products and APIs OpenShift Pipeline, let's first have a look at the Sandbox default catalog that was created when the IBM API Connect Configuration OpenShift Pipeline from the previous section created your Provider Organization is actually empty. You can check your catalog from your IBM API Connect API Manager: Tip You can open your IBM API Connect API Manager through your Red Hat OpenShift web console. Go to Networking --> Routes under the tools project and click on the Location value for the apic-clust-xxxxx-api-manager route. Then, use the credentials you specified in your provider organization configuration to log in. Click on the apic-publish-products-apis-pipeline . Then, click on Start on the Actions drop down menu at the top right corner. On the Start Pipeline OpenShift Pipeline configuration menu that pops up, make sure you provide the appropriate values for the following pipeline parameters: git-apic-pipeline-git-url which is the GitHub url where the pipeline scripts are stored. These are the scripts the pipeline will execute. This is the GitHub repository your forked into your GitHub organization in the previous Configure APIC section. git-apic-yaml-url which is the GitHub url where the products and APIs definitions you want the pipeline to work with are stored. Once again, you should have forked this repository into your GitHub organization in the previous Configure APIC section. Click Start . This will create a Pipeline Run object that represents this specific execution of the pipeline. You can follow along the execution of this OpenShift Pipeline Run by clicking the Logs tab at the top bar. If this execution of the IBM API Connect configuration pipeline finishes successfully you should see SUCCESS messages at the bottom of the logs as well as a green check mark on the task the Pipeline Run has executed on the left hand side: Info Do not worry about seeing \"Test result\": 404 messages since this is expected. We are publishing APIs that do not point to any running application. If you switch back to the Details tab of the Pipeline Run , you should also see green check marks both at the Pipeline Run name at the top and below it in the task that has been executed by this Pipeline Run that is displayed on the Pipeline Run Details section: And in the main Pipelines section of your OpenShift web console you should also see the last Pipeline Run for the IBM API Connect config Pipeline in green: If you go to your IBM API Connect API Manager and check your default Sandbox catalog, you should now see your just published Cloud Provider product. If you click on the menu icon at the right hand side of any of your products, a drop down menu should display. On that menu, click on Manage APIs : You should see the different APIs that have been configured to be part of that product that your IBM API Connect Gateway component will expose: Congratulations! You have successfully completed the in-depth tutorial for IBM API Connect Cluster use case. We strongly recommend you check out the other in-depth tutorials from the left hand side menu of this Cloud Pak Production Deployment Guides, specially the IBM API Connect Multi-Cluster use case. Thank you.","title":"IBM Cloud"},{"location":"guides/cp4i/apic/publish/ibm-cloud/#ibm-api-connect-publish-apis-pipeline-on-ibm-cloud","text":"","title":"IBM API Connect publish APIs pipeline on IBM Cloud"},{"location":"guides/cp4i/apic/publish/ibm-cloud/#overview","text":"You have reached the last section of this IBM API Connect Cluster in-depth tutorial. By now, you should have a working IBM API Connect Cluster instance on IBM Cloud that sits on the Red Hat OpenShift Kubernetes Service. However, there is one last step to be taken. If you remember in the first place, the goal of deploying IBM API Connect was to be able to securely expose your application APIs through the IBM API Connect Gateway component, be able to gather analytics on those API requests through the IBM API Connect Analytics component and have all your application APIs published, socialized and managed from a central solution that is IBM API Connect in itself. And, of course, you would want to have this process as much automated as possible which is what the recommendation for a production environment is. And remember that this tutorial, and the Cloud Pak Production Deployment Guides in fact, is all about best practices and recommendations as far as how to deploy and manage IBM Software in production on Red Hat OpenShift. The IBM API Connect citizens you would want to work with in this section are products and APIs. Products are a logical grouping of APIs and Plans. The Catalog is where API Publishing and Consumption is handled and a Catalog has a one to one relationship with a Portal Site. An API Product is staged to a Catalog, published to the Portal site and then API consumers can begin subscribing and making API calls. Catalogs provide isolated API run-time environments within a Provider Organization. If you remember from the previous Configure APIC section where you ran the IBM API Connect Configuration OpenShift Pipeline to get your recently deployed IBM API Connect Cluster instance configured, you got a Provider Organization created and a default Sandbox catalog created in it. As a result, you should be ready to publish APIs into products and publish those products into your catalog, which would make those application APIs accessible through your IBM API Connect Gateway component. For more information about IBM API Connect terminology and logical architecture, we strongly recommend you read the IBM API Connect v10.x Deployment WhitePaper written by Chris Phillips. In this section then, you will be looking at the second of the OpenShift Pipelines your multi-tenancy-gitops repository created for you. This IBM API Connect Publish Products and APIs OpenShift Pipeline automates the process for getting products and APIs, which are stored as code in GitHub, and publish those to your IBM API Connect Cluster instance. Hence, if any change is delivered to those product and APIs definitions in GitHub, the IBM API Connect Publish Products and APIs OpenShift Pipeline will make sure those changes are applied to your IBM API Connect Cluster instance providing a sort of GitOps approach to managing your products and APIs. More specifically, your IBM API Connect Publish Products and APIs OpenShift Pipeline will: Download and process product definitions. Download corresponding API definitions for the products. Publish products and APIs to catalog.","title":"Overview"},{"location":"guides/cp4i/apic/publish/ibm-cloud/#openshift-pipeline","text":"The OpenShift Pipeline just introduced above has been created in your Red Hat OpenShift custer. If you go to Pipelines --> Pipelines on the left hand side menu of your Red Hat OpenShift web console under the ci Project, you will see two OpenShift pipelines have been created as a result of what you had specified in your GitOps repositories: Info The pipeline you are interested in for getting your instance of IBM API Connect configured is called apic-publish-products-apis-pipeline Important Make sure you have read that previous Configure APIC section to better understand where this pipeline comes from within your GitOps repositories, how it is created and the GitHub repositories behind it. More importantly, make sure you have forked the pipelines repositories into your own GitHub organization and created your GitHub Personal Access Token as explained in the OpenShift pipeline subsection of that previous section.","title":"OpenShift pipeline"},{"location":"guides/cp4i/apic/publish/ibm-cloud/#products-and-apis-definitions","text":"The GitHub repository that will store those products and APIs you want your IBM API Connect Publish Products and APIs OpenShift Pipeline to work with is https://github.com/cloud-native-toolkit/apic-products-apis-yaml , which you should have already forked into your GitHub organization. If you inspect that repository under the Demo folder, you will see the following APIs and products you would want to get published through your IBM API Connect Publish Products and APIs OpenShift Pipeline: apic-products-apis-yaml/Demo \u251c\u2500\u2500 APIs \u2502 \u251c\u2500\u2500 account-consent-open-banking-standard_1.0.0.yaml \u2502 \u251c\u2500\u2500 corporate-multi-payment-online_1.0.0.yaml \u2502 \u251c\u2500\u2500 retail-single-payment-open-banking_1.0.0.yaml \u2502 \u251c\u2500\u2500 retrieve-account-details-open-banking-standard_1.0.0.yaml \u2502 \u251c\u2500\u2500 retrieve-balance-details-open-banking-standard_1.0.0.yaml \u2502 \u2514\u2500\u2500 retrieve-transaction-details-open-banking-standard_1.0.0.yaml \u2514\u2500\u2500 Products \u251c\u2500\u2500 accounts-open-banking-standard_1.0.0.yaml \u251c\u2500\u2500 corp-multi-payment-online-open-banking-standard_1.0.0.yaml \u2514\u2500\u2500 retail-single-payment-open-banking-standard_1.0.0.yaml You can inspect those files for better understanding if you would like to.","title":"Products and APIs definitions"},{"location":"guides/cp4i/apic/publish/ibm-cloud/#run-the-ibm-api-connect-configuration-pipeline","text":"Before you execute your IBM API Connect Publish Products and APIs OpenShift Pipeline, let's first have a look at the Sandbox default catalog that was created when the IBM API Connect Configuration OpenShift Pipeline from the previous section created your Provider Organization is actually empty. You can check your catalog from your IBM API Connect API Manager: Tip You can open your IBM API Connect API Manager through your Red Hat OpenShift web console. Go to Networking --> Routes under the tools project and click on the Location value for the apic-clust-xxxxx-api-manager route. Then, use the credentials you specified in your provider organization configuration to log in. Click on the apic-publish-products-apis-pipeline . Then, click on Start on the Actions drop down menu at the top right corner. On the Start Pipeline OpenShift Pipeline configuration menu that pops up, make sure you provide the appropriate values for the following pipeline parameters: git-apic-pipeline-git-url which is the GitHub url where the pipeline scripts are stored. These are the scripts the pipeline will execute. This is the GitHub repository your forked into your GitHub organization in the previous Configure APIC section. git-apic-yaml-url which is the GitHub url where the products and APIs definitions you want the pipeline to work with are stored. Once again, you should have forked this repository into your GitHub organization in the previous Configure APIC section. Click Start . This will create a Pipeline Run object that represents this specific execution of the pipeline. You can follow along the execution of this OpenShift Pipeline Run by clicking the Logs tab at the top bar. If this execution of the IBM API Connect configuration pipeline finishes successfully you should see SUCCESS messages at the bottom of the logs as well as a green check mark on the task the Pipeline Run has executed on the left hand side: Info Do not worry about seeing \"Test result\": 404 messages since this is expected. We are publishing APIs that do not point to any running application. If you switch back to the Details tab of the Pipeline Run , you should also see green check marks both at the Pipeline Run name at the top and below it in the task that has been executed by this Pipeline Run that is displayed on the Pipeline Run Details section: And in the main Pipelines section of your OpenShift web console you should also see the last Pipeline Run for the IBM API Connect config Pipeline in green: If you go to your IBM API Connect API Manager and check your default Sandbox catalog, you should now see your just published Cloud Provider product. If you click on the menu icon at the right hand side of any of your products, a drop down menu should display. On that menu, click on Manage APIs : You should see the different APIs that have been configured to be part of that product that your IBM API Connect Gateway component will expose: Congratulations! You have successfully completed the in-depth tutorial for IBM API Connect Cluster use case. We strongly recommend you check out the other in-depth tutorials from the left hand side menu of this Cloud Pak Production Deployment Guides, specially the IBM API Connect Multi-Cluster use case. Thank you.","title":"Run the IBM API Connect Configuration pipeline"},{"location":"guides/cp4i/apic/scalability/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/apic/scalability/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/apic/security/overview/","text":"IBM API Connect Security on OpenShift \u00b6 Abstract This document describes the security aspects and components in making your IBM API Connect deployment and the APIs it manages and exposes secure. Overview \u00b6 The following network diagram example helps to explain the different communication channels that exist within your IBM API Connect channels taking into account the most common topology where each of those components might be deployed to (as far as the various zones, both public and private, in a network). Usage description 1 API request/response \u2013 Users invoking the provided APIs 2 DataPower\u00ae administration \u2013 Internal operators who are managing the Gateway servers 3 API Manager \u2013 Internal business users who are defining and monitoring APIs 4 Cloud Manager \u2013 Internal operators who are administering the Cloud 5 Developer Portal administration \u2013 Internal operators who are managing the Portal servers 6 Gateway servers post traffic to Analytics service 7 Push configuration \u2013 Management servers communicate bi-directionally with Gateway servers 8 Push configuration/webhooks \u2013 Management servers push configuration and webhooks to the Developer Portal 9 Pull configuration/make API calls \u2013 Developer Portal servers pull configuration and call REST APIs 10 Developer Portal \u2013 External developers who are accessing the Developer Portal 11 Push API definition to Management server. Pick up credential for microservice code push 12 Analytics offload 13 Analytics accesses NTP 14 Analytics access DNS 15 Management service queries Analytics service 16 The Portal service invokes an API (GET) on the Analytics service to retrieve data 17 External billing service \u2013 Management cluster connecting to external billing service (when configured for billing) 18 Developer Portal cluster must be able to access its own site endpoints You can read more about the IBM API Connect endpoints here as well as the firewall requirements here . Certificates \u00b6 IBM API Connect allows you to provide existing custom certificates that you already own, for example DigiCert certificates, or generate new custom certificates using a package such as the Kubernetes Cert Manager , which gets installed as a component of the IBM Cloud Pak Foundational Services, off a Certificate Authority that you provide. Here is a list of the certificates required for each of the IBM API Connect subsystems (as well as the internal certificates used within those subsystems) to interact with each other on a secure manner using mutual TLS (mTLS). Important It is important that you understand, for each of the IBM API Connect subsystems, what endpoints and their respective certificates must be signed by the same Certificate Authority as their counterpart endpoint certificate for the IBM API Connect subsystems they want to establish secure mTLS communication with. Basically, if you want to establish secure mTLS communication between two IBM API Connect subsystems (or systems in general), these subsystems endpoints' certificates need to be signed by the same Certificate Authority (or a public Certificate Authority) so that the signature for the counterpart endpoints' certificates can be verified at both sides of the communication channel. More detail on mTLS here . Here is the official IBM API Connect Certificates in a Kubernetes environment documentation which explains how to configure the IBM API Connect subsystems to either use existing custom certificates that the client already own or generate new custom certificates, either using the Kubernetes Cert Manager (whether you provide an existing custom Certificate Authority that the client already own or creating a new self-signed Certificate Authority) or letting the IBM API Connect Operator create those certificates. Tip For deploying IBM API Connect on OpenShift in production it is highly recommended that either the client provides their own existing custom certificates or they provide their own existing Certificate Authority and let the Kubernetes Cert Manager to manage the required certificates by the different IBM API Connect subsystems. Managing authentication and security \u00b6 Secure Cloud Manager and API Manager as well as your Catalogs with a user registry. Secure your APIs with OAuth. Create TLS profiles to ensure that information you share among web servers will not be stolen or tampered with. As Cloud Administrator or Topology Administrator (or with a custom role that contains the Settings:Manage permission), you can configure the following authentication and security mechanisms: User registries to authenticate users of Cloud Manager and API Manager and of your Catalogs and APIs. OAuth providers to provide protection for APIs. TLS profiles to secure transmission of data through the Gateway to external web sites and among web servers. Find the official IBM API Connect documentation for managing authentication and security here . Important API Connect includes several default TLS profiles to help you get started working with the application. The default profiles should not be used in a production environment. It is important to create your own profiles to ensure a secure network. Also, find more information about the different IBM API Connect user roles for the Cloud Manager, API Manager and Developer Portal here and how to administer members and roles here .","title":"Security"},{"location":"guides/cp4i/apic/security/overview/#ibm-api-connect-security-on-openshift","text":"Abstract This document describes the security aspects and components in making your IBM API Connect deployment and the APIs it manages and exposes secure.","title":"IBM API Connect Security on OpenShift"},{"location":"guides/cp4i/apic/security/overview/#overview","text":"The following network diagram example helps to explain the different communication channels that exist within your IBM API Connect channels taking into account the most common topology where each of those components might be deployed to (as far as the various zones, both public and private, in a network). Usage description 1 API request/response \u2013 Users invoking the provided APIs 2 DataPower\u00ae administration \u2013 Internal operators who are managing the Gateway servers 3 API Manager \u2013 Internal business users who are defining and monitoring APIs 4 Cloud Manager \u2013 Internal operators who are administering the Cloud 5 Developer Portal administration \u2013 Internal operators who are managing the Portal servers 6 Gateway servers post traffic to Analytics service 7 Push configuration \u2013 Management servers communicate bi-directionally with Gateway servers 8 Push configuration/webhooks \u2013 Management servers push configuration and webhooks to the Developer Portal 9 Pull configuration/make API calls \u2013 Developer Portal servers pull configuration and call REST APIs 10 Developer Portal \u2013 External developers who are accessing the Developer Portal 11 Push API definition to Management server. Pick up credential for microservice code push 12 Analytics offload 13 Analytics accesses NTP 14 Analytics access DNS 15 Management service queries Analytics service 16 The Portal service invokes an API (GET) on the Analytics service to retrieve data 17 External billing service \u2013 Management cluster connecting to external billing service (when configured for billing) 18 Developer Portal cluster must be able to access its own site endpoints You can read more about the IBM API Connect endpoints here as well as the firewall requirements here .","title":"Overview"},{"location":"guides/cp4i/apic/security/overview/#certificates","text":"IBM API Connect allows you to provide existing custom certificates that you already own, for example DigiCert certificates, or generate new custom certificates using a package such as the Kubernetes Cert Manager , which gets installed as a component of the IBM Cloud Pak Foundational Services, off a Certificate Authority that you provide. Here is a list of the certificates required for each of the IBM API Connect subsystems (as well as the internal certificates used within those subsystems) to interact with each other on a secure manner using mutual TLS (mTLS). Important It is important that you understand, for each of the IBM API Connect subsystems, what endpoints and their respective certificates must be signed by the same Certificate Authority as their counterpart endpoint certificate for the IBM API Connect subsystems they want to establish secure mTLS communication with. Basically, if you want to establish secure mTLS communication between two IBM API Connect subsystems (or systems in general), these subsystems endpoints' certificates need to be signed by the same Certificate Authority (or a public Certificate Authority) so that the signature for the counterpart endpoints' certificates can be verified at both sides of the communication channel. More detail on mTLS here . Here is the official IBM API Connect Certificates in a Kubernetes environment documentation which explains how to configure the IBM API Connect subsystems to either use existing custom certificates that the client already own or generate new custom certificates, either using the Kubernetes Cert Manager (whether you provide an existing custom Certificate Authority that the client already own or creating a new self-signed Certificate Authority) or letting the IBM API Connect Operator create those certificates. Tip For deploying IBM API Connect on OpenShift in production it is highly recommended that either the client provides their own existing custom certificates or they provide their own existing Certificate Authority and let the Kubernetes Cert Manager to manage the required certificates by the different IBM API Connect subsystems.","title":"Certificates"},{"location":"guides/cp4i/apic/security/overview/#managing-authentication-and-security","text":"Secure Cloud Manager and API Manager as well as your Catalogs with a user registry. Secure your APIs with OAuth. Create TLS profiles to ensure that information you share among web servers will not be stolen or tampered with. As Cloud Administrator or Topology Administrator (or with a custom role that contains the Settings:Manage permission), you can configure the following authentication and security mechanisms: User registries to authenticate users of Cloud Manager and API Manager and of your Catalogs and APIs. OAuth providers to provide protection for APIs. TLS profiles to secure transmission of data through the Gateway to external web sites and among web servers. Find the official IBM API Connect documentation for managing authentication and security here . Important API Connect includes several default TLS profiles to help you get started working with the application. The default profiles should not be used in a production environment. It is important to create your own profiles to ensure a secure network. Also, find more information about the different IBM API Connect user roles for the Cloud Manager, API Manager and Developer Portal here and how to administer members and roles here .","title":"Managing authentication and security"},{"location":"guides/cp4i/mq/ad/","text":"< Back to main page Decision Index \u00b6 Subject Area ID Topic Description Build AD01 Baking vs Frying Options to manage MQ configurations Infra AD02 Cluster Design Considerations to segregate environments and applications using clusters and/or namespaces","title":"Decision Index"},{"location":"guides/cp4i/mq/ad/#decision-index","text":"Subject Area ID Topic Description Build AD01 Baking vs Frying Options to manage MQ configurations Infra AD02 Cluster Design Considerations to segregate environments and applications using clusters and/or namespaces","title":"Decision Index"},{"location":"guides/cp4i/mq/ad/ad01-baking-vs-frying/","text":"AD01 - Baking vs Frying \u00b6 < Back to architecture decision registry MQ - Baking vs Frying \u00b6 Subject Area: Build Issue or Problem Statement \u00b6 Baking and Frying are terms used to describe the process of deploying a software component and it's configuration in a controlled fashion. In the context of MQ Containers, we define the terms as: Baking: Extension of the official MQ image for the purpose of embedding configuration or adding third party binaries. Baking happens at build time. Frying: Addition of configuration at deployment time using a ConfigMap . It is important to denote the fact that an MQ server is a stateful component. Once the initial instance of a queue manager is deployed the MQSC configuration is processed and stored in the underlying persistent volume. For subsequent deployment, the Queue Manager will already exists and so any modification to the MQSC instruction should take it into considerations. Tests should cover this scenario and ensure that the configuration achieves the desired outcome. The remainder of this architecture decision focusses primarily on the MQ configuration and ignore the scenario where binaries need to be added (see Givens & Assumptions ). Should organization create custom images with MQ config or should that configuration be injected at deployment time? Givens & Assumptions \u00b6 Given - Where required third-party binaries are always baked into a custom image. Given - MQ tests must verify that the queue manager can upgraded from the previous version. Assumption - Deployment is automated and controlled using a gitops approach. Motivation \u00b6 Either option explored in this architecture decision are equally technically viable. However, the choice made here has a direct impact on: * the design of the CI/CD pipeline, * the testing strategy, * local development testing. As such, it is critical that organization make a conscious choice and understand it's implication. Alternatives \u00b6 1. Bake the MQ configuration \u00b6 In this option, all MQ configuration is added to a custom image during the build phase. In order to enable the pipeline to properly test the upgrade from the previous version, a proper SemVer standard must be adopted. The pipeline can then deploy the current version and then upgrade to the new one, validating that the configuration remains valid. Considerations : + Queue Manager can be tested with a basic docker engine. + No need to pull additional GitHub branch/releases. + Simple to deploy the Queue Manager on a local dev environment. + Image version makes it easier to assess if multiple instance share the same configuration. - Slower CI process due to the image creation. - Image registry contains multiple image versions. - MQ Upgrades require a rebuilding of every custom images. - Risk of version specified by MQ CR being out of sync with custom image. 2. Fry the complete MQ configuration \u00b6 In this option, there is no need to build custom images. The build phase, takes the MQSC configuration and adds it to a ConfigMap which gets pushed to the GitOps repository. During the continuous integration, the system needs to have access to the current and new Queue Manager configuration. The CI pipeline can access this information either through GitHub branch/release pull or through an existing environment. It then pulls the current ConfigMap, deploys a Queue Manager instance and then apply the new ConfigMap, validating that the configuration remains valid. Considerations : + Faster CI process since no need to push images every time. + MQ Upgrades can be fully controlled by the Operators. - Cannot be tested locally using Docker without some manipulation. - Added complexity due to the need to pull the current version in addition to the new configuration. - Local dev testing requires access to an OpenShift cluster or a local deployment of CRC. 3. Bake the common MQ configuration, fry the rest \u00b6 This is an hybrid approach combining the two previous options. In this scenario, the common configuration shared across all queue manager is baked into an Enterprise image. From that point, every Queue Manager deployed is based off the Enterprise image and all configuration unique to a queue manager is fried. Considerations : + Ensure consistency of common config across all queue managers + Provides a control point to manage MQ versions the Enterprise supports Includes all the pro's and con's from the Option 2. Justification \u00b6 N/A Implications \u00b6 N/A Derived requirements N/A Related decisions N/A","title":"AD01 - Baking vs Frying"},{"location":"guides/cp4i/mq/ad/ad01-baking-vs-frying/#ad01-baking-vs-frying","text":"< Back to architecture decision registry","title":"AD01 - Baking vs Frying"},{"location":"guides/cp4i/mq/ad/ad01-baking-vs-frying/#mq-baking-vs-frying","text":"Subject Area: Build","title":"MQ - Baking vs Frying"},{"location":"guides/cp4i/mq/ad/ad01-baking-vs-frying/#issue-or-problem-statement","text":"Baking and Frying are terms used to describe the process of deploying a software component and it's configuration in a controlled fashion. In the context of MQ Containers, we define the terms as: Baking: Extension of the official MQ image for the purpose of embedding configuration or adding third party binaries. Baking happens at build time. Frying: Addition of configuration at deployment time using a ConfigMap . It is important to denote the fact that an MQ server is a stateful component. Once the initial instance of a queue manager is deployed the MQSC configuration is processed and stored in the underlying persistent volume. For subsequent deployment, the Queue Manager will already exists and so any modification to the MQSC instruction should take it into considerations. Tests should cover this scenario and ensure that the configuration achieves the desired outcome. The remainder of this architecture decision focusses primarily on the MQ configuration and ignore the scenario where binaries need to be added (see Givens & Assumptions ). Should organization create custom images with MQ config or should that configuration be injected at deployment time?","title":"Issue or Problem Statement"},{"location":"guides/cp4i/mq/ad/ad01-baking-vs-frying/#givens-assumptions","text":"Given - Where required third-party binaries are always baked into a custom image. Given - MQ tests must verify that the queue manager can upgraded from the previous version. Assumption - Deployment is automated and controlled using a gitops approach.","title":"Givens &amp; Assumptions"},{"location":"guides/cp4i/mq/ad/ad01-baking-vs-frying/#motivation","text":"Either option explored in this architecture decision are equally technically viable. However, the choice made here has a direct impact on: * the design of the CI/CD pipeline, * the testing strategy, * local development testing. As such, it is critical that organization make a conscious choice and understand it's implication.","title":"Motivation"},{"location":"guides/cp4i/mq/ad/ad01-baking-vs-frying/#alternatives","text":"","title":"Alternatives"},{"location":"guides/cp4i/mq/ad/ad01-baking-vs-frying/#justification","text":"N/A","title":"Justification"},{"location":"guides/cp4i/mq/ad/ad01-baking-vs-frying/#implications","text":"N/A Derived requirements N/A Related decisions N/A","title":"Implications"},{"location":"guides/cp4i/mq/ad/ad02-clusters-design/","text":"AD02 - Clusters design \u00b6 < Back to architecture decision registry Clusters design \u00b6 Subject Area: Infra Issue or Problem Statement \u00b6 This architecture decision focuses on the boundaries of production and non-production environments. In traditional data centers, resources such as compute, storage, and network are partitioned into different hardware or zone to minimize the chance that workloads impact each other. This approach has also been useful to enforce segregation of duties between various stakeholders. In the context of OpenShift, partitioning can happen at the level of clusters or projects and namespace. A project can be assigned a quota on resources allocated and furthermore, the RBAC model can ensure that only a specific set of users can access resources contained within the project. For an on-premise data center, an OpenShift cluster offers a great opportunity to increase workload density. Workload density means running more workloads on a same set of compute. This is achieved thanks to the workload scheduler which will allocate workloads where capacity exists and where it meets the requirement stated by the deployment descriptor of your application. By avoiding the proliferation of master nodes (via discrete clusters), more nodes can be dedicated to run business workloads, helping reduce operational costs and improve the density. However, from a cloud provider perspective, this argument is not always true. Since some cloud provider do not charge for their master nodes, the cost and density arguments goes away. Finally, in planning the cluster and project strategy, there is the considerations of cluster level operations like OpenShift upgrades. Consolidating workloads on a single cluster, implicitly means that cluster upgrades become a riskier operation. In that context, what is the better option when it comes to managing and partitioning Dev, QA, Staging, Production environment? Givens & Assumptions \u00b6 Given The decision focusses only on the cloud-native application. Given Organization may decide to specialize clusters for a specific function (ie. Power System with GPU for AI). The considerations for this approach falls outside the scope of this decision. Given Segregation of duty is achieved in a similar fashion no matter which option is selected: User are assigned roles that allows them to access a subset of namespaces within the cluster. Assumption Additional sandbox clusters can be temporarily provision to assess new OpenShift versions. Motivation \u00b6 This architecture decision is critical as it can have a significant impact on costs, risks, operational complexity. It also has direct implication on the deployment process. Alternatives \u00b6 NOTE 1: The alternatives listed below could be combined together. For example, it is possible to combine option 3 (pre-prod/prod) with option 4 (cluster per app) to achieve a model with a pre-prod and a production cluster per application. Similar considerations would apply to the presented alternatives, so they are not included here. NOTE 2: In the diagrams across the various alternatives, the \"TOOLS\" refer to the Cloud Native Toolkit tools. 1. Single cluster for all environments \u00b6 Because all environments and applications are hosted on the same cluster, we rely on namespace naming convention to include a prefix or suffix that indicates the type of environment they host. Each application and services may have more then one namespace, but they should all have the prefix/suffix attached to the namespace. The implication of this standardization is that any reference from a component in one namespace to a different namespace will need to be configurable. This option leads to higher workload density since all applications and environment are hosted on the same cluster, however you should be mindful of the tested capacity limit . From a cluster upgrade perspective, it does require temporary sandbox to test new versions and organization adopting this approach need to take a \"fix-forward\" approach to their cluster management, since once the patch is applied, it can be impossible to return to previous state. NOTE: While this option is included here from a completeness perspective. We do not advocate such a strategy given the inherent risks. Considerations : + Great for workload density. + Simplify the cluster management day-to-day tasks. - Significant operational risks during cluster maintenance. - Cluster outages takes out all environment and applications. - Production workload segregation needs to be carefully planned. - Cluster sizing must take into account tested limits. 2. Single cluster for non-production environments, a separate one for production \u00b6 In this option, production workloads are deployed in a cluster dedicated to this environment. Pre-production workloads share a single cluster and environments are segregated by projects/namespaces. This option is in effect an extension of the first one but with the added production workload being segregated. As such, the namespace standardization still applies. From a cluster upgrade perspective, the infrastructure team may initially test a subset of workloads in a sandbox like environment and once ready have a single environment (the non-production cluster) to perform the final testing, assess the stability of the new version before proceeding with the production cluster. Considerations : + Good for workload density. + Provide ability to completely segregate production resources from other environments. + Keeps the cluster life-cycle management overhead to a minimum. + Minimizes the amount of resources allocated to control planes. - Requires that lookup between namespace be configurable. - Only a single environment to assess the impact of a new OpenShift version on existing workloads. 3. One cluster per environment \u00b6 As the name says, each environment are supported by a separate cluster. Within each environment, application teams deploy their application and services within the namespaces dictated by their solution design. Important to emphasize that the cluster represents the environment and that it can contain multiple independent applications and services. For a cluster upgrade perspective, each environment needs to be upgraded separately. As upgrade progress, the staging environment provides the replica of production, ensuring that until production is ready to be upgraded, at least a single non-production environment remains available for testing. The implication of that option is the associated additional resources that are dedicated to the control plane of each clusters and the associated operational costs of maintaining multiple clusters across the landscape. Considerations : + Highest degree of segregation between environments. + Provides most flexibility when it comes to cluster upgrades. - Increases costs associated with additional control plane resources. - Increases operations complexity. - Reduces workload density, since environments are not sharing the same resource pool. 4. Cluster per application \u00b6 By separating each cluster by application (or application domain), it enables organization to manage each cluster as an independent unit and cost and operational resources can be measured at the cluster level. Namespace standardization is again required to ensure that the application environments can be properly segregated. From a cluster upgrade perspective, the application faces similar challenges as the first option, meaning that sandbox cluster must be created to assess new version impacts. Considerations : + Highest degree of segregation between applications. + Simplest model to allocate costs to an application. + In situation of outages, blast radius tends to be simpler to identify. - Leads to proliferation of clusters. - Does not favour resource sharing; Many clusters running with low resource usage. - Risk of application clusters falling behind the upgrade life-cycle. - Higher risks of divergence across clusters, leading to operational and security risks. 5. Management and Resources plane clusters \u00b6 This approach separates concerns between a management and a resource plane. We define the two as being: Management plane: run key processes responsible for the applications life-cycle. These includes the CI and CD components. It can also run components like Red Hat Advanced Cluster Management to ensure consistency of configuration and operations. Resource plane: This is the set of clusters that are running the application and services components. This enables organization to track and automate the cluster upgrades via the management plane. Considerations : + Provides opportunity to federate all clusters under one view. + Management plane can drive automation of key activities. + Enforces consistency across clusters by leveraging a single source for all environments. - Requires additional resources to run the management plane. Justification \u00b6 N/A Implications \u00b6 N/A Derived requirements N/A Related decisions N/A","title":"AD02 - Clusters design"},{"location":"guides/cp4i/mq/ad/ad02-clusters-design/#ad02-clusters-design","text":"< Back to architecture decision registry","title":"AD02 - Clusters design"},{"location":"guides/cp4i/mq/ad/ad02-clusters-design/#clusters-design","text":"Subject Area: Infra","title":"Clusters design"},{"location":"guides/cp4i/mq/ad/ad02-clusters-design/#issue-or-problem-statement","text":"This architecture decision focuses on the boundaries of production and non-production environments. In traditional data centers, resources such as compute, storage, and network are partitioned into different hardware or zone to minimize the chance that workloads impact each other. This approach has also been useful to enforce segregation of duties between various stakeholders. In the context of OpenShift, partitioning can happen at the level of clusters or projects and namespace. A project can be assigned a quota on resources allocated and furthermore, the RBAC model can ensure that only a specific set of users can access resources contained within the project. For an on-premise data center, an OpenShift cluster offers a great opportunity to increase workload density. Workload density means running more workloads on a same set of compute. This is achieved thanks to the workload scheduler which will allocate workloads where capacity exists and where it meets the requirement stated by the deployment descriptor of your application. By avoiding the proliferation of master nodes (via discrete clusters), more nodes can be dedicated to run business workloads, helping reduce operational costs and improve the density. However, from a cloud provider perspective, this argument is not always true. Since some cloud provider do not charge for their master nodes, the cost and density arguments goes away. Finally, in planning the cluster and project strategy, there is the considerations of cluster level operations like OpenShift upgrades. Consolidating workloads on a single cluster, implicitly means that cluster upgrades become a riskier operation. In that context, what is the better option when it comes to managing and partitioning Dev, QA, Staging, Production environment?","title":"Issue or Problem Statement"},{"location":"guides/cp4i/mq/ad/ad02-clusters-design/#givens-assumptions","text":"Given The decision focusses only on the cloud-native application. Given Organization may decide to specialize clusters for a specific function (ie. Power System with GPU for AI). The considerations for this approach falls outside the scope of this decision. Given Segregation of duty is achieved in a similar fashion no matter which option is selected: User are assigned roles that allows them to access a subset of namespaces within the cluster. Assumption Additional sandbox clusters can be temporarily provision to assess new OpenShift versions.","title":"Givens &amp; Assumptions"},{"location":"guides/cp4i/mq/ad/ad02-clusters-design/#motivation","text":"This architecture decision is critical as it can have a significant impact on costs, risks, operational complexity. It also has direct implication on the deployment process.","title":"Motivation"},{"location":"guides/cp4i/mq/ad/ad02-clusters-design/#alternatives","text":"NOTE 1: The alternatives listed below could be combined together. For example, it is possible to combine option 3 (pre-prod/prod) with option 4 (cluster per app) to achieve a model with a pre-prod and a production cluster per application. Similar considerations would apply to the presented alternatives, so they are not included here. NOTE 2: In the diagrams across the various alternatives, the \"TOOLS\" refer to the Cloud Native Toolkit tools.","title":"Alternatives"},{"location":"guides/cp4i/mq/ad/ad02-clusters-design/#justification","text":"N/A","title":"Justification"},{"location":"guides/cp4i/mq/ad/ad02-clusters-design/#implications","text":"N/A Derived requirements N/A Related decisions N/A","title":"Implications"},{"location":"guides/cp4i/mq/app-pipeline/topic1/","text":"Application overview \u00b6 Overview \u00b6 Audience : Architects, Application developers, Administrators In this topic, we're going to: Review how an MQ application connects to a queue manager Configure a sample application using a GitHub repository Examine the structure of the sample application An MQ application is a client that consumes messaging services provided by an MQ queue manager server. For example, an application connects to a queue manager to put and get a messages from a queue; or alternatively publish and subscribe to a topic. A client application can be written in a variety of programming languages. The following diagram shows two examples of an application consuming simple messaging services provided by a queue manager. In the first example, the application is local to the queue manager; it is in the same cluster. In the second example the application is remote to the queue manager; it might be in a different cluster, or not in a Kubernetes environment. Our sample application exposes the following REST interface, making it easy to consume by other applications and services both inside and outside the cluster: /api/send-hello-world to put a default Hello world message to a queue /api/recv to get a message from the queue /api/send-json to put a JSON message to a queue /health to check the application is running The /health function allows Kubernetes to check the application is running and healthy and to automatically heal it if any problems occur. We will test each of these REST interfaces in a later topic. The application uses the Spring framework and the IBM MQ Spring Boot Starter to simplify application development and configuration. We use the Spring framework for the development of our REST and MQ interfaces because it simplifies the development of microservice and web based applications often found in a containerized environment such as Kubernetes. Pre-requisites \u00b6 Before attempting this section, make sure you have followed the section in configuring the cluster . Specifically, you must have completed the following tasks: You have installed the git and tree command line tools. You have created a GitHub access token for your application repository's account. You have completed the previous chapter to deploy a Queue Manager instance. Creating the application repository \u00b6 It\u2019s wise to use a new terminal window for this chapter. It will help us switch between the client application repository and GitOps repository as we examine the different steps in the CICD process. Follow these instructions to download the application source code. Navigate to the following sample configuration repository and create a fork: https://github.com/cloud-native-toolkit/mq-spring-app Set up a GitHub environment variable If you've not done it already, set up environment variable $GIT_ORG , with your GitHub organization. We'll use this variable in subsequent commands. Open a new terminal window. Replace <git-org> in the following command: export GIT_ORG = <git-org> Clone the fork to your local machine git clone https://github.com/ $GIT_ORG /mq-spring-app.git Change to the local clone's folder cd mq-spring-app Set up environment variable for Git branch The sample repository has a single master branch. We're going to create a new branch that is initially populated with this branch and work in it. Create a new branch $GIT_BRANCH_SPRING for you using the $GIT_ORG environment variable we just set up: export GIT_BRANCH_SPRING = mq-spring- $GIT_ORG git checkout -b $GIT_BRANCH_SPRING you'll see something like: Switched to a new branch 'mq-spring-prod-ref-guide' Push the changes: git push -u origin $GIT_BRANCH_SPRING Total 0 (delta 0), reused 0 (delta 0) remote: remote: Create a pull request for 'mq-spring-prod-ref-guide' on GitHub by visiting: remote: https://github.com/prod-ref-guide/mq-spring-app/pull/new/mq-spring-prod-ref-guide remote: To https://github.com/prod-ref-guide/mq-spring-app.git * [new branch] mq-spring-prod-ref-guide -> mq-spring-prod-ref-guide Branch 'mq-spring-prod-ref-guide' set up to track remote branch 'mq-spring-prod-ref-guide' from 'origin'. Notice how we've created a new branch $GIT_BRANCH_SPRING based on the master branch. All changes will be made in $GIT_BRANCH_SPRING ; whereas master will remain unchanged. Also note that we use $GIT_BRANCH_SPRING to name the mq-spring-app client application branch in contrast to $GIT_BRANCH for the multi-tenancy-gitops-apps GitOps branch. This helps to prevent us from accidentally promoting a change if we use the wrong terminal window in the tutorial. Review the application source directories tree -d The following diagram shows the directory structure for the MQ application: . \u251c\u2500\u2500 architecture \u251c\u2500\u2500 chart \u2502 \u2514\u2500\u2500 base \u2502 \u2514\u2500\u2500 templates \u251c\u2500\u2500 jmeter \u251c\u2500\u2500 local \u2502 \u251c\u2500\u2500 certs \u2502 \u251c\u2500\u2500 jaeger \u2502 \u251c\u2500\u2500 ldap \u2502 \u2514\u2500\u2500 mq \u251c\u2500\u2500 postman \u2514\u2500\u2500 src \u251c\u2500\u2500 main \u2502 \u251c\u2500\u2500 java \u2502 \u2502 \u2514\u2500\u2500 com \u2502 \u2502 \u2514\u2500\u2500 ibm \u2502 \u2502 \u251c\u2500\u2500 cloud_garage \u2502 \u2502 \u2502 \u2514\u2500\u2500 swagger \u2502 \u2502 \u251c\u2500\u2500 health \u2502 \u2502 \u2514\u2500\u2500 mqclient \u2502 \u2502 \u251c\u2500\u2500 app \u2502 \u2502 \u251c\u2500\u2500 config \u2502 \u2502 \u251c\u2500\u2500 controller \u2502 \u2502 \u251c\u2500\u2500 exceptions \u2502 \u2502 \u251c\u2500\u2500 model \u2502 \u2502 \u2514\u2500\u2500 service \u2502 \u2514\u2500\u2500 resources \u2502 \u2514\u2500\u2500 public \u2514\u2500\u2500 test \u2514\u2500\u2500 java \u2514\u2500\u2500 com \u2514\u2500\u2500 ibm \u251c\u2500\u2500 cloud_garage \u2502 \u2514\u2500\u2500 swagger \u251c\u2500\u2500 health \u2514\u2500\u2500 mqclient \u2514\u2500\u2500 controller Notice the top-level folder structure: architecture contains the application architecture reference diagram. chart contains the helm templates required to deploy the application. jmeter contains sample application testing artifacts to demonstrate application and service availability. local contains the configuration required to run the application locally. postman contains files used by postman to test the application. src/main contains the application source files written using the Spring Boot framework. src/test contains the test code used by the pipeline to test the application. Spend a few minutes browsing these folders; we'll explore them in more detail throughout this chapter. Congratulations! You have successfully cloned and reviewed the application source code. In the next topic, we will build the application using a Tekton pipeline, deploy it using an ArgoCD application, and test it using the curl command.","title":"Sample application"},{"location":"guides/cp4i/mq/app-pipeline/topic1/#application-overview","text":"","title":"Application overview"},{"location":"guides/cp4i/mq/app-pipeline/topic1/#overview","text":"Audience : Architects, Application developers, Administrators In this topic, we're going to: Review how an MQ application connects to a queue manager Configure a sample application using a GitHub repository Examine the structure of the sample application An MQ application is a client that consumes messaging services provided by an MQ queue manager server. For example, an application connects to a queue manager to put and get a messages from a queue; or alternatively publish and subscribe to a topic. A client application can be written in a variety of programming languages. The following diagram shows two examples of an application consuming simple messaging services provided by a queue manager. In the first example, the application is local to the queue manager; it is in the same cluster. In the second example the application is remote to the queue manager; it might be in a different cluster, or not in a Kubernetes environment. Our sample application exposes the following REST interface, making it easy to consume by other applications and services both inside and outside the cluster: /api/send-hello-world to put a default Hello world message to a queue /api/recv to get a message from the queue /api/send-json to put a JSON message to a queue /health to check the application is running The /health function allows Kubernetes to check the application is running and healthy and to automatically heal it if any problems occur. We will test each of these REST interfaces in a later topic. The application uses the Spring framework and the IBM MQ Spring Boot Starter to simplify application development and configuration. We use the Spring framework for the development of our REST and MQ interfaces because it simplifies the development of microservice and web based applications often found in a containerized environment such as Kubernetes.","title":"Overview"},{"location":"guides/cp4i/mq/app-pipeline/topic1/#pre-requisites","text":"Before attempting this section, make sure you have followed the section in configuring the cluster . Specifically, you must have completed the following tasks: You have installed the git and tree command line tools. You have created a GitHub access token for your application repository's account. You have completed the previous chapter to deploy a Queue Manager instance.","title":"Pre-requisites"},{"location":"guides/cp4i/mq/app-pipeline/topic1/#creating-the-application-repository","text":"It\u2019s wise to use a new terminal window for this chapter. It will help us switch between the client application repository and GitOps repository as we examine the different steps in the CICD process. Follow these instructions to download the application source code. Navigate to the following sample configuration repository and create a fork: https://github.com/cloud-native-toolkit/mq-spring-app Set up a GitHub environment variable If you've not done it already, set up environment variable $GIT_ORG , with your GitHub organization. We'll use this variable in subsequent commands. Open a new terminal window. Replace <git-org> in the following command: export GIT_ORG = <git-org> Clone the fork to your local machine git clone https://github.com/ $GIT_ORG /mq-spring-app.git Change to the local clone's folder cd mq-spring-app Set up environment variable for Git branch The sample repository has a single master branch. We're going to create a new branch that is initially populated with this branch and work in it. Create a new branch $GIT_BRANCH_SPRING for you using the $GIT_ORG environment variable we just set up: export GIT_BRANCH_SPRING = mq-spring- $GIT_ORG git checkout -b $GIT_BRANCH_SPRING you'll see something like: Switched to a new branch 'mq-spring-prod-ref-guide' Push the changes: git push -u origin $GIT_BRANCH_SPRING Total 0 (delta 0), reused 0 (delta 0) remote: remote: Create a pull request for 'mq-spring-prod-ref-guide' on GitHub by visiting: remote: https://github.com/prod-ref-guide/mq-spring-app/pull/new/mq-spring-prod-ref-guide remote: To https://github.com/prod-ref-guide/mq-spring-app.git * [new branch] mq-spring-prod-ref-guide -> mq-spring-prod-ref-guide Branch 'mq-spring-prod-ref-guide' set up to track remote branch 'mq-spring-prod-ref-guide' from 'origin'. Notice how we've created a new branch $GIT_BRANCH_SPRING based on the master branch. All changes will be made in $GIT_BRANCH_SPRING ; whereas master will remain unchanged. Also note that we use $GIT_BRANCH_SPRING to name the mq-spring-app client application branch in contrast to $GIT_BRANCH for the multi-tenancy-gitops-apps GitOps branch. This helps to prevent us from accidentally promoting a change if we use the wrong terminal window in the tutorial. Review the application source directories tree -d The following diagram shows the directory structure for the MQ application: . \u251c\u2500\u2500 architecture \u251c\u2500\u2500 chart \u2502 \u2514\u2500\u2500 base \u2502 \u2514\u2500\u2500 templates \u251c\u2500\u2500 jmeter \u251c\u2500\u2500 local \u2502 \u251c\u2500\u2500 certs \u2502 \u251c\u2500\u2500 jaeger \u2502 \u251c\u2500\u2500 ldap \u2502 \u2514\u2500\u2500 mq \u251c\u2500\u2500 postman \u2514\u2500\u2500 src \u251c\u2500\u2500 main \u2502 \u251c\u2500\u2500 java \u2502 \u2502 \u2514\u2500\u2500 com \u2502 \u2502 \u2514\u2500\u2500 ibm \u2502 \u2502 \u251c\u2500\u2500 cloud_garage \u2502 \u2502 \u2502 \u2514\u2500\u2500 swagger \u2502 \u2502 \u251c\u2500\u2500 health \u2502 \u2502 \u2514\u2500\u2500 mqclient \u2502 \u2502 \u251c\u2500\u2500 app \u2502 \u2502 \u251c\u2500\u2500 config \u2502 \u2502 \u251c\u2500\u2500 controller \u2502 \u2502 \u251c\u2500\u2500 exceptions \u2502 \u2502 \u251c\u2500\u2500 model \u2502 \u2502 \u2514\u2500\u2500 service \u2502 \u2514\u2500\u2500 resources \u2502 \u2514\u2500\u2500 public \u2514\u2500\u2500 test \u2514\u2500\u2500 java \u2514\u2500\u2500 com \u2514\u2500\u2500 ibm \u251c\u2500\u2500 cloud_garage \u2502 \u2514\u2500\u2500 swagger \u251c\u2500\u2500 health \u2514\u2500\u2500 mqclient \u2514\u2500\u2500 controller Notice the top-level folder structure: architecture contains the application architecture reference diagram. chart contains the helm templates required to deploy the application. jmeter contains sample application testing artifacts to demonstrate application and service availability. local contains the configuration required to run the application locally. postman contains files used by postman to test the application. src/main contains the application source files written using the Spring Boot framework. src/test contains the test code used by the pipeline to test the application. Spend a few minutes browsing these folders; we'll explore them in more detail throughout this chapter. Congratulations! You have successfully cloned and reviewed the application source code. In the next topic, we will build the application using a Tekton pipeline, deploy it using an ArgoCD application, and test it using the curl command.","title":"Creating the application repository"},{"location":"guides/cp4i/mq/app-pipeline/topic2/","text":"Application pipeline \u00b6 Overview \u00b6 Audience : Application developers, Administrators In this topic, we're going to: Create an MQ application pipeline Examine the pipeline tasks Run the pipeline Review the resultant GitOps application folders Explore how ArgoCD automatically deploys the application Test the application using curl The following diagram illustrates how CICD is achieved using Tekton pipelines and ArgoCD applications. In this topic, we are going to focus on the highlighted application CICD. The application pipeline executes a set of tasks designed to ensure that the application builds successfully, including passing a set of functional and security tests. If every step is successful, the pipeline creates a new application image and GitOps configuration. Subsequently, a dedicated ArgoCD application watches for new MQ application images and configurations, which it deploys to the cluster. Notice the separation of concerns between Tekton and ArgoCD. Tekton's role is to perform continuous integration, ensuring that the best current version of the application is available for deployment. ArgoCD's role is to perform continuous deployment, ensuring that the cluster is always kept up-to-date with the latest built application that has passed all required tests. Pre-requisites \u00b6 In order to deploy the pipelines for the application we require the following prerequisites to be configured and installed: Create the cluster on the cloud platform of your choice. Configure the cluster for GitOps, and install the cluster infrastructure and cloud native services. Complete the steps in the previous topic to clone the application source. Ensure you are logged into your cluster from the command line. Build and test the application \u00b6 Let's now review the application pipeline to build, smoke test and configure the application. The production reference guide includes a number of pre-configured application pipelines for different programming languages and frameworks. These pipelines illustrate best practices for continuous integration. They are not meant to be exhaustive; you can modify them for your own projects. For this section, we return to the terminal window we used in previous chapters for interacting with the GitOps repository rather than the one we\u2019ve just used to clone the mq-spring-app client application source repository. Open a new terminal window for the multi-tenancy-gitops-apps repository if necessary. Review the pre-configured pipeline Open the OpenShift console and navigate to Pipelines > Pipelines . You will want to select the ci Project from the drop-down at the top of the page. Click the mq-spring-app-dev Pipeline to view the client application build and deployment pipeline. You can see the different stages of the pipeline, from building the Java application, unit testing, through to updating the image registry and GitOps repo. You can also view the pipeline from the command line: oc describe Pipeline mq-spring-app-dev -n ci Kickoff a pipeline run From the Actions dropdown menu in the upper-right corner, select Start . Configure the run as follows: Set git-url to your fork of the mq-spring-app repository Set git-revision to master . (Later in the tutorial, we will use a new branch.) Set scan-image: false (temporary fix while issues with UBI are resolved) Click Start and wait! It may take a little while for the pipeline to run. Keep checking until all steps have completed. Re-merging local clone to view Helm chart in GitOps repository The mq-spring-app-dev pipeline run updated the GitOps repository with the Helm chart. This means that our local clone of the GitOps repository is one commit behind GitHub. Before we can push any more changes to the GitOps repository, we must re-merge our local clone with GitHub. Return to the terminal window you're using for the multi-tenancy-gitops-apps GitOps apps repository. (Rather than the terminal window you're using for the mq-spring-app-dev source repository.) git fetch origin git merge origin/ $GIT_BRANCH which shows that Updating b8ed336..792f3c1 Fast-forward mq/environments/dev/mq-infra/requirements.yaml | 2 +- mq/environments/dev/mq-spring-app/Chart.yaml | 4 ++++ mq/environments/dev/mq-spring-app/requirements.yaml | 4 ++++ mq/environments/dev/mq-spring-app/values.yaml | 3 +++ 4 files changed, 12 insertions ( + ) , 1 deletion ( - ) create mode 100644 mq/environments/dev/mq-spring-app/Chart.yaml create mode 100644 mq/environments/dev/mq-spring-app/requirements.yaml create mode 100644 mq/environments/dev/mq-spring-app/values.yaml We're now in a consistent state with the GitOps apps repository, so let's construct our next set of changes to push. The ArgoCD application for MQ sample application MQ sample application has its deployment to the cluster managed by a dedicated ArgoCD application called dev-mq-spring-app-instance . This follows the separation of concerns pattern where one ArgoCD application manages a set of related Kubernetes resources deployed to a cluster; in this case, all those resources associated with MQ sample application in the dev namespace. Issue the following command to show the ArgoCD application details: cat mq/config/argocd/dev/dev-mq-spring-app-instance.yaml which shows a YAML file typical of those we've seen before: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : dev-mq-spring-app-instance annotations : argocd.argoproj.io/sync-wave : \"300\" finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : dev server : https://kubernetes.default.svc project : applications source : path : mq/environments/dev/mq-spring-app repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops-apps targetRevision : master helm : valueFiles : - values.yaml syncPolicy : automated : prune : true selfHeal : true See how the Helm chart is referenced by path: mq/environments/dev/mq-spring-app : mq/environments/dev/mq-spring-app/ \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml The ArgoCD application applies this Helm chart to the cluster to instantiate MQ sample application as a set of cluster resources. Look at active MQ sample application ArgoCD application Let's examine MQ sample application and its Kubernetes resources using the ArgoCD UI. In the ArgoCD UI search the Applications view with the keyword mq-spring-app : ( You may need to launch the ArgoCD UI again. Refer to these instructions . ) We can now see the below ArgoCD Application: A new dev-mq-spring-app-instance ArgoCD application that is managing QM1 resources deployed to the cluster. View the new MQ sample application Kubernetes resources We can look at the deployed instance of mq-spring-app and its dependent kubernetes resources. Click on the dev-mq-infra-instance ArgoCD application: Verify application deployment \u00b6 Review the deployed application The application is deployed within the cluster using a deployment manifest. The deployment creates a replica set to manage the application's pod. A service is also created to manage the port, and a route allows external connectivity to the application via the service. This is the deployment for the application, where we can see 1 pod has been created for development and testing purposes: You can also view the deployment from the command line: oc project dev oc describe deployment mq-spring-app The application writes logs to stdout. These can be viewed from the command line. First of all find the name of the running mq-spring-app pod: oc get pods -l app.kubernetes.io/name = mq-spring-app Using the name of the running mq-spring-app pod, the following commands displays its log for the last 60 seconds and will stream any further messages: oc logs mq-spring-app-d9474564c-85mf6 -f --since = 60s Review the application's service This shows the corresponding service , where we can see the application's port 80 inside the application pod is being mapped to port 8080 at the cluster level: You can also view the service from the command line: oc describe service mq-spring-app Review the application's route Finally the route shows the external url (location) we use to interact with the application: You can also view the route from the command line: oc describe route mq-spring-app Check the application is running Using the location value from the route we can call the application to check its health by appending /health , for example: export APP_URL = $( oc get route -n dev mq-spring-app -o jsonpath = \"{.spec.host}\" ) curl -X GET https:// $APP_URL /actuator/health The function should return something similar to the JSON-formatted response below: { \"status\" : \"UP\" , \"components\" : { \"HeathEndPointMain\" : { \"status\" : \"UP\" }, \"diskSpace\" : { \"status\" : \"UP\" , \"details\" : { ... } }, \"jms\" : { \"status\" : \"UP\" , \"details\" : { \"provider\" : \"IBM MQ JMS Provider\" } }, \"livenessState\" : { \"status\" : \"UP\" }, \"ping\" : { \"status\" : \"UP\" }, \"readinessState\" : { \"status\" : \"UP\" } }, \"groups\" : [ \"liveness\" , \"readiness\" ] } Call the application to put a message to a queue Using the same location value we can call the application to to put a message to a queue on the queue manager, for example: curl -X GET https:// $APP_URL /api/send-hello-world Call the application to get a message from a queue Using the same location value we can call the application to to get a message from a queue on the queue manager, for example: curl -X GET https:// $APP_URL /api/recv Call the application to put a JSON message to a queue Using the same location value we can call the application to to put a message with a JSON payload to a queue on the queue manager, for example: curl -X POST -H \"Content-Type: application/json\" https:// $APP_URL /api/send-json --data \"{\\\"data\\\":\\\"Hola Munda\\\"}\" Note the use of the POST verb in the above command. This can be followed with a call to get the message from the queue. Congratulations! You've now created and run the pipeline for the application. In the next section we're going to make a change to the application that will trigger the pipeline to build a new version of the application image.","title":"Configuring the pipeline"},{"location":"guides/cp4i/mq/app-pipeline/topic2/#application-pipeline","text":"","title":"Application pipeline"},{"location":"guides/cp4i/mq/app-pipeline/topic2/#overview","text":"Audience : Application developers, Administrators In this topic, we're going to: Create an MQ application pipeline Examine the pipeline tasks Run the pipeline Review the resultant GitOps application folders Explore how ArgoCD automatically deploys the application Test the application using curl The following diagram illustrates how CICD is achieved using Tekton pipelines and ArgoCD applications. In this topic, we are going to focus on the highlighted application CICD. The application pipeline executes a set of tasks designed to ensure that the application builds successfully, including passing a set of functional and security tests. If every step is successful, the pipeline creates a new application image and GitOps configuration. Subsequently, a dedicated ArgoCD application watches for new MQ application images and configurations, which it deploys to the cluster. Notice the separation of concerns between Tekton and ArgoCD. Tekton's role is to perform continuous integration, ensuring that the best current version of the application is available for deployment. ArgoCD's role is to perform continuous deployment, ensuring that the cluster is always kept up-to-date with the latest built application that has passed all required tests.","title":"Overview"},{"location":"guides/cp4i/mq/app-pipeline/topic2/#pre-requisites","text":"In order to deploy the pipelines for the application we require the following prerequisites to be configured and installed: Create the cluster on the cloud platform of your choice. Configure the cluster for GitOps, and install the cluster infrastructure and cloud native services. Complete the steps in the previous topic to clone the application source. Ensure you are logged into your cluster from the command line.","title":"Pre-requisites"},{"location":"guides/cp4i/mq/app-pipeline/topic2/#build-and-test-the-application","text":"Let's now review the application pipeline to build, smoke test and configure the application. The production reference guide includes a number of pre-configured application pipelines for different programming languages and frameworks. These pipelines illustrate best practices for continuous integration. They are not meant to be exhaustive; you can modify them for your own projects. For this section, we return to the terminal window we used in previous chapters for interacting with the GitOps repository rather than the one we\u2019ve just used to clone the mq-spring-app client application source repository. Open a new terminal window for the multi-tenancy-gitops-apps repository if necessary. Review the pre-configured pipeline Open the OpenShift console and navigate to Pipelines > Pipelines . You will want to select the ci Project from the drop-down at the top of the page. Click the mq-spring-app-dev Pipeline to view the client application build and deployment pipeline. You can see the different stages of the pipeline, from building the Java application, unit testing, through to updating the image registry and GitOps repo. You can also view the pipeline from the command line: oc describe Pipeline mq-spring-app-dev -n ci Kickoff a pipeline run From the Actions dropdown menu in the upper-right corner, select Start . Configure the run as follows: Set git-url to your fork of the mq-spring-app repository Set git-revision to master . (Later in the tutorial, we will use a new branch.) Set scan-image: false (temporary fix while issues with UBI are resolved) Click Start and wait! It may take a little while for the pipeline to run. Keep checking until all steps have completed. Re-merging local clone to view Helm chart in GitOps repository The mq-spring-app-dev pipeline run updated the GitOps repository with the Helm chart. This means that our local clone of the GitOps repository is one commit behind GitHub. Before we can push any more changes to the GitOps repository, we must re-merge our local clone with GitHub. Return to the terminal window you're using for the multi-tenancy-gitops-apps GitOps apps repository. (Rather than the terminal window you're using for the mq-spring-app-dev source repository.) git fetch origin git merge origin/ $GIT_BRANCH which shows that Updating b8ed336..792f3c1 Fast-forward mq/environments/dev/mq-infra/requirements.yaml | 2 +- mq/environments/dev/mq-spring-app/Chart.yaml | 4 ++++ mq/environments/dev/mq-spring-app/requirements.yaml | 4 ++++ mq/environments/dev/mq-spring-app/values.yaml | 3 +++ 4 files changed, 12 insertions ( + ) , 1 deletion ( - ) create mode 100644 mq/environments/dev/mq-spring-app/Chart.yaml create mode 100644 mq/environments/dev/mq-spring-app/requirements.yaml create mode 100644 mq/environments/dev/mq-spring-app/values.yaml We're now in a consistent state with the GitOps apps repository, so let's construct our next set of changes to push. The ArgoCD application for MQ sample application MQ sample application has its deployment to the cluster managed by a dedicated ArgoCD application called dev-mq-spring-app-instance . This follows the separation of concerns pattern where one ArgoCD application manages a set of related Kubernetes resources deployed to a cluster; in this case, all those resources associated with MQ sample application in the dev namespace. Issue the following command to show the ArgoCD application details: cat mq/config/argocd/dev/dev-mq-spring-app-instance.yaml which shows a YAML file typical of those we've seen before: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : dev-mq-spring-app-instance annotations : argocd.argoproj.io/sync-wave : \"300\" finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : dev server : https://kubernetes.default.svc project : applications source : path : mq/environments/dev/mq-spring-app repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops-apps targetRevision : master helm : valueFiles : - values.yaml syncPolicy : automated : prune : true selfHeal : true See how the Helm chart is referenced by path: mq/environments/dev/mq-spring-app : mq/environments/dev/mq-spring-app/ \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml The ArgoCD application applies this Helm chart to the cluster to instantiate MQ sample application as a set of cluster resources. Look at active MQ sample application ArgoCD application Let's examine MQ sample application and its Kubernetes resources using the ArgoCD UI. In the ArgoCD UI search the Applications view with the keyword mq-spring-app : ( You may need to launch the ArgoCD UI again. Refer to these instructions . ) We can now see the below ArgoCD Application: A new dev-mq-spring-app-instance ArgoCD application that is managing QM1 resources deployed to the cluster. View the new MQ sample application Kubernetes resources We can look at the deployed instance of mq-spring-app and its dependent kubernetes resources. Click on the dev-mq-infra-instance ArgoCD application:","title":"Build and test the application"},{"location":"guides/cp4i/mq/app-pipeline/topic2/#verify-application-deployment","text":"Review the deployed application The application is deployed within the cluster using a deployment manifest. The deployment creates a replica set to manage the application's pod. A service is also created to manage the port, and a route allows external connectivity to the application via the service. This is the deployment for the application, where we can see 1 pod has been created for development and testing purposes: You can also view the deployment from the command line: oc project dev oc describe deployment mq-spring-app The application writes logs to stdout. These can be viewed from the command line. First of all find the name of the running mq-spring-app pod: oc get pods -l app.kubernetes.io/name = mq-spring-app Using the name of the running mq-spring-app pod, the following commands displays its log for the last 60 seconds and will stream any further messages: oc logs mq-spring-app-d9474564c-85mf6 -f --since = 60s Review the application's service This shows the corresponding service , where we can see the application's port 80 inside the application pod is being mapped to port 8080 at the cluster level: You can also view the service from the command line: oc describe service mq-spring-app Review the application's route Finally the route shows the external url (location) we use to interact with the application: You can also view the route from the command line: oc describe route mq-spring-app Check the application is running Using the location value from the route we can call the application to check its health by appending /health , for example: export APP_URL = $( oc get route -n dev mq-spring-app -o jsonpath = \"{.spec.host}\" ) curl -X GET https:// $APP_URL /actuator/health The function should return something similar to the JSON-formatted response below: { \"status\" : \"UP\" , \"components\" : { \"HeathEndPointMain\" : { \"status\" : \"UP\" }, \"diskSpace\" : { \"status\" : \"UP\" , \"details\" : { ... } }, \"jms\" : { \"status\" : \"UP\" , \"details\" : { \"provider\" : \"IBM MQ JMS Provider\" } }, \"livenessState\" : { \"status\" : \"UP\" }, \"ping\" : { \"status\" : \"UP\" }, \"readinessState\" : { \"status\" : \"UP\" } }, \"groups\" : [ \"liveness\" , \"readiness\" ] } Call the application to put a message to a queue Using the same location value we can call the application to to put a message to a queue on the queue manager, for example: curl -X GET https:// $APP_URL /api/send-hello-world Call the application to get a message from a queue Using the same location value we can call the application to to get a message from a queue on the queue manager, for example: curl -X GET https:// $APP_URL /api/recv Call the application to put a JSON message to a queue Using the same location value we can call the application to to put a message with a JSON payload to a queue on the queue manager, for example: curl -X POST -H \"Content-Type: application/json\" https:// $APP_URL /api/send-json --data \"{\\\"data\\\":\\\"Hola Munda\\\"}\" Note the use of the POST verb in the above command. This can be followed with a call to get the message from the queue. Congratulations! You've now created and run the pipeline for the application. In the next section we're going to make a change to the application that will trigger the pipeline to build a new version of the application image.","title":"Verify application deployment"},{"location":"guides/cp4i/mq/app-pipeline/topic3/","text":"Changing the application \u00b6 Overview \u00b6 Audience : Application developers, Administrators In this topic we add a new function to the application, and push this change to our application repository on GitHub which triggers the pipeline to rebuild the application. The new function puts the hello message to a specified queue rather than the default. The new queue name is stored in memory by the application, and so subsequent gets of the message will be from the specified queue. Later in this tutorial we will use this new capability to demonstrate changing the queue manager configuration by adding an extra queue. Pre-requisites \u00b6 Follow these instructions to create and run the application pipelines. Set up the webhook and its processing \u00b6 Your GitOps apps repository contains a sample event listener. Let's customize this sample with the relevant values for our cluster. We'll then deploy the event listener resources and trigger resources to the cluster using GitOps. Select the GitOps terminal window Return to the terminal window you're using for the multi-tenancy-gitops-apps GitOps repository. We're going to make use of a script in the sample GitOps apps repository that makes use of the $GIT_ORG and $GIT_BRANCH_SPRING environment variables you've seen earlier. Previously we used them in the mq-spring-app terminal window; now we're going to use this in the multi-tenancy-gitops-apps terminal window. Again, make sure in you're in the multi-tenancy-gitops-apps terminal window. Issue the following command to ensure you're in the correct starting folder: cd $HOME /git/multi-tenancy-gitops-apps Set up the $GIT_ORG and $GIT_BRANCH_SPRING environment variables Let's set up the environment variables that are used by the customization script. Replace <git-org> in the following command with your GitHub user name: export GIT_ORG = <git-org> Then create the $GIT_BRANCH_SPRING environment variable: export GIT_BRANCH_SPRING = mq-spring- $GIT_ORG Verify that your $GIT_USER and $GIT_BRANCH environment variables are set: echo $GIT_ORG echo $GIT_BRANCH_SPRING For example: ( base ) anthonyodowd/git/multi-tenancy-gitops echo GIT_ORG prod-ref-guide ( base ) anthonyodowd/git/multi-tenancy-gitops echo GIT_BRANCH_QM1 mq-spring-prod-ref-guide The sample event listener The GitOps repository contains a template for the event listener. Issue the following command to view the template of the event listener YAML: cat mq/environments/ci/eventlisteners/cntk-event-listener.yaml_template apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - name : mq-infra-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_QM1}' && body.repository.full_name == '${GIT_ORG}/mq-infra'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-infra-dev # - name: mq-spring-app-dev # interceptors: # - ref: # name: \"cel\" # params: # - name: \"filter\" # value: \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_SPRING}' && body.repository.full_name == '${GIT_ORG}/mq-spring-app'\" # bindings: # - ref: cnkt-trigger-binding # template: # ref: mq-spring-app-dev Open mq/environments/ci/eventlisteners/cntk-event-listener.yaml_template in the editor and uncomment the below lines. - name : mq-spring-app-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_SPRING}' && body.repository.full_name == '${GIT_ORG}/mq-spring-app'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-spring-app-dev Once edited, it looks like below: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - name : mq-infra-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_QM1}' && body.repository.full_name == '${GIT_ORG}/mq-infra'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-infra-dev - name : mq-spring-app-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_SPRING}' && body.repository.full_name == '${GIT_ORG}/mq-spring-app'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-spring-app-dev If you are using OpenShift 4.7, replace the above template with the following: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - bindings : - kind : TriggerBinding ref : cnkt-trigger-binding interceptors : - cel : filter : header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_QM1}' && body.repository.full_name == '${GIT_ORG}/mq-infra' name : mq-infra-dev template : ref : mq-infra-dev - bindings : - kind : TriggerBinding ref : cnkt-trigger-binding interceptors : - cel : filter : header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_SPRING}' && body.repository.full_name == '${GIT_ORG}/mq-spring-app' name : mq-spring-app-dev template : ref : mq-spring-app-dev Customize the event listeners Create the YAML for the event listener as follows. Issue the following command: cd mq/environments/ci/eventlisteners/ sh ./cntk-event-listener.sh cd ../../../../ Once, this script is run successfully, you should see a new file named cntk-event-listener.yaml . Exploring the event listener customization Let's examine a customized event listener in your local clone. We'll see how it maps to our diagram above, and has been customized for our source repository. Issue the following command: cat mq/environments/ci/eventlisteners/cntk-event-listener.yaml to show the customized event listener: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - name : mq-infra-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/qm1-prod-ref-guide-test' && body.repository.full_name == 'prod-ref-guide-test/mq-infra'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-infra-dev - name : mq-spring-app-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/mq-spring-prod-ref-guide-test' && body.repository.full_name == 'prod-ref-guide-test/mq-spring-app'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-spring-app-dev If using OpenShift 4.7, it will be as follows: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - bindings : - kind : TriggerBinding ref : cnkt-trigger-binding interceptors : - cel : filter : header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/qm1-prod-ref-guide-test' && body.repository.full_name == 'prod-ref-guide-test/mq-infra' name : mq-infra-dev template : ref : mq-infra-dev # - bindings: # - kind: TriggerBinding # ref: cnkt-trigger-binding # interceptors: # - cel: # filter: header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/qm1-prod-ref-guide-test' && body.repository.full_name == 'prod-ref-guide-test/mq-spring-app' # name: mq-spring-app-dev # template: # ref: mq-spring-app-dev An event listener can produce one or more triggers as specified by the list spec.triggers: . Now, our event listener additionally produces a trigger called mq-spring-app-dev along with mq-infra-dev . This trigger comprises the template mq-spring-app-dev and template binding cntk-trigger-binding . Push GitOps changes to GitHub Let\u2019s make these GitOps changes and push them. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Customizing the event trigger\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 15, done. Counting objects: 100% (15/15), done. Delta compression using up to 8 threads Compressing objects: 100% (8/8), done. Writing objects: 100% (8/8), 699 bytes | 699.00 KiB/s, done. Total 8 (delta 7), reused 0 (delta 0) remote: Resolving deltas: 100% (7/7), completed with 7 local objects. To https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps.git 792f3c1..5d937d7 master -> master Activate the webhook components Now that we've customized the event listener for our source repository, let's activate all the components necessary to process our webhook: the route, event listener, trigger template and trigger binding. Open the kustomization.yaml that is under mq/environments/ci/ . This contains all the resources that belong to the ci namespace. cat mq/environments/ci/kustomization.yaml resources : #- certificates/ci-mq-client-certificate.yaml #- certificates/ci-mq-server-certificate.yaml - configmaps/gitops-repo-configmap.yaml - eventlisteners/cntk-event-listener.yaml - triggerbindings/cntk-binding.yaml - triggertemplates/mq-infra-dev.yaml #- triggertemplates/mq-spring-app-dev.yaml - pipelines/ibm-test-pipeline-for-dev.yaml - pipelines/ibm-test-pipeline-for-stage.yaml #- pipelines/java-maven-dev-pipeline.yaml - pipelines/mq-pipeline-dev.yaml - pipelines/mq-spring-app-dev-pipeline.yaml - roles/custom-pipeline-sa-clusterrole.yaml - roles/custom-pipeline-sa-role.yaml - roles/custom-ci-pipeline-sa-rolebinding.yaml - roles/custom-dev-pipeline-sa-rolebinding.yaml - roles/custom-staging-pipeline-sa-rolebinding.yaml - roles/custom-prod-pipeline-sa-rolebinding.yaml - routes/cntk-route.yaml - secrets/artifactory-access-secret.yaml - secrets/git-credentials-secret.yaml - secrets/ibm-entitled-registry-credentials-secret.yaml #- secrets/mq-client-jks-password-secret.yaml - tasks/10-gitops.yaml - tasks/12-functional-tests.yaml - tasks/13-jmeter-performance-test.yaml - tasks/4-smoke-tests-mq.yaml - tasks/4-smoke-tests.yaml - tasks/ibm-build-tag-push-v2-6-13.yaml - tasks/ibm-helm-release-v2-6-13.yaml - tasks/ibm-img-release-v2-6-13.yaml - tasks/ibm-img-scan-v2-6-13.yaml - tasks/ibm-java-maven-test-v2-6-13.yaml - tasks/ibm-setup-v2-6-13.yaml - tasks/ibm-tag-release-v2-6-13.yaml Open this file in your editor and uncomment the below resources. - triggertemplates/mq-spring-app-dev.yaml You will have the following resources un-commented: resources : #- certificates/ci-mq-client-certificate.yaml #- certificates/ci-mq-server-certificate.yaml - configmaps/gitops-repo-configmap.yaml - eventlisteners/cntk-event-listener.yaml - triggerbindings/cntk-binding.yaml - triggertemplates/mq-infra-dev.yaml - triggertemplates/mq-spring-app-dev.yaml - pipelines/ibm-test-pipeline-for-dev.yaml - pipelines/ibm-test-pipeline-for-stage.yaml #- pipelines/java-maven-dev-pipeline.yaml - pipelines/mq-pipeline-dev.yaml - pipelines/mq-spring-app-dev-pipeline.yaml - roles/custom-pipeline-sa-clusterrole.yaml - roles/custom-pipeline-sa-role.yaml - roles/custom-ci-pipeline-sa-rolebinding.yaml - roles/custom-dev-pipeline-sa-rolebinding.yaml - roles/custom-staging-pipeline-sa-rolebinding.yaml - roles/custom-prod-pipeline-sa-rolebinding.yaml - routes/cntk-route.yaml - secrets/artifactory-access-secret.yaml - secrets/git-credentials-secret.yaml - secrets/ibm-entitled-registry-credentials-secret.yaml #- secrets/mq-client-jks-password-secret.yaml - tasks/10-gitops.yaml - tasks/12-functional-tests.yaml - tasks/13-jmeter-performance-test.yaml - tasks/4-smoke-tests-mq.yaml - tasks/4-smoke-tests.yaml - tasks/ibm-build-tag-push-v2-6-13.yaml - tasks/ibm-helm-release-v2-6-13.yaml - tasks/ibm-img-release-v2-6-13.yaml - tasks/ibm-img-scan-v2-6-13.yaml - tasks/ibm-java-maven-test-v2-6-13.yaml - tasks/ibm-setup-v2-6-13.yaml - tasks/ibm-tag-release-v2-6-13.yaml Update the GitOps repository Let\u2019s commit these changes to make the event listener and trigger resources active in the cluster. Issue the following command: git add mq/environments/ci/kustomization.yaml git commit -s -m \"Add app pipeline webhook\" git push origin $GIT_BRANCH which shows that the changes are pushed to GitHub: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 508 bytes | 508.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps.git 5d937d7..7ed3cd1 master -> master This change to the GitOps repository can now be used by ArgoCD. The apps-mq-rest-ci-1 argocd application Let's examine the ArgoCD application that manage the applications in ci namespace . In the ArgoCD UI Applications view, click on the icon for the apps-mq-rest-ci-1 application: You will see all the necessary configurations. Now, you will see trigger template, trigger binding and event listener we just created under this application. The GitHub webhook UI Let's now use the GitHub UI, to configure a webhook that creates an event and sends it to the route whenever the queue manager source repository changes. We'll configure the webhook using the GitHub UI. Issue the following command to determine the URL for the UI: echo https://github.com/ ${ GIT_ORG } /mq-spring-app/settings/hooks/new for example: https://github.com/prod-ref-guide/mq-spring-app/settings/hooks/new Copy the URL into your browser to launch the GitHub webpage to configure a new webhook for your mq-spring-app repository: We need to complete these details to configure our webhook. Configure the webhook To configure a webhook, we need to identify the URL it will call when it generates an event, the format of the event, and for which GitHub events we'll generate an event. Issue the following command to determine the URL of the event listener route: echo http:// $( oc get route el-cnkt-event-listener -n ci -o jsonpath = '{.spec.host}' ) for example: http://el-cnkt-event-listener-ci.xxxxx.containers.appdomain.cloud Here's a sample webhook configuration: Configure the following arguments: Set the Payload URL using the event listener route address. Set Content type to application/json . Select Let me select individual event Select Pull requests and Pushes from the list of available events. Click on Add webhook to create the webhook. A new webhook has been added In the GitHub UI, you can see that a new webhook has been added: Notice the webhook's name and that it's generating an event whenever a pull-request or push is issued against this repository. Let's now make a change to the MQ sample application source repository and watch the webhook at work. Change the application \u00b6 Confirm the new function doesn't exist Return to the terminal window you\u2019re using for the mq-spring-app source repository. (Rather than the terminal window you\u2019re using for the multi-tenancy-gitops-apps GitOps repository.) First of all we check the new function doesn't already exist. Using the application's route URL from the previous topic enter the following command: export APP_URL = $( oc get route -n dev mq-spring-app -o jsonpath = \"{.spec.host}\" ) curl -G -X GET https:// $APP_URL /api/send-to-queue -d queueName = test Note the addition of the -G parameter as we are adding a parameter to the GET. You should get a Not Found response: { \"timestamp\" : \"2021-05-04T13:16:51.365+0000\" , \"status\" : 404 , \"error\" : \"Not Found\" , \"message\" : \"No message available\" , \"path\" : \"/api/send-to-queue\" } Add the new function We're going to add the function /api/send-to-queue to the MQClientController class which includes the existing functions to put and get MQ messages. 2.1. Change source directory From the main directory of the cloned application source, change to the src/main/java/com/ibm/mqclient/controller directory cd src/main/java/com/ibm/mqclient/controller 2.2. Update source file Edit MQClientController.java using your editor of choice: code MQClientController.java 2.3. Add required imports Add the following code to the end of the import section at the top of the MQClientController.java file: import org.springframework.web.bind.annotation.RequestParam ; 2.4. Add new function Add the following code to the end of the MQClientController class: @GetMapping ( value = \"/api/send-to-queue\" ) @ApiOperation ( value = \"Put a 'Hello World!' message on the MQ queue specified as parameter.\" , notes = \"This api puts a hello world text message on the MQ queue specified as parameter.\" ) @ApiResponses ( value = { @ApiResponse ( code = 200 , message = \"Successfully put message on the specified queue.\" ), @ApiResponse ( code = 500 , message = \"Error putting message on the specified queue.\" )}) ResponseData sendHelloToQueueName ( @RequestParam String queueName ) { mqService . setQueueName ( queueName ); String dataSentToQueue = mqService . sendHelloWorld (); final String text = \"Successfully sent message to queue \" + mqService . getQueueName (); ResponseData responseData = new ResponseData ( \"OK\" , text , dataSentToQueue ); return responseData ; } Ensure the new function is within the braces {} of the main class, for example: public class MQClientController { ... @GetMapping ( value = \"/api/send-to-queue\" ) ResponseData sendHelloToQueueName ( @RequestParam String queueName ) { mqService . setQueueName ( queueName ); String dataSentToQueue = mqService . sendHelloWorld (); final String text = \"Successfully sent message to queue \" + mqService . getQueueName (); ResponseData responseData = new ResponseData ( \"OK\" , text , dataSentToQueue ); return responseData ; } } 2.5 Save the changes to the file Add get and set methods to the MQService class We also need change the MQService class to allow the queueName variable to be changed. 3.1. Change directory From current directory change to '../service': cd ../service 3.2. Update source file Edit MQService.java using your editor of choice: code MQService.java 3.3. Change code Change 'line 34' to include the queue name: LOG . debug ( \"Successfully Sent message: {} to the queue \" + queueName , helloWorld ); Change 'line 37' to include the queue name throw new AppException ( \"MQAPP001\" , \"Error sending message to the queue \" + queueName , ex ); 3.5 Add the new get and set functions After the sendHelloWorld() function add: public void setQueueName ( String newQueueName ) { this . queueName = newQueueName ; return ; } public String getQueueName () { return this . queueName ; } Commit and push the new changes 4.1. Commit the change to your local git repository git commit -a -m \"Add /api/send-to-queue function\" 4.2. Push the changes from your local repository git push origin ${ GIT_BRANCH_SPRING } 4.3. Observe the pipeline building, testing, and deploying the application Monitor the active PipelineRun while it is active. Once completed, you can return to ArgoCD to monitor the mq-spring-app-dev ArgoCD Application and it's rollout status. Test the new function We call the same function as we did in step 1. curl -G -X GET https:// $APP_URL /api/send-to-queue -d queueName = IBM.DEMO.Q This time we should get the following response showing the application now includes our new function: { \"status\" : \"OK\" , \"statusMessage\" : \"Successfully sent message to queue IBM.DEMO.Q\" , \"data\" : \"Hello World!\" } Congratulations! You've successfully modified the application by adding a new function. You then pushed this change to your Git repository which triggered the pipelines to build, test and deploy the change. Finally you called the new function using the curl command.","title":"Managing an application change"},{"location":"guides/cp4i/mq/app-pipeline/topic3/#changing-the-application","text":"","title":"Changing the application"},{"location":"guides/cp4i/mq/app-pipeline/topic3/#overview","text":"Audience : Application developers, Administrators In this topic we add a new function to the application, and push this change to our application repository on GitHub which triggers the pipeline to rebuild the application. The new function puts the hello message to a specified queue rather than the default. The new queue name is stored in memory by the application, and so subsequent gets of the message will be from the specified queue. Later in this tutorial we will use this new capability to demonstrate changing the queue manager configuration by adding an extra queue.","title":"Overview"},{"location":"guides/cp4i/mq/app-pipeline/topic3/#pre-requisites","text":"Follow these instructions to create and run the application pipelines.","title":"Pre-requisites"},{"location":"guides/cp4i/mq/app-pipeline/topic3/#set-up-the-webhook-and-its-processing","text":"Your GitOps apps repository contains a sample event listener. Let's customize this sample with the relevant values for our cluster. We'll then deploy the event listener resources and trigger resources to the cluster using GitOps. Select the GitOps terminal window Return to the terminal window you're using for the multi-tenancy-gitops-apps GitOps repository. We're going to make use of a script in the sample GitOps apps repository that makes use of the $GIT_ORG and $GIT_BRANCH_SPRING environment variables you've seen earlier. Previously we used them in the mq-spring-app terminal window; now we're going to use this in the multi-tenancy-gitops-apps terminal window. Again, make sure in you're in the multi-tenancy-gitops-apps terminal window. Issue the following command to ensure you're in the correct starting folder: cd $HOME /git/multi-tenancy-gitops-apps Set up the $GIT_ORG and $GIT_BRANCH_SPRING environment variables Let's set up the environment variables that are used by the customization script. Replace <git-org> in the following command with your GitHub user name: export GIT_ORG = <git-org> Then create the $GIT_BRANCH_SPRING environment variable: export GIT_BRANCH_SPRING = mq-spring- $GIT_ORG Verify that your $GIT_USER and $GIT_BRANCH environment variables are set: echo $GIT_ORG echo $GIT_BRANCH_SPRING For example: ( base ) anthonyodowd/git/multi-tenancy-gitops echo GIT_ORG prod-ref-guide ( base ) anthonyodowd/git/multi-tenancy-gitops echo GIT_BRANCH_QM1 mq-spring-prod-ref-guide The sample event listener The GitOps repository contains a template for the event listener. Issue the following command to view the template of the event listener YAML: cat mq/environments/ci/eventlisteners/cntk-event-listener.yaml_template apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - name : mq-infra-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_QM1}' && body.repository.full_name == '${GIT_ORG}/mq-infra'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-infra-dev # - name: mq-spring-app-dev # interceptors: # - ref: # name: \"cel\" # params: # - name: \"filter\" # value: \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_SPRING}' && body.repository.full_name == '${GIT_ORG}/mq-spring-app'\" # bindings: # - ref: cnkt-trigger-binding # template: # ref: mq-spring-app-dev Open mq/environments/ci/eventlisteners/cntk-event-listener.yaml_template in the editor and uncomment the below lines. - name : mq-spring-app-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_SPRING}' && body.repository.full_name == '${GIT_ORG}/mq-spring-app'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-spring-app-dev Once edited, it looks like below: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - name : mq-infra-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_QM1}' && body.repository.full_name == '${GIT_ORG}/mq-infra'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-infra-dev - name : mq-spring-app-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_SPRING}' && body.repository.full_name == '${GIT_ORG}/mq-spring-app'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-spring-app-dev If you are using OpenShift 4.7, replace the above template with the following: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - bindings : - kind : TriggerBinding ref : cnkt-trigger-binding interceptors : - cel : filter : header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_QM1}' && body.repository.full_name == '${GIT_ORG}/mq-infra' name : mq-infra-dev template : ref : mq-infra-dev - bindings : - kind : TriggerBinding ref : cnkt-trigger-binding interceptors : - cel : filter : header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_SPRING}' && body.repository.full_name == '${GIT_ORG}/mq-spring-app' name : mq-spring-app-dev template : ref : mq-spring-app-dev Customize the event listeners Create the YAML for the event listener as follows. Issue the following command: cd mq/environments/ci/eventlisteners/ sh ./cntk-event-listener.sh cd ../../../../ Once, this script is run successfully, you should see a new file named cntk-event-listener.yaml . Exploring the event listener customization Let's examine a customized event listener in your local clone. We'll see how it maps to our diagram above, and has been customized for our source repository. Issue the following command: cat mq/environments/ci/eventlisteners/cntk-event-listener.yaml to show the customized event listener: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - name : mq-infra-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/qm1-prod-ref-guide-test' && body.repository.full_name == 'prod-ref-guide-test/mq-infra'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-infra-dev - name : mq-spring-app-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/mq-spring-prod-ref-guide-test' && body.repository.full_name == 'prod-ref-guide-test/mq-spring-app'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-spring-app-dev If using OpenShift 4.7, it will be as follows: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - bindings : - kind : TriggerBinding ref : cnkt-trigger-binding interceptors : - cel : filter : header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/qm1-prod-ref-guide-test' && body.repository.full_name == 'prod-ref-guide-test/mq-infra' name : mq-infra-dev template : ref : mq-infra-dev # - bindings: # - kind: TriggerBinding # ref: cnkt-trigger-binding # interceptors: # - cel: # filter: header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/qm1-prod-ref-guide-test' && body.repository.full_name == 'prod-ref-guide-test/mq-spring-app' # name: mq-spring-app-dev # template: # ref: mq-spring-app-dev An event listener can produce one or more triggers as specified by the list spec.triggers: . Now, our event listener additionally produces a trigger called mq-spring-app-dev along with mq-infra-dev . This trigger comprises the template mq-spring-app-dev and template binding cntk-trigger-binding . Push GitOps changes to GitHub Let\u2019s make these GitOps changes and push them. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Customizing the event trigger\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 15, done. Counting objects: 100% (15/15), done. Delta compression using up to 8 threads Compressing objects: 100% (8/8), done. Writing objects: 100% (8/8), 699 bytes | 699.00 KiB/s, done. Total 8 (delta 7), reused 0 (delta 0) remote: Resolving deltas: 100% (7/7), completed with 7 local objects. To https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps.git 792f3c1..5d937d7 master -> master Activate the webhook components Now that we've customized the event listener for our source repository, let's activate all the components necessary to process our webhook: the route, event listener, trigger template and trigger binding. Open the kustomization.yaml that is under mq/environments/ci/ . This contains all the resources that belong to the ci namespace. cat mq/environments/ci/kustomization.yaml resources : #- certificates/ci-mq-client-certificate.yaml #- certificates/ci-mq-server-certificate.yaml - configmaps/gitops-repo-configmap.yaml - eventlisteners/cntk-event-listener.yaml - triggerbindings/cntk-binding.yaml - triggertemplates/mq-infra-dev.yaml #- triggertemplates/mq-spring-app-dev.yaml - pipelines/ibm-test-pipeline-for-dev.yaml - pipelines/ibm-test-pipeline-for-stage.yaml #- pipelines/java-maven-dev-pipeline.yaml - pipelines/mq-pipeline-dev.yaml - pipelines/mq-spring-app-dev-pipeline.yaml - roles/custom-pipeline-sa-clusterrole.yaml - roles/custom-pipeline-sa-role.yaml - roles/custom-ci-pipeline-sa-rolebinding.yaml - roles/custom-dev-pipeline-sa-rolebinding.yaml - roles/custom-staging-pipeline-sa-rolebinding.yaml - roles/custom-prod-pipeline-sa-rolebinding.yaml - routes/cntk-route.yaml - secrets/artifactory-access-secret.yaml - secrets/git-credentials-secret.yaml - secrets/ibm-entitled-registry-credentials-secret.yaml #- secrets/mq-client-jks-password-secret.yaml - tasks/10-gitops.yaml - tasks/12-functional-tests.yaml - tasks/13-jmeter-performance-test.yaml - tasks/4-smoke-tests-mq.yaml - tasks/4-smoke-tests.yaml - tasks/ibm-build-tag-push-v2-6-13.yaml - tasks/ibm-helm-release-v2-6-13.yaml - tasks/ibm-img-release-v2-6-13.yaml - tasks/ibm-img-scan-v2-6-13.yaml - tasks/ibm-java-maven-test-v2-6-13.yaml - tasks/ibm-setup-v2-6-13.yaml - tasks/ibm-tag-release-v2-6-13.yaml Open this file in your editor and uncomment the below resources. - triggertemplates/mq-spring-app-dev.yaml You will have the following resources un-commented: resources : #- certificates/ci-mq-client-certificate.yaml #- certificates/ci-mq-server-certificate.yaml - configmaps/gitops-repo-configmap.yaml - eventlisteners/cntk-event-listener.yaml - triggerbindings/cntk-binding.yaml - triggertemplates/mq-infra-dev.yaml - triggertemplates/mq-spring-app-dev.yaml - pipelines/ibm-test-pipeline-for-dev.yaml - pipelines/ibm-test-pipeline-for-stage.yaml #- pipelines/java-maven-dev-pipeline.yaml - pipelines/mq-pipeline-dev.yaml - pipelines/mq-spring-app-dev-pipeline.yaml - roles/custom-pipeline-sa-clusterrole.yaml - roles/custom-pipeline-sa-role.yaml - roles/custom-ci-pipeline-sa-rolebinding.yaml - roles/custom-dev-pipeline-sa-rolebinding.yaml - roles/custom-staging-pipeline-sa-rolebinding.yaml - roles/custom-prod-pipeline-sa-rolebinding.yaml - routes/cntk-route.yaml - secrets/artifactory-access-secret.yaml - secrets/git-credentials-secret.yaml - secrets/ibm-entitled-registry-credentials-secret.yaml #- secrets/mq-client-jks-password-secret.yaml - tasks/10-gitops.yaml - tasks/12-functional-tests.yaml - tasks/13-jmeter-performance-test.yaml - tasks/4-smoke-tests-mq.yaml - tasks/4-smoke-tests.yaml - tasks/ibm-build-tag-push-v2-6-13.yaml - tasks/ibm-helm-release-v2-6-13.yaml - tasks/ibm-img-release-v2-6-13.yaml - tasks/ibm-img-scan-v2-6-13.yaml - tasks/ibm-java-maven-test-v2-6-13.yaml - tasks/ibm-setup-v2-6-13.yaml - tasks/ibm-tag-release-v2-6-13.yaml Update the GitOps repository Let\u2019s commit these changes to make the event listener and trigger resources active in the cluster. Issue the following command: git add mq/environments/ci/kustomization.yaml git commit -s -m \"Add app pipeline webhook\" git push origin $GIT_BRANCH which shows that the changes are pushed to GitHub: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 508 bytes | 508.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps.git 5d937d7..7ed3cd1 master -> master This change to the GitOps repository can now be used by ArgoCD. The apps-mq-rest-ci-1 argocd application Let's examine the ArgoCD application that manage the applications in ci namespace . In the ArgoCD UI Applications view, click on the icon for the apps-mq-rest-ci-1 application: You will see all the necessary configurations. Now, you will see trigger template, trigger binding and event listener we just created under this application. The GitHub webhook UI Let's now use the GitHub UI, to configure a webhook that creates an event and sends it to the route whenever the queue manager source repository changes. We'll configure the webhook using the GitHub UI. Issue the following command to determine the URL for the UI: echo https://github.com/ ${ GIT_ORG } /mq-spring-app/settings/hooks/new for example: https://github.com/prod-ref-guide/mq-spring-app/settings/hooks/new Copy the URL into your browser to launch the GitHub webpage to configure a new webhook for your mq-spring-app repository: We need to complete these details to configure our webhook. Configure the webhook To configure a webhook, we need to identify the URL it will call when it generates an event, the format of the event, and for which GitHub events we'll generate an event. Issue the following command to determine the URL of the event listener route: echo http:// $( oc get route el-cnkt-event-listener -n ci -o jsonpath = '{.spec.host}' ) for example: http://el-cnkt-event-listener-ci.xxxxx.containers.appdomain.cloud Here's a sample webhook configuration: Configure the following arguments: Set the Payload URL using the event listener route address. Set Content type to application/json . Select Let me select individual event Select Pull requests and Pushes from the list of available events. Click on Add webhook to create the webhook. A new webhook has been added In the GitHub UI, you can see that a new webhook has been added: Notice the webhook's name and that it's generating an event whenever a pull-request or push is issued against this repository. Let's now make a change to the MQ sample application source repository and watch the webhook at work.","title":"Set up the webhook and its processing"},{"location":"guides/cp4i/mq/app-pipeline/topic3/#change-the-application","text":"Confirm the new function doesn't exist Return to the terminal window you\u2019re using for the mq-spring-app source repository. (Rather than the terminal window you\u2019re using for the multi-tenancy-gitops-apps GitOps repository.) First of all we check the new function doesn't already exist. Using the application's route URL from the previous topic enter the following command: export APP_URL = $( oc get route -n dev mq-spring-app -o jsonpath = \"{.spec.host}\" ) curl -G -X GET https:// $APP_URL /api/send-to-queue -d queueName = test Note the addition of the -G parameter as we are adding a parameter to the GET. You should get a Not Found response: { \"timestamp\" : \"2021-05-04T13:16:51.365+0000\" , \"status\" : 404 , \"error\" : \"Not Found\" , \"message\" : \"No message available\" , \"path\" : \"/api/send-to-queue\" } Add the new function We're going to add the function /api/send-to-queue to the MQClientController class which includes the existing functions to put and get MQ messages. 2.1. Change source directory From the main directory of the cloned application source, change to the src/main/java/com/ibm/mqclient/controller directory cd src/main/java/com/ibm/mqclient/controller 2.2. Update source file Edit MQClientController.java using your editor of choice: code MQClientController.java 2.3. Add required imports Add the following code to the end of the import section at the top of the MQClientController.java file: import org.springframework.web.bind.annotation.RequestParam ; 2.4. Add new function Add the following code to the end of the MQClientController class: @GetMapping ( value = \"/api/send-to-queue\" ) @ApiOperation ( value = \"Put a 'Hello World!' message on the MQ queue specified as parameter.\" , notes = \"This api puts a hello world text message on the MQ queue specified as parameter.\" ) @ApiResponses ( value = { @ApiResponse ( code = 200 , message = \"Successfully put message on the specified queue.\" ), @ApiResponse ( code = 500 , message = \"Error putting message on the specified queue.\" )}) ResponseData sendHelloToQueueName ( @RequestParam String queueName ) { mqService . setQueueName ( queueName ); String dataSentToQueue = mqService . sendHelloWorld (); final String text = \"Successfully sent message to queue \" + mqService . getQueueName (); ResponseData responseData = new ResponseData ( \"OK\" , text , dataSentToQueue ); return responseData ; } Ensure the new function is within the braces {} of the main class, for example: public class MQClientController { ... @GetMapping ( value = \"/api/send-to-queue\" ) ResponseData sendHelloToQueueName ( @RequestParam String queueName ) { mqService . setQueueName ( queueName ); String dataSentToQueue = mqService . sendHelloWorld (); final String text = \"Successfully sent message to queue \" + mqService . getQueueName (); ResponseData responseData = new ResponseData ( \"OK\" , text , dataSentToQueue ); return responseData ; } } 2.5 Save the changes to the file Add get and set methods to the MQService class We also need change the MQService class to allow the queueName variable to be changed. 3.1. Change directory From current directory change to '../service': cd ../service 3.2. Update source file Edit MQService.java using your editor of choice: code MQService.java 3.3. Change code Change 'line 34' to include the queue name: LOG . debug ( \"Successfully Sent message: {} to the queue \" + queueName , helloWorld ); Change 'line 37' to include the queue name throw new AppException ( \"MQAPP001\" , \"Error sending message to the queue \" + queueName , ex ); 3.5 Add the new get and set functions After the sendHelloWorld() function add: public void setQueueName ( String newQueueName ) { this . queueName = newQueueName ; return ; } public String getQueueName () { return this . queueName ; } Commit and push the new changes 4.1. Commit the change to your local git repository git commit -a -m \"Add /api/send-to-queue function\" 4.2. Push the changes from your local repository git push origin ${ GIT_BRANCH_SPRING } 4.3. Observe the pipeline building, testing, and deploying the application Monitor the active PipelineRun while it is active. Once completed, you can return to ArgoCD to monitor the mq-spring-app-dev ArgoCD Application and it's rollout status. Test the new function We call the same function as we did in step 1. curl -G -X GET https:// $APP_URL /api/send-to-queue -d queueName = IBM.DEMO.Q This time we should get the following response showing the application now includes our new function: { \"status\" : \"OK\" , \"statusMessage\" : \"Successfully sent message to queue IBM.DEMO.Q\" , \"data\" : \"Hello World!\" } Congratulations! You've successfully modified the application by adding a new function. You then pushed this change to your Git repository which triggered the pipelines to build, test and deploy the change. Finally you called the new function using the curl command.","title":"Change the application"},{"location":"guides/cp4i/mq/app-pipeline/topic4/","text":"Application configuration \u00b6 This pattern needs to be revisited as these edits should be made in the gitops repository and not the upstream application source repository. Overview \u00b6 Audience : Application developers, Administrators In this topic we review where the MQ configuration is stored, and how this can be changed. An MQ application requires the following configuration information to connect to a queue manager: queue manager name channel connection user password application name Note, User and Password are dependent on the security configuration for the queue manager. Pre-requisites \u00b6 In order to view and change the MQ configuration for the application we require the following prerequisites to be configured and installed: * Create the cluster on your infrastructure of choice. * Configure the cluster . This includes installing the Cloud Native Toolkit. * Complete the steps here to clone the application source. * Complete the steps here to review and run the pipeline. * Ensure you are logged into your cluster from the command line. Review MQ configuration \u00b6 Review the current MQ configuration From the main directory of the cloned application source, change to the chart/base directory: Display the content of values.yml by running: cat values.yml The MQ configuration can be seen near the end of this file: Here we can see the MQ configuration that will be added to a ConfigMap and Secret (both called mq-spring-app ) in the cluster by the ArgoCD pipeline. The application deployment manifest is configured to include these. Review the ConfigMap Display the resultant ConfigMap called mq-spring-app : oc get ConfigMap mq-spring-app -o yaml Review the Secret Display the resultant Secret called mq-spring-app : oc get Secret mq-spring-app -o yaml Note: The values for USER and PASSWORD are shown in Base64 . You can run the following command to decode them: echo \"cGFzc3cwcmQ=\" | base64 -d Change MQ configuration To change the MQ configuration, we follow the same process as for an application change . 4.1. Change the content of values.yml 4.2. Commit the change to your local git repository git commit -a -m \"Change MQ configuration\" 4.3. Push the changes from your local repository git push 4.4. Create a pull-request if required 4.5. Observe the pipeline being triggered to build, test and deploy the application Congratulations! You have successfully reviewed and modified the MQ configuration for the application.","title":"Application configuration"},{"location":"guides/cp4i/mq/app-pipeline/topic4/#application-configuration","text":"This pattern needs to be revisited as these edits should be made in the gitops repository and not the upstream application source repository.","title":"Application configuration"},{"location":"guides/cp4i/mq/app-pipeline/topic4/#overview","text":"Audience : Application developers, Administrators In this topic we review where the MQ configuration is stored, and how this can be changed. An MQ application requires the following configuration information to connect to a queue manager: queue manager name channel connection user password application name Note, User and Password are dependent on the security configuration for the queue manager.","title":"Overview"},{"location":"guides/cp4i/mq/app-pipeline/topic4/#pre-requisites","text":"In order to view and change the MQ configuration for the application we require the following prerequisites to be configured and installed: * Create the cluster on your infrastructure of choice. * Configure the cluster . This includes installing the Cloud Native Toolkit. * Complete the steps here to clone the application source. * Complete the steps here to review and run the pipeline. * Ensure you are logged into your cluster from the command line.","title":"Pre-requisites"},{"location":"guides/cp4i/mq/app-pipeline/topic4/#review-mq-configuration","text":"Review the current MQ configuration From the main directory of the cloned application source, change to the chart/base directory: Display the content of values.yml by running: cat values.yml The MQ configuration can be seen near the end of this file: Here we can see the MQ configuration that will be added to a ConfigMap and Secret (both called mq-spring-app ) in the cluster by the ArgoCD pipeline. The application deployment manifest is configured to include these. Review the ConfigMap Display the resultant ConfigMap called mq-spring-app : oc get ConfigMap mq-spring-app -o yaml Review the Secret Display the resultant Secret called mq-spring-app : oc get Secret mq-spring-app -o yaml Note: The values for USER and PASSWORD are shown in Base64 . You can run the following command to decode them: echo \"cGFzc3cwcmQ=\" | base64 -d Change MQ configuration To change the MQ configuration, we follow the same process as for an application change . 4.1. Change the content of values.yml 4.2. Commit the change to your local git repository git commit -a -m \"Change MQ configuration\" 4.3. Push the changes from your local repository git push 4.4. Create a pull-request if required 4.5. Observe the pipeline being triggered to build, test and deploy the application Congratulations! You have successfully reviewed and modified the MQ configuration for the application.","title":"Review MQ configuration"},{"location":"guides/cp4i/mq/cloud-native/automate-promotion/","text":"Automate Promotion Pipeline Workflow \u00b6 Overview \u00b6 In the previous topics, we promoted the queue manager and sample application from dev to staging environment and then from staging to prod environment. We then deployed them using ArgoCD. In this topic, we're going to make this promotion processes fully automatic. As we perviously saw, a change to the queue manager source repository or sample application source repository will automatically result in a pipeline run for it. If the pipeline is successful, a new Helm chart will be built and automatically deployed by ArgoCD into the dev namespace. Once the Queuemanager or the sample application is successfully deployed in dev namespace, the argocd post sync trigger in dev namespace will automatically generate a pipelinerun. If the pipeline is successful, a new Helm chart will be built and automatically deployed by ArgoCD into the stage namespace. Following the same approach again, once the Queuemanager or the sample application is successfully deployed in stage namespace, the argocd post sync trigger in stage namespace will automatically generate a pipelinerun. If the pipeline is successful, a new Helm chart will be built and automatically deployed by ArgoCD into the prod namespace. To recap, the whole process will be automatic from end-to-end. Look at the following diagram: We've already seen most of the highlighted components. Notice that we've added a post sync trigger . This trigger will be responsible to fire promotion pipelines every time the QM1 or Sample application is successfully deployed in the respective namespaces. In this topic, we're going to: Set up post sync triggers. Test the end to end promotion process. By the end of this topic we'll have a fully automated promotion process set up in place to deploy the latest changes made to QueueManager or Sample application to the cluster across all environments in an automated fashion. Pre-requisites \u00b6 Before attempting this topic, you should have successfully completed the previous topic . Set up the ArgoCD Post-sync hook \u00b6 An ArgoCD PostSync hook is useful to run certain checks after the deployment. For instance, if we want to validate the health or perform some integration tests, we can use PostSync hook to do these things for us. Basically, this hook executes after all the the Sync hooks of the application are successful and completed. If you want to know more about this, check Resource Hooks out. In this topic section we're going to set up the components we need for our PostSync hook: A PostSync hook will be triggered by ArgoCD once the deployment is successful and is sent to an event listener . In Kubernetes, this event listener comprises a normal route and service front-ending a Tekton eventlistener custom resource. The event listener is driven by the ArgoCD PostSync hook, and runs the pipeline using a Tekton trigger . Set up trigger templates \u00b6 Select the GitOps terminal window Return to the terminal window you're using for the multi-tenancy-gitops-apps GitOps repository. We're going to make use of a script in the sample GitOps apps repository that makes use of the $GIT_ORG and $GIT_BRANCH_QM1 environment variables you've seen earlier. Previously we used them in the mq-infra terminal window; now we're going to use this in the multi-tenancy-gitops-apps terminal window. Again, make sure in you're in the multi-tenancy-gitops-apps terminal window. Issue the following command to ensure you're in the correct starting folder: cd $HOME /git/multi-tenancy-gitops-apps Set up the $GIT_ORG and $GIT_BRANCH environment variables Let's set up the environment variables. Replace <git-org> in the following command with your GitHub user name: export GIT_ORG = <git-org> Then create the $GIT_BRANCH environment variable: export GIT_BRANCH = master Verify that your $GIT_USER and $GIT_BRANCH environment variables are set: echo $GIT_ORG echo $GIT_BRANCH For example: (base) user/git/multi-tenancy-gitops echo GIT_ORG prod-ref-guide (base) user/git/multi-tenancy-gitops echo GIT_BRANCH master The trigger templates for Queuemanager PostSync hook in dev environment Issue the following commands to generate a trigger template for Queuemanager PostSync hook in dev environment: vi mq/environments/ci/triggertemplates/mq-infra-dev-triggertemplate.yaml Alternatively, you can also open your favourite editor and create mq-infra-dev-triggertemplate.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/triggertemplates . Copy the below contents and save them to mq-infra-dev-triggertemplate.yaml . Before doing this, make sure you replace ${GIT_ORG} with the name of your organization. To know your org, just issue echo ${GIT_ORG} . apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-infra-post-dev name : mq-infra-post-dev spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-dev-stage generateName : ibm-mq-promote-dev-stage- spec : params : - name : git-url value : https://github.com/${GIT_ORG}/mq-infra.git pipelineRef : name : ibm-mq-promote-dev-stage Issue the following command to verify: cat mq/environments/ci/triggertemplates/mq-infra-dev-triggertemplate.yaml to show the trigger template we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-infra-post-dev name : mq-infra-post-dev spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-dev-stage generateName : ibm-mq-promote-dev-stage- spec : params : - name : git-url value : https://github.com/prod-ref-guide/mq-infra.git pipelineRef : name : ibm-mq-promote-dev-stage The trigger templates for MQ sample application PostSync hook in dev environment Issue the following commands to generate a trigger template for MQ Sample application PostSync hook in dev environment: vi mq/environments/ci/triggertemplates/mq-spring-app-dev-triggertemplate.yaml Alternatively, you can also open your favourite editor and create mq-spring-app-dev-triggertemplate.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/triggertemplates . Copy the below contents and save them to mq-spring-app-dev-triggertemplate.yaml . Before doing this, make sure you replace ${GIT_ORG} with the name of your organization. To know your org, just issue echo ${GIT_ORG} . apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-spring-app-post-dev name : mq-spring-app-post-dev spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-dev-stage generateName : ibm-mq-promote-dev-stage- spec : params : - name : git-url value : https://github.com/${GIT_ORG}/mq-spring-app.git pipelineRef : name : ibm-mq-promote-dev-stage Issue the following command to verify: cat mq/environments/ci/triggertemplates/mq-spring-app-dev-triggertemplate.yaml to show the trigger template we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-spring-app-post-dev name : mq-spring-app-post-dev spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-dev-stage generateName : ibm-mq-promote-dev-stage- spec : params : - name : git-url value : https://github.com/prod-ref-guide/mq-spring-app.git pipelineRef : name : ibm-mq-promote-dev-stage The trigger templates for Queuemanager PostSync hook in stage environment Issue the following commands to generate a trigger template for Queuemanager PostSync hook in stage environment: vi mq/environments/ci/triggertemplates/mq-infra-stage-triggertemplate.yaml Alternatively, you can also open your favourite editor and create mq-infra-stage-triggertemplate.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/triggertemplates . Copy the below contents and save them to mq-infra-stage-triggertemplate.yaml . Before doing this, make sure you replace ${GIT_ORG} with the name of your organization. To know your org, just issue echo ${GIT_ORG} . apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-infra-post-stage name : mq-infra-post-stage spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-stage-prod generateName : ibm-mq-promote-stage-prod- spec : params : - name : git-url value : https://github.com/${GIT_ORG}/mq-infra.git pipelineRef : name : ibm-mq-promote-stage-prod Issue the following command to verify: cat mq/environments/ci/triggertemplates/mq-infra-stage-triggertemplate.yaml to show the trigger template we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-infra-post-stage name : mq-infra-post-stage spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-stage-prod generateName : ibm-mq-promote-stage-prod- spec : params : - name : git-url value : https://github.com/prod-ref-guide/mq-infra.git pipelineRef : name : ibm-mq-promote-stage-prod The trigger templates for MQ sample application PostSync hook in stage environment Issue the following commands to generate a trigger template for MQ Sample application PostSync hook in stage environment: vi mq/environments/ci/triggertemplates/mq-spring-app-stage-triggertemplate.yaml Alternatively, you can also open your favourite editor and create mq-spring-app-stage-triggertemplate.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/triggertemplates . Copy the below contents and save them to mq-spring-app-stage-triggertemplate.yaml . Before doing this, make sure you replace ${GIT_ORG} with the name of your organization. To know your org, just issue echo ${GIT_ORG} . apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-spring-app-post-stage name : mq-spring-app-post-stage spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-stage-prod generateName : ibm-mq-promote-stage-prod- spec : params : - name : git-url value : https://github.com/${GIT_ORG}/mq-spring-app.git pipelineRef : name : ibm-mq-promote-stage-prod Issue the following command to verify: cat mq/environments/ci/triggertemplates/mq-spring-app-stage-triggertemplate.yaml to show the trigger template we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-spring-app-post-stage name : mq-spring-app-post-stage spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-stage-prod generateName : ibm-mq-promote-stage-prod- spec : params : - name : git-url value : https://github.com/prod-ref-guide/mq-spring-app.git pipelineRef : name : ibm-mq-promote-stage-prod Push GitOps changes to GitHub Let\u2019s make these GitOps changes and push them. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add mq/environments/ci/triggertemplates git commit -s -m \"Defining trigger templates for promotion automation\" git push origin $GIT_BRANCH Set up eventlisteners \u00b6 The eventlistener for QueueManager in dev environment Issue the following commands to generate an eventlistener in dev environment for the QueueManager: vi mq/environments/ci/eventlisteners/mq-infra-dev-eventlistener.yaml Alternatively, you can also open your favourite editor and create mq-infra-dev-eventlistener.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/eventlisteners . Copy the below contents and save them to mq-infra-dev-eventlistener.yaml . apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-infra-post-dev spec : serviceAccountName : pipeline triggers : - name : mq-infra-post-dev template : ref : mq-infra-post-dev Issue the following command to verify: cat mq/environments/ci/eventlisteners/mq-infra-dev-eventlistener.yaml to show the eventlistener we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-infra-post-dev spec : serviceAccountName : pipeline triggers : - name : mq-infra-post-dev template : ref : mq-infra-post-dev The eventlistener for MQ sample application in dev environment Issue the following commands to generate an eventlistener in dev environment for the MQ sample application: vi mq/environments/ci/eventlisteners/mq-spring-app-dev-eventlistener.yaml Alternatively, you can also open your favourite editor and create mq-spring-app-dev-eventlistener.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/eventlisteners . Copy the below contents and save them to mq-spring-app-dev-eventlistener.yaml . apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-spring-app-post-dev spec : serviceAccountName : pipeline triggers : - name : mq-spring-app-post-dev template : ref : mq-spring-app-post-dev Issue the following command to verify: cat mq/environments/ci/eventlisteners/mq-spring-app-dev-eventlistener.yaml to show the eventlistener we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-spring-app-post-dev spec : serviceAccountName : pipeline triggers : - name : mq-spring-app-post-dev template : ref : mq-spring-app-post-dev The eventlistener for QueueManager in stage environment Issue the following commands to generate an eventlistener in stage environment for the QueueManager: vi mq/environments/ci/eventlisteners/mq-infra-stage-eventlistener.yaml Alternatively, you can also open your favourite editor and create mq-infra-stage-eventlistener.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/eventlisteners . Copy the below contents and save them to mq-infra-stage-eventlistener.yaml . apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-infra-post-stage spec : serviceAccountName : pipeline triggers : - name : mq-infra-post-stage template : ref : mq-infra-post-stage Issue the following command to verify: cat mq/environments/ci/eventlisteners/mq-infra-stage-eventlistener.yaml to show the eventlistener we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-infra-post-stage spec : serviceAccountName : pipeline triggers : - name : mq-infra-post-stage template : ref : mq-infra-post-stage The eventlistener for MQ sample application in stage environment Issue the following commands to generate an eventlistener in stage environment for the MQ sample application: vi mq/environments/ci/eventlisteners/mq-spring-app-stage-eventlistener.yaml Alternatively, you can also open your favourite editor and create mq-spring-app-stage-eventlistener.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/eventlisteners . Copy the below contents and save them to mq-spring-app-stage-eventlistener.yaml . apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-spring-app-post-stage spec : serviceAccountName : pipeline triggers : - name : mq-spring-app-post-stage template : ref : mq-spring-app-post-stage Issue the following command to verify: cat mq/environments/ci/eventlisteners/mq-spring-app-stage-eventlistener.yaml to show the eventlistener we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-spring-app-post-stage spec : serviceAccountName : pipeline triggers : - name : mq-spring-app-post-stage template : ref : mq-spring-app-post-stage Push GitOps changes to GitHub Let\u2019s make these GitOps changes and push them. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add mq/environments/ci/eventlisteners/ git commit -s -m \"Defining eventlisteners for promotion automation\" git push origin $GIT_BRANCH Set up routes for the eventlisteners \u00b6 The route for QueueManager eventlistener in dev environment Issue the following commands to generate an eventlistener route in dev environment for the QueueManager: vi mq/environments/ci/routes/mq-infra-dev-route.yaml Alternatively, you can also open your favourite editor and create mq-infra-dev-route.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/routes . Copy the below contents and save them to mq-infra-dev-route.yaml . apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-infra-post-dev labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-infra-post-dev annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-infra-post-dev weight : 100 port : targetPort : http-listener wildcardPolicy : None Issue the following command to verify: cat mq/environments/ci/routes/mq-infra-dev-route.yaml to show the route we created earlier: apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-infra-post-dev labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-infra-post-dev annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-infra-post-dev weight : 100 port : targetPort : http-listener wildcardPolicy : None The route for MQ sample application eventlistener in dev environment Issue the following commands to generate an eventlistener route in dev environment for the MQ sample application: vi mq/environments/ci/routes/mq-spring-app-dev-route.yaml Alternatively, you can also open your favourite editor and create mq-spring-app-dev-route.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/routes . Copy the below contents and save them to mq-spring-app-dev-route.yaml . apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-spring-app-post-dev labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-spring-app-post-dev annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-spring-app-post-dev weight : 100 port : targetPort : http-listener wildcardPolicy : None Issue the following command to verify: cat mq/environments/ci/routes/mq-spring-app-dev-route.yaml to show the eventlistener we created earlier: apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-spring-app-post-dev labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-spring-app-post-dev annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-spring-app-post-dev weight : 100 port : targetPort : http-listener wildcardPolicy : None The route for QueueManager eventlistener in stage environment Issue the following commands to generate an eventlistener route in stage environment for the QueueManager: vi mq/environments/ci/routes/mq-infra-stage-route.yaml Alternatively, you can also open your favourite editor and create mq-infra-stage-route.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/routes . Copy the below contents and save them to mq-infra-stage-route.yaml . apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-infra-post-stage labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-infra-post-stage annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-infra-post-stage weight : 100 port : targetPort : http-listener wildcardPolicy : None Issue the following command to verify: cat mq/environments/ci/routes/mq-infra-stage-route.yaml to show the route we created earlier: apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-infra-post-stage labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-infra-post-stage annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-infra-post-stage weight : 100 port : targetPort : http-listener wildcardPolicy : None The route for MQ sample application eventlistener in stage environment Issue the following commands to generate an eventlistener route in stage environment for the MQ sample application: vi mq/environments/ci/routes/mq-spring-app-stage-route.yaml Alternatively, you can also open your favourite editor and create mq-spring-app-stage-route.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/routes . Copy the below contents and save them to mq-spring-app-stage-route.yaml . apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-spring-app-post-stage labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-spring-app-post-stage annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-spring-app-post-stage weight : 100 port : targetPort : http-listener wildcardPolicy : None Issue the following command to verify: cat mq/environments/ci/routes/mq-spring-app-stage-route.yaml to show the eventlistener we created earlier: apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-spring-app-post-stage labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-spring-app-post-stage annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-spring-app-post-stage weight : 100 port : targetPort : http-listener wildcardPolicy : None Push GitOps changes to GitHub Let\u2019s make these GitOps changes and push them. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add mq/environments/ci/routes/ git commit -s -m \"Defining routes for promotion automation\" git push origin $GIT_BRANCH Activate the Triggers \u00b6 Activate the PostSync hook components Now that we've created the event listener and triggers, let's activate all the components necessary to process our PostSync hook: the route, event listener and trigger template. Open the kustomization.yaml that is under mq/environments/ci/ . This contains all the resources that belong to the ci namespace. cat mq/environments/ci/kustomization.yaml resources : #- certificates/ci-mq-client-certificate.yaml #- certificates/ci-mq-server-certificate.yaml - configmaps/gitops-repo-configmap.yaml - eventlisteners/cntk-event-listener.yaml - triggerbindings/cntk-binding.yaml - triggertemplates/mq-infra-dev.yaml - triggertemplates/mq-spring-app-dev.yaml - pipelines/ibm-test-pipeline-for-dev.yaml - pipelines/ibm-test-pipeline-for-stage.yaml #- pipelines/java-maven-dev-pipeline.yaml - pipelines/mq-pipeline-dev.yaml - pipelines/mq-spring-app-dev-pipeline.yaml - roles/custom-pipeline-sa-clusterrole.yaml - roles/custom-pipeline-sa-role.yaml - roles/custom-ci-pipeline-sa-rolebinding.yaml - roles/custom-dev-pipeline-sa-rolebinding.yaml - roles/custom-staging-pipeline-sa-rolebinding.yaml - roles/custom-prod-pipeline-sa-rolebinding.yaml - routes/cntk-route.yaml - secrets/artifactory-access-secret.yaml - secrets/git-credentials-secret.yaml - secrets/ibm-entitled-registry-credentials-secret.yaml #- secrets/mq-client-jks-password-secret.yaml - tasks/10-gitops.yaml - tasks/12-functional-tests.yaml - tasks/13-jmeter-performance-test.yaml - tasks/4-smoke-tests-mq.yaml - tasks/4-smoke-tests.yaml - tasks/ibm-build-tag-push-v2-6-13.yaml - tasks/ibm-helm-release-v2-6-13.yaml - tasks/ibm-img-release-v2-6-13.yaml - tasks/ibm-img-scan-v2-6-13.yaml - tasks/ibm-java-maven-test-v2-6-13.yaml - tasks/ibm-setup-v2-6-13.yaml - tasks/ibm-tag-release-v2-6-13.yaml Open this file in your editor and add the below resources. - triggertemplates/mq-infra-dev-triggertemplate.yaml - eventlisteners/mq-infra-dev-eventlistener.yaml - routes/mq-infra-dev-route.yaml - triggertemplates/mq-spring-app-dev-triggertemplate.yaml - eventlisteners/mq-spring-app-dev-eventlistener.yaml - routes/mq-spring-app-dev-route.yaml - triggertemplates/mq-infra-stage-triggertemplate.yaml - eventlisteners/mq-infra-stage-eventlistener.yaml - routes/mq-infra-stage-route.yaml - triggertemplates/mq-spring-app-stage-triggertemplate.yaml - eventlisteners/mq-spring-app-stage-eventlistener.yaml - routes/mq-spring-app-stage-route.yaml You will have the following resources now: resources : #- certificates/ci-mq-client-certificate.yaml #- certificates/ci-mq-server-certificate.yaml - configmaps/gitops-repo-configmap.yaml - eventlisteners/cntk-event-listener.yaml - triggerbindings/cntk-binding.yaml - triggertemplates/mq-infra-dev.yaml - triggertemplates/mq-spring-app-dev.yaml - pipelines/ibm-test-pipeline-for-dev.yaml - pipelines/ibm-test-pipeline-for-stage.yaml #- pipelines/java-maven-dev-pipeline.yaml - pipelines/mq-pipeline-dev.yaml - pipelines/mq-spring-app-dev-pipeline.yaml - roles/custom-pipeline-sa-clusterrole.yaml - roles/custom-pipeline-sa-role.yaml - roles/custom-ci-pipeline-sa-rolebinding.yaml - roles/custom-dev-pipeline-sa-rolebinding.yaml - roles/custom-staging-pipeline-sa-rolebinding.yaml - roles/custom-prod-pipeline-sa-rolebinding.yaml - routes/cntk-route.yaml - secrets/artifactory-access-secret.yaml - secrets/git-credentials-secret.yaml - secrets/ibm-entitled-registry-credentials-secret.yaml #- secrets/mq-client-jks-password-secret.yaml - tasks/10-gitops.yaml - tasks/12-functional-tests.yaml - tasks/13-jmeter-performance-test.yaml - tasks/4-smoke-tests-mq.yaml - tasks/4-smoke-tests.yaml - tasks/ibm-build-tag-push-v2-6-13.yaml - tasks/ibm-helm-release-v2-6-13.yaml - tasks/ibm-img-release-v2-6-13.yaml - tasks/ibm-img-scan-v2-6-13.yaml - tasks/ibm-java-maven-test-v2-6-13.yaml - tasks/ibm-setup-v2-6-13.yaml - tasks/ibm-tag-release-v2-6-13.yaml - triggertemplates/mq-infra-dev-triggertemplate.yaml - eventlisteners/mq-infra-dev-eventlistener.yaml - routes/mq-infra-dev-route.yaml - triggertemplates/mq-spring-app-dev-triggertemplate.yaml - eventlisteners/mq-spring-app-dev-eventlistener.yaml - routes/mq-spring-app-dev-route.yaml - triggertemplates/mq-infra-stage-triggertemplate.yaml - eventlisteners/mq-infra-stage-eventlistener.yaml - routes/mq-infra-stage-route.yaml - triggertemplates/mq-spring-app-stage-triggertemplate.yaml - eventlisteners/mq-spring-app-stage-eventlistener.yaml - routes/mq-spring-app-stage-route.yaml Update the GitOps repository Let\u2019s commit these changes to make the event listener and trigger resources active in the cluster. Issue the following command: git add mq/environments/ci/kustomization.yaml git commit -s -m \"Add PostSync hook components\" git push origin $GIT_BRANCH This change to the GitOps repository can now be used by ArgoCD. Modify the charts in dev \u00b6 Add PostSync hook to mq-infra chart Issue the following commands to modify the mq-infra chart requirements.yaml : vi mq/environments/dev/mq-infra/requirements.yaml Alternatively, you can also open your favourite editor and open requirements.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/dev/mq-infra/ . Add the below content under dependencies in mq/environments/dev/mq-infra/requirements.yaml : - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Issue the following command to verify: cat mq/environments/dev/mq-infra/requirements.yaml to show the requirements.yaml we created earlier: dependencies : - name : mq-infra version : 0.0.1 repository : http://artifactory-artifactory.tools:8082/artifactory/generic-local/ci - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Configure the PostSync hook in mq-infra chart Issue the following commands to configure the PostSync hook in mq-infra chart values.yaml : vi mq/environments/dev/mq-infra/values.yaml Alternatively, you can also open your favourite editor and open values.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/dev/mq-infra/ . We need to grab the QueueManager eventlistener route in dev namespace. This will be used below. echo http:// $( oc get route mq-infra-post-dev -n ci -o jsonpath = '{.spec.host}' ) You will see something like below: http://mq-infra-post-dev-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud Use this in place of <replace with the mq-infra dev eventlistener route> . Add the below content in mq/environments/dev/mq-infra/values.yaml : post-sync-job : name : mq-infra-dev-to-stage-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : \"registry.access.redhat.com/ubi8\" imagePullPolicy : \"Always\" command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"<replace with the mq-infra dev eventlistener route>\" Issue the following command to verify: cat mq/environments/dev/mq-infra/values.yaml to show the values.yaml we modified earlier: global : {} mq-infra : replicaCount : 1 post-sync-job : name : mq-infra-dev-to-stage-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : \"registry.access.redhat.com/ubi8\" imagePullPolicy : \"Always\" command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"http://mq-infra-post-dev-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud\" Add PostSync hook to mq-spring-app chart Issue the following commands to modify the mq-spring-app chart requirements.yaml : vi mq/environments/dev/mq-spring-app/requirements.yaml Alternatively, you can also open your favourite editor and open requirements.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/dev/mq-spring-app/ . Add the below content under dependencies in mq/environments/dev/mq-spring-app/requirements.yaml : - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Issue the following command to verify: cat mq/environments/dev/mq-spring-app/requirements.yaml to show the requirements.yaml we created earlier: dependencies : - name : mq-spring-app version : 0.0.11 repository : http://artifactory-artifactory.tools:8082/artifactory/generic-local/ci - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Configure the PostSync hook in mq-spring-app chart Issue the following commands to configure the PostSync hook in mq-spring-app chart values.yaml : vi mq/environments/dev/mq-spring-app/values.yaml Alternatively, you can also open your favourite editor and open values.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/dev/mq-spring-app/ . We need to grab the MQ Sample application eventlistener route in dev namespace. This will be used below. echo http:// $( oc get route mq-spring-app-post-dev -n ci -o jsonpath = '{.spec.host}' ) You will see something like below: http://mq-spring-app-post-dev-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud Use this in place of <replace with the mq-spring-app dev eventlistener route> . Add the below content in mq/environments/dev/mq-spring-app/values.yaml : post-sync-job : name : mq-spring-app-dev-to-stage-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : \"registry.access.redhat.com/ubi8\" imagePullPolicy : \"Always\" command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"<replace with the mq-spring-app dev eventlistener route>\" Issue the following command to verify: cat mq/environments/dev/mq-spring-app/values.yaml to show the values.yaml we modified earlier: global : {} mq-spring-app : replicaCount : 1 post-sync-job : name : mq-spring-app-dev-to-stage-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : \"registry.access.redhat.com/ubi8\" imagePullPolicy : \"Always\" command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"http://mq-spring-app-post-dev-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud\" Modify the charts in stage \u00b6 Add PostSync hook to mq-infra chart Issue the following commands to modify the mq-infra chart requirements.yaml : vi mq/environments/staging/mq-infra/requirements.yaml Alternatively, you can also open your favourite editor and open requirements.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/staging/mq-infra/ . Add the below content under dependencies in mq/environments/staging/mq-infra/requirements.yaml : - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Issue the following command to verify: cat mq/environments/staging/mq-infra/requirements.yaml to show the requirements.yaml we created earlier: dependencies : - name : mq-infra version : 0.0.1 repository : http://artifactory-artifactory.tools:8082/artifactory/generic-local/ci - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Configure the PostSync hook in mq-infra chart Issue the following commands to configure the PostSync hook in mq-infra chart values.yaml : vi mq/environments/staging/mq-infra/values.yaml Alternatively, you can also open your favourite editor and open values.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/staging/mq-infra/ . We need to grab the QueueManager eventlistener route in staging namespace. This will be used below. echo http:// $( oc get route mq-infra-post-stage -n ci -o jsonpath = '{.spec.host}' ) You will see something like below: http://mq-infra-post-stage-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud Use this in place of <replace with the mq-infra stage eventlistener route> . Add the below content in mq/environments/staging/mq-infra/values.yaml : post-sync-job : name : mq-infra-stage-to-prod-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : registry.access.redhat.com/ubi8 imagePullPolicy : Always command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"<replace with the mq-infra stage eventlistener route>\" Issue the following command to verify: cat mq/environments/staging/mq-infra/values.yaml to show the values.yaml we modified earlier: global : {} mq-infra : replicaCount : 1 post-sync-job : name : mq-infra-stage-to-prod-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : registry.access.redhat.com/ubi8 imagePullPolicy : Always command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"http://mq-infra-post-stage-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud\" Add PostSync hook to mq-spring-app chart Issue the following commands to modify the mq-spring-app chart requirements.yaml : vi mq/environments/staging/mq-spring-app/requirements.yaml Alternatively, you can also open your favourite editor and open requirements.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/staging/mq-spring-app/ . Add the below content under dependencies in mq/environments/staging/mq-spring-app/requirements.yaml : - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Issue the following command to verify: cat mq/environments/staging/mq-spring-app/requirements.yaml to show the requirements.yaml we created earlier: dependencies : - name : mq-spring-app version : 0.0.11 repository : http://artifactory-artifactory.tools:8082/artifactory/generic-local/ci - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Configure the PostSync hook in mq-spring-app chart Issue the following commands to configure the PostSync hook in mq-spring-app chart values.yaml : vi mq/environments/staging/mq-spring-app/values.yaml Alternatively, you can also open your favourite editor and open values.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/staging/mq-spring-app/ . We need to grab the MQ Sample application eventlistener route in staging namespace. This will be used below. echo http:// $( oc get route mq-spring-app-post-stage -n ci -o jsonpath = '{.spec.host}' ) You will see something like below: http://mq-spring-app-post-stage-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud Use this in place of <replace with the mq-spring-app stage eventlistener route> . Add the below content in mq/environments/staging/mq-spring-app/values.yaml : post-sync-job : name : mq-spring-app-stage-to-prod-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : \"registry.access.redhat.com/ubi8\" imagePullPolicy : \"Always\" command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"<replace with the mq-spring-app stage eventlistener route>\" Issue the following command to verify: cat mq/environments/staging/mq-spring-app/values.yaml to show the values.yaml we modified earlier: global : {} mq-spring-app : replicaCount : 1 post-sync-job : name : mq-spring-app-stage-to-prod-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : \"registry.access.redhat.com/ubi8\" imagePullPolicy : \"Always\" command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"http://mq-spring-app-post-stage-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud\" Push the changes to GitOps repository Let\u2019s commit these changes. Issue the following command: git add mq/environments/ git commit -s -m \"Add PostSync hook components\" git push origin $GIT_BRANCH Congratulations! You've now got a fully automated promotion process for your queue manager and MQ sample application. You started the chapter by configuring all the necessary components for a ArgoCD PostSync resource hook. As part of it, you created event listeners and triggers in your cluster to process these events and start the promotion pipeline run using the data in the event. After updating the QM1 source repository or MQ Sample Application source repository, respective pipeline runs was triggered automatically. Initially when the changes are made, mq-infra-dev or mq-spring-app-dev pipeline will run. Once those are deployed in the dev namespace, ArgoCD PostSync hook in dev will trigger the ibm-mq-promote-dev-stage pipeline automatically which leaves a PR in the GitOps apps repository for stage resources. Once this PR is merged and the deployments are done in stage namespace, ArgoCD PostSync hook in stage will trigger the ibm-mq-promote-stage-prod pipeline automatically which leaves a PR in the GitOps apps repository for prod resources. Once this PR is merged, deployments will be done in prod namespace.","title":"Automate promotion"},{"location":"guides/cp4i/mq/cloud-native/automate-promotion/#automate-promotion-pipeline-workflow","text":"","title":"Automate Promotion Pipeline Workflow"},{"location":"guides/cp4i/mq/cloud-native/automate-promotion/#overview","text":"In the previous topics, we promoted the queue manager and sample application from dev to staging environment and then from staging to prod environment. We then deployed them using ArgoCD. In this topic, we're going to make this promotion processes fully automatic. As we perviously saw, a change to the queue manager source repository or sample application source repository will automatically result in a pipeline run for it. If the pipeline is successful, a new Helm chart will be built and automatically deployed by ArgoCD into the dev namespace. Once the Queuemanager or the sample application is successfully deployed in dev namespace, the argocd post sync trigger in dev namespace will automatically generate a pipelinerun. If the pipeline is successful, a new Helm chart will be built and automatically deployed by ArgoCD into the stage namespace. Following the same approach again, once the Queuemanager or the sample application is successfully deployed in stage namespace, the argocd post sync trigger in stage namespace will automatically generate a pipelinerun. If the pipeline is successful, a new Helm chart will be built and automatically deployed by ArgoCD into the prod namespace. To recap, the whole process will be automatic from end-to-end. Look at the following diagram: We've already seen most of the highlighted components. Notice that we've added a post sync trigger . This trigger will be responsible to fire promotion pipelines every time the QM1 or Sample application is successfully deployed in the respective namespaces. In this topic, we're going to: Set up post sync triggers. Test the end to end promotion process. By the end of this topic we'll have a fully automated promotion process set up in place to deploy the latest changes made to QueueManager or Sample application to the cluster across all environments in an automated fashion.","title":"Overview"},{"location":"guides/cp4i/mq/cloud-native/automate-promotion/#pre-requisites","text":"Before attempting this topic, you should have successfully completed the previous topic .","title":"Pre-requisites"},{"location":"guides/cp4i/mq/cloud-native/automate-promotion/#set-up-the-argocd-post-sync-hook","text":"An ArgoCD PostSync hook is useful to run certain checks after the deployment. For instance, if we want to validate the health or perform some integration tests, we can use PostSync hook to do these things for us. Basically, this hook executes after all the the Sync hooks of the application are successful and completed. If you want to know more about this, check Resource Hooks out. In this topic section we're going to set up the components we need for our PostSync hook: A PostSync hook will be triggered by ArgoCD once the deployment is successful and is sent to an event listener . In Kubernetes, this event listener comprises a normal route and service front-ending a Tekton eventlistener custom resource. The event listener is driven by the ArgoCD PostSync hook, and runs the pipeline using a Tekton trigger .","title":"Set up the ArgoCD Post-sync hook"},{"location":"guides/cp4i/mq/cloud-native/automate-promotion/#set-up-trigger-templates","text":"Select the GitOps terminal window Return to the terminal window you're using for the multi-tenancy-gitops-apps GitOps repository. We're going to make use of a script in the sample GitOps apps repository that makes use of the $GIT_ORG and $GIT_BRANCH_QM1 environment variables you've seen earlier. Previously we used them in the mq-infra terminal window; now we're going to use this in the multi-tenancy-gitops-apps terminal window. Again, make sure in you're in the multi-tenancy-gitops-apps terminal window. Issue the following command to ensure you're in the correct starting folder: cd $HOME /git/multi-tenancy-gitops-apps Set up the $GIT_ORG and $GIT_BRANCH environment variables Let's set up the environment variables. Replace <git-org> in the following command with your GitHub user name: export GIT_ORG = <git-org> Then create the $GIT_BRANCH environment variable: export GIT_BRANCH = master Verify that your $GIT_USER and $GIT_BRANCH environment variables are set: echo $GIT_ORG echo $GIT_BRANCH For example: (base) user/git/multi-tenancy-gitops echo GIT_ORG prod-ref-guide (base) user/git/multi-tenancy-gitops echo GIT_BRANCH master The trigger templates for Queuemanager PostSync hook in dev environment Issue the following commands to generate a trigger template for Queuemanager PostSync hook in dev environment: vi mq/environments/ci/triggertemplates/mq-infra-dev-triggertemplate.yaml Alternatively, you can also open your favourite editor and create mq-infra-dev-triggertemplate.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/triggertemplates . Copy the below contents and save them to mq-infra-dev-triggertemplate.yaml . Before doing this, make sure you replace ${GIT_ORG} with the name of your organization. To know your org, just issue echo ${GIT_ORG} . apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-infra-post-dev name : mq-infra-post-dev spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-dev-stage generateName : ibm-mq-promote-dev-stage- spec : params : - name : git-url value : https://github.com/${GIT_ORG}/mq-infra.git pipelineRef : name : ibm-mq-promote-dev-stage Issue the following command to verify: cat mq/environments/ci/triggertemplates/mq-infra-dev-triggertemplate.yaml to show the trigger template we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-infra-post-dev name : mq-infra-post-dev spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-dev-stage generateName : ibm-mq-promote-dev-stage- spec : params : - name : git-url value : https://github.com/prod-ref-guide/mq-infra.git pipelineRef : name : ibm-mq-promote-dev-stage The trigger templates for MQ sample application PostSync hook in dev environment Issue the following commands to generate a trigger template for MQ Sample application PostSync hook in dev environment: vi mq/environments/ci/triggertemplates/mq-spring-app-dev-triggertemplate.yaml Alternatively, you can also open your favourite editor and create mq-spring-app-dev-triggertemplate.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/triggertemplates . Copy the below contents and save them to mq-spring-app-dev-triggertemplate.yaml . Before doing this, make sure you replace ${GIT_ORG} with the name of your organization. To know your org, just issue echo ${GIT_ORG} . apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-spring-app-post-dev name : mq-spring-app-post-dev spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-dev-stage generateName : ibm-mq-promote-dev-stage- spec : params : - name : git-url value : https://github.com/${GIT_ORG}/mq-spring-app.git pipelineRef : name : ibm-mq-promote-dev-stage Issue the following command to verify: cat mq/environments/ci/triggertemplates/mq-spring-app-dev-triggertemplate.yaml to show the trigger template we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-spring-app-post-dev name : mq-spring-app-post-dev spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-dev-stage generateName : ibm-mq-promote-dev-stage- spec : params : - name : git-url value : https://github.com/prod-ref-guide/mq-spring-app.git pipelineRef : name : ibm-mq-promote-dev-stage The trigger templates for Queuemanager PostSync hook in stage environment Issue the following commands to generate a trigger template for Queuemanager PostSync hook in stage environment: vi mq/environments/ci/triggertemplates/mq-infra-stage-triggertemplate.yaml Alternatively, you can also open your favourite editor and create mq-infra-stage-triggertemplate.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/triggertemplates . Copy the below contents and save them to mq-infra-stage-triggertemplate.yaml . Before doing this, make sure you replace ${GIT_ORG} with the name of your organization. To know your org, just issue echo ${GIT_ORG} . apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-infra-post-stage name : mq-infra-post-stage spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-stage-prod generateName : ibm-mq-promote-stage-prod- spec : params : - name : git-url value : https://github.com/${GIT_ORG}/mq-infra.git pipelineRef : name : ibm-mq-promote-stage-prod Issue the following command to verify: cat mq/environments/ci/triggertemplates/mq-infra-stage-triggertemplate.yaml to show the trigger template we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-infra-post-stage name : mq-infra-post-stage spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-stage-prod generateName : ibm-mq-promote-stage-prod- spec : params : - name : git-url value : https://github.com/prod-ref-guide/mq-infra.git pipelineRef : name : ibm-mq-promote-stage-prod The trigger templates for MQ sample application PostSync hook in stage environment Issue the following commands to generate a trigger template for MQ Sample application PostSync hook in stage environment: vi mq/environments/ci/triggertemplates/mq-spring-app-stage-triggertemplate.yaml Alternatively, you can also open your favourite editor and create mq-spring-app-stage-triggertemplate.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/triggertemplates . Copy the below contents and save them to mq-spring-app-stage-triggertemplate.yaml . Before doing this, make sure you replace ${GIT_ORG} with the name of your organization. To know your org, just issue echo ${GIT_ORG} . apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-spring-app-post-stage name : mq-spring-app-post-stage spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-stage-prod generateName : ibm-mq-promote-stage-prod- spec : params : - name : git-url value : https://github.com/${GIT_ORG}/mq-spring-app.git pipelineRef : name : ibm-mq-promote-stage-prod Issue the following command to verify: cat mq/environments/ci/triggertemplates/mq-spring-app-stage-triggertemplate.yaml to show the trigger template we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : mq-spring-app-post-stage name : mq-spring-app-post-stage spec : resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous argocd.argoproj.io/sync-options : Prune=false labels : tekton.dev/pipeline : ibm-mq-promote-stage-prod generateName : ibm-mq-promote-stage-prod- spec : params : - name : git-url value : https://github.com/prod-ref-guide/mq-spring-app.git pipelineRef : name : ibm-mq-promote-stage-prod Push GitOps changes to GitHub Let\u2019s make these GitOps changes and push them. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add mq/environments/ci/triggertemplates git commit -s -m \"Defining trigger templates for promotion automation\" git push origin $GIT_BRANCH","title":"Set up trigger templates"},{"location":"guides/cp4i/mq/cloud-native/automate-promotion/#set-up-eventlisteners","text":"The eventlistener for QueueManager in dev environment Issue the following commands to generate an eventlistener in dev environment for the QueueManager: vi mq/environments/ci/eventlisteners/mq-infra-dev-eventlistener.yaml Alternatively, you can also open your favourite editor and create mq-infra-dev-eventlistener.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/eventlisteners . Copy the below contents and save them to mq-infra-dev-eventlistener.yaml . apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-infra-post-dev spec : serviceAccountName : pipeline triggers : - name : mq-infra-post-dev template : ref : mq-infra-post-dev Issue the following command to verify: cat mq/environments/ci/eventlisteners/mq-infra-dev-eventlistener.yaml to show the eventlistener we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-infra-post-dev spec : serviceAccountName : pipeline triggers : - name : mq-infra-post-dev template : ref : mq-infra-post-dev The eventlistener for MQ sample application in dev environment Issue the following commands to generate an eventlistener in dev environment for the MQ sample application: vi mq/environments/ci/eventlisteners/mq-spring-app-dev-eventlistener.yaml Alternatively, you can also open your favourite editor and create mq-spring-app-dev-eventlistener.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/eventlisteners . Copy the below contents and save them to mq-spring-app-dev-eventlistener.yaml . apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-spring-app-post-dev spec : serviceAccountName : pipeline triggers : - name : mq-spring-app-post-dev template : ref : mq-spring-app-post-dev Issue the following command to verify: cat mq/environments/ci/eventlisteners/mq-spring-app-dev-eventlistener.yaml to show the eventlistener we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-spring-app-post-dev spec : serviceAccountName : pipeline triggers : - name : mq-spring-app-post-dev template : ref : mq-spring-app-post-dev The eventlistener for QueueManager in stage environment Issue the following commands to generate an eventlistener in stage environment for the QueueManager: vi mq/environments/ci/eventlisteners/mq-infra-stage-eventlistener.yaml Alternatively, you can also open your favourite editor and create mq-infra-stage-eventlistener.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/eventlisteners . Copy the below contents and save them to mq-infra-stage-eventlistener.yaml . apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-infra-post-stage spec : serviceAccountName : pipeline triggers : - name : mq-infra-post-stage template : ref : mq-infra-post-stage Issue the following command to verify: cat mq/environments/ci/eventlisteners/mq-infra-stage-eventlistener.yaml to show the eventlistener we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-infra-post-stage spec : serviceAccountName : pipeline triggers : - name : mq-infra-post-stage template : ref : mq-infra-post-stage The eventlistener for MQ sample application in stage environment Issue the following commands to generate an eventlistener in stage environment for the MQ sample application: vi mq/environments/ci/eventlisteners/mq-spring-app-stage-eventlistener.yaml Alternatively, you can also open your favourite editor and create mq-spring-app-stage-eventlistener.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/eventlisteners . Copy the below contents and save them to mq-spring-app-stage-eventlistener.yaml . apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-spring-app-post-stage spec : serviceAccountName : pipeline triggers : - name : mq-spring-app-post-stage template : ref : mq-spring-app-post-stage Issue the following command to verify: cat mq/environments/ci/eventlisteners/mq-spring-app-stage-eventlistener.yaml to show the eventlistener we created earlier: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : name : mq-spring-app-post-stage spec : serviceAccountName : pipeline triggers : - name : mq-spring-app-post-stage template : ref : mq-spring-app-post-stage Push GitOps changes to GitHub Let\u2019s make these GitOps changes and push them. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add mq/environments/ci/eventlisteners/ git commit -s -m \"Defining eventlisteners for promotion automation\" git push origin $GIT_BRANCH","title":"Set up eventlisteners"},{"location":"guides/cp4i/mq/cloud-native/automate-promotion/#set-up-routes-for-the-eventlisteners","text":"The route for QueueManager eventlistener in dev environment Issue the following commands to generate an eventlistener route in dev environment for the QueueManager: vi mq/environments/ci/routes/mq-infra-dev-route.yaml Alternatively, you can also open your favourite editor and create mq-infra-dev-route.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/routes . Copy the below contents and save them to mq-infra-dev-route.yaml . apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-infra-post-dev labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-infra-post-dev annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-infra-post-dev weight : 100 port : targetPort : http-listener wildcardPolicy : None Issue the following command to verify: cat mq/environments/ci/routes/mq-infra-dev-route.yaml to show the route we created earlier: apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-infra-post-dev labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-infra-post-dev annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-infra-post-dev weight : 100 port : targetPort : http-listener wildcardPolicy : None The route for MQ sample application eventlistener in dev environment Issue the following commands to generate an eventlistener route in dev environment for the MQ sample application: vi mq/environments/ci/routes/mq-spring-app-dev-route.yaml Alternatively, you can also open your favourite editor and create mq-spring-app-dev-route.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/routes . Copy the below contents and save them to mq-spring-app-dev-route.yaml . apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-spring-app-post-dev labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-spring-app-post-dev annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-spring-app-post-dev weight : 100 port : targetPort : http-listener wildcardPolicy : None Issue the following command to verify: cat mq/environments/ci/routes/mq-spring-app-dev-route.yaml to show the eventlistener we created earlier: apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-spring-app-post-dev labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-spring-app-post-dev annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-spring-app-post-dev weight : 100 port : targetPort : http-listener wildcardPolicy : None The route for QueueManager eventlistener in stage environment Issue the following commands to generate an eventlistener route in stage environment for the QueueManager: vi mq/environments/ci/routes/mq-infra-stage-route.yaml Alternatively, you can also open your favourite editor and create mq-infra-stage-route.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/routes . Copy the below contents and save them to mq-infra-stage-route.yaml . apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-infra-post-stage labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-infra-post-stage annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-infra-post-stage weight : 100 port : targetPort : http-listener wildcardPolicy : None Issue the following command to verify: cat mq/environments/ci/routes/mq-infra-stage-route.yaml to show the route we created earlier: apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-infra-post-stage labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-infra-post-stage annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-infra-post-stage weight : 100 port : targetPort : http-listener wildcardPolicy : None The route for MQ sample application eventlistener in stage environment Issue the following commands to generate an eventlistener route in stage environment for the MQ sample application: vi mq/environments/ci/routes/mq-spring-app-stage-route.yaml Alternatively, you can also open your favourite editor and create mq-spring-app-stage-route.yaml under the path multi-tenancy-gitops-apps/mq/environments/ci/routes . Copy the below contents and save them to mq-spring-app-stage-route.yaml . apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-spring-app-post-stage labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-spring-app-post-stage annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-spring-app-post-stage weight : 100 port : targetPort : http-listener wildcardPolicy : None Issue the following command to verify: cat mq/environments/ci/routes/mq-spring-app-stage-route.yaml to show the eventlistener we created earlier: apiVersion : route.openshift.io/v1 kind : Route metadata : name : mq-spring-app-post-stage labels : app.kubernetes.io/managed-by : EventListener app.kubernetes.io/part-of : Triggers eventlistener : mq-spring-app-post-stage annotations : openshift.io/host.generated : 'true' spec : to : kind : \"Service\" name : el-mq-spring-app-post-stage weight : 100 port : targetPort : http-listener wildcardPolicy : None Push GitOps changes to GitHub Let\u2019s make these GitOps changes and push them. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add mq/environments/ci/routes/ git commit -s -m \"Defining routes for promotion automation\" git push origin $GIT_BRANCH","title":"Set up routes for the eventlisteners"},{"location":"guides/cp4i/mq/cloud-native/automate-promotion/#activate-the-triggers","text":"Activate the PostSync hook components Now that we've created the event listener and triggers, let's activate all the components necessary to process our PostSync hook: the route, event listener and trigger template. Open the kustomization.yaml that is under mq/environments/ci/ . This contains all the resources that belong to the ci namespace. cat mq/environments/ci/kustomization.yaml resources : #- certificates/ci-mq-client-certificate.yaml #- certificates/ci-mq-server-certificate.yaml - configmaps/gitops-repo-configmap.yaml - eventlisteners/cntk-event-listener.yaml - triggerbindings/cntk-binding.yaml - triggertemplates/mq-infra-dev.yaml - triggertemplates/mq-spring-app-dev.yaml - pipelines/ibm-test-pipeline-for-dev.yaml - pipelines/ibm-test-pipeline-for-stage.yaml #- pipelines/java-maven-dev-pipeline.yaml - pipelines/mq-pipeline-dev.yaml - pipelines/mq-spring-app-dev-pipeline.yaml - roles/custom-pipeline-sa-clusterrole.yaml - roles/custom-pipeline-sa-role.yaml - roles/custom-ci-pipeline-sa-rolebinding.yaml - roles/custom-dev-pipeline-sa-rolebinding.yaml - roles/custom-staging-pipeline-sa-rolebinding.yaml - roles/custom-prod-pipeline-sa-rolebinding.yaml - routes/cntk-route.yaml - secrets/artifactory-access-secret.yaml - secrets/git-credentials-secret.yaml - secrets/ibm-entitled-registry-credentials-secret.yaml #- secrets/mq-client-jks-password-secret.yaml - tasks/10-gitops.yaml - tasks/12-functional-tests.yaml - tasks/13-jmeter-performance-test.yaml - tasks/4-smoke-tests-mq.yaml - tasks/4-smoke-tests.yaml - tasks/ibm-build-tag-push-v2-6-13.yaml - tasks/ibm-helm-release-v2-6-13.yaml - tasks/ibm-img-release-v2-6-13.yaml - tasks/ibm-img-scan-v2-6-13.yaml - tasks/ibm-java-maven-test-v2-6-13.yaml - tasks/ibm-setup-v2-6-13.yaml - tasks/ibm-tag-release-v2-6-13.yaml Open this file in your editor and add the below resources. - triggertemplates/mq-infra-dev-triggertemplate.yaml - eventlisteners/mq-infra-dev-eventlistener.yaml - routes/mq-infra-dev-route.yaml - triggertemplates/mq-spring-app-dev-triggertemplate.yaml - eventlisteners/mq-spring-app-dev-eventlistener.yaml - routes/mq-spring-app-dev-route.yaml - triggertemplates/mq-infra-stage-triggertemplate.yaml - eventlisteners/mq-infra-stage-eventlistener.yaml - routes/mq-infra-stage-route.yaml - triggertemplates/mq-spring-app-stage-triggertemplate.yaml - eventlisteners/mq-spring-app-stage-eventlistener.yaml - routes/mq-spring-app-stage-route.yaml You will have the following resources now: resources : #- certificates/ci-mq-client-certificate.yaml #- certificates/ci-mq-server-certificate.yaml - configmaps/gitops-repo-configmap.yaml - eventlisteners/cntk-event-listener.yaml - triggerbindings/cntk-binding.yaml - triggertemplates/mq-infra-dev.yaml - triggertemplates/mq-spring-app-dev.yaml - pipelines/ibm-test-pipeline-for-dev.yaml - pipelines/ibm-test-pipeline-for-stage.yaml #- pipelines/java-maven-dev-pipeline.yaml - pipelines/mq-pipeline-dev.yaml - pipelines/mq-spring-app-dev-pipeline.yaml - roles/custom-pipeline-sa-clusterrole.yaml - roles/custom-pipeline-sa-role.yaml - roles/custom-ci-pipeline-sa-rolebinding.yaml - roles/custom-dev-pipeline-sa-rolebinding.yaml - roles/custom-staging-pipeline-sa-rolebinding.yaml - roles/custom-prod-pipeline-sa-rolebinding.yaml - routes/cntk-route.yaml - secrets/artifactory-access-secret.yaml - secrets/git-credentials-secret.yaml - secrets/ibm-entitled-registry-credentials-secret.yaml #- secrets/mq-client-jks-password-secret.yaml - tasks/10-gitops.yaml - tasks/12-functional-tests.yaml - tasks/13-jmeter-performance-test.yaml - tasks/4-smoke-tests-mq.yaml - tasks/4-smoke-tests.yaml - tasks/ibm-build-tag-push-v2-6-13.yaml - tasks/ibm-helm-release-v2-6-13.yaml - tasks/ibm-img-release-v2-6-13.yaml - tasks/ibm-img-scan-v2-6-13.yaml - tasks/ibm-java-maven-test-v2-6-13.yaml - tasks/ibm-setup-v2-6-13.yaml - tasks/ibm-tag-release-v2-6-13.yaml - triggertemplates/mq-infra-dev-triggertemplate.yaml - eventlisteners/mq-infra-dev-eventlistener.yaml - routes/mq-infra-dev-route.yaml - triggertemplates/mq-spring-app-dev-triggertemplate.yaml - eventlisteners/mq-spring-app-dev-eventlistener.yaml - routes/mq-spring-app-dev-route.yaml - triggertemplates/mq-infra-stage-triggertemplate.yaml - eventlisteners/mq-infra-stage-eventlistener.yaml - routes/mq-infra-stage-route.yaml - triggertemplates/mq-spring-app-stage-triggertemplate.yaml - eventlisteners/mq-spring-app-stage-eventlistener.yaml - routes/mq-spring-app-stage-route.yaml Update the GitOps repository Let\u2019s commit these changes to make the event listener and trigger resources active in the cluster. Issue the following command: git add mq/environments/ci/kustomization.yaml git commit -s -m \"Add PostSync hook components\" git push origin $GIT_BRANCH This change to the GitOps repository can now be used by ArgoCD.","title":"Activate the Triggers"},{"location":"guides/cp4i/mq/cloud-native/automate-promotion/#modify-the-charts-in-dev","text":"Add PostSync hook to mq-infra chart Issue the following commands to modify the mq-infra chart requirements.yaml : vi mq/environments/dev/mq-infra/requirements.yaml Alternatively, you can also open your favourite editor and open requirements.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/dev/mq-infra/ . Add the below content under dependencies in mq/environments/dev/mq-infra/requirements.yaml : - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Issue the following command to verify: cat mq/environments/dev/mq-infra/requirements.yaml to show the requirements.yaml we created earlier: dependencies : - name : mq-infra version : 0.0.1 repository : http://artifactory-artifactory.tools:8082/artifactory/generic-local/ci - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Configure the PostSync hook in mq-infra chart Issue the following commands to configure the PostSync hook in mq-infra chart values.yaml : vi mq/environments/dev/mq-infra/values.yaml Alternatively, you can also open your favourite editor and open values.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/dev/mq-infra/ . We need to grab the QueueManager eventlistener route in dev namespace. This will be used below. echo http:// $( oc get route mq-infra-post-dev -n ci -o jsonpath = '{.spec.host}' ) You will see something like below: http://mq-infra-post-dev-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud Use this in place of <replace with the mq-infra dev eventlistener route> . Add the below content in mq/environments/dev/mq-infra/values.yaml : post-sync-job : name : mq-infra-dev-to-stage-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : \"registry.access.redhat.com/ubi8\" imagePullPolicy : \"Always\" command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"<replace with the mq-infra dev eventlistener route>\" Issue the following command to verify: cat mq/environments/dev/mq-infra/values.yaml to show the values.yaml we modified earlier: global : {} mq-infra : replicaCount : 1 post-sync-job : name : mq-infra-dev-to-stage-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : \"registry.access.redhat.com/ubi8\" imagePullPolicy : \"Always\" command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"http://mq-infra-post-dev-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud\" Add PostSync hook to mq-spring-app chart Issue the following commands to modify the mq-spring-app chart requirements.yaml : vi mq/environments/dev/mq-spring-app/requirements.yaml Alternatively, you can also open your favourite editor and open requirements.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/dev/mq-spring-app/ . Add the below content under dependencies in mq/environments/dev/mq-spring-app/requirements.yaml : - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Issue the following command to verify: cat mq/environments/dev/mq-spring-app/requirements.yaml to show the requirements.yaml we created earlier: dependencies : - name : mq-spring-app version : 0.0.11 repository : http://artifactory-artifactory.tools:8082/artifactory/generic-local/ci - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Configure the PostSync hook in mq-spring-app chart Issue the following commands to configure the PostSync hook in mq-spring-app chart values.yaml : vi mq/environments/dev/mq-spring-app/values.yaml Alternatively, you can also open your favourite editor and open values.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/dev/mq-spring-app/ . We need to grab the MQ Sample application eventlistener route in dev namespace. This will be used below. echo http:// $( oc get route mq-spring-app-post-dev -n ci -o jsonpath = '{.spec.host}' ) You will see something like below: http://mq-spring-app-post-dev-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud Use this in place of <replace with the mq-spring-app dev eventlistener route> . Add the below content in mq/environments/dev/mq-spring-app/values.yaml : post-sync-job : name : mq-spring-app-dev-to-stage-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : \"registry.access.redhat.com/ubi8\" imagePullPolicy : \"Always\" command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"<replace with the mq-spring-app dev eventlistener route>\" Issue the following command to verify: cat mq/environments/dev/mq-spring-app/values.yaml to show the values.yaml we modified earlier: global : {} mq-spring-app : replicaCount : 1 post-sync-job : name : mq-spring-app-dev-to-stage-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : \"registry.access.redhat.com/ubi8\" imagePullPolicy : \"Always\" command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"http://mq-spring-app-post-dev-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud\"","title":"Modify the charts in dev"},{"location":"guides/cp4i/mq/cloud-native/automate-promotion/#modify-the-charts-in-stage","text":"Add PostSync hook to mq-infra chart Issue the following commands to modify the mq-infra chart requirements.yaml : vi mq/environments/staging/mq-infra/requirements.yaml Alternatively, you can also open your favourite editor and open requirements.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/staging/mq-infra/ . Add the below content under dependencies in mq/environments/staging/mq-infra/requirements.yaml : - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Issue the following command to verify: cat mq/environments/staging/mq-infra/requirements.yaml to show the requirements.yaml we created earlier: dependencies : - name : mq-infra version : 0.0.1 repository : http://artifactory-artifactory.tools:8082/artifactory/generic-local/ci - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Configure the PostSync hook in mq-infra chart Issue the following commands to configure the PostSync hook in mq-infra chart values.yaml : vi mq/environments/staging/mq-infra/values.yaml Alternatively, you can also open your favourite editor and open values.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/staging/mq-infra/ . We need to grab the QueueManager eventlistener route in staging namespace. This will be used below. echo http:// $( oc get route mq-infra-post-stage -n ci -o jsonpath = '{.spec.host}' ) You will see something like below: http://mq-infra-post-stage-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud Use this in place of <replace with the mq-infra stage eventlistener route> . Add the below content in mq/environments/staging/mq-infra/values.yaml : post-sync-job : name : mq-infra-stage-to-prod-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : registry.access.redhat.com/ubi8 imagePullPolicy : Always command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"<replace with the mq-infra stage eventlistener route>\" Issue the following command to verify: cat mq/environments/staging/mq-infra/values.yaml to show the values.yaml we modified earlier: global : {} mq-infra : replicaCount : 1 post-sync-job : name : mq-infra-stage-to-prod-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : registry.access.redhat.com/ubi8 imagePullPolicy : Always command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"http://mq-infra-post-stage-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud\" Add PostSync hook to mq-spring-app chart Issue the following commands to modify the mq-spring-app chart requirements.yaml : vi mq/environments/staging/mq-spring-app/requirements.yaml Alternatively, you can also open your favourite editor and open requirements.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/staging/mq-spring-app/ . Add the below content under dependencies in mq/environments/staging/mq-spring-app/requirements.yaml : - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Issue the following command to verify: cat mq/environments/staging/mq-spring-app/requirements.yaml to show the requirements.yaml we created earlier: dependencies : - name : mq-spring-app version : 0.0.11 repository : http://artifactory-artifactory.tools:8082/artifactory/generic-local/ci - name : post-sync-job version : 0.1.0 repository : https://cloud-native-toolkit.github.io/prod-ref-guide-charts/ Configure the PostSync hook in mq-spring-app chart Issue the following commands to configure the PostSync hook in mq-spring-app chart values.yaml : vi mq/environments/staging/mq-spring-app/values.yaml Alternatively, you can also open your favourite editor and open values.yaml navigating to the path multi-tenancy-gitops-apps/mq/environments/staging/mq-spring-app/ . We need to grab the MQ Sample application eventlistener route in staging namespace. This will be used below. echo http:// $( oc get route mq-spring-app-post-stage -n ci -o jsonpath = '{.spec.host}' ) You will see something like below: http://mq-spring-app-post-stage-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud Use this in place of <replace with the mq-spring-app stage eventlistener route> . Add the below content in mq/environments/staging/mq-spring-app/values.yaml : post-sync-job : name : mq-spring-app-stage-to-prod-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : \"registry.access.redhat.com/ubi8\" imagePullPolicy : \"Always\" command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"<replace with the mq-spring-app stage eventlistener route>\" Issue the following command to verify: cat mq/environments/staging/mq-spring-app/values.yaml to show the values.yaml we modified earlier: global : {} mq-spring-app : replicaCount : 1 post-sync-job : name : mq-spring-app-stage-to-prod-postsync-trigger argocdhook : PostSync serviceaccount : pipeline container : image : \"registry.access.redhat.com/ubi8\" imagePullPolicy : \"Always\" command : \"curl\" args : - \"-X\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"--data\" - \"{}\" - \"http://mq-spring-app-post-stage-ci.itzroks-5500068atp-tgcncr-4b4a324f027aea19c5cbc0c3275c4656-0000.hkg02.containers.appdomain.cloud\" Push the changes to GitOps repository Let\u2019s commit these changes. Issue the following command: git add mq/environments/ git commit -s -m \"Add PostSync hook components\" git push origin $GIT_BRANCH Congratulations! You've now got a fully automated promotion process for your queue manager and MQ sample application. You started the chapter by configuring all the necessary components for a ArgoCD PostSync resource hook. As part of it, you created event listeners and triggers in your cluster to process these events and start the promotion pipeline run using the data in the event. After updating the QM1 source repository or MQ Sample Application source repository, respective pipeline runs was triggered automatically. Initially when the changes are made, mq-infra-dev or mq-spring-app-dev pipeline will run. Once those are deployed in the dev namespace, ArgoCD PostSync hook in dev will trigger the ibm-mq-promote-dev-stage pipeline automatically which leaves a PR in the GitOps apps repository for stage resources. Once this PR is merged and the deployments are done in stage namespace, ArgoCD PostSync hook in stage will trigger the ibm-mq-promote-stage-prod pipeline automatically which leaves a PR in the GitOps apps repository for prod resources. Once this PR is merged, deployments will be done in prod namespace.","title":"Modify the charts in stage"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/","text":"Promoting across environments \u00b6 Overview \u00b6 Audience : Architects, Application developers Promoting across different environments means typically we move the services or applications from one level to the other. For instance, we start with development environment. This will eventually be promoted to staging for testing once the developers make sure things are working fine in development environment. After thorough testing, this will be promoted to production environment which will be the final environment that will be used by the intended users. In this topic, we're going to: Deployment environments Promoting deliverables across environments CI Environment Development Environment Functional Testing Staging Environment Performance Testing Production Environment Deployment environments \u00b6 For the basic promotion process, in our CI/CD process, typical environments include development, staging and production. Also, we are using a dedicated environment to run our CI/CD pipelines. These environments are defined as distinct projects within a cluster. Dev, Stage and Prod environments \u00b6 Dev environment is the primary environment and it is used for sandboxing. During the development process, developers use this environment to test their code. This will allow them to thoroughly test the sample MQ application code, as well as the infrastructure code which contains the Queue Manager configurations. Staging environment is sort of pre prod environment. All the sample MQ application deliverables that are ready to be pushed to the prod environment should be thouroughly tested here. Prod environment will host all live services that belong to the MQ workloads. These deliverables will be consumed by the end user. CI environment \u00b6 CI environment is used run all the MQ related pipelines. This includes all the tasks (custom / reusable task from cloud native toolkit ), pipelines, necessary triggers etc. Promoting deliverables across environments \u00b6 For our sample use case, end to end basic promotion process is as follows. For this sample usecase, we are maintaining two source code repositories for maintaining Queue Manager and a sample application that helps us to demonstrate the work flow. Webhooks are configured for all these repositories hooking them up with corresponding pipelines. If there are any updates to the source code, you can push changes to the respective repositories. When the changes are pushed, underlying webhooks will trigger corresponding pipelines. When we make changes to the mq-infra repository, it will trigger mq-infra-dev pipeline. Similarly, when we changes to the mq-spring-app repository, it will trigger mq-spring-app-dev pipeline. Once these pipelines are run successfully, the updated resources are pushed to the dev destination in the Gitops Apps repository. Subsequently, a dedicated ArgoCD application watches for the updates, which it deploys to the cluster in the DEV environment. Once the deployment is successful, the ibm-mq-promote-dev-stage pipeline will be triggered. If ibm-mq-promote-dev-stage pipeline runs successfully, it will make changes to the staging resources and leave a Pull Request in the Gitops Apps repository. Then the Pull Request will be reviewed manually and the changes to the staging resources are merged into the Gitops Apps repository. Subsequently, a dedicated ArgoCD application watches for the updates, which it deploys to the cluster in the STAGING environment. Once the deployment is successful, the ibm-mq-promote-stage-prod pipeline will be triggered. If ibm-mq-promote-stage-prod pipeline runs successfully, it will make changes to the prod resources and leave a Pull Request in the Gitops Apps repository. Then the Pull Request will be reviewed manually and the changes to the prod resources are merged into the Gitops Apps repository. Subsequently, a dedicated ArgoCD application watches for the updates, which it deploys to the cluster in the PROD environment. CI Environment \u00b6 CI environment is our sample Continuous Integration space where we hosted all the necessary resources. This environment holds things like all the pipelines, tasks, triggers, etc. Available pipelines \u00b6 Below are the four pipelines that are being used by the promotion process for the sample MQ workflow. mq-infra-dev pipeline \u00b6 When we add in new changes to the MQ Infra repository, this pipeline will be triggered. This pipeline executes a set of tasks designed to ensure that the Queue Manager builds successfully. If every step is successful, the pipeline creates a new image and GitOps configuration. mq-spring-app-dev pipeline \u00b6 When we add in new changes to the MQ Sample Application repository, this pipeline will be triggered. This pipeline executes a set of tasks designed to ensure that the Sample application builds successfully. If every step is successful, the pipeline creates a new image and GitOps configuration. ibm-mq-promote-dev-stage pipeline \u00b6 The pipeline executes a set of tasks designed to ensure that the Queue Manager or the MQ Sample application is passing a set of functional tests. If every step is successful, the pipeline updates Staging related GitOps configuration and leaves a Pull Request. ibm-mq-promote-stage-prod pipeline \u00b6 The pipeline executes a set of tasks designed to ensure that the Queue Manager or the MQ Sample application is passing a set of performance tests. If every step is successful, the pipeline updates Production related GitOps configuration and leaves a Pull Request. Development Environment \u00b6 When the changes are made to the source repositories, respective pipelines will run and build the necessary deliverables. These deliverables will be deployed to the development environment and will be subjected to first phase of testing allowing to validate the deployment. This deployment in development environment is done by ArgoCD. ArgoCD\u2019s role is to perform continuous deployment, ensuring that the cluster is always kept up-to-date with the latest built application. This is the first environment where the deliverable is deployed. If this deployment is a success, then this will be the first step that ensures that this deliverable can be promoted to production at some point of time. Once the deliverable is successfully deployed, we can validate different properties of the system which may functional or non-functional. Not only, we can also validate the configurations of the deliverable. In our sample promotion process, we showed an example of functional tests that are part of functional test pipeline. Depending on the usecases this pipeline can be further extended or modified. If everything works fine, we can promote the deliverable to next environment and deploy it there. Functional Testing \u00b6 Functional test Pipeline \u00b6 dev-instance-test Clones the source repositories. Grabs the route of the deliverable. Performs functional testing using newman test scripts. Finally, returns the name of the deliverable. gitops Clones the gitops apps repository. Copies the deliverables from dev to staging. Updates the helm chart with latest resources. Generate a Pull Request with the changes to gitops apps repository. Functional Test Task \u00b6 In our sample functional test task, we are using Newman as our functional testing tool. Note The contents in this guide are only examples. We demonstrated some of the practices using some sample usecases. For real world scenarios, it should be used in conjunction with existing QA practices and this content is not a replacement. For assistance, CSMs should reach out to QA SMEs. Postman \u00b6 Postman is a tool that is used for API automation. This tools helps us to automate many tests like unit tests, integration tests, end to end tests etc. Based on the test scripts we define, this validates the respective APIs by making sure that the responses received are valid. It also allows us to save a bunch of API requests as a collection. Grouping the requests together will allow easier access. These collections can be exported as JSON files and stored. Test scripts \u00b6 For our sample, the Postman collection is exported as mq-spring-app.postman_collection.json and you can have a look at it here . In this test script, we defined couple of API automations as follows. Send message Receive message Send message POST health readiness liveness Newman \u00b6 Newman is the command-line collection runner. It allows us to run the test scripts from the command line. This one can be easily used in the continuous integration and build systems. Like mentioned earlier, the collection that is exported as json files from Postman can be used to run the tests using Newman. We are using the below command as part of our functional testing task. newman run \\ --env-var base-url = $route \\ $test_file We are passing the necessary route information along with the test json file. Advantages \u00b6 Some advantages are: Ensures all the APIs are working properly. Detects if any bugs exists. Helps us to improve the application and make it better. Staging Environment \u00b6 When the Pull Request with the changes in staging resources get merged into the GitOps apps repository, the deliverables for staging environment will be updated. These deliverables will be deployed to the stage environment and will be subjected to second phase of testing allowing to validate the deployment. This deployment in staging environment is done by ArgoCD. ArgoCD\u2019s role is to perform continuous deployment, ensuring that the cluster is always kept up-to-date with the latest built application. This is the next environment after dev where the deliverable is deployed and is exact replica of production environment. If this deployment is a success, then this will ensure that this deliverable can be promoted to production environment. Once the deliverable is successfully deployed, we can perform different tests like User Acceptance Tests (UAT), load/stress testing, chaos engineering tests etc. In our sample promotion process, we showed an example of load tests that are part of performance test pipeline. Depending on the usecases this pipeline can be further extended or modified. If everything works fine, we can promote the deliverable to next environment and deploy it there. Performance Testing \u00b6 Performance test Pipeline \u00b6 stage-instance-test Clones the source repositories. Grabs the route of the deliverable. Performs load testing using jmeter test scripts. Finally, returns the name of the deliverable. gitops Clones the gitops apps repository. Copies the deliverables from staging to prod. Updates the helm chart with latest resources. Generate a Pull Request with the changes to gitops apps repository. Performance Test Task \u00b6 In our sample performance test task, we are using Jmeter as our performance load testing tool. The live data from Jmeter will be fed into a datastore. Based on this data, the metrics can be visualized using Grafana dashboard. Note The contents in this guide are only examples. We demonstrated some of the practices using some sample usecases. For real world scenarios, it should be used in conjunction with existing performance practices and this content is not a replacement. For assistance, CSMs should reach out to performance SMEs. Apache Jmeter \u00b6 Jmeter is an open source java based tool. It helps us to measure the performance of the application. We can basically analyze the load functional behavior of the application. Jmeter simulates the load, for instance it simulates a group of users and keep sending requests to the target server. We can define a test plan script. All the necessary configurations will be provided here. Once we run the Jmeter tests, results will be available. Sample test plan \u00b6 Our sample app test plan can be accessed here . If you have a look at the test plan, you will see different configuration elements. For example, you can see __P(route,mq-spring-app-dev.gitops-mq-ha-6ccd7f378ae819553d37d5f2ee142bd6-0000.tor01.containers.appdomain.cloud) in one of the ThreadGroup elements. The default route here is mq-spring-app-dev.gitops-mq-ha-6ccd7f378ae819553d37d5f2ee142bd6-0000.tor01.containers.appdomain.cloud , but the route url is parameterized. We will be passing this information as part of pipeline. Running jmeter tests \u00b6 Below is the jmeter command we used as part of our tests. ${ JMETER_BIN } /jmeter.sh \\ -n -t $( params.test-plan ) \\ -Jroute = $route \\ -l /source/report.jtl Here we are providing the test plan script along with the route to the Jmeter. Once the command is executed successfully, it loads the results into /source/report.jtl file. Advantages \u00b6 Some of the advantages are: Allows us to identify performance bottlenecks. Identify bugs. System downtime can be reduced. Production Environment \u00b6 When the Pull Request with the changes in prod resources get merged into the GitOps apps repository, the deliverables for prod environment will be updated. These deliverables will be deployed to the prod environment. This deployment in production environment is done by ArgoCD. ArgoCD\u2019s role is to perform continuous deployment, ensuring that the cluster is always kept up-to-date with the latest built application. This is the next environment after stage where the deliverable is finally deployed. Once the deliverable is successfully deployed, it will be made available to the end user.","title":"Promoting across environments"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#promoting-across-environments","text":"","title":"Promoting across environments"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#overview","text":"Audience : Architects, Application developers Promoting across different environments means typically we move the services or applications from one level to the other. For instance, we start with development environment. This will eventually be promoted to staging for testing once the developers make sure things are working fine in development environment. After thorough testing, this will be promoted to production environment which will be the final environment that will be used by the intended users. In this topic, we're going to: Deployment environments Promoting deliverables across environments CI Environment Development Environment Functional Testing Staging Environment Performance Testing Production Environment","title":"Overview"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#deployment-environments","text":"For the basic promotion process, in our CI/CD process, typical environments include development, staging and production. Also, we are using a dedicated environment to run our CI/CD pipelines. These environments are defined as distinct projects within a cluster.","title":"Deployment environments"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#dev-stage-and-prod-environments","text":"Dev environment is the primary environment and it is used for sandboxing. During the development process, developers use this environment to test their code. This will allow them to thoroughly test the sample MQ application code, as well as the infrastructure code which contains the Queue Manager configurations. Staging environment is sort of pre prod environment. All the sample MQ application deliverables that are ready to be pushed to the prod environment should be thouroughly tested here. Prod environment will host all live services that belong to the MQ workloads. These deliverables will be consumed by the end user.","title":"Dev, Stage and Prod environments"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#ci-environment","text":"CI environment is used run all the MQ related pipelines. This includes all the tasks (custom / reusable task from cloud native toolkit ), pipelines, necessary triggers etc.","title":"CI environment"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#promoting-deliverables-across-environments","text":"For our sample use case, end to end basic promotion process is as follows. For this sample usecase, we are maintaining two source code repositories for maintaining Queue Manager and a sample application that helps us to demonstrate the work flow. Webhooks are configured for all these repositories hooking them up with corresponding pipelines. If there are any updates to the source code, you can push changes to the respective repositories. When the changes are pushed, underlying webhooks will trigger corresponding pipelines. When we make changes to the mq-infra repository, it will trigger mq-infra-dev pipeline. Similarly, when we changes to the mq-spring-app repository, it will trigger mq-spring-app-dev pipeline. Once these pipelines are run successfully, the updated resources are pushed to the dev destination in the Gitops Apps repository. Subsequently, a dedicated ArgoCD application watches for the updates, which it deploys to the cluster in the DEV environment. Once the deployment is successful, the ibm-mq-promote-dev-stage pipeline will be triggered. If ibm-mq-promote-dev-stage pipeline runs successfully, it will make changes to the staging resources and leave a Pull Request in the Gitops Apps repository. Then the Pull Request will be reviewed manually and the changes to the staging resources are merged into the Gitops Apps repository. Subsequently, a dedicated ArgoCD application watches for the updates, which it deploys to the cluster in the STAGING environment. Once the deployment is successful, the ibm-mq-promote-stage-prod pipeline will be triggered. If ibm-mq-promote-stage-prod pipeline runs successfully, it will make changes to the prod resources and leave a Pull Request in the Gitops Apps repository. Then the Pull Request will be reviewed manually and the changes to the prod resources are merged into the Gitops Apps repository. Subsequently, a dedicated ArgoCD application watches for the updates, which it deploys to the cluster in the PROD environment.","title":"Promoting deliverables across environments"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#ci-environment_1","text":"CI environment is our sample Continuous Integration space where we hosted all the necessary resources. This environment holds things like all the pipelines, tasks, triggers, etc.","title":"CI Environment"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#available-pipelines","text":"Below are the four pipelines that are being used by the promotion process for the sample MQ workflow.","title":"Available pipelines"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#development-environment","text":"When the changes are made to the source repositories, respective pipelines will run and build the necessary deliverables. These deliverables will be deployed to the development environment and will be subjected to first phase of testing allowing to validate the deployment. This deployment in development environment is done by ArgoCD. ArgoCD\u2019s role is to perform continuous deployment, ensuring that the cluster is always kept up-to-date with the latest built application. This is the first environment where the deliverable is deployed. If this deployment is a success, then this will be the first step that ensures that this deliverable can be promoted to production at some point of time. Once the deliverable is successfully deployed, we can validate different properties of the system which may functional or non-functional. Not only, we can also validate the configurations of the deliverable. In our sample promotion process, we showed an example of functional tests that are part of functional test pipeline. Depending on the usecases this pipeline can be further extended or modified. If everything works fine, we can promote the deliverable to next environment and deploy it there.","title":"Development Environment"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#functional-testing","text":"","title":"Functional Testing"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#functional-test-pipeline","text":"dev-instance-test Clones the source repositories. Grabs the route of the deliverable. Performs functional testing using newman test scripts. Finally, returns the name of the deliverable. gitops Clones the gitops apps repository. Copies the deliverables from dev to staging. Updates the helm chart with latest resources. Generate a Pull Request with the changes to gitops apps repository.","title":"Functional test Pipeline"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#functional-test-task","text":"In our sample functional test task, we are using Newman as our functional testing tool. Note The contents in this guide are only examples. We demonstrated some of the practices using some sample usecases. For real world scenarios, it should be used in conjunction with existing QA practices and this content is not a replacement. For assistance, CSMs should reach out to QA SMEs.","title":"Functional Test Task"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#staging-environment","text":"When the Pull Request with the changes in staging resources get merged into the GitOps apps repository, the deliverables for staging environment will be updated. These deliverables will be deployed to the stage environment and will be subjected to second phase of testing allowing to validate the deployment. This deployment in staging environment is done by ArgoCD. ArgoCD\u2019s role is to perform continuous deployment, ensuring that the cluster is always kept up-to-date with the latest built application. This is the next environment after dev where the deliverable is deployed and is exact replica of production environment. If this deployment is a success, then this will ensure that this deliverable can be promoted to production environment. Once the deliverable is successfully deployed, we can perform different tests like User Acceptance Tests (UAT), load/stress testing, chaos engineering tests etc. In our sample promotion process, we showed an example of load tests that are part of performance test pipeline. Depending on the usecases this pipeline can be further extended or modified. If everything works fine, we can promote the deliverable to next environment and deploy it there.","title":"Staging Environment"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#performance-testing","text":"","title":"Performance Testing"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#performance-test-pipeline","text":"stage-instance-test Clones the source repositories. Grabs the route of the deliverable. Performs load testing using jmeter test scripts. Finally, returns the name of the deliverable. gitops Clones the gitops apps repository. Copies the deliverables from staging to prod. Updates the helm chart with latest resources. Generate a Pull Request with the changes to gitops apps repository.","title":"Performance test Pipeline"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#performance-test-task","text":"In our sample performance test task, we are using Jmeter as our performance load testing tool. The live data from Jmeter will be fed into a datastore. Based on this data, the metrics can be visualized using Grafana dashboard. Note The contents in this guide are only examples. We demonstrated some of the practices using some sample usecases. For real world scenarios, it should be used in conjunction with existing performance practices and this content is not a replacement. For assistance, CSMs should reach out to performance SMEs.","title":"Performance Test Task"},{"location":"guides/cp4i/mq/cloud-native/promote-environments/#production-environment","text":"When the Pull Request with the changes in prod resources get merged into the GitOps apps repository, the deliverables for prod environment will be updated. These deliverables will be deployed to the prod environment. This deployment in production environment is done by ArgoCD. ArgoCD\u2019s role is to perform continuous deployment, ensuring that the cluster is always kept up-to-date with the latest built application. This is the next environment after stage where the deliverable is finally deployed. Once the deliverable is successfully deployed, it will be made available to the end user.","title":"Production Environment"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/","text":"Promoting from Dev to Staging environment \u00b6 Overview \u00b6 Audience : Application developers, Administrators In the previous topic of this chapter, we ran the application pipeline using the source repository for MQ sample application. The pipeline successfully built and tested the application as well as creating versioned resources in the image registry, Artifactory and the GitOps repository. In this topic, you will set up a promotion pipeline that promotes the Queue Manager and MQ sample application from dev environment to staging environment. We'll activate the ArgoCD application that will watch the GitOps folder containing the Helm charts and use it and its dependent resources to deploy a running queue manager and sample application to the staging namespace in the cluster. Look at the following diagram: We've highlighted the components we're going to explore in this topic: The promotion pipeline from dev to staging will perform a set of tasks for functional testing. If successful, the pipeline leaves a PR in the GitOps apps repository. The GitOps apps repository will have a PR for the latest good build and test represented as a Helm chart for staging environment. This chart will be subsequently used by ArgoCD to deploy to the cluster. An ArgoCD application will monitor the GitOps folder where its Helm chart is held. Whenever this folder is updated, this ArgoCD application will apply these updates to the cluster, resulting in a new deployment. The instance of Queue Manager running in the cluster is active staging queue manager ready for use by MQ applications under staging. The instance of MQ Sample Application running in the cluster is active staging sample application under staging. In this topic, we're going to: Access the promotion pipeline Run the promotion pipeline for Queue Manager Run the promotion pipeline for MQ Sample Application Explore the promotion pipeline. Activate ArgoCD application for Queue Manager and MQ Sample Application for staging namespace. Review the resultant GitOps application folders for Queue Manager and MQ Sample Application . Examine the activated ArgoCD application that deploys the Queue Manager and MQ Sample Application to the cluster. Validate Queue Manager and MQ Sample Application . By the end of this topic we'll have a fully functioning dev to staging promotion pipeline that we can use to promote queue manager and sample application from dev environment to staging environment. Pre-requisites \u00b6 Before attempting this topic, you should have successfully completed the previous chapter . Access the promotion pipeline \u00b6 As we can see in the diagram above, the promotion pipeline is responsible for promoting the Queue Manager and MQ Sample application from dev to staging . If successful, it leaves a PR with the updated resources for staging in the GitOps apps repository, which are used by ArgoCD to deploy the updated Queue Manager and application to the cluster. It's usually the case a pipeline runs automatically when the Queue Manager or the Sample application is successfully deployed in dev environment. However, our first pipeline run is manual so that you can be in control, making it easier to understand what's happening. Locate the ci pipelines in the web console Let's find the promotion pipeline using the OpenShift web console. Navigate to Pipelines->Pipelines in the left hand pane, and select Project: ci , to show all pipelines in the ci namespace: You can see all the pipelines that we installed into the ci namespace in the previous chapter. We'll use different pipelines for different activities throughout the tutorial. For this topic, we're going to use the ibm-mq-promote-dev-stage pipeline. When the Queue Manager or Sample application are successfully deployed to the dev namespace, this pipeline will perform functional testing and on a successful run, it will promote the existing resources in dev to the staging namespace. The ibm-mq-promote-dev-stage promotion pipeline Select the ibm-mq-promote-dev-stage pipeline from the list of all pipelines: Like all pipelines, ibm-mq-promote-dev-stage is composed from a set of tasks : dev-instance-tests gitops The task name often provides a strong hint of each task's specific function. We'll examine these tasks in detail as the pipeline runs. The oc command as an alternative to the Web console As well as using the OpenShift web console, you can also interact with pipeline using the oc or tekton commands. Ensure you're logged in to cluster, and issue the following command to list the ibm-mq-promote-dev-stage pipeline details: oc get pipeline ibm-mq-promote-dev-stage -n ci which shows a brief summary of the pipeline: NAME AGE ibm-mq-promote-dev-stage 4d17h You can get more detail by adding the -o yaml option; we'll do that later. Run the promotion pipeline \u00b6 Queue Manager \u00b6 Configure your first pipeline run In pipeline details view above, you'll see an Actions button. Select Start to configure a pipeline run. You'll be presented with the following dialog: The supplied arguments allow the user of the pipeline to configure its behavior. For example, a user can use this pipeline with different queue manager source repositories. Configure the run as follows: Set git-url to your fork of the mq-infra repository Set git-revision to master . Set src-environment to dev . Set dest-environment to staging . Set the app-path pointing to your GitOps apps repository which in our case is mq/environments . Set git-pr to true . Set test-file pointing to your postman collections . If your source repository don't have any postman collections, just leave the defaults. Note At the moment, Queue Manager mq-infra repository do not have any functional tests defined in the form of Postman Collections. This pipeline is designed in a way to skip this tests if the definitions are not present. However, MQ Sample application mq-spring-app repository have these tests defined and we will be coming across it in the later section of this tutorial. Hit Start to start the pipeline run. You can also use the command line to run a pipeline; we'll explore that option later. Watch a pipeline run executing The pipeline run has now started. Notice how the view changes to Pipeline Run details : We're now looking at the live output from a pipeline run, rather than the pipeline used to create the run. Notice that the pipeline run name ibm-mq-promote-dev-stage-3269qi is based on the pipeline name -- with a unique suffix. Every new pipeline run will have a unique name. See also how the first dev-instance-tests task is running, while the remaining tasks are waiting to start. Hover over dev-instance-tests task and you will see the steps that comprise it. Watch pipeline steps complete As the pipeline run proceeds, notice all the steps got completed: This pipeline will take about a minute or two to complete. MQ Sample Application \u00b6 Configure your first pipeline run In pipeline details view above, you'll see an Actions button. Select Start to configure a pipeline run. You'll be presented with the following dialog: The supplied arguments allow the user of the pipeline to configure its behavior. For example, a user can use this pipeline with different sample application source repositories. Configure the run as follows: Set git-url to your fork of the mq-spring-app repository Set git-revision to master . Set src-environment to dev . Set dest-environment to staging . Set the app-path pointing to your GitOps apps repository which in our case is mq/environments . Set git-pr to true . Set test-file pointing to your postman collections . If your source repository don't have any postman collections, just leave the defaults. mq-spring-app repository have the postman collections defined at postman/mq-spring-app.postman_collection.json . Hit Start to start the pipeline run. You can also use the command line to run a pipeline; we'll explore that option later. Watch a pipeline run executing The pipeline run has now started. Notice how the view changes to Pipeline Run details : We're now looking at the live output from a pipeline run, rather than the pipeline used to create the run. Notice that the pipeline run name ibm-mq-promote-dev-stage-fzgqmv is based on the pipeline name -- with a unique suffix. Every new pipeline run will have a unique name. See also how the first dev-instance-tests task is running, while the remaining tasks are waiting to start. Hover over dev-instance-tests task and you will see the steps that comprise it. Watch pipeline steps complete As the pipeline run proceeds, notice all the steps got completed: This pipeline will take about a minute or two to complete. Let's explore its tasks in a little more detail. Explore the promotion pipeline \u00b6 Let's look more closely at how the ibm-mq-promote-dev-stage pipeline is structured. Let's also examine the tasks and steps that make up the pipeline, and how they progressively validate the deployments in dev namespace, resulting in the production of a Helm chart ready for deployment in staging namespace. Pipeline, Task, Step Let's start with the pipeline, its tasks and steps. Hover over the dev-instance-tests task: See how our pipeline is made up of tasks such as dev-instance-tests and gitops . These run in the order defined by the pipeline. Our pipeline is linear, though Tekton supports more sophisticated pipeline graphs if necessary. See how each task comprises a set of steps such as git-clone or setup . These run in the order defined by the task. The pipeline run logs When a pipeline runs, all its output is captured in a set of logs, one for each task. Click on the dev-instance-tests task to show its logs: ( Alternatively, you can select the Logs tab from the UI, and then select the tasks on the left pane within the pipeline run view. ) See how the dev-instance-tests task has its output in a dedicated log. You can select any log for any task that has completed or is executing. When a pipeline run completes, all its logs remain available, which can help diagnosing problems for example. Exploring an example task output: dev-instance-tests It's easy to examine a task log; you simply select the relevant task and scroll the log up or down. For active tasks the log will be dynamically updated. Click on the dev-instance-tests task: This console window shows the output generated by dev-instance-tests task. As the task script proceeds, its output is captured; that's what we can see in this window. Notice that the dev-instance-tests task output is from the first step in the dev-instance-tests task. This step is called STEP-GIT-CLONE . Note how the step names are capitalized in the web console output. Let's look at another task and its steps more closely. Exploring a task step in detail : STEP-RUN-TEST A task is built of multiple steps. Let's explore the dev-instance-tests task and its step-run-test step. Select the dev-instance-tests task and scroll through its logs to see its third step , STEP-RUN-TEST : See how the step-run-test output is captured in the same log as the previous step git-clone . The pipeline definition Up to this point, we've examined the pipeline run and the logs it generates. Let's now look at how a pipeline is defined. Issue the following command to show the ibm-mq-promote-dev-stage pipeline: oc describe pipeline ibm-mq-promote-dev-stage -n ci which shows the pipeline YAML in considerable detail: Name : ibm-mq-promote-dev-stage Namespace : ci Labels : app.kubernetes.io/instance=apps-mq-rest-ci-1 Annotations : app.openshift.io/runtime : test API Version : tekton.dev/v1beta1 Kind : Pipeline Metadata : Creation Timestamp : 2021-09-23T17:29:24Z Generation : 2 Managed Fields : API Version : tekton.dev/v1beta1 Fields Type : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : f:app.openshift.io/runtime : f:kubectl.kubernetes.io/last-applied-configuration : f:labels : . : f:app.kubernetes.io/instance : f:spec : . : f:params : f:tasks : Manager : argocd-application-controller Operation : Update Time : 2021-09-23T17:29:24Z Resource Version : 10006118 Self Link : /apis/tekton.dev/v1beta1/namespaces/ci/pipelines/ibm-mq-promote-dev-stage UID : b9adbe4c-e88f-49f4-867a-986c4304df19 Spec : Params : Description : The url for the git repository Name : git-url Type : string Default : master Description : git branch for the test app Name : git-revision Type : string Default : dev Description : environment Name : src-environment Type : string Default : staging Description : environment Name : dest-environment Type : string Default : mq/environments Description : Path in gitops repo Name : app-path Type : string Default : true Description : Enable the pipeline to do a PR for the gitops repo Name : git-pr Type : string Default : postman/test.json Description : Path of the postman collection (postman/mq-spring-app.postman_collection.json) Name : test-file Type : string Tasks : Name : dev-instance-tests Params : Name : git-url Value : $(params.git-url) Name : src-environment Value : $(params.src-environment) Name : test-file Value : $(params.test-file) Task Ref : Kind : Task Name : ibm-functional-test Name : gitops Params : Name : app-name Value : $(tasks.dev-instance-tests.results.app-name) Name : src-environment Value : $(params.src-environment) Name : dest-environment Value : $(params.dest-environment) Name : app-path Value : $(params.app-path) Name : git-pr Value : $(params.git-pr) Run After : dev-instance-tests Task Ref : Kind : Task Name : ibm-gitops Events : <none> Don't be intimidated by this output -- it's actually just a more detailed source view of the information shown for the ibm-mq-promote-dev-stage pipeline in the web console. Locate the following key structures in the Pipeline YAML: API Version: tekton.dev/v1beta1 and Kind: Pipeline identify this as a Tekton pipeline. Spec: Params identifies the pipeline input parameters Tasks: is a list of the tasks in this pipeline, each of which has A Name: naming the task A set of Params: identifying the task input parameters An optional Run After: value indicating when the task is run A Task Ref: identifying the actual task code to be run using the task's input Notice that there's no code in the pipeline definition; instead, the definition specifies the required inputs to the pipeline, as well as the set of required tasks and their order of execution. The code executed by each task is identified by Task Ref: , rather than in the pipeline; the pipeline definition merely defines the order of task execution and how parameters are marshaled between tasks. Let's now examine the pipeline definition in a little more detail. The pipeline input Spec: Params When we configure a pipeline run, the arguments map precisely to Spec: Params in the pipeline YAML file. Below, we've just shown the Spec: Params: for the ibm-mq-promote-dev-stage pipeline: Spec : Params : Description : The url for the git repository Name : git-url Type : string Default : master Description : git branch for the test app Name : git-revision Type : string Default : dev Description : environment Name : src-environment Type : string Default : staging Description : environment Name : dest-environment Type : string Default : mq/environments Description : Path in gitops repo Name : app-path Type : string Default : true Description : Enable the pipeline to do a PR for the gitops repo Name : git-pr Type : string Default : postman/test.json Description : Path of the postman collection (postman/mq-spring-app.postman_collection.json) Name : test-file Type : string Spend a few moments mapping each of these parameters maps to those on the Start Pipeline input dialog where you specified the pipeline run arguments. For example, map Name: , Description: and Default: to the different fields in the dialog. Parameters for the first setup task We can see that the first task in the pipeline is called dev-instance-tests . Let's examine its YAML to see how it gets its input parameters: Tasks : Name : dev-instance-tests Params : Name : git-url Value : $(params.git-url) Name : src-environment Value : $(params.src-environment) Name : test-file Value : $(params.test-file) Task Ref : Kind : Task Name : ibm-functional-test See how the dev-instance-tests task derives its git-url parameter using the pipeline input parameter value $(params.git-url) . See how src-environment and test-file work in a similar way. The first task in a pipeline typically works like this -- its parameters are mapped from the pipeline's input parameters. Notice also that some pipeline input parameters are not referred to by the dev-instance-tests task; they will be used by subsequent tasks using the appropriate $(params.) value. Passing arguments between tasks As each task completes, the pipeline proceeds. When a new task starts it often requires one or more results generated by a previous task. We can see a good example of this in the gitops task: Name : gitops Params : Name : app-name Value : $(tasks.dev-instance-tests.results.app-name) Name : src-environment Value : $(params.src-environment) Name : dest-environment Value : $(params.dest-environment) Name : app-path Value : $(params.app-path) Name : git-pr Value : $(params.git-pr) Run After : dev-instance-tests Task Ref : Kind : Task Name : ibm-gitops See how the gitops task specifies that the app-name parameter should use value generated by the dev-instance-tests task using the syntax: $(tasks.dev-instance-tests.results.app-name) . Also notice how the gitops tasks uses Run After: to specify that it should execute after the dev-instance-tests task. This follows the Kubernetes idiom of resources being declarative -- the order of execution is defined by RunAfter: rather than the order in which tasks appear in the YAML. Again, notice that the gitops task doesn't contain the code that the task executes. This is contained in Task Ref: which identifies ibm-gitops as the task to execute using the specified parameters. It's the code in ibm-gitops which generates the log output for the task; we'll examine it later. The way pipelines tasks are designed makes them highly reusable. As we'll see later, tasks are written with defined inputs and outputs such that they can be re-used by different pipelines. Pipelines focus on organizing the order of task execution and how parameters are marshaled into and between tasks; it's the tasks that do the actual work. Locating the pipeline and tasks source YAMLs Finally, let's locate the source for the ibm-mq-promote-dev-stage pipeline and the tasks within it. It is located in the following folder: cd $HOME /git cd multi-tenancy-gitops-apps tree mq/environments/ci/pipelines/ We can see the other pipelines for the ci namespace in this folder: mq/environments/ci/pipelines/ \u251c\u2500\u2500 ibm-test-pipeline-for-dev.yaml \u251c\u2500\u2500 ibm-test-pipeline-for-stage.yaml \u251c\u2500\u2500 java-maven-dev-pipeline.yaml \u251c\u2500\u2500 mq-pipeline-dev.yaml \u2514\u2500\u2500 mq-spring-app-dev-pipeline.yaml These map to the MQ-related pipelines we saw in the Pipelines->Pipelines view in the web console. Exploring the ibm-mq-promote-dev-stage source YAML View the source for the ibm-test-pipeline-for-dev.yaml pipeline with the command: cat mq/environments/ci/pipelines/ibm-test-pipeline-for-dev.yaml which shows the source YAML for the ibm-mq-promote-dev-stage pipeline: apiVersion : tekton.dev/v1beta1 kind : Pipeline metadata : name : ibm-mq-promote-dev-stage annotations : app.openshift.io/runtime : test spec : params : - name : git-url description : The url for the git repository - name : git-revision description : git branch for the test app default : master - name : src-environment description : environment default : dev - name : dest-environment description : environment default : staging - name : app-path description : Path in gitops repo default : mq/environments - name : git-pr description : Enable the pipeline to do a PR for the gitops repo default : \"true\" - name : test-file description : Path of the postman collection (postman/mq-spring-app.postman_collection.json) default : \"postman/test.json\" tasks : - name : dev-instance-tests taskRef : name : ibm-functional-test params : - name : git-url value : \"$(params.git-url)\" - name : src-environment value : \"$(params.src-environment)\" - name : test-file value : \"$(params.test-file)\" - name : gitops taskRef : name : ibm-gitops runAfter : - dev-instance-tests params : - name : app-name value : \"$(tasks.dev-instance-tests.results.app-name)\" - name : src-environment value : \"$(params.src-environment)\" - name : dest-environment value : \"$(params.dest-environment)\" - name : app-path value : \"$(params.app-path)\" - name : git-pr value : \"$(params.git-pr)\" This YAML is slightly different to the output of the oc get pipeline command, because extra information is added during deployment such as Creation Timestamp: . Finding the ArgoCD application that manages pipelines You can see how the ibm-mq-promote-dev-stage and other pipeline YAMLs were deployed by examining the ArgoCD application that watches the folder containing the ci namespace pipelines. Issue the following command: cat mq/config/argocd/ci/ci-app-rest.yaml which shows the apps-mq-rest-ci-1 ArgoCD application that watches for updates: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : apps-mq-rest-ci-1 annotations : argocd.argoproj.io/sync-wave : \"300\" finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : ci server : https://kubernetes.default.svc project : applications source : path : mq/environments/ci repoURL : https://github.com/hp-gitops-test/multi-tenancy-gitops-apps targetRevision : master syncPolicy : automated : prune : true selfHeal : true See how this apps-mq-rest-ci-1 watches: path: mq/environments/ci . As we know, this folder contains the YAML for the ibm-mq-promote-dev-stage and other pipelines under pipelines folder. When this ArgoCD application was made active in the cluster, it installed all the pipelines along with other resources in this folder. Activate ArgoCD Applications \u00b6 We're now going to activate the ArgoCD applications to manage the deployment of Queue Manager and MQ Sample application to the staging namespace. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Select resources to deploy Access the 0-bootstrap/single-cluster/3-apps/kustomization.yaml : cat 0 -bootstrap/single-cluster/3-apps/kustomization.yaml Open 0-bootstrap/single-cluster/3-apps/kustomization.yaml and uncomment the below: - argocd/mq/stage.yaml Once modified, it should be something like below: resources: #- argocd/ace/cicd.yaml #- argocd/ace/dev.yaml #- argocd/ace/stage.yaml #- argocd/ace/prod.yaml - argocd/mq/cicd.yaml - argocd/mq/dev.yaml - argocd/mq/stage.yaml #- argocd/mq/prod.yaml #- argocd/apic/cicd.yaml #- argocd/apic/dev.yaml #- argocd/apic/stage.yaml #- argocd/apic/prod.yaml #- argocd/bookinfo/cicd.yaml #- argocd/bookinfo/dev.yaml #- argocd/bookinfo/stage.yaml #- argocd/bookinfo/prod.yaml #- argocd/soapserver/soapserver.yaml patches: - target: group: argoproj.io kind: Application labelSelector: \"gitops.tier.layer=applications\" patch: | - - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD applications application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Activating staging instance for queue manager and sample app\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11 , done . Counting objects: 100 % ( 11 /11 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 6 /6 ) , done . Writing objects: 100 % ( 6 /6 ) , 517 bytes | 517 .00 KiB/s, done . Total 6 ( delta 5 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 5 /5 ) , completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 2de3f9c..3225847 master -> master This change to the GitOps repository can now be used by ArgoCD. The applications argocd application Let's examine the ArgoCD application that manage the applications in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the applications application: Review the resultant GitOps application folders \u00b6 The result of our previously successful ibm-mq-promote-dev-stage pipeline runs was to leave a PR in a GitOps folder for Queue Manager and MQ Sample application . This PR create a Helm chart if one is not present in the staging folder. If the Helm chart is already there, it updates the version in the requirements.yaml to match the latest deployment currently residing in the dev namespace. Ensure you've set up the $GIT_ORG environment variable This chapter also uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Open a new terminal window . Replace <git-org> in the following command with your GitHub user name: export GIT_ORG = <git-org> You can verify your $GIT_ORG as follows: echo $GIT_ORG which will show your GitHub Organization, for example: prod-ref-guide Grab the multi-tenancy-gitops-apps repository url Issue the below command to get the GitOps apps repository url. echo https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps/pulls which will give you an url, for instance: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps/pulls Review the Pull Requests created by ibm-mq-promote-dev-stage pipeline Open the above url in your browser and you will see Pull Requests generated by the ibm-mq-promote-dev-stage pipeline. Merge these Pull Requests. Re-merging local clone to view staging resources in GitOps apps repository The ibm-mq-promote-dev-stage pipeline left a Pull request and we merged the changes in the previous step. This means that our local clone of the GitOps repository is one commit behind GitHub. To allow us to view the changes locally, we must re-merge our local branch with GitHub. Return to the terminal window you're using for the multi-tenancy-gitops-apps GitOps apps repository. Issue the following command to change to your GitOps apps repository: cd $HOME /git cd multi-tenancy-gitops-apps Issue the following commands to merge the local branch: git fetch origin git merge origin/ $GIT_BRANCH which shows our local branch being updated: Updating 3be3b10..ea6706d Fast-forward mq/environments/staging/mq-infra/Chart.yaml | 4 ++++ mq/environments/staging/mq-infra/requirements.yaml | 4 ++++ mq/environments/staging/mq-infra/values.yaml | 3 +++ mq/environments/staging/mq-spring-app/Chart.yaml | 4 ++++ mq/environments/staging/mq-spring-app/requirements.yaml | 4 ++++ mq/environments/staging/mq-spring-app/values.yaml | 3 +++ 6 files changed, 22 insertions ( + ) create mode 100644 mq/environments/staging/mq-infra/Chart.yaml create mode 100644 mq/environments/staging/mq-infra/requirements.yaml create mode 100644 mq/environments/staging/mq-infra/values.yaml create mode 100644 mq/environments/staging/mq-spring-app/Chart.yaml create mode 100644 mq/environments/staging/mq-spring-app/requirements.yaml create mode 100644 mq/environments/staging/mq-spring-app/values.yaml Notice how these files correspond to the new Helm chart created in the GitOps apps repository by the ibm-mq-promote-dev-stage pipeline run. Explore the Helm chart in the GitOps Apps repository Let's examine the newly produced Helm charts in the GitOps apps repository; it was created by the previous pipeline runs. Issue the following command: tree mq/environments/staging/mq-infra/ which shows the newly produced Helm chart: mq/environments/staging/mq-infra/ \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml Notice that: The chart for Queue Manager was created in the mq/environments/ folder to reflect the fact this queue manager is part of the applications layer. The chart was created in the staging subfolder to reflect the fact that it's going to be deployed to the staging namespace. The chart was created in a new folder /mq-infra . This folder is dedicated to Queue Manager . Issue the following command: tree mq/environments/staging/mq-spring-app/ which shows the newly produced Helm chart: mq/environments/staging/mq-spring-app/ \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml Notice that: The chart for MQ Sample application was created in the mq/environments/ folder to reflect the fact this application is part of the applications layer. The chart was created in the staging subfolder to reflect the fact that it's going to be deployed to the staging namespace. The chart was created in a new folder /mq-spring-app . This folder is dedicated to MQ Sample application . Examine the activated ArgoCD applications \u00b6 We're now going to examine the activated ArgoCD applications that uses the above Helm charts to manage the deployment of Queue Manager and MQ Sample application to the staging namespace. Queue Manager \u00b6 The ArgoCD application for Queue Manager QM1 has its deployment to the cluster managed by a dedicated ArgoCD application called staging-mq-infra-instance . Issue the following command to show the ArgoCD application details: cat mq/config/argocd/staging/staging-mq-infra-instance.yaml which shows a YAML file typical of those we've seen before: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: staging-mq-infra-instance annotations: argocd.argoproj.io/sync-wave: \"300\" finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: namespace: staging server: https://kubernetes.default.svc project: applications source: path: mq/environments/staging/mq-infra repoURL: https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps targetRevision: master helm: valueFiles: - values.yaml syncPolicy: automated: prune: true selfHeal: true syncOptions: - Replace = true See how the Helm chart we explored in the previous section of this topic is referenced by path: mq/environments/staging/mq-infra : mq/environments/staging/mq-infra \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml The ArgoCD application applies this Helm chart to the cluster to instantiate QM1 as a set of cluster resources. Look at active Queue Manager ArgoCD application Let's examine QM1 and its Kubernetes resources using the ArgoCD UI. In the ArgoCD UI search the Applications view with the keyword staging-mq-infra : ( You may need to launch the ArgoCD UI again. Refer to these instructions . ) View the new Queue Manager Kubernetes resources We can look at the deployed instance of QM1 and its dependent kubernetes resources. Click on the staging-mq-infra-instance ArgoCD application: MQ Sample Application \u00b6 The ArgoCD application for MQ Sample Application MQ Sample Application has its deployment to the cluster managed by a dedicated ArgoCD application called staging-mq-spring-app-instance . Issue the following command to show the ArgoCD application details: cat mq/config/argocd/staging/staging-mq-spring-app-instance.yaml which shows a YAML file typical of those we've seen before: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: staging-mq-spring-app-instance annotations: argocd.argoproj.io/sync-wave: \"300\" finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: namespace: staging server: https://kubernetes.default.svc project: applications source: path: mq/environments/staging/mq-spring-app repoURL: https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps targetRevision: master helm: valueFiles: - values.yaml syncPolicy: automated: prune: true selfHeal: true See how the Helm chart we explored in the previous section of this topic is referenced by path: mq/environments/staging/mq-spring-app : mq/environments/staging/mq-spring-app \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml The ArgoCD application applies this Helm chart to the cluster to instantiate MQ Sample Application as a set of cluster resources. Look at active MQ Sample Application ArgoCD application Let's examine MQ Sample Application and its Kubernetes resources using the ArgoCD UI. In the ArgoCD UI search the Applications view with the keyword staging-mq-spring : ( You may need to launch the ArgoCD UI again. Refer to these instructions . ) View the new MQ Sample Application Kubernetes resources We can look at the deployed instance of MQ Sample Application and its dependent kubernetes resources. Click on the staging-mq-spring-app-instance ArgoCD application: Validate the Queue Manager \u00b6 Connect to the queue manager pod Let's connect to this container, so that we can explore the queue manager. Issue the following command: oc exec -n staging qm-dev-ibm-mq-0 -it -- /bin/bash to connect you to the pod default container, where you'll see the bash prompt: bash-4.4$ Your terminal is now connected to the container running the queue manager. Use MQ command line The container we've connected to has a fully running instance of the queue manager configured according to the YAML generated by the Helm chart. We can use regular MQ commands to verify that this is a regular queue manager. Issue the following command: dspmq -o all to see a full list of the queue managers running in the container: QMNAME ( QM1 ) STATUS ( Running ) DEFAULT ( yes ) STANDBY ( Permitted ) INSTNAME ( Installation1 ) INSTPATH ( /opt/mqm ) INSTVER ( 9 .2.3.0 ) ROLE ( Not configured ) INSTANCE () INSYNC () QUORUM () We can see that QM1 is running, it's the default queue manager, where it's installed in the container file system, and it's version. Display queue manager properties We can also run an MQSC command to display the queue manager properties. These properties were defined in the MQSC file. Issue the following command: runmqsc QM1 <<< \"dis qmgr\" to see the full set of queue manager properties for QM1 : 5724 -H72 ( C ) Copyright IBM Corp. 1994 , 2021 . Starting MQSC for queue manager QM1. 1 : dis qmgr AMQ8408I: Display Queue Manager details. QMNAME ( QM1 ) ACCTCONO ( DISABLED ) ACCTINT ( 1800 ) ACCTMQI ( OFF ) ACCTQ ( OFF ) ACTIVREC ( MSG ) ACTVCONO ( DISABLED ) ACTVTRC ( OFF ) ADVCAP ( ENABLED ) ALTDATE ( 2021 -09-29 ) ALTTIME ( 09 .20.18 ) AMQPCAP ( NO ) AUTHOREV ( DISABLED ) CCSID ( 819 ) CERTLABL ( ) CERTVPOL ( ANY ) CHAD ( DISABLED ) CHADEV ( DISABLED ) CHADEXIT ( ) CHLEV ( DISABLED ) CHLAUTH ( DISABLED ) CLWLDATA ( ) CLWLEXIT ( ) CLWLLEN ( 100 ) CLWLMRUC ( 999999999 ) CLWLUSEQ ( LOCAL ) CMDEV ( DISABLED ) CMDLEVEL ( 923 ) COMMANDQ ( SYSTEM.ADMIN.COMMAND.QUEUE ) CONFIGEV ( DISABLED ) CONNAUTH ( SYSTEM.DEFAULT.AUTHINFO.IDPWOS ) CRDATE ( 2021 -09-29 ) CRTIME ( 09 .20.14 ) CUSTOM ( ) DEADQ ( DEV.DEAD.LETTER.QUEUE ) DEFCLXQ ( SCTQ ) DEFXMITQ ( ) DESCR ( ) DISTL ( YES ) IMGINTVL ( 60 ) IMGLOGLN ( OFF ) IMGRCOVO ( YES ) IMGRCOVQ ( YES ) IMGSCHED ( MANUAL ) INHIBTEV ( DISABLED ) IPADDRV ( IPV4 ) LOCALEV ( DISABLED ) LOGGEREV ( DISABLED ) MARKINT ( 5000 ) MAXHANDS ( 256 ) MAXMSGL ( 4194304 ) MAXPROPL ( NOLIMIT ) MAXPRTY ( 9 ) MAXUMSGS ( 10000 ) MONACLS ( QMGR ) MONCHL ( OFF ) MONQ ( OFF ) PARENT ( ) PERFMEV ( DISABLED ) PLATFORM ( UNIX ) PSMODE ( ENABLED ) PSCLUS ( ENABLED ) PSNPMSG ( DISCARD ) PSNPRES ( NORMAL ) PSRTYCNT ( 5 ) PSSYNCPT ( IFPER ) QMID ( QM1_2021-09-29_09.20.14 ) REMOTEEV ( DISABLED ) REPOS ( ) REPOSNL ( ) REVDNS ( ENABLED ) ROUTEREC ( MSG ) SCHINIT ( QMGR ) SCMDSERV ( QMGR ) SPLCAP ( ENABLED ) SSLCRLNL ( ) SSLCRYP ( ) SSLEV ( DISABLED ) SSLFIPS ( NO ) SSLKEYR ( /run/runmqserver/tls/key ) SSLRKEYC ( 0 ) STATACLS ( QMGR ) STATCHL ( OFF ) STATINT ( 1800 ) STATMQI ( OFF ) STATQ ( OFF ) STRSTPEV ( ENABLED ) SUITEB ( NONE ) SYNCPT TREELIFE ( 1800 ) TRIGINT ( 999999999 ) VERSION ( 09020300 ) XRCAP ( NO ) One MQSC command read. No commands have a syntax error. All valid MQSC commands were processed. Note how CHLAUTH(DISABLED) has been set. This was set in the MQSC file configuration for QM1 in via the mqsc-configmap configmap. Feel free to run other runmqsc commands to explore the queue manager properties. Exit from queue manager container When we're finished exploring the queue manager container, we can exit it. Issue the following command: exit to return to your local machine's command prompt: exit .../git/multi-tenancy-gitops-apps Validate the Application \u00b6 Check the application is running Using the location value from the route we can call the application to check its health by appending /health , for example: export STAGING_APP_URL = $( oc get route -n staging mq-spring-app -o jsonpath = \"{.spec.host}\" ) curl -X GET https:// $STAGING_APP_URL /actuator/health { \"status\" : \"UP\" , \"components\" : { \"HeathEndPointMain\" : { \"status\" : \"UP\" } , \"diskSpace\" : { \"status\" : \"UP\" , \"details\" : { \"total\" :105552732160, \"free\" :74874388480, \"threshold\" :10485760, \"exists\" :true }} , \"jms\" : { \"status\" : \"UP\" , \"details\" : { \"provider\" : \"IBM MQ JMS Provider\" }} , \"livenessState\" : { \"status\" : \"UP\" } , \"ping\" : { \"status\" : \"UP\" } , \"readinessState\" : { \"status\" : \"UP\" }} , \"groups\" : [ \"liveness\" , \"readiness\" ]} Call the application to put a message to a queue Using the same location value we can call the application to to put a message to a queue on the queue manager, for example: curl -X GET https:// $STAGING_APP_URL /api/send-hello-world Call the application to get a message from a queue Using the same location value we can call the application to to get a message from a queue on the queue manager, for example: curl -X GET https:// $STAGING_APP_URL /api/recv Call the application to put a JSON message to a queue Using the same location value we can call the application to to put a message with a JSON payload to a queue on the queue manager, for example: curl -X POST -H \"Content-Type: application/json\" https:// $STAGING_APP_URL /api/send-json --data \"{\\\"data\\\":\\\"Hola Munda\\\"}\" Note the use of the POST verb in the above command. This can be followed with a call to get the message from the queue. Congratulations! You've completed your first run of the promotion pipeline from dev to staging. Feel free to run the ibm-mq-promote-dev-stage pipeline more than once to get a feeling for how it works. You've used it to promote an instance of QM1 and MQ sample application from dev environment to staging environment in the cluster. You've explored how the promotion pipeline for dev to staging is structured as tasks and steps. You've examined it's pipeline run log to understand how this pipeline works and how tasks are implemented. Finally, you've examined how the GitOps apps repository is updated with the staging resources from a successful run of the pipeline. You've completed set up continuous deployment for the Queue Manager as well as the MQ sample application and deployed their instances to the staging namespace in the cluster. You've interacted with this queue manager using the command line as well as verified the application. In the next topic of this chapter we're going to promote the changes from staging environment to prod environment.","title":"Promote from dev to staging"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/#promoting-from-dev-to-staging-environment","text":"","title":"Promoting from Dev to Staging environment"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/#overview","text":"Audience : Application developers, Administrators In the previous topic of this chapter, we ran the application pipeline using the source repository for MQ sample application. The pipeline successfully built and tested the application as well as creating versioned resources in the image registry, Artifactory and the GitOps repository. In this topic, you will set up a promotion pipeline that promotes the Queue Manager and MQ sample application from dev environment to staging environment. We'll activate the ArgoCD application that will watch the GitOps folder containing the Helm charts and use it and its dependent resources to deploy a running queue manager and sample application to the staging namespace in the cluster. Look at the following diagram: We've highlighted the components we're going to explore in this topic: The promotion pipeline from dev to staging will perform a set of tasks for functional testing. If successful, the pipeline leaves a PR in the GitOps apps repository. The GitOps apps repository will have a PR for the latest good build and test represented as a Helm chart for staging environment. This chart will be subsequently used by ArgoCD to deploy to the cluster. An ArgoCD application will monitor the GitOps folder where its Helm chart is held. Whenever this folder is updated, this ArgoCD application will apply these updates to the cluster, resulting in a new deployment. The instance of Queue Manager running in the cluster is active staging queue manager ready for use by MQ applications under staging. The instance of MQ Sample Application running in the cluster is active staging sample application under staging. In this topic, we're going to: Access the promotion pipeline Run the promotion pipeline for Queue Manager Run the promotion pipeline for MQ Sample Application Explore the promotion pipeline. Activate ArgoCD application for Queue Manager and MQ Sample Application for staging namespace. Review the resultant GitOps application folders for Queue Manager and MQ Sample Application . Examine the activated ArgoCD application that deploys the Queue Manager and MQ Sample Application to the cluster. Validate Queue Manager and MQ Sample Application . By the end of this topic we'll have a fully functioning dev to staging promotion pipeline that we can use to promote queue manager and sample application from dev environment to staging environment.","title":"Overview"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/#pre-requisites","text":"Before attempting this topic, you should have successfully completed the previous chapter .","title":"Pre-requisites"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/#access-the-promotion-pipeline","text":"As we can see in the diagram above, the promotion pipeline is responsible for promoting the Queue Manager and MQ Sample application from dev to staging . If successful, it leaves a PR with the updated resources for staging in the GitOps apps repository, which are used by ArgoCD to deploy the updated Queue Manager and application to the cluster. It's usually the case a pipeline runs automatically when the Queue Manager or the Sample application is successfully deployed in dev environment. However, our first pipeline run is manual so that you can be in control, making it easier to understand what's happening. Locate the ci pipelines in the web console Let's find the promotion pipeline using the OpenShift web console. Navigate to Pipelines->Pipelines in the left hand pane, and select Project: ci , to show all pipelines in the ci namespace: You can see all the pipelines that we installed into the ci namespace in the previous chapter. We'll use different pipelines for different activities throughout the tutorial. For this topic, we're going to use the ibm-mq-promote-dev-stage pipeline. When the Queue Manager or Sample application are successfully deployed to the dev namespace, this pipeline will perform functional testing and on a successful run, it will promote the existing resources in dev to the staging namespace. The ibm-mq-promote-dev-stage promotion pipeline Select the ibm-mq-promote-dev-stage pipeline from the list of all pipelines: Like all pipelines, ibm-mq-promote-dev-stage is composed from a set of tasks : dev-instance-tests gitops The task name often provides a strong hint of each task's specific function. We'll examine these tasks in detail as the pipeline runs. The oc command as an alternative to the Web console As well as using the OpenShift web console, you can also interact with pipeline using the oc or tekton commands. Ensure you're logged in to cluster, and issue the following command to list the ibm-mq-promote-dev-stage pipeline details: oc get pipeline ibm-mq-promote-dev-stage -n ci which shows a brief summary of the pipeline: NAME AGE ibm-mq-promote-dev-stage 4d17h You can get more detail by adding the -o yaml option; we'll do that later.","title":"Access the promotion pipeline"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/#run-the-promotion-pipeline","text":"","title":"Run the promotion pipeline"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/#queue-manager","text":"Configure your first pipeline run In pipeline details view above, you'll see an Actions button. Select Start to configure a pipeline run. You'll be presented with the following dialog: The supplied arguments allow the user of the pipeline to configure its behavior. For example, a user can use this pipeline with different queue manager source repositories. Configure the run as follows: Set git-url to your fork of the mq-infra repository Set git-revision to master . Set src-environment to dev . Set dest-environment to staging . Set the app-path pointing to your GitOps apps repository which in our case is mq/environments . Set git-pr to true . Set test-file pointing to your postman collections . If your source repository don't have any postman collections, just leave the defaults. Note At the moment, Queue Manager mq-infra repository do not have any functional tests defined in the form of Postman Collections. This pipeline is designed in a way to skip this tests if the definitions are not present. However, MQ Sample application mq-spring-app repository have these tests defined and we will be coming across it in the later section of this tutorial. Hit Start to start the pipeline run. You can also use the command line to run a pipeline; we'll explore that option later. Watch a pipeline run executing The pipeline run has now started. Notice how the view changes to Pipeline Run details : We're now looking at the live output from a pipeline run, rather than the pipeline used to create the run. Notice that the pipeline run name ibm-mq-promote-dev-stage-3269qi is based on the pipeline name -- with a unique suffix. Every new pipeline run will have a unique name. See also how the first dev-instance-tests task is running, while the remaining tasks are waiting to start. Hover over dev-instance-tests task and you will see the steps that comprise it. Watch pipeline steps complete As the pipeline run proceeds, notice all the steps got completed: This pipeline will take about a minute or two to complete.","title":"Queue Manager"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/#mq-sample-application","text":"Configure your first pipeline run In pipeline details view above, you'll see an Actions button. Select Start to configure a pipeline run. You'll be presented with the following dialog: The supplied arguments allow the user of the pipeline to configure its behavior. For example, a user can use this pipeline with different sample application source repositories. Configure the run as follows: Set git-url to your fork of the mq-spring-app repository Set git-revision to master . Set src-environment to dev . Set dest-environment to staging . Set the app-path pointing to your GitOps apps repository which in our case is mq/environments . Set git-pr to true . Set test-file pointing to your postman collections . If your source repository don't have any postman collections, just leave the defaults. mq-spring-app repository have the postman collections defined at postman/mq-spring-app.postman_collection.json . Hit Start to start the pipeline run. You can also use the command line to run a pipeline; we'll explore that option later. Watch a pipeline run executing The pipeline run has now started. Notice how the view changes to Pipeline Run details : We're now looking at the live output from a pipeline run, rather than the pipeline used to create the run. Notice that the pipeline run name ibm-mq-promote-dev-stage-fzgqmv is based on the pipeline name -- with a unique suffix. Every new pipeline run will have a unique name. See also how the first dev-instance-tests task is running, while the remaining tasks are waiting to start. Hover over dev-instance-tests task and you will see the steps that comprise it. Watch pipeline steps complete As the pipeline run proceeds, notice all the steps got completed: This pipeline will take about a minute or two to complete. Let's explore its tasks in a little more detail.","title":"MQ Sample Application"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/#explore-the-promotion-pipeline","text":"Let's look more closely at how the ibm-mq-promote-dev-stage pipeline is structured. Let's also examine the tasks and steps that make up the pipeline, and how they progressively validate the deployments in dev namespace, resulting in the production of a Helm chart ready for deployment in staging namespace. Pipeline, Task, Step Let's start with the pipeline, its tasks and steps. Hover over the dev-instance-tests task: See how our pipeline is made up of tasks such as dev-instance-tests and gitops . These run in the order defined by the pipeline. Our pipeline is linear, though Tekton supports more sophisticated pipeline graphs if necessary. See how each task comprises a set of steps such as git-clone or setup . These run in the order defined by the task. The pipeline run logs When a pipeline runs, all its output is captured in a set of logs, one for each task. Click on the dev-instance-tests task to show its logs: ( Alternatively, you can select the Logs tab from the UI, and then select the tasks on the left pane within the pipeline run view. ) See how the dev-instance-tests task has its output in a dedicated log. You can select any log for any task that has completed or is executing. When a pipeline run completes, all its logs remain available, which can help diagnosing problems for example. Exploring an example task output: dev-instance-tests It's easy to examine a task log; you simply select the relevant task and scroll the log up or down. For active tasks the log will be dynamically updated. Click on the dev-instance-tests task: This console window shows the output generated by dev-instance-tests task. As the task script proceeds, its output is captured; that's what we can see in this window. Notice that the dev-instance-tests task output is from the first step in the dev-instance-tests task. This step is called STEP-GIT-CLONE . Note how the step names are capitalized in the web console output. Let's look at another task and its steps more closely. Exploring a task step in detail : STEP-RUN-TEST A task is built of multiple steps. Let's explore the dev-instance-tests task and its step-run-test step. Select the dev-instance-tests task and scroll through its logs to see its third step , STEP-RUN-TEST : See how the step-run-test output is captured in the same log as the previous step git-clone . The pipeline definition Up to this point, we've examined the pipeline run and the logs it generates. Let's now look at how a pipeline is defined. Issue the following command to show the ibm-mq-promote-dev-stage pipeline: oc describe pipeline ibm-mq-promote-dev-stage -n ci which shows the pipeline YAML in considerable detail: Name : ibm-mq-promote-dev-stage Namespace : ci Labels : app.kubernetes.io/instance=apps-mq-rest-ci-1 Annotations : app.openshift.io/runtime : test API Version : tekton.dev/v1beta1 Kind : Pipeline Metadata : Creation Timestamp : 2021-09-23T17:29:24Z Generation : 2 Managed Fields : API Version : tekton.dev/v1beta1 Fields Type : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : f:app.openshift.io/runtime : f:kubectl.kubernetes.io/last-applied-configuration : f:labels : . : f:app.kubernetes.io/instance : f:spec : . : f:params : f:tasks : Manager : argocd-application-controller Operation : Update Time : 2021-09-23T17:29:24Z Resource Version : 10006118 Self Link : /apis/tekton.dev/v1beta1/namespaces/ci/pipelines/ibm-mq-promote-dev-stage UID : b9adbe4c-e88f-49f4-867a-986c4304df19 Spec : Params : Description : The url for the git repository Name : git-url Type : string Default : master Description : git branch for the test app Name : git-revision Type : string Default : dev Description : environment Name : src-environment Type : string Default : staging Description : environment Name : dest-environment Type : string Default : mq/environments Description : Path in gitops repo Name : app-path Type : string Default : true Description : Enable the pipeline to do a PR for the gitops repo Name : git-pr Type : string Default : postman/test.json Description : Path of the postman collection (postman/mq-spring-app.postman_collection.json) Name : test-file Type : string Tasks : Name : dev-instance-tests Params : Name : git-url Value : $(params.git-url) Name : src-environment Value : $(params.src-environment) Name : test-file Value : $(params.test-file) Task Ref : Kind : Task Name : ibm-functional-test Name : gitops Params : Name : app-name Value : $(tasks.dev-instance-tests.results.app-name) Name : src-environment Value : $(params.src-environment) Name : dest-environment Value : $(params.dest-environment) Name : app-path Value : $(params.app-path) Name : git-pr Value : $(params.git-pr) Run After : dev-instance-tests Task Ref : Kind : Task Name : ibm-gitops Events : <none> Don't be intimidated by this output -- it's actually just a more detailed source view of the information shown for the ibm-mq-promote-dev-stage pipeline in the web console. Locate the following key structures in the Pipeline YAML: API Version: tekton.dev/v1beta1 and Kind: Pipeline identify this as a Tekton pipeline. Spec: Params identifies the pipeline input parameters Tasks: is a list of the tasks in this pipeline, each of which has A Name: naming the task A set of Params: identifying the task input parameters An optional Run After: value indicating when the task is run A Task Ref: identifying the actual task code to be run using the task's input Notice that there's no code in the pipeline definition; instead, the definition specifies the required inputs to the pipeline, as well as the set of required tasks and their order of execution. The code executed by each task is identified by Task Ref: , rather than in the pipeline; the pipeline definition merely defines the order of task execution and how parameters are marshaled between tasks. Let's now examine the pipeline definition in a little more detail. The pipeline input Spec: Params When we configure a pipeline run, the arguments map precisely to Spec: Params in the pipeline YAML file. Below, we've just shown the Spec: Params: for the ibm-mq-promote-dev-stage pipeline: Spec : Params : Description : The url for the git repository Name : git-url Type : string Default : master Description : git branch for the test app Name : git-revision Type : string Default : dev Description : environment Name : src-environment Type : string Default : staging Description : environment Name : dest-environment Type : string Default : mq/environments Description : Path in gitops repo Name : app-path Type : string Default : true Description : Enable the pipeline to do a PR for the gitops repo Name : git-pr Type : string Default : postman/test.json Description : Path of the postman collection (postman/mq-spring-app.postman_collection.json) Name : test-file Type : string Spend a few moments mapping each of these parameters maps to those on the Start Pipeline input dialog where you specified the pipeline run arguments. For example, map Name: , Description: and Default: to the different fields in the dialog. Parameters for the first setup task We can see that the first task in the pipeline is called dev-instance-tests . Let's examine its YAML to see how it gets its input parameters: Tasks : Name : dev-instance-tests Params : Name : git-url Value : $(params.git-url) Name : src-environment Value : $(params.src-environment) Name : test-file Value : $(params.test-file) Task Ref : Kind : Task Name : ibm-functional-test See how the dev-instance-tests task derives its git-url parameter using the pipeline input parameter value $(params.git-url) . See how src-environment and test-file work in a similar way. The first task in a pipeline typically works like this -- its parameters are mapped from the pipeline's input parameters. Notice also that some pipeline input parameters are not referred to by the dev-instance-tests task; they will be used by subsequent tasks using the appropriate $(params.) value. Passing arguments between tasks As each task completes, the pipeline proceeds. When a new task starts it often requires one or more results generated by a previous task. We can see a good example of this in the gitops task: Name : gitops Params : Name : app-name Value : $(tasks.dev-instance-tests.results.app-name) Name : src-environment Value : $(params.src-environment) Name : dest-environment Value : $(params.dest-environment) Name : app-path Value : $(params.app-path) Name : git-pr Value : $(params.git-pr) Run After : dev-instance-tests Task Ref : Kind : Task Name : ibm-gitops See how the gitops task specifies that the app-name parameter should use value generated by the dev-instance-tests task using the syntax: $(tasks.dev-instance-tests.results.app-name) . Also notice how the gitops tasks uses Run After: to specify that it should execute after the dev-instance-tests task. This follows the Kubernetes idiom of resources being declarative -- the order of execution is defined by RunAfter: rather than the order in which tasks appear in the YAML. Again, notice that the gitops task doesn't contain the code that the task executes. This is contained in Task Ref: which identifies ibm-gitops as the task to execute using the specified parameters. It's the code in ibm-gitops which generates the log output for the task; we'll examine it later. The way pipelines tasks are designed makes them highly reusable. As we'll see later, tasks are written with defined inputs and outputs such that they can be re-used by different pipelines. Pipelines focus on organizing the order of task execution and how parameters are marshaled into and between tasks; it's the tasks that do the actual work. Locating the pipeline and tasks source YAMLs Finally, let's locate the source for the ibm-mq-promote-dev-stage pipeline and the tasks within it. It is located in the following folder: cd $HOME /git cd multi-tenancy-gitops-apps tree mq/environments/ci/pipelines/ We can see the other pipelines for the ci namespace in this folder: mq/environments/ci/pipelines/ \u251c\u2500\u2500 ibm-test-pipeline-for-dev.yaml \u251c\u2500\u2500 ibm-test-pipeline-for-stage.yaml \u251c\u2500\u2500 java-maven-dev-pipeline.yaml \u251c\u2500\u2500 mq-pipeline-dev.yaml \u2514\u2500\u2500 mq-spring-app-dev-pipeline.yaml These map to the MQ-related pipelines we saw in the Pipelines->Pipelines view in the web console. Exploring the ibm-mq-promote-dev-stage source YAML View the source for the ibm-test-pipeline-for-dev.yaml pipeline with the command: cat mq/environments/ci/pipelines/ibm-test-pipeline-for-dev.yaml which shows the source YAML for the ibm-mq-promote-dev-stage pipeline: apiVersion : tekton.dev/v1beta1 kind : Pipeline metadata : name : ibm-mq-promote-dev-stage annotations : app.openshift.io/runtime : test spec : params : - name : git-url description : The url for the git repository - name : git-revision description : git branch for the test app default : master - name : src-environment description : environment default : dev - name : dest-environment description : environment default : staging - name : app-path description : Path in gitops repo default : mq/environments - name : git-pr description : Enable the pipeline to do a PR for the gitops repo default : \"true\" - name : test-file description : Path of the postman collection (postman/mq-spring-app.postman_collection.json) default : \"postman/test.json\" tasks : - name : dev-instance-tests taskRef : name : ibm-functional-test params : - name : git-url value : \"$(params.git-url)\" - name : src-environment value : \"$(params.src-environment)\" - name : test-file value : \"$(params.test-file)\" - name : gitops taskRef : name : ibm-gitops runAfter : - dev-instance-tests params : - name : app-name value : \"$(tasks.dev-instance-tests.results.app-name)\" - name : src-environment value : \"$(params.src-environment)\" - name : dest-environment value : \"$(params.dest-environment)\" - name : app-path value : \"$(params.app-path)\" - name : git-pr value : \"$(params.git-pr)\" This YAML is slightly different to the output of the oc get pipeline command, because extra information is added during deployment such as Creation Timestamp: . Finding the ArgoCD application that manages pipelines You can see how the ibm-mq-promote-dev-stage and other pipeline YAMLs were deployed by examining the ArgoCD application that watches the folder containing the ci namespace pipelines. Issue the following command: cat mq/config/argocd/ci/ci-app-rest.yaml which shows the apps-mq-rest-ci-1 ArgoCD application that watches for updates: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : apps-mq-rest-ci-1 annotations : argocd.argoproj.io/sync-wave : \"300\" finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : ci server : https://kubernetes.default.svc project : applications source : path : mq/environments/ci repoURL : https://github.com/hp-gitops-test/multi-tenancy-gitops-apps targetRevision : master syncPolicy : automated : prune : true selfHeal : true See how this apps-mq-rest-ci-1 watches: path: mq/environments/ci . As we know, this folder contains the YAML for the ibm-mq-promote-dev-stage and other pipelines under pipelines folder. When this ArgoCD application was made active in the cluster, it installed all the pipelines along with other resources in this folder.","title":"Explore the promotion pipeline"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/#activate-argocd-applications","text":"We're now going to activate the ArgoCD applications to manage the deployment of Queue Manager and MQ Sample application to the staging namespace. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Select resources to deploy Access the 0-bootstrap/single-cluster/3-apps/kustomization.yaml : cat 0 -bootstrap/single-cluster/3-apps/kustomization.yaml Open 0-bootstrap/single-cluster/3-apps/kustomization.yaml and uncomment the below: - argocd/mq/stage.yaml Once modified, it should be something like below: resources: #- argocd/ace/cicd.yaml #- argocd/ace/dev.yaml #- argocd/ace/stage.yaml #- argocd/ace/prod.yaml - argocd/mq/cicd.yaml - argocd/mq/dev.yaml - argocd/mq/stage.yaml #- argocd/mq/prod.yaml #- argocd/apic/cicd.yaml #- argocd/apic/dev.yaml #- argocd/apic/stage.yaml #- argocd/apic/prod.yaml #- argocd/bookinfo/cicd.yaml #- argocd/bookinfo/dev.yaml #- argocd/bookinfo/stage.yaml #- argocd/bookinfo/prod.yaml #- argocd/soapserver/soapserver.yaml patches: - target: group: argoproj.io kind: Application labelSelector: \"gitops.tier.layer=applications\" patch: | - - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD applications application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Activating staging instance for queue manager and sample app\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11 , done . Counting objects: 100 % ( 11 /11 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 6 /6 ) , done . Writing objects: 100 % ( 6 /6 ) , 517 bytes | 517 .00 KiB/s, done . Total 6 ( delta 5 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 5 /5 ) , completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 2de3f9c..3225847 master -> master This change to the GitOps repository can now be used by ArgoCD. The applications argocd application Let's examine the ArgoCD application that manage the applications in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the applications application:","title":"Activate ArgoCD Applications"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/#review-the-resultant-gitops-application-folders","text":"The result of our previously successful ibm-mq-promote-dev-stage pipeline runs was to leave a PR in a GitOps folder for Queue Manager and MQ Sample application . This PR create a Helm chart if one is not present in the staging folder. If the Helm chart is already there, it updates the version in the requirements.yaml to match the latest deployment currently residing in the dev namespace. Ensure you've set up the $GIT_ORG environment variable This chapter also uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Open a new terminal window . Replace <git-org> in the following command with your GitHub user name: export GIT_ORG = <git-org> You can verify your $GIT_ORG as follows: echo $GIT_ORG which will show your GitHub Organization, for example: prod-ref-guide Grab the multi-tenancy-gitops-apps repository url Issue the below command to get the GitOps apps repository url. echo https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps/pulls which will give you an url, for instance: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps/pulls Review the Pull Requests created by ibm-mq-promote-dev-stage pipeline Open the above url in your browser and you will see Pull Requests generated by the ibm-mq-promote-dev-stage pipeline. Merge these Pull Requests. Re-merging local clone to view staging resources in GitOps apps repository The ibm-mq-promote-dev-stage pipeline left a Pull request and we merged the changes in the previous step. This means that our local clone of the GitOps repository is one commit behind GitHub. To allow us to view the changes locally, we must re-merge our local branch with GitHub. Return to the terminal window you're using for the multi-tenancy-gitops-apps GitOps apps repository. Issue the following command to change to your GitOps apps repository: cd $HOME /git cd multi-tenancy-gitops-apps Issue the following commands to merge the local branch: git fetch origin git merge origin/ $GIT_BRANCH which shows our local branch being updated: Updating 3be3b10..ea6706d Fast-forward mq/environments/staging/mq-infra/Chart.yaml | 4 ++++ mq/environments/staging/mq-infra/requirements.yaml | 4 ++++ mq/environments/staging/mq-infra/values.yaml | 3 +++ mq/environments/staging/mq-spring-app/Chart.yaml | 4 ++++ mq/environments/staging/mq-spring-app/requirements.yaml | 4 ++++ mq/environments/staging/mq-spring-app/values.yaml | 3 +++ 6 files changed, 22 insertions ( + ) create mode 100644 mq/environments/staging/mq-infra/Chart.yaml create mode 100644 mq/environments/staging/mq-infra/requirements.yaml create mode 100644 mq/environments/staging/mq-infra/values.yaml create mode 100644 mq/environments/staging/mq-spring-app/Chart.yaml create mode 100644 mq/environments/staging/mq-spring-app/requirements.yaml create mode 100644 mq/environments/staging/mq-spring-app/values.yaml Notice how these files correspond to the new Helm chart created in the GitOps apps repository by the ibm-mq-promote-dev-stage pipeline run. Explore the Helm chart in the GitOps Apps repository Let's examine the newly produced Helm charts in the GitOps apps repository; it was created by the previous pipeline runs. Issue the following command: tree mq/environments/staging/mq-infra/ which shows the newly produced Helm chart: mq/environments/staging/mq-infra/ \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml Notice that: The chart for Queue Manager was created in the mq/environments/ folder to reflect the fact this queue manager is part of the applications layer. The chart was created in the staging subfolder to reflect the fact that it's going to be deployed to the staging namespace. The chart was created in a new folder /mq-infra . This folder is dedicated to Queue Manager . Issue the following command: tree mq/environments/staging/mq-spring-app/ which shows the newly produced Helm chart: mq/environments/staging/mq-spring-app/ \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml Notice that: The chart for MQ Sample application was created in the mq/environments/ folder to reflect the fact this application is part of the applications layer. The chart was created in the staging subfolder to reflect the fact that it's going to be deployed to the staging namespace. The chart was created in a new folder /mq-spring-app . This folder is dedicated to MQ Sample application .","title":"Review the resultant GitOps application folders"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/#examine-the-activated-argocd-applications","text":"We're now going to examine the activated ArgoCD applications that uses the above Helm charts to manage the deployment of Queue Manager and MQ Sample application to the staging namespace.","title":"Examine the activated ArgoCD applications"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/#queue-manager_1","text":"The ArgoCD application for Queue Manager QM1 has its deployment to the cluster managed by a dedicated ArgoCD application called staging-mq-infra-instance . Issue the following command to show the ArgoCD application details: cat mq/config/argocd/staging/staging-mq-infra-instance.yaml which shows a YAML file typical of those we've seen before: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: staging-mq-infra-instance annotations: argocd.argoproj.io/sync-wave: \"300\" finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: namespace: staging server: https://kubernetes.default.svc project: applications source: path: mq/environments/staging/mq-infra repoURL: https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps targetRevision: master helm: valueFiles: - values.yaml syncPolicy: automated: prune: true selfHeal: true syncOptions: - Replace = true See how the Helm chart we explored in the previous section of this topic is referenced by path: mq/environments/staging/mq-infra : mq/environments/staging/mq-infra \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml The ArgoCD application applies this Helm chart to the cluster to instantiate QM1 as a set of cluster resources. Look at active Queue Manager ArgoCD application Let's examine QM1 and its Kubernetes resources using the ArgoCD UI. In the ArgoCD UI search the Applications view with the keyword staging-mq-infra : ( You may need to launch the ArgoCD UI again. Refer to these instructions . ) View the new Queue Manager Kubernetes resources We can look at the deployed instance of QM1 and its dependent kubernetes resources. Click on the staging-mq-infra-instance ArgoCD application:","title":"Queue Manager"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/#mq-sample-application_1","text":"The ArgoCD application for MQ Sample Application MQ Sample Application has its deployment to the cluster managed by a dedicated ArgoCD application called staging-mq-spring-app-instance . Issue the following command to show the ArgoCD application details: cat mq/config/argocd/staging/staging-mq-spring-app-instance.yaml which shows a YAML file typical of those we've seen before: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: staging-mq-spring-app-instance annotations: argocd.argoproj.io/sync-wave: \"300\" finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: namespace: staging server: https://kubernetes.default.svc project: applications source: path: mq/environments/staging/mq-spring-app repoURL: https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps targetRevision: master helm: valueFiles: - values.yaml syncPolicy: automated: prune: true selfHeal: true See how the Helm chart we explored in the previous section of this topic is referenced by path: mq/environments/staging/mq-spring-app : mq/environments/staging/mq-spring-app \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml The ArgoCD application applies this Helm chart to the cluster to instantiate MQ Sample Application as a set of cluster resources. Look at active MQ Sample Application ArgoCD application Let's examine MQ Sample Application and its Kubernetes resources using the ArgoCD UI. In the ArgoCD UI search the Applications view with the keyword staging-mq-spring : ( You may need to launch the ArgoCD UI again. Refer to these instructions . ) View the new MQ Sample Application Kubernetes resources We can look at the deployed instance of MQ Sample Application and its dependent kubernetes resources. Click on the staging-mq-spring-app-instance ArgoCD application:","title":"MQ Sample Application"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/#validate-the-queue-manager","text":"Connect to the queue manager pod Let's connect to this container, so that we can explore the queue manager. Issue the following command: oc exec -n staging qm-dev-ibm-mq-0 -it -- /bin/bash to connect you to the pod default container, where you'll see the bash prompt: bash-4.4$ Your terminal is now connected to the container running the queue manager. Use MQ command line The container we've connected to has a fully running instance of the queue manager configured according to the YAML generated by the Helm chart. We can use regular MQ commands to verify that this is a regular queue manager. Issue the following command: dspmq -o all to see a full list of the queue managers running in the container: QMNAME ( QM1 ) STATUS ( Running ) DEFAULT ( yes ) STANDBY ( Permitted ) INSTNAME ( Installation1 ) INSTPATH ( /opt/mqm ) INSTVER ( 9 .2.3.0 ) ROLE ( Not configured ) INSTANCE () INSYNC () QUORUM () We can see that QM1 is running, it's the default queue manager, where it's installed in the container file system, and it's version. Display queue manager properties We can also run an MQSC command to display the queue manager properties. These properties were defined in the MQSC file. Issue the following command: runmqsc QM1 <<< \"dis qmgr\" to see the full set of queue manager properties for QM1 : 5724 -H72 ( C ) Copyright IBM Corp. 1994 , 2021 . Starting MQSC for queue manager QM1. 1 : dis qmgr AMQ8408I: Display Queue Manager details. QMNAME ( QM1 ) ACCTCONO ( DISABLED ) ACCTINT ( 1800 ) ACCTMQI ( OFF ) ACCTQ ( OFF ) ACTIVREC ( MSG ) ACTVCONO ( DISABLED ) ACTVTRC ( OFF ) ADVCAP ( ENABLED ) ALTDATE ( 2021 -09-29 ) ALTTIME ( 09 .20.18 ) AMQPCAP ( NO ) AUTHOREV ( DISABLED ) CCSID ( 819 ) CERTLABL ( ) CERTVPOL ( ANY ) CHAD ( DISABLED ) CHADEV ( DISABLED ) CHADEXIT ( ) CHLEV ( DISABLED ) CHLAUTH ( DISABLED ) CLWLDATA ( ) CLWLEXIT ( ) CLWLLEN ( 100 ) CLWLMRUC ( 999999999 ) CLWLUSEQ ( LOCAL ) CMDEV ( DISABLED ) CMDLEVEL ( 923 ) COMMANDQ ( SYSTEM.ADMIN.COMMAND.QUEUE ) CONFIGEV ( DISABLED ) CONNAUTH ( SYSTEM.DEFAULT.AUTHINFO.IDPWOS ) CRDATE ( 2021 -09-29 ) CRTIME ( 09 .20.14 ) CUSTOM ( ) DEADQ ( DEV.DEAD.LETTER.QUEUE ) DEFCLXQ ( SCTQ ) DEFXMITQ ( ) DESCR ( ) DISTL ( YES ) IMGINTVL ( 60 ) IMGLOGLN ( OFF ) IMGRCOVO ( YES ) IMGRCOVQ ( YES ) IMGSCHED ( MANUAL ) INHIBTEV ( DISABLED ) IPADDRV ( IPV4 ) LOCALEV ( DISABLED ) LOGGEREV ( DISABLED ) MARKINT ( 5000 ) MAXHANDS ( 256 ) MAXMSGL ( 4194304 ) MAXPROPL ( NOLIMIT ) MAXPRTY ( 9 ) MAXUMSGS ( 10000 ) MONACLS ( QMGR ) MONCHL ( OFF ) MONQ ( OFF ) PARENT ( ) PERFMEV ( DISABLED ) PLATFORM ( UNIX ) PSMODE ( ENABLED ) PSCLUS ( ENABLED ) PSNPMSG ( DISCARD ) PSNPRES ( NORMAL ) PSRTYCNT ( 5 ) PSSYNCPT ( IFPER ) QMID ( QM1_2021-09-29_09.20.14 ) REMOTEEV ( DISABLED ) REPOS ( ) REPOSNL ( ) REVDNS ( ENABLED ) ROUTEREC ( MSG ) SCHINIT ( QMGR ) SCMDSERV ( QMGR ) SPLCAP ( ENABLED ) SSLCRLNL ( ) SSLCRYP ( ) SSLEV ( DISABLED ) SSLFIPS ( NO ) SSLKEYR ( /run/runmqserver/tls/key ) SSLRKEYC ( 0 ) STATACLS ( QMGR ) STATCHL ( OFF ) STATINT ( 1800 ) STATMQI ( OFF ) STATQ ( OFF ) STRSTPEV ( ENABLED ) SUITEB ( NONE ) SYNCPT TREELIFE ( 1800 ) TRIGINT ( 999999999 ) VERSION ( 09020300 ) XRCAP ( NO ) One MQSC command read. No commands have a syntax error. All valid MQSC commands were processed. Note how CHLAUTH(DISABLED) has been set. This was set in the MQSC file configuration for QM1 in via the mqsc-configmap configmap. Feel free to run other runmqsc commands to explore the queue manager properties. Exit from queue manager container When we're finished exploring the queue manager container, we can exit it. Issue the following command: exit to return to your local machine's command prompt: exit .../git/multi-tenancy-gitops-apps","title":"Validate the Queue Manager"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-dev-stage/#validate-the-application","text":"Check the application is running Using the location value from the route we can call the application to check its health by appending /health , for example: export STAGING_APP_URL = $( oc get route -n staging mq-spring-app -o jsonpath = \"{.spec.host}\" ) curl -X GET https:// $STAGING_APP_URL /actuator/health { \"status\" : \"UP\" , \"components\" : { \"HeathEndPointMain\" : { \"status\" : \"UP\" } , \"diskSpace\" : { \"status\" : \"UP\" , \"details\" : { \"total\" :105552732160, \"free\" :74874388480, \"threshold\" :10485760, \"exists\" :true }} , \"jms\" : { \"status\" : \"UP\" , \"details\" : { \"provider\" : \"IBM MQ JMS Provider\" }} , \"livenessState\" : { \"status\" : \"UP\" } , \"ping\" : { \"status\" : \"UP\" } , \"readinessState\" : { \"status\" : \"UP\" }} , \"groups\" : [ \"liveness\" , \"readiness\" ]} Call the application to put a message to a queue Using the same location value we can call the application to to put a message to a queue on the queue manager, for example: curl -X GET https:// $STAGING_APP_URL /api/send-hello-world Call the application to get a message from a queue Using the same location value we can call the application to to get a message from a queue on the queue manager, for example: curl -X GET https:// $STAGING_APP_URL /api/recv Call the application to put a JSON message to a queue Using the same location value we can call the application to to put a message with a JSON payload to a queue on the queue manager, for example: curl -X POST -H \"Content-Type: application/json\" https:// $STAGING_APP_URL /api/send-json --data \"{\\\"data\\\":\\\"Hola Munda\\\"}\" Note the use of the POST verb in the above command. This can be followed with a call to get the message from the queue. Congratulations! You've completed your first run of the promotion pipeline from dev to staging. Feel free to run the ibm-mq-promote-dev-stage pipeline more than once to get a feeling for how it works. You've used it to promote an instance of QM1 and MQ sample application from dev environment to staging environment in the cluster. You've explored how the promotion pipeline for dev to staging is structured as tasks and steps. You've examined it's pipeline run log to understand how this pipeline works and how tasks are implemented. Finally, you've examined how the GitOps apps repository is updated with the staging resources from a successful run of the pipeline. You've completed set up continuous deployment for the Queue Manager as well as the MQ sample application and deployed their instances to the staging namespace in the cluster. You've interacted with this queue manager using the command line as well as verified the application. In the next topic of this chapter we're going to promote the changes from staging environment to prod environment.","title":"Validate the Application"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/","text":"Promoting from Staging to Prod environment \u00b6 Overview \u00b6 Audience : Application developers, Administrators In the previous topic of this chapter, we ran the promotion pipeline for dev to staging. The pipeline successfully promoted the Queue Manager instance and MQ Sample application instance from dev to staging . In this topic, you will set up a promotion pipeline that promotes the Queue Manager and MQ sample application from staging environment to prod environment. We'll activate the ArgoCD application that will watch the GitOps folder containing the Helm charts and use it and its dependent resources to deploy a running queue manager and sample application to the prod namespace in the cluster. Look at the following diagram: We've highlighted the components we're going to explore in this topic: The promotion pipeline from staging to prod will perform a set of tasks for performance testing. If successful, the pipeline leaves a PR in the GitOps apps repository. The GitOps apps repository will have a PR for the latest good build and test represented as a Helm chart for prod environment. This chart will be subsequently used by ArgoCD to deploy to the cluster. An ArgoCD application will monitor the GitOps folder where its Helm chart is held. Whenever this folder is updated, this ArgoCD application will apply these updates to the cluster, resulting in a new deployment. The instance of Queue Manager running in the cluster is active prod queue manager ready for use by MQ applications under prod. The instance of MQ Sample Application running in the cluster is active prod sample application under prod. In this topic, we're going to: Access the promotion pipeline Run the promotion pipeline for Queue Manager Run the promotion pipeline for MQ Sample Application Explore the promotion pipeline. Activate ArgoCD application for Queue Manager and MQ Sample Application for prod namespace. Review the resultant GitOps application folders for Queue Manager and MQ Sample Application . Examine the activated ArgoCD application that deploys the Queue Manager and MQ Sample Application to the cluster. Validate Queue Manager and MQ Sample Application . By the end of this topic we'll have a fully functioning staging to prod promotion pipeline that we can use to promote queue manager and sample application from staging environment to prod environment. Pre-requisites \u00b6 Before attempting this topic, you should have successfully completed the previous chapter . Access the promotion pipeline \u00b6 As we can see in the diagram above, the promotion pipeline is responsible for promoting the Queue Manager and MQ Sample application from staging to prod . If successful, it leaves a PR with the updated resources for prod in the GitOps apps repository, which are used by ArgoCD to deploy the updated Queue Manager and application to the cluster. It's usually the case a pipeline runs automatically when the Queue Manager or the Sample application is successfully deployed in staging environment. However, our first pipeline run is manual so that you can be in control, making it easier to understand what's happening. Locate the ci pipelines in the web console Let's find the promotion pipeline using the OpenShift web console. Navigate to Pipelines->Pipelines in the left hand pane, and select Project: ci , to show all pipelines in the ci namespace: You can see all the pipelines that we installed into the ci namespace in the previous chapter. We'll use different pipelines for different activities throughout the tutorial. For this topic, we're going to use the ibm-mq-promote-stage-prod pipeline. When the Queue Manager or Sample application are successfully deployed to the staging namespace, this pipeline will perform performance testing and on a successful run, it will promote the existing resources in staging to the prod namespace. The ibm-mq-promote-stage-prod promotion pipeline Select the ibm-mq-promote-stage-prod pipeline from the list of all pipelines: Like all pipelines, ibm-mq-promote-stage-prod is composed from a set of tasks : stage-instance-tests gitops The task name often provides a strong hint of each task's specific function. We'll examine these tasks in detail as the pipeline runs. The oc command as an alternative to the Web console As well as using the OpenShift web console, you can also interact with pipeline using the oc or tekton commands. Ensure you're logged in to cluster, and issue the following command to list the ibm-mq-promote-stage-prod pipeline details: oc get pipeline ibm-mq-promote-stage-prod -n ci which shows a brief summary of the pipeline: NAME AGE ibm-mq-promote-stage-prod 5d18h You can get more detail by adding the -o yaml option; we'll do that later. Run the promotion pipeline \u00b6 Queue Manager \u00b6 Configure your first pipeline run In pipeline details view above, you'll see an Actions button. Select Start to configure a pipeline run. You'll be presented with the following dialog: The supplied arguments allow the user of the pipeline to configure its behavior. For example, a user can use this pipeline with different queue manager source repositories. Configure the run as follows: Set git-url to your fork of the mq-infra repository Set git-revision to master . Set src-environment to staging . Set dest-environment to prod . Set the app-path pointing to your GitOps apps repository which in our case is mq/environments . Set git-pr to true . Set test-file pointing to your jmeter jmx file . If your source repository don't have any jmx file defined, just leave the defaults. Note At the moment, Queue Manager mq-infra repository do not have any performance tests defined using Jmeter. This pipeline is designed in a way to skip this tests if the definitions are not present. However, MQ Sample application mq-spring-app repository have these tests defined and we will be coming across it in the later section of this tutorial. Hit Start to start the pipeline run. You can also use the command line to run a pipeline; we'll explore that option later. Watch a pipeline run executing The pipeline run has now started. Notice how the view changes to Pipeline Run details : We're now looking at the live output from a pipeline run, rather than the pipeline used to create the run. Notice that the pipeline run name ibm-mq-promote-stage-prod-spl3ow is based on the pipeline name -- with a unique suffix. Every new pipeline run will have a unique name. See also how the first stage-instance-tests task is running, while the remaining tasks are waiting to start. Hover over stage-instance-tests task and you will see the steps that comprise it. Watch pipeline steps complete As the pipeline run proceeds, notice all the steps got completed: This pipeline will take about a minute or two to complete. MQ Sample Application \u00b6 Configure your first pipeline run In pipeline details view above, you'll see an Actions button. Select Start to configure a pipeline run. You'll be presented with the following dialog: The supplied arguments allow the user of the pipeline to configure its behavior. For example, a user can use this pipeline with different sample application source repositories. Configure the run as follows: Set git-url to your fork of the mq-spring-app repository Set git-revision to master . Set src-environment to staging . Set dest-environment to prod . Set the app-path pointing to your GitOps apps repository which in our case is mq/environments . Set git-pr to true . Set test-plan pointing to your jmeter jmx file . If your source repository don't have any jmx file, just leave the defaults. mq-spring-app repository have the jmx file defined at jmeter/mq-spring-app.jmx using Jmeter. Hit Start to start the pipeline run. You can also use the command line to run a pipeline; we'll explore that option later. Watch a pipeline run executing The pipeline run has now started. Notice how the view changes to Pipeline Run details : We're now looking at the live output from a pipeline run, rather than the pipeline used to create the run. Notice that the pipeline run name ibm-mq-promote-stage-prod-gw8mgi is based on the pipeline name -- with a unique suffix. Every new pipeline run will have a unique name. See also how the first stage-instance-tests task is running, while the remaining tasks are waiting to start. Hover over stage-instance-tests task and you will see the steps that comprise it. Watch pipeline steps complete As the pipeline run proceeds, notice all the steps got completed: This pipeline will take about a minute or two to complete. Explore the promotion pipeline \u00b6 Let's look more closely at how the ibm-mq-promote-stage-prod pipeline is structured. Let's also examine the tasks and steps that make up the pipeline, and how they progressively validate the deployments in staging namespace, resulting in the production of a Helm chart ready for deployment in prod namespace. Pipeline, Task, Step Let's start with the pipeline, its tasks and steps. Hover over the stage-instance-tests task: See how our pipeline is made up of tasks such as stage-instance-tests and gitops . These run in the order defined by the pipeline. Our pipeline is linear, though Tekton supports more sophisticated pipeline graphs if necessary. See how each task comprises a set of steps such as git-clone or setup . These run in the order defined by the task. The pipeline run logs When a pipeline runs, all its output is captured in a set of logs, one for each task. Click on the stage-instance-tests task to show its logs: ( Alternatively, you can select the Logs tab from the UI, and then select the tasks on the left pane within the pipeline run view. ) See how the stage-instance-tests task has its output in a dedicated log. You can select any log for any task that has completed or is executing. When a pipeline run completes, all its logs remain available, which can help diagnosing problems for example. Exploring an example task output: stage-instance-tests It's easy to examine a task log; you simply select the relevant task and scroll the log up or down. For active tasks the log will be dynamically updated. Click on the stage-instance-tests task: This console window shows the output generated by stage-instance-tests task. As the task script proceeds, its output is captured; that's what we can see in this window. Notice that the stage-instance-tests task output is from the first step in the stage-instance-tests task. This step is called STEP-GIT-CLONE . Note how the step names are capitalized in the web console output. Let's look at another task and its steps more closely. Exploring a task step in detail : STEP-JMETER-IT A task is built of multiple steps. Let's explore the stage-instance-tests task and its step-jmeter-it step. Select the stage-instance-tests task and scroll through its logs to see its third step , STEP-JMETER-IT : See how the step-jmeter-it output is captured in the same log as the previous step git-clone . The pipeline definition Up to this point, we've examined the pipeline run and the logs it generates. Let's now look at how a pipeline is defined. Issue the following command to show the ibm-mq-promote-stage-prod pipeline: oc describe pipeline ibm-mq-promote-stage-prod -n ci which shows the pipeline YAML in considerable detail: Name : ibm-mq-promote-stage-prod Namespace : ci Labels : app.kubernetes.io/instance=apps-mq-rest-ci-1 Annotations : app.openshift.io/runtime : test API Version : tekton.dev/v1beta1 Kind : Pipeline Metadata : Creation Timestamp : 2021-09-23T17:29:24Z Generation : 1 Managed Fields : API Version : tekton.dev/v1beta1 Fields Type : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : f:app.openshift.io/runtime : f:kubectl.kubernetes.io/last-applied-configuration : f:labels : . : f:app.kubernetes.io/instance : f:spec : . : f:params : f:tasks : Manager : argocd-application-controller Operation : Update Time : 2021-09-23T17:29:24Z Resource Version : 2478082 Self Link : /apis/tekton.dev/v1beta1/namespaces/ci/pipelines/ibm-mq-promote-stage-prod UID : d7ed2886-97f4-4c89-beb7-dd9c343dabf1 Spec : Params : Description : The url for the git repository Name : git-url Type : string Default : master Description : git branch for the test app Name : git-revision Type : string Default : staging Description : environment Name : src-environment Type : string Default : prod Description : environment Name : dest-environment Type : string Default : mq/environments Description : Path in gitops repo Name : app-path Type : string Default : true Description : Enable the pipeline to do a PR for the gitops repo Name : git-pr Type : string Default : jmeter/test.jmx Description : Path of the postman collection (jmeter/mq-spring-app.jmx) Name : test-plan Type : string Tasks : Name : stage-instance-tests Params : Name : git-url Value : $(params.git-url) Name : src-environment Value : $(params.src-environment) Name : test-plan Value : $(params.test-plan) Task Ref : Kind : Task Name : ibm-jmeter-performance-test Name : gitops Params : Name : app-name Value : $(tasks.stage-instance-tests.results.app-name) Name : src-environment Value : $(params.src-environment) Name : dest-environment Value : $(params.dest-environment) Name : app-path Value : $(params.app-path) Name : git-pr Value : $(params.git-pr) Run After : stage-instance-tests Task Ref : Kind : Task Name : ibm-gitops Events : <none> Don't be intimidated by this output -- it's actually just a more detailed source view of the information shown for the ibm-mq-promote-stage-prod pipeline in the web console. Locate the following key structures in the Pipeline YAML: API Version: tekton.dev/v1beta1 and Kind: Pipeline identify this as a Tekton pipeline. Spec: Params identifies the pipeline input parameters Tasks: is a list of the tasks in this pipeline, each of which has A Name: naming the task A set of Params: identifying the task input parameters An optional Run After: value indicating when the task is run A Task Ref: identifying the actual task code to be run using the task's input Notice that there's no code in the pipeline definition; instead, the definition specifies the required inputs to the pipeline, as well as the set of required tasks and their order of execution. The code executed by each task is identified by Task Ref: , rather than in the pipeline; the pipeline definition merely defines the order of task execution and how parameters are marshaled between tasks. Let's now examine the pipeline definition in a little more detail. The pipeline input Spec: Params When we configure a pipeline run, the arguments map precisely to Spec: Params in the pipeline YAML file. Below, we've just shown the Spec: Params: for the ibm-mq-promote-stage-prod pipeline: Spec : Params : Description : The url for the git repository Name : git-url Type : string Default : master Description : git branch for the test app Name : git-revision Type : string Default : staging Description : environment Name : src-environment Type : string Default : prod Description : environment Name : dest-environment Type : string Default : mq/environments Description : Path in gitops repo Name : app-path Type : string Default : true Description : Enable the pipeline to do a PR for the gitops repo Name : git-pr Type : string Default : jmeter/test.jmx Description : Path of the postman collection (jmeter/mq-spring-app.jmx) Name : test-plan Type : string Spend a few moments mapping each of these parameters maps to those on the Start Pipeline input dialog where you specified the pipeline run arguments. For example, map Name: , Description: and Default: to the different fields in the dialog. Parameters for the first setup task We can see that the first task in the pipeline is called stage-instance-tests . Let's examine its YAML to see how it gets its input parameters: Tasks : Name : stage-instance-tests Params : Name : git-url Value : $(params.git-url) Name : src-environment Value : $(params.src-environment) Name : test-plan Value : $(params.test-plan) Task Ref : Kind : Task Name : ibm-jmeter-performance-test See how the dev-instance-tests task derives its git-url parameter using the pipeline input parameter value $(params.git-url) . See how src-environment and test-plan work in a similar way. The first task in a pipeline typically works like this -- its parameters are mapped from the pipeline's input parameters. Notice also that some pipeline input parameters are not referred to by the stage-instance-tests task; they will be used by subsequent tasks using the appropriate $(params.) value. Passing arguments between tasks As each task completes, the pipeline proceeds. When a new task starts it often requires one or more results generated by a previous task. We can see a good example of this in the gitops task: Name : gitops Params : Name : app-name Value : $(tasks.stage-instance-tests.results.app-name) Name : src-environment Value : $(params.src-environment) Name : dest-environment Value : $(params.dest-environment) Name : app-path Value : $(params.app-path) Name : git-pr Value : $(params.git-pr) Run After : stage-instance-tests Task Ref : Kind : Task Name : ibm-gitops See how the gitops task specifies that the app-name parameter should use value generated by the stage-instance-tests task using the syntax: $(tasks.stage-instance-tests.results.app-name) . Also notice how the gitops tasks uses Run After: to specify that it should execute after the stage-instance-tests task. This follows the Kubernetes idiom of resources being declarative -- the order of execution is defined by RunAfter: rather than the order in which tasks appear in the YAML. Again, notice that the gitops task doesn't contain the code that the task executes. This is contained in Task Ref: which identifies ibm-gitops as the task to execute using the specified parameters. It's the code in ibm-gitops which generates the log output for the task; we'll examine it later. The way pipelines tasks are designed makes them highly reusable. As we'll see later, tasks are written with defined inputs and outputs such that they can be re-used by different pipelines. Pipelines focus on organizing the order of task execution and how parameters are marshaled into and between tasks; it's the tasks that do the actual work. Locating the pipeline and tasks source YAMLs Finally, let's locate the source for the ibm-mq-promote-stage-prod pipeline and the tasks within it. It is located in the following folder: cd $HOME /git cd multi-tenancy-gitops-apps tree mq/environments/ci/pipelines/ We can see the other pipelines for the ci namespace in this folder: mq/environments/ci/pipelines/ \u251c\u2500\u2500 ibm-test-pipeline-for-dev.yaml \u251c\u2500\u2500 ibm-test-pipeline-for-stage.yaml \u251c\u2500\u2500 java-maven-dev-pipeline.yaml \u251c\u2500\u2500 mq-pipeline-dev.yaml \u2514\u2500\u2500 mq-spring-app-dev-pipeline.yaml These map to the MQ-related pipelines we saw in the Pipelines->Pipelines view in the web console. Exploring the ibm-mq-promote-stage-prod source YAML View the source for the ibm-test-pipeline-for-stage.yaml pipeline with the command: cat mq/environments/ci/pipelines/ibm-test-pipeline-for-stage.yaml which shows the source YAML for the ibm-mq-promote-dev-stage pipeline: apiVersion : tekton.dev/v1beta1 kind : Pipeline metadata : name : ibm-mq-promote-stage-prod annotations : app.openshift.io/runtime : test spec : params : - name : git-url description : The url for the git repository - name : git-revision description : git branch for the test app default : master - name : src-environment description : environment default : staging - name : dest-environment description : environment default : prod - name : app-path description : Path in gitops repo default : mq/environments - name : git-pr description : Enable the pipeline to do a PR for the gitops repo default : \"true\" - name : test-plan description : Path of the postman collection (jmeter/mq-spring-app.jmx) default : \"jmeter/test.jmx\" tasks : - name : stage-instance-tests taskRef : name : ibm-jmeter-performance-test params : - name : git-url value : \"$(params.git-url)\" - name : src-environment value : \"$(params.src-environment)\" - name : test-plan value : \"$(params.test-plan)\" - name : gitops taskRef : name : ibm-gitops runAfter : - stage-instance-tests params : - name : app-name value : \"$(tasks.stage-instance-tests.results.app-name)\" - name : src-environment value : \"$(params.src-environment)\" - name : dest-environment value : \"$(params.dest-environment)\" - name : app-path value : \"$(params.app-path)\" - name : git-pr value : \"$(params.git-pr)\" This YAML is slightly different to the output of the oc get pipeline command, because extra information is added during deployment such as Creation Timestamp: . Finding the ArgoCD application that manages pipelines You can see how the ibm-mq-promote-stage-prod and other pipeline YAMLs were deployed by examining the ArgoCD application that watches the folder containing the ci namespace pipelines. Issue the following command: cat mq/config/argocd/ci/ci-app-rest.yaml which shows the apps-mq-rest-ci-1 ArgoCD application that watches for updates: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : apps-mq-rest-ci-1 annotations : argocd.argoproj.io/sync-wave : \"300\" finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : ci server : https://kubernetes.default.svc project : applications source : path : mq/environments/ci repoURL : https://github.com/hp-gitops-test/multi-tenancy-gitops-apps targetRevision : master syncPolicy : automated : prune : true selfHeal : true See how this apps-mq-rest-ci-1 watches: path: mq/environments/ci . As we know, this folder contains the YAML for the ibm-mq-promote-stage-prod and other pipelines under pipelines folder. When this ArgoCD application was made active in the cluster, it installed all the pipelines along with other resources in this folder. Activate ArgoCD Applications \u00b6 We're now going to activate the ArgoCD applications to manage the deployment of Queue Manager and MQ Sample application to the prod namespace. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Select resources to deploy Access the 0-bootstrap/single-cluster/3-apps/kustomization.yaml : cat 0 -bootstrap/single-cluster/3-apps/kustomization.yaml Open 0-bootstrap/single-cluster/3-apps/kustomization.yaml and uncomment the below: - argocd/mq/prod.yaml Once modified, it should be something like below: resources: #- argocd/ace/cicd.yaml #- argocd/ace/dev.yaml #- argocd/ace/stage.yaml #- argocd/ace/prod.yaml - argocd/mq/cicd.yaml - argocd/mq/dev.yaml - argocd/mq/stage.yaml - argocd/mq/prod.yaml #- argocd/apic/cicd.yaml #- argocd/apic/dev.yaml #- argocd/apic/stage.yaml #- argocd/apic/prod.yaml #- argocd/bookinfo/cicd.yaml #- argocd/bookinfo/dev.yaml #- argocd/bookinfo/stage.yaml #- argocd/bookinfo/prod.yaml #- argocd/soapserver/soapserver.yaml patches: - target: group: argoproj.io kind: Application labelSelector: \"gitops.tier.layer=applications\" patch: | - - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD applications application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Activating prod instance for queue manager and sample app\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11 , done . Counting objects: 100 % ( 11 /11 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 6 /6 ) , done . Writing objects: 100 % ( 6 /6 ) , 515 bytes | 257 .00 KiB/s, done . Total 6 ( delta 5 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 5 /5 ) , completed with 5 local objects. To https://github.com/hp-gitops-mq/multi-tenancy-gitops.git 3225847 ..25f7459 master -> master This change to the GitOps repository can now be used by ArgoCD. The applications argocd application Let's examine the ArgoCD application that manage the applications in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the applications application: Review the resultant GitOps application folders \u00b6 The result of our previously successful ibm-mq-promote-stage-prod pipeline runs was to leave a PR in a GitOps folder for Queue Manager and MQ Sample application . This PR create a Helm chart if one is not present in the prod folder. If the Helm chart is already there, it updates the version in the requirements.yaml to match the latest deployment currently residing in the staging namespace. Ensure you've set up the $GIT_ORG environment variable This chapter also uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Open a new terminal window . Replace <git-org> in the following command with your GitHub user name: export GIT_ORG = <git-org> You can verify your $GIT_ORG as follows: echo $GIT_ORG which will show your GitHub Organization, for example: prod-ref-guide Grab the multi-tenancy-gitops-apps repository url Issue the below command to get the GitOps apps repository url. echo https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps/pulls which will give you an url, for instance: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps/pulls Review the Pull Requests created by ibm-mq-promote-dev-stage pipeline Open the above url in your browser and you will see Pull Requests generated by the ibm-mq-promote-dev-stage pipeline. Merge these Pull Requests. Re-merging local clone to view prod resources in GitOps apps repository The ibm-mq-promote-stage-prod pipeline left a Pull request and we merged the changes in the previous step. This means that our local clone of the GitOps repository is one commit behind GitHub. To allow us to view the changes locally, we must re-merge our local branch with GitHub. Return to the terminal window you're using for the multi-tenancy-gitops-apps GitOps apps repository. Issue the following command to change to your GitOps apps repository: cd $HOME /git cd multi-tenancy-gitops-apps Issue the following commands to merge the local branch: git fetch origin git merge origin/ $GIT_BRANCH which shows our local branch being updated: Updating ea6706d..8882bb9 Fast-forward mq/environments/prod/mq-infra/Chart.yaml | 4 ++++ mq/environments/prod/mq-infra/requirements.yaml | 4 ++++ mq/environments/prod/mq-infra/values.yaml | 3 +++ mq/environments/prod/mq-spring-app/Chart.yaml | 4 ++++ mq/environments/prod/mq-spring-app/requirements.yaml | 4 ++++ mq/environments/prod/mq-spring-app/values.yaml | 3 +++ 6 files changed, 22 insertions ( + ) create mode 100644 mq/environments/prod/mq-infra/Chart.yaml create mode 100644 mq/environments/prod/mq-infra/requirements.yaml create mode 100644 mq/environments/prod/mq-infra/values.yaml create mode 100644 mq/environments/prod/mq-spring-app/Chart.yaml create mode 100644 mq/environments/prod/mq-spring-app/requirements.yaml create mode 100644 mq/environments/prod/mq-spring-app/values.yaml Notice how these files correspond to the new Helm chart created in the GitOps apps repository by the ibm-mq-promote-stage-prod pipeline run. Explore the Helm chart in the GitOps Apps repository Let's examine the newly produced Helm charts in the GitOps apps repository; it was created by the previous pipeline runs. Issue the following command: tree mq/environments/prod/mq-infra/ which shows the newly produced Helm chart: mq/environments/prod/mq-infra/ \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml Notice that: The chart for Queue Manager was created in the mq/environments/ folder to reflect the fact this queue manager is part of the applications layer. The chart was created in the prod subfolder to reflect the fact that it's going to be deployed to the prod namespace. The chart was created in a new folder /mq-infra . This folder is dedicated to Queue Manager . Issue the following command: tree mq/environments/prod/mq-spring-app/ which shows the newly produced Helm chart: mq/environments/prod/mq-spring-app/ \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml Notice that: The chart for MQ Sample application was created in the mq/environments/ folder to reflect the fact this application is part of the applications layer. The chart was created in the prod subfolder to reflect the fact that it's going to be deployed to the prod namespace. The chart was created in a new folder /mq-spring-app . This folder is dedicated to MQ Sample application . Examine the activated ArgoCD applications \u00b6 We're now going to examine the activated ArgoCD applications that uses the above Helm charts to manage the deployment of Queue Manager and MQ Sample application to the prod namespace. Queue Manager \u00b6 The ArgoCD application for Queue Manager QM1 has its deployment to the cluster managed by a dedicated ArgoCD application called prod-mq-infra-instance . Issue the following command to show the ArgoCD application details: cat mq/config/argocd/prod/prod-mq-infra-instance.yaml which shows a YAML file typical of those we've seen before: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: prod-mq-infra-instance annotations: argocd.argoproj.io/sync-wave: \"300\" finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: namespace: prod server: https://kubernetes.default.svc project: applications source: path: mq/environments/prod/mq-infra repoURL: https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps targetRevision: master helm: valueFiles: - values.yaml syncPolicy: automated: prune: true selfHeal: true See how the Helm chart we explored in the previous section of this topic is referenced by path: mq/environments/prod/mq-infra : mq/environments/prod/mq-infra \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml The ArgoCD application applies this Helm chart to the cluster to instantiate QM1 as a set of cluster resources. Look at active Queue Manager ArgoCD application Let's examine QM1 and its Kubernetes resources using the ArgoCD UI. In the ArgoCD UI search the Applications view with the keyword prod-mq-infra : ( You may need to launch the ArgoCD UI again. Refer to these instructions . ) View the new Queue Manager Kubernetes resources We can look at the deployed instance of QM1 and its dependent kubernetes resources. Click on the prod-mq-infra-instance ArgoCD application: MQ Sample Application \u00b6 The ArgoCD application for MQ Sample Application MQ Sample Application has its deployment to the cluster managed by a dedicated ArgoCD application called prod-mq-spring-app-instance . Issue the following command to show the ArgoCD application details: cat mq/config/argocd/prod/prod-mq-spring-app-instance.yaml which shows a YAML file typical of those we've seen before: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: prod-mq-spring-app-instance annotations: argocd.argoproj.io/sync-wave: \"300\" finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: namespace: prod server: https://kubernetes.default.svc project: applications source: path: mq/environments/prod/mq-spring-app repoURL: https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps targetRevision: master helm: valueFiles: - values.yaml syncPolicy: automated: prune: true selfHeal: true See how the Helm chart we explored in the previous section of this topic is referenced by path: mq/environments/prod/mq-spring-app : mq/environments/prod/mq-spring-app \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml The ArgoCD application applies this Helm chart to the cluster to instantiate MQ Sample Application as a set of cluster resources. Look at active MQ Sample Application ArgoCD application Let's examine MQ Sample Application and its Kubernetes resources using the ArgoCD UI. In the ArgoCD UI search the Applications view with the keyword prod-mq-spring : ( You may need to launch the ArgoCD UI again. Refer to these instructions . ) View the new MQ Sample Application Kubernetes resources We can look at the deployed instance of MQ Sample Application and its dependent kubernetes resources. Click on the prod-mq-spring-app-instance ArgoCD application: Validate the Queue Manager \u00b6 Connect to the queue manager pod Let's connect to this container, so that we can explore the queue manager. Issue the following command: oc exec -n prod qm-dev-ibm-mq-0 -it -- /bin/bash to connect you to the pod default container, where you'll see the bash prompt: bash-4.4$ Your terminal is now connected to the container running the queue manager. Use MQ command line The container we've connected to has a fully running instance of the queue manager configured according to the YAML generated by the Helm chart. We can use regular MQ commands to verify that this is a regular queue manager. Issue the following command: dspmq -o all to see a full list of the queue managers running in the container: QMNAME ( QM1 ) STATUS ( Running ) DEFAULT ( yes ) STANDBY ( Permitted ) INSTNAME ( Installation1 ) INSTPATH ( /opt/mqm ) INSTVER ( 9 .2.3.0 ) ROLE ( Not configured ) INSTANCE () INSYNC () QUORUM () We can see that QM1 is running, it's the default queue manager, where it's installed in the container file system, and it's version. Display queue manager properties We can also run an MQSC command to display the queue manager properties. These properties were defined in the MQSC file. Issue the following command: runmqsc QM1 <<< \"dis qmgr\" to see the full set of queue manager properties for QM1 : 5724 -H72 ( C ) Copyright IBM Corp. 1994 , 2021 . Starting MQSC for queue manager QM1. 1 : dis qmgr AMQ8408I: Display Queue Manager details. QMNAME ( QM1 ) ACCTCONO ( DISABLED ) ACCTINT ( 1800 ) ACCTMQI ( OFF ) ACCTQ ( OFF ) ACTIVREC ( MSG ) ACTVCONO ( DISABLED ) ACTVTRC ( OFF ) ADVCAP ( ENABLED ) ALTDATE ( 2021 -09-29 ) ALTTIME ( 14 .38.48 ) AMQPCAP ( NO ) AUTHOREV ( DISABLED ) CCSID ( 819 ) CERTLABL ( ) CERTVPOL ( ANY ) CHAD ( DISABLED ) CHADEV ( DISABLED ) CHADEXIT ( ) CHLEV ( DISABLED ) CHLAUTH ( DISABLED ) CLWLDATA ( ) CLWLEXIT ( ) CLWLLEN ( 100 ) CLWLMRUC ( 999999999 ) CLWLUSEQ ( LOCAL ) CMDEV ( DISABLED ) CMDLEVEL ( 923 ) COMMANDQ ( SYSTEM.ADMIN.COMMAND.QUEUE ) CONFIGEV ( DISABLED ) CONNAUTH ( SYSTEM.DEFAULT.AUTHINFO.IDPWOS ) CRDATE ( 2021 -09-29 ) CRTIME ( 14 .38.45 ) CUSTOM ( ) DEADQ ( DEV.DEAD.LETTER.QUEUE ) DEFCLXQ ( SCTQ ) DEFXMITQ ( ) DESCR ( ) DISTL ( YES ) IMGINTVL ( 60 ) IMGLOGLN ( OFF ) IMGRCOVO ( YES ) IMGRCOVQ ( YES ) IMGSCHED ( MANUAL ) INHIBTEV ( DISABLED ) IPADDRV ( IPV4 ) LOCALEV ( DISABLED ) LOGGEREV ( DISABLED ) MARKINT ( 5000 ) MAXHANDS ( 256 ) MAXMSGL ( 4194304 ) MAXPROPL ( NOLIMIT ) MAXPRTY ( 9 ) MAXUMSGS ( 10000 ) MONACLS ( QMGR ) MONCHL ( OFF ) MONQ ( OFF ) PARENT ( ) PERFMEV ( DISABLED ) PLATFORM ( UNIX ) PSMODE ( ENABLED ) PSCLUS ( ENABLED ) PSNPMSG ( DISCARD ) PSNPRES ( NORMAL ) PSRTYCNT ( 5 ) PSSYNCPT ( IFPER ) QMID ( QM1_2021-09-29_14.38.45 ) REMOTEEV ( DISABLED ) REPOS ( ) REPOSNL ( ) REVDNS ( ENABLED ) ROUTEREC ( MSG ) SCHINIT ( QMGR ) SCMDSERV ( QMGR ) SPLCAP ( ENABLED ) SSLCRLNL ( ) SSLCRYP ( ) SSLEV ( DISABLED ) SSLFIPS ( NO ) SSLKEYR ( /run/runmqserver/tls/key ) SSLRKEYC ( 0 ) STATACLS ( QMGR ) STATCHL ( OFF ) STATINT ( 1800 ) STATMQI ( OFF ) STATQ ( OFF ) STRSTPEV ( ENABLED ) SUITEB ( NONE ) SYNCPT TREELIFE ( 1800 ) TRIGINT ( 999999999 ) VERSION ( 09020300 ) XRCAP ( NO ) One MQSC command read. No commands have a syntax error. All valid MQSC commands were processed. Note how CHLAUTH(DISABLED) has been set. This was set in the MQSC file configuration for QM1 in via the mqsc-configmap configmap. Feel free to run other runmqsc commands to explore the queue manager properties. Exit from queue manager container When we're finished exploring the queue manager container, we can exit it. Issue the following command: exit to return to your local machine's command prompt: exit .../git/multi-tenancy-gitops-apps Validate the Application \u00b6 Check the application is running Using the location value from the route we can call the application to check its health by appending /health , for example: export PROD_APP_URL = $( oc get route -n staging mq-spring-app -o jsonpath = \"{.spec.host}\" ) curl -X GET https:// $PROD_APP_URL /actuator/health { \"status\" : \"UP\" , \"components\" : { \"HeathEndPointMain\" : { \"status\" : \"UP\" } , \"diskSpace\" : { \"status\" : \"UP\" , \"details\" : { \"total\" :105552732160, \"free\" :74679922688, \"threshold\" :10485760, \"exists\" :true }} , \"jms\" : { \"status\" : \"UP\" , \"details\" : { \"provider\" : \"IBM MQ JMS Provider\" }} , \"livenessState\" : { \"status\" : \"UP\" } , \"ping\" : { \"status\" : \"UP\" } , \"readinessState\" : { \"status\" : \"UP\" }} , \"groups\" : [ \"liveness\" , \"readiness\" ]} Call the application to put a message to a queue Using the same location value we can call the application to to put a message to a queue on the queue manager, for example: curl -X GET https:// $PROD_APP_URL /api/send-hello-world Call the application to get a message from a queue Using the same location value we can call the application to to get a message from a queue on the queue manager, for example: curl -X GET https:// $PROD_APP_URL /api/recv Call the application to put a JSON message to a queue Using the same location value we can call the application to to put a message with a JSON payload to a queue on the queue manager, for example: curl -X POST -H \"Content-Type: application/json\" https:// $PROD_APP_URL /api/send-json --data \"{\\\"data\\\":\\\"Hola Munda\\\"}\" Note the use of the POST verb in the above command. This can be followed with a call to get the message from the queue. Congratulations! You've completed your first run of the promotion pipeline from dev to staging. Feel free to run the ibm-mq-promote-stage-prod pipeline more than once to get a feeling for how it works. You've used it to promote an instance of QM1 and MQ sample application from staging environment to prod environment in the cluster. You've explored how the promotion pipeline for staging to prod is structured as tasks and steps. You've examined it's pipeline run log to understand how this pipeline works and how tasks are implemented. Finally, you've examined how the GitOps apps repository is updated with the prod resources from a successful run of the pipeline. You've completed set up continuous deployment for the Queue Manager as well as the MQ sample application and deployed their instances to the prod namespace in the cluster. You've interacted with this queue manager using the command line as well as verified the application. In the next topic of this chapter we're going to see how we can automatically promote the changes across environments.","title":"Promote from staging to prod"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/#promoting-from-staging-to-prod-environment","text":"","title":"Promoting from Staging to Prod environment"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/#overview","text":"Audience : Application developers, Administrators In the previous topic of this chapter, we ran the promotion pipeline for dev to staging. The pipeline successfully promoted the Queue Manager instance and MQ Sample application instance from dev to staging . In this topic, you will set up a promotion pipeline that promotes the Queue Manager and MQ sample application from staging environment to prod environment. We'll activate the ArgoCD application that will watch the GitOps folder containing the Helm charts and use it and its dependent resources to deploy a running queue manager and sample application to the prod namespace in the cluster. Look at the following diagram: We've highlighted the components we're going to explore in this topic: The promotion pipeline from staging to prod will perform a set of tasks for performance testing. If successful, the pipeline leaves a PR in the GitOps apps repository. The GitOps apps repository will have a PR for the latest good build and test represented as a Helm chart for prod environment. This chart will be subsequently used by ArgoCD to deploy to the cluster. An ArgoCD application will monitor the GitOps folder where its Helm chart is held. Whenever this folder is updated, this ArgoCD application will apply these updates to the cluster, resulting in a new deployment. The instance of Queue Manager running in the cluster is active prod queue manager ready for use by MQ applications under prod. The instance of MQ Sample Application running in the cluster is active prod sample application under prod. In this topic, we're going to: Access the promotion pipeline Run the promotion pipeline for Queue Manager Run the promotion pipeline for MQ Sample Application Explore the promotion pipeline. Activate ArgoCD application for Queue Manager and MQ Sample Application for prod namespace. Review the resultant GitOps application folders for Queue Manager and MQ Sample Application . Examine the activated ArgoCD application that deploys the Queue Manager and MQ Sample Application to the cluster. Validate Queue Manager and MQ Sample Application . By the end of this topic we'll have a fully functioning staging to prod promotion pipeline that we can use to promote queue manager and sample application from staging environment to prod environment.","title":"Overview"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/#pre-requisites","text":"Before attempting this topic, you should have successfully completed the previous chapter .","title":"Pre-requisites"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/#access-the-promotion-pipeline","text":"As we can see in the diagram above, the promotion pipeline is responsible for promoting the Queue Manager and MQ Sample application from staging to prod . If successful, it leaves a PR with the updated resources for prod in the GitOps apps repository, which are used by ArgoCD to deploy the updated Queue Manager and application to the cluster. It's usually the case a pipeline runs automatically when the Queue Manager or the Sample application is successfully deployed in staging environment. However, our first pipeline run is manual so that you can be in control, making it easier to understand what's happening. Locate the ci pipelines in the web console Let's find the promotion pipeline using the OpenShift web console. Navigate to Pipelines->Pipelines in the left hand pane, and select Project: ci , to show all pipelines in the ci namespace: You can see all the pipelines that we installed into the ci namespace in the previous chapter. We'll use different pipelines for different activities throughout the tutorial. For this topic, we're going to use the ibm-mq-promote-stage-prod pipeline. When the Queue Manager or Sample application are successfully deployed to the staging namespace, this pipeline will perform performance testing and on a successful run, it will promote the existing resources in staging to the prod namespace. The ibm-mq-promote-stage-prod promotion pipeline Select the ibm-mq-promote-stage-prod pipeline from the list of all pipelines: Like all pipelines, ibm-mq-promote-stage-prod is composed from a set of tasks : stage-instance-tests gitops The task name often provides a strong hint of each task's specific function. We'll examine these tasks in detail as the pipeline runs. The oc command as an alternative to the Web console As well as using the OpenShift web console, you can also interact with pipeline using the oc or tekton commands. Ensure you're logged in to cluster, and issue the following command to list the ibm-mq-promote-stage-prod pipeline details: oc get pipeline ibm-mq-promote-stage-prod -n ci which shows a brief summary of the pipeline: NAME AGE ibm-mq-promote-stage-prod 5d18h You can get more detail by adding the -o yaml option; we'll do that later.","title":"Access the promotion pipeline"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/#run-the-promotion-pipeline","text":"","title":"Run the promotion pipeline"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/#queue-manager","text":"Configure your first pipeline run In pipeline details view above, you'll see an Actions button. Select Start to configure a pipeline run. You'll be presented with the following dialog: The supplied arguments allow the user of the pipeline to configure its behavior. For example, a user can use this pipeline with different queue manager source repositories. Configure the run as follows: Set git-url to your fork of the mq-infra repository Set git-revision to master . Set src-environment to staging . Set dest-environment to prod . Set the app-path pointing to your GitOps apps repository which in our case is mq/environments . Set git-pr to true . Set test-file pointing to your jmeter jmx file . If your source repository don't have any jmx file defined, just leave the defaults. Note At the moment, Queue Manager mq-infra repository do not have any performance tests defined using Jmeter. This pipeline is designed in a way to skip this tests if the definitions are not present. However, MQ Sample application mq-spring-app repository have these tests defined and we will be coming across it in the later section of this tutorial. Hit Start to start the pipeline run. You can also use the command line to run a pipeline; we'll explore that option later. Watch a pipeline run executing The pipeline run has now started. Notice how the view changes to Pipeline Run details : We're now looking at the live output from a pipeline run, rather than the pipeline used to create the run. Notice that the pipeline run name ibm-mq-promote-stage-prod-spl3ow is based on the pipeline name -- with a unique suffix. Every new pipeline run will have a unique name. See also how the first stage-instance-tests task is running, while the remaining tasks are waiting to start. Hover over stage-instance-tests task and you will see the steps that comprise it. Watch pipeline steps complete As the pipeline run proceeds, notice all the steps got completed: This pipeline will take about a minute or two to complete.","title":"Queue Manager"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/#mq-sample-application","text":"Configure your first pipeline run In pipeline details view above, you'll see an Actions button. Select Start to configure a pipeline run. You'll be presented with the following dialog: The supplied arguments allow the user of the pipeline to configure its behavior. For example, a user can use this pipeline with different sample application source repositories. Configure the run as follows: Set git-url to your fork of the mq-spring-app repository Set git-revision to master . Set src-environment to staging . Set dest-environment to prod . Set the app-path pointing to your GitOps apps repository which in our case is mq/environments . Set git-pr to true . Set test-plan pointing to your jmeter jmx file . If your source repository don't have any jmx file, just leave the defaults. mq-spring-app repository have the jmx file defined at jmeter/mq-spring-app.jmx using Jmeter. Hit Start to start the pipeline run. You can also use the command line to run a pipeline; we'll explore that option later. Watch a pipeline run executing The pipeline run has now started. Notice how the view changes to Pipeline Run details : We're now looking at the live output from a pipeline run, rather than the pipeline used to create the run. Notice that the pipeline run name ibm-mq-promote-stage-prod-gw8mgi is based on the pipeline name -- with a unique suffix. Every new pipeline run will have a unique name. See also how the first stage-instance-tests task is running, while the remaining tasks are waiting to start. Hover over stage-instance-tests task and you will see the steps that comprise it. Watch pipeline steps complete As the pipeline run proceeds, notice all the steps got completed: This pipeline will take about a minute or two to complete.","title":"MQ Sample Application"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/#explore-the-promotion-pipeline","text":"Let's look more closely at how the ibm-mq-promote-stage-prod pipeline is structured. Let's also examine the tasks and steps that make up the pipeline, and how they progressively validate the deployments in staging namespace, resulting in the production of a Helm chart ready for deployment in prod namespace. Pipeline, Task, Step Let's start with the pipeline, its tasks and steps. Hover over the stage-instance-tests task: See how our pipeline is made up of tasks such as stage-instance-tests and gitops . These run in the order defined by the pipeline. Our pipeline is linear, though Tekton supports more sophisticated pipeline graphs if necessary. See how each task comprises a set of steps such as git-clone or setup . These run in the order defined by the task. The pipeline run logs When a pipeline runs, all its output is captured in a set of logs, one for each task. Click on the stage-instance-tests task to show its logs: ( Alternatively, you can select the Logs tab from the UI, and then select the tasks on the left pane within the pipeline run view. ) See how the stage-instance-tests task has its output in a dedicated log. You can select any log for any task that has completed or is executing. When a pipeline run completes, all its logs remain available, which can help diagnosing problems for example. Exploring an example task output: stage-instance-tests It's easy to examine a task log; you simply select the relevant task and scroll the log up or down. For active tasks the log will be dynamically updated. Click on the stage-instance-tests task: This console window shows the output generated by stage-instance-tests task. As the task script proceeds, its output is captured; that's what we can see in this window. Notice that the stage-instance-tests task output is from the first step in the stage-instance-tests task. This step is called STEP-GIT-CLONE . Note how the step names are capitalized in the web console output. Let's look at another task and its steps more closely. Exploring a task step in detail : STEP-JMETER-IT A task is built of multiple steps. Let's explore the stage-instance-tests task and its step-jmeter-it step. Select the stage-instance-tests task and scroll through its logs to see its third step , STEP-JMETER-IT : See how the step-jmeter-it output is captured in the same log as the previous step git-clone . The pipeline definition Up to this point, we've examined the pipeline run and the logs it generates. Let's now look at how a pipeline is defined. Issue the following command to show the ibm-mq-promote-stage-prod pipeline: oc describe pipeline ibm-mq-promote-stage-prod -n ci which shows the pipeline YAML in considerable detail: Name : ibm-mq-promote-stage-prod Namespace : ci Labels : app.kubernetes.io/instance=apps-mq-rest-ci-1 Annotations : app.openshift.io/runtime : test API Version : tekton.dev/v1beta1 Kind : Pipeline Metadata : Creation Timestamp : 2021-09-23T17:29:24Z Generation : 1 Managed Fields : API Version : tekton.dev/v1beta1 Fields Type : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : f:app.openshift.io/runtime : f:kubectl.kubernetes.io/last-applied-configuration : f:labels : . : f:app.kubernetes.io/instance : f:spec : . : f:params : f:tasks : Manager : argocd-application-controller Operation : Update Time : 2021-09-23T17:29:24Z Resource Version : 2478082 Self Link : /apis/tekton.dev/v1beta1/namespaces/ci/pipelines/ibm-mq-promote-stage-prod UID : d7ed2886-97f4-4c89-beb7-dd9c343dabf1 Spec : Params : Description : The url for the git repository Name : git-url Type : string Default : master Description : git branch for the test app Name : git-revision Type : string Default : staging Description : environment Name : src-environment Type : string Default : prod Description : environment Name : dest-environment Type : string Default : mq/environments Description : Path in gitops repo Name : app-path Type : string Default : true Description : Enable the pipeline to do a PR for the gitops repo Name : git-pr Type : string Default : jmeter/test.jmx Description : Path of the postman collection (jmeter/mq-spring-app.jmx) Name : test-plan Type : string Tasks : Name : stage-instance-tests Params : Name : git-url Value : $(params.git-url) Name : src-environment Value : $(params.src-environment) Name : test-plan Value : $(params.test-plan) Task Ref : Kind : Task Name : ibm-jmeter-performance-test Name : gitops Params : Name : app-name Value : $(tasks.stage-instance-tests.results.app-name) Name : src-environment Value : $(params.src-environment) Name : dest-environment Value : $(params.dest-environment) Name : app-path Value : $(params.app-path) Name : git-pr Value : $(params.git-pr) Run After : stage-instance-tests Task Ref : Kind : Task Name : ibm-gitops Events : <none> Don't be intimidated by this output -- it's actually just a more detailed source view of the information shown for the ibm-mq-promote-stage-prod pipeline in the web console. Locate the following key structures in the Pipeline YAML: API Version: tekton.dev/v1beta1 and Kind: Pipeline identify this as a Tekton pipeline. Spec: Params identifies the pipeline input parameters Tasks: is a list of the tasks in this pipeline, each of which has A Name: naming the task A set of Params: identifying the task input parameters An optional Run After: value indicating when the task is run A Task Ref: identifying the actual task code to be run using the task's input Notice that there's no code in the pipeline definition; instead, the definition specifies the required inputs to the pipeline, as well as the set of required tasks and their order of execution. The code executed by each task is identified by Task Ref: , rather than in the pipeline; the pipeline definition merely defines the order of task execution and how parameters are marshaled between tasks. Let's now examine the pipeline definition in a little more detail. The pipeline input Spec: Params When we configure a pipeline run, the arguments map precisely to Spec: Params in the pipeline YAML file. Below, we've just shown the Spec: Params: for the ibm-mq-promote-stage-prod pipeline: Spec : Params : Description : The url for the git repository Name : git-url Type : string Default : master Description : git branch for the test app Name : git-revision Type : string Default : staging Description : environment Name : src-environment Type : string Default : prod Description : environment Name : dest-environment Type : string Default : mq/environments Description : Path in gitops repo Name : app-path Type : string Default : true Description : Enable the pipeline to do a PR for the gitops repo Name : git-pr Type : string Default : jmeter/test.jmx Description : Path of the postman collection (jmeter/mq-spring-app.jmx) Name : test-plan Type : string Spend a few moments mapping each of these parameters maps to those on the Start Pipeline input dialog where you specified the pipeline run arguments. For example, map Name: , Description: and Default: to the different fields in the dialog. Parameters for the first setup task We can see that the first task in the pipeline is called stage-instance-tests . Let's examine its YAML to see how it gets its input parameters: Tasks : Name : stage-instance-tests Params : Name : git-url Value : $(params.git-url) Name : src-environment Value : $(params.src-environment) Name : test-plan Value : $(params.test-plan) Task Ref : Kind : Task Name : ibm-jmeter-performance-test See how the dev-instance-tests task derives its git-url parameter using the pipeline input parameter value $(params.git-url) . See how src-environment and test-plan work in a similar way. The first task in a pipeline typically works like this -- its parameters are mapped from the pipeline's input parameters. Notice also that some pipeline input parameters are not referred to by the stage-instance-tests task; they will be used by subsequent tasks using the appropriate $(params.) value. Passing arguments between tasks As each task completes, the pipeline proceeds. When a new task starts it often requires one or more results generated by a previous task. We can see a good example of this in the gitops task: Name : gitops Params : Name : app-name Value : $(tasks.stage-instance-tests.results.app-name) Name : src-environment Value : $(params.src-environment) Name : dest-environment Value : $(params.dest-environment) Name : app-path Value : $(params.app-path) Name : git-pr Value : $(params.git-pr) Run After : stage-instance-tests Task Ref : Kind : Task Name : ibm-gitops See how the gitops task specifies that the app-name parameter should use value generated by the stage-instance-tests task using the syntax: $(tasks.stage-instance-tests.results.app-name) . Also notice how the gitops tasks uses Run After: to specify that it should execute after the stage-instance-tests task. This follows the Kubernetes idiom of resources being declarative -- the order of execution is defined by RunAfter: rather than the order in which tasks appear in the YAML. Again, notice that the gitops task doesn't contain the code that the task executes. This is contained in Task Ref: which identifies ibm-gitops as the task to execute using the specified parameters. It's the code in ibm-gitops which generates the log output for the task; we'll examine it later. The way pipelines tasks are designed makes them highly reusable. As we'll see later, tasks are written with defined inputs and outputs such that they can be re-used by different pipelines. Pipelines focus on organizing the order of task execution and how parameters are marshaled into and between tasks; it's the tasks that do the actual work. Locating the pipeline and tasks source YAMLs Finally, let's locate the source for the ibm-mq-promote-stage-prod pipeline and the tasks within it. It is located in the following folder: cd $HOME /git cd multi-tenancy-gitops-apps tree mq/environments/ci/pipelines/ We can see the other pipelines for the ci namespace in this folder: mq/environments/ci/pipelines/ \u251c\u2500\u2500 ibm-test-pipeline-for-dev.yaml \u251c\u2500\u2500 ibm-test-pipeline-for-stage.yaml \u251c\u2500\u2500 java-maven-dev-pipeline.yaml \u251c\u2500\u2500 mq-pipeline-dev.yaml \u2514\u2500\u2500 mq-spring-app-dev-pipeline.yaml These map to the MQ-related pipelines we saw in the Pipelines->Pipelines view in the web console. Exploring the ibm-mq-promote-stage-prod source YAML View the source for the ibm-test-pipeline-for-stage.yaml pipeline with the command: cat mq/environments/ci/pipelines/ibm-test-pipeline-for-stage.yaml which shows the source YAML for the ibm-mq-promote-dev-stage pipeline: apiVersion : tekton.dev/v1beta1 kind : Pipeline metadata : name : ibm-mq-promote-stage-prod annotations : app.openshift.io/runtime : test spec : params : - name : git-url description : The url for the git repository - name : git-revision description : git branch for the test app default : master - name : src-environment description : environment default : staging - name : dest-environment description : environment default : prod - name : app-path description : Path in gitops repo default : mq/environments - name : git-pr description : Enable the pipeline to do a PR for the gitops repo default : \"true\" - name : test-plan description : Path of the postman collection (jmeter/mq-spring-app.jmx) default : \"jmeter/test.jmx\" tasks : - name : stage-instance-tests taskRef : name : ibm-jmeter-performance-test params : - name : git-url value : \"$(params.git-url)\" - name : src-environment value : \"$(params.src-environment)\" - name : test-plan value : \"$(params.test-plan)\" - name : gitops taskRef : name : ibm-gitops runAfter : - stage-instance-tests params : - name : app-name value : \"$(tasks.stage-instance-tests.results.app-name)\" - name : src-environment value : \"$(params.src-environment)\" - name : dest-environment value : \"$(params.dest-environment)\" - name : app-path value : \"$(params.app-path)\" - name : git-pr value : \"$(params.git-pr)\" This YAML is slightly different to the output of the oc get pipeline command, because extra information is added during deployment such as Creation Timestamp: . Finding the ArgoCD application that manages pipelines You can see how the ibm-mq-promote-stage-prod and other pipeline YAMLs were deployed by examining the ArgoCD application that watches the folder containing the ci namespace pipelines. Issue the following command: cat mq/config/argocd/ci/ci-app-rest.yaml which shows the apps-mq-rest-ci-1 ArgoCD application that watches for updates: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : apps-mq-rest-ci-1 annotations : argocd.argoproj.io/sync-wave : \"300\" finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : ci server : https://kubernetes.default.svc project : applications source : path : mq/environments/ci repoURL : https://github.com/hp-gitops-test/multi-tenancy-gitops-apps targetRevision : master syncPolicy : automated : prune : true selfHeal : true See how this apps-mq-rest-ci-1 watches: path: mq/environments/ci . As we know, this folder contains the YAML for the ibm-mq-promote-stage-prod and other pipelines under pipelines folder. When this ArgoCD application was made active in the cluster, it installed all the pipelines along with other resources in this folder.","title":"Explore the promotion pipeline"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/#activate-argocd-applications","text":"We're now going to activate the ArgoCD applications to manage the deployment of Queue Manager and MQ Sample application to the prod namespace. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Select resources to deploy Access the 0-bootstrap/single-cluster/3-apps/kustomization.yaml : cat 0 -bootstrap/single-cluster/3-apps/kustomization.yaml Open 0-bootstrap/single-cluster/3-apps/kustomization.yaml and uncomment the below: - argocd/mq/prod.yaml Once modified, it should be something like below: resources: #- argocd/ace/cicd.yaml #- argocd/ace/dev.yaml #- argocd/ace/stage.yaml #- argocd/ace/prod.yaml - argocd/mq/cicd.yaml - argocd/mq/dev.yaml - argocd/mq/stage.yaml - argocd/mq/prod.yaml #- argocd/apic/cicd.yaml #- argocd/apic/dev.yaml #- argocd/apic/stage.yaml #- argocd/apic/prod.yaml #- argocd/bookinfo/cicd.yaml #- argocd/bookinfo/dev.yaml #- argocd/bookinfo/stage.yaml #- argocd/bookinfo/prod.yaml #- argocd/soapserver/soapserver.yaml patches: - target: group: argoproj.io kind: Application labelSelector: \"gitops.tier.layer=applications\" patch: | - - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD applications application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Activating prod instance for queue manager and sample app\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11 , done . Counting objects: 100 % ( 11 /11 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 6 /6 ) , done . Writing objects: 100 % ( 6 /6 ) , 515 bytes | 257 .00 KiB/s, done . Total 6 ( delta 5 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 5 /5 ) , completed with 5 local objects. To https://github.com/hp-gitops-mq/multi-tenancy-gitops.git 3225847 ..25f7459 master -> master This change to the GitOps repository can now be used by ArgoCD. The applications argocd application Let's examine the ArgoCD application that manage the applications in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the applications application:","title":"Activate ArgoCD Applications"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/#review-the-resultant-gitops-application-folders","text":"The result of our previously successful ibm-mq-promote-stage-prod pipeline runs was to leave a PR in a GitOps folder for Queue Manager and MQ Sample application . This PR create a Helm chart if one is not present in the prod folder. If the Helm chart is already there, it updates the version in the requirements.yaml to match the latest deployment currently residing in the staging namespace. Ensure you've set up the $GIT_ORG environment variable This chapter also uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Open a new terminal window . Replace <git-org> in the following command with your GitHub user name: export GIT_ORG = <git-org> You can verify your $GIT_ORG as follows: echo $GIT_ORG which will show your GitHub Organization, for example: prod-ref-guide Grab the multi-tenancy-gitops-apps repository url Issue the below command to get the GitOps apps repository url. echo https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps/pulls which will give you an url, for instance: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps/pulls Review the Pull Requests created by ibm-mq-promote-dev-stage pipeline Open the above url in your browser and you will see Pull Requests generated by the ibm-mq-promote-dev-stage pipeline. Merge these Pull Requests. Re-merging local clone to view prod resources in GitOps apps repository The ibm-mq-promote-stage-prod pipeline left a Pull request and we merged the changes in the previous step. This means that our local clone of the GitOps repository is one commit behind GitHub. To allow us to view the changes locally, we must re-merge our local branch with GitHub. Return to the terminal window you're using for the multi-tenancy-gitops-apps GitOps apps repository. Issue the following command to change to your GitOps apps repository: cd $HOME /git cd multi-tenancy-gitops-apps Issue the following commands to merge the local branch: git fetch origin git merge origin/ $GIT_BRANCH which shows our local branch being updated: Updating ea6706d..8882bb9 Fast-forward mq/environments/prod/mq-infra/Chart.yaml | 4 ++++ mq/environments/prod/mq-infra/requirements.yaml | 4 ++++ mq/environments/prod/mq-infra/values.yaml | 3 +++ mq/environments/prod/mq-spring-app/Chart.yaml | 4 ++++ mq/environments/prod/mq-spring-app/requirements.yaml | 4 ++++ mq/environments/prod/mq-spring-app/values.yaml | 3 +++ 6 files changed, 22 insertions ( + ) create mode 100644 mq/environments/prod/mq-infra/Chart.yaml create mode 100644 mq/environments/prod/mq-infra/requirements.yaml create mode 100644 mq/environments/prod/mq-infra/values.yaml create mode 100644 mq/environments/prod/mq-spring-app/Chart.yaml create mode 100644 mq/environments/prod/mq-spring-app/requirements.yaml create mode 100644 mq/environments/prod/mq-spring-app/values.yaml Notice how these files correspond to the new Helm chart created in the GitOps apps repository by the ibm-mq-promote-stage-prod pipeline run. Explore the Helm chart in the GitOps Apps repository Let's examine the newly produced Helm charts in the GitOps apps repository; it was created by the previous pipeline runs. Issue the following command: tree mq/environments/prod/mq-infra/ which shows the newly produced Helm chart: mq/environments/prod/mq-infra/ \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml Notice that: The chart for Queue Manager was created in the mq/environments/ folder to reflect the fact this queue manager is part of the applications layer. The chart was created in the prod subfolder to reflect the fact that it's going to be deployed to the prod namespace. The chart was created in a new folder /mq-infra . This folder is dedicated to Queue Manager . Issue the following command: tree mq/environments/prod/mq-spring-app/ which shows the newly produced Helm chart: mq/environments/prod/mq-spring-app/ \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml Notice that: The chart for MQ Sample application was created in the mq/environments/ folder to reflect the fact this application is part of the applications layer. The chart was created in the prod subfolder to reflect the fact that it's going to be deployed to the prod namespace. The chart was created in a new folder /mq-spring-app . This folder is dedicated to MQ Sample application .","title":"Review the resultant GitOps application folders"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/#examine-the-activated-argocd-applications","text":"We're now going to examine the activated ArgoCD applications that uses the above Helm charts to manage the deployment of Queue Manager and MQ Sample application to the prod namespace.","title":"Examine the activated ArgoCD applications"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/#queue-manager_1","text":"The ArgoCD application for Queue Manager QM1 has its deployment to the cluster managed by a dedicated ArgoCD application called prod-mq-infra-instance . Issue the following command to show the ArgoCD application details: cat mq/config/argocd/prod/prod-mq-infra-instance.yaml which shows a YAML file typical of those we've seen before: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: prod-mq-infra-instance annotations: argocd.argoproj.io/sync-wave: \"300\" finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: namespace: prod server: https://kubernetes.default.svc project: applications source: path: mq/environments/prod/mq-infra repoURL: https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps targetRevision: master helm: valueFiles: - values.yaml syncPolicy: automated: prune: true selfHeal: true See how the Helm chart we explored in the previous section of this topic is referenced by path: mq/environments/prod/mq-infra : mq/environments/prod/mq-infra \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml The ArgoCD application applies this Helm chart to the cluster to instantiate QM1 as a set of cluster resources. Look at active Queue Manager ArgoCD application Let's examine QM1 and its Kubernetes resources using the ArgoCD UI. In the ArgoCD UI search the Applications view with the keyword prod-mq-infra : ( You may need to launch the ArgoCD UI again. Refer to these instructions . ) View the new Queue Manager Kubernetes resources We can look at the deployed instance of QM1 and its dependent kubernetes resources. Click on the prod-mq-infra-instance ArgoCD application:","title":"Queue Manager"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/#mq-sample-application_1","text":"The ArgoCD application for MQ Sample Application MQ Sample Application has its deployment to the cluster managed by a dedicated ArgoCD application called prod-mq-spring-app-instance . Issue the following command to show the ArgoCD application details: cat mq/config/argocd/prod/prod-mq-spring-app-instance.yaml which shows a YAML file typical of those we've seen before: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: prod-mq-spring-app-instance annotations: argocd.argoproj.io/sync-wave: \"300\" finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: namespace: prod server: https://kubernetes.default.svc project: applications source: path: mq/environments/prod/mq-spring-app repoURL: https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps targetRevision: master helm: valueFiles: - values.yaml syncPolicy: automated: prune: true selfHeal: true See how the Helm chart we explored in the previous section of this topic is referenced by path: mq/environments/prod/mq-spring-app : mq/environments/prod/mq-spring-app \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml The ArgoCD application applies this Helm chart to the cluster to instantiate MQ Sample Application as a set of cluster resources. Look at active MQ Sample Application ArgoCD application Let's examine MQ Sample Application and its Kubernetes resources using the ArgoCD UI. In the ArgoCD UI search the Applications view with the keyword prod-mq-spring : ( You may need to launch the ArgoCD UI again. Refer to these instructions . ) View the new MQ Sample Application Kubernetes resources We can look at the deployed instance of MQ Sample Application and its dependent kubernetes resources. Click on the prod-mq-spring-app-instance ArgoCD application:","title":"MQ Sample Application"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/#validate-the-queue-manager","text":"Connect to the queue manager pod Let's connect to this container, so that we can explore the queue manager. Issue the following command: oc exec -n prod qm-dev-ibm-mq-0 -it -- /bin/bash to connect you to the pod default container, where you'll see the bash prompt: bash-4.4$ Your terminal is now connected to the container running the queue manager. Use MQ command line The container we've connected to has a fully running instance of the queue manager configured according to the YAML generated by the Helm chart. We can use regular MQ commands to verify that this is a regular queue manager. Issue the following command: dspmq -o all to see a full list of the queue managers running in the container: QMNAME ( QM1 ) STATUS ( Running ) DEFAULT ( yes ) STANDBY ( Permitted ) INSTNAME ( Installation1 ) INSTPATH ( /opt/mqm ) INSTVER ( 9 .2.3.0 ) ROLE ( Not configured ) INSTANCE () INSYNC () QUORUM () We can see that QM1 is running, it's the default queue manager, where it's installed in the container file system, and it's version. Display queue manager properties We can also run an MQSC command to display the queue manager properties. These properties were defined in the MQSC file. Issue the following command: runmqsc QM1 <<< \"dis qmgr\" to see the full set of queue manager properties for QM1 : 5724 -H72 ( C ) Copyright IBM Corp. 1994 , 2021 . Starting MQSC for queue manager QM1. 1 : dis qmgr AMQ8408I: Display Queue Manager details. QMNAME ( QM1 ) ACCTCONO ( DISABLED ) ACCTINT ( 1800 ) ACCTMQI ( OFF ) ACCTQ ( OFF ) ACTIVREC ( MSG ) ACTVCONO ( DISABLED ) ACTVTRC ( OFF ) ADVCAP ( ENABLED ) ALTDATE ( 2021 -09-29 ) ALTTIME ( 14 .38.48 ) AMQPCAP ( NO ) AUTHOREV ( DISABLED ) CCSID ( 819 ) CERTLABL ( ) CERTVPOL ( ANY ) CHAD ( DISABLED ) CHADEV ( DISABLED ) CHADEXIT ( ) CHLEV ( DISABLED ) CHLAUTH ( DISABLED ) CLWLDATA ( ) CLWLEXIT ( ) CLWLLEN ( 100 ) CLWLMRUC ( 999999999 ) CLWLUSEQ ( LOCAL ) CMDEV ( DISABLED ) CMDLEVEL ( 923 ) COMMANDQ ( SYSTEM.ADMIN.COMMAND.QUEUE ) CONFIGEV ( DISABLED ) CONNAUTH ( SYSTEM.DEFAULT.AUTHINFO.IDPWOS ) CRDATE ( 2021 -09-29 ) CRTIME ( 14 .38.45 ) CUSTOM ( ) DEADQ ( DEV.DEAD.LETTER.QUEUE ) DEFCLXQ ( SCTQ ) DEFXMITQ ( ) DESCR ( ) DISTL ( YES ) IMGINTVL ( 60 ) IMGLOGLN ( OFF ) IMGRCOVO ( YES ) IMGRCOVQ ( YES ) IMGSCHED ( MANUAL ) INHIBTEV ( DISABLED ) IPADDRV ( IPV4 ) LOCALEV ( DISABLED ) LOGGEREV ( DISABLED ) MARKINT ( 5000 ) MAXHANDS ( 256 ) MAXMSGL ( 4194304 ) MAXPROPL ( NOLIMIT ) MAXPRTY ( 9 ) MAXUMSGS ( 10000 ) MONACLS ( QMGR ) MONCHL ( OFF ) MONQ ( OFF ) PARENT ( ) PERFMEV ( DISABLED ) PLATFORM ( UNIX ) PSMODE ( ENABLED ) PSCLUS ( ENABLED ) PSNPMSG ( DISCARD ) PSNPRES ( NORMAL ) PSRTYCNT ( 5 ) PSSYNCPT ( IFPER ) QMID ( QM1_2021-09-29_14.38.45 ) REMOTEEV ( DISABLED ) REPOS ( ) REPOSNL ( ) REVDNS ( ENABLED ) ROUTEREC ( MSG ) SCHINIT ( QMGR ) SCMDSERV ( QMGR ) SPLCAP ( ENABLED ) SSLCRLNL ( ) SSLCRYP ( ) SSLEV ( DISABLED ) SSLFIPS ( NO ) SSLKEYR ( /run/runmqserver/tls/key ) SSLRKEYC ( 0 ) STATACLS ( QMGR ) STATCHL ( OFF ) STATINT ( 1800 ) STATMQI ( OFF ) STATQ ( OFF ) STRSTPEV ( ENABLED ) SUITEB ( NONE ) SYNCPT TREELIFE ( 1800 ) TRIGINT ( 999999999 ) VERSION ( 09020300 ) XRCAP ( NO ) One MQSC command read. No commands have a syntax error. All valid MQSC commands were processed. Note how CHLAUTH(DISABLED) has been set. This was set in the MQSC file configuration for QM1 in via the mqsc-configmap configmap. Feel free to run other runmqsc commands to explore the queue manager properties. Exit from queue manager container When we're finished exploring the queue manager container, we can exit it. Issue the following command: exit to return to your local machine's command prompt: exit .../git/multi-tenancy-gitops-apps","title":"Validate the Queue Manager"},{"location":"guides/cp4i/mq/cloud-native/promoting-environments-stage-prod/#validate-the-application","text":"Check the application is running Using the location value from the route we can call the application to check its health by appending /health , for example: export PROD_APP_URL = $( oc get route -n staging mq-spring-app -o jsonpath = \"{.spec.host}\" ) curl -X GET https:// $PROD_APP_URL /actuator/health { \"status\" : \"UP\" , \"components\" : { \"HeathEndPointMain\" : { \"status\" : \"UP\" } , \"diskSpace\" : { \"status\" : \"UP\" , \"details\" : { \"total\" :105552732160, \"free\" :74679922688, \"threshold\" :10485760, \"exists\" :true }} , \"jms\" : { \"status\" : \"UP\" , \"details\" : { \"provider\" : \"IBM MQ JMS Provider\" }} , \"livenessState\" : { \"status\" : \"UP\" } , \"ping\" : { \"status\" : \"UP\" } , \"readinessState\" : { \"status\" : \"UP\" }} , \"groups\" : [ \"liveness\" , \"readiness\" ]} Call the application to put a message to a queue Using the same location value we can call the application to to put a message to a queue on the queue manager, for example: curl -X GET https:// $PROD_APP_URL /api/send-hello-world Call the application to get a message from a queue Using the same location value we can call the application to to get a message from a queue on the queue manager, for example: curl -X GET https:// $PROD_APP_URL /api/recv Call the application to put a JSON message to a queue Using the same location value we can call the application to to put a message with a JSON payload to a queue on the queue manager, for example: curl -X POST -H \"Content-Type: application/json\" https:// $PROD_APP_URL /api/send-json --data \"{\\\"data\\\":\\\"Hola Munda\\\"}\" Note the use of the POST verb in the above command. This can be followed with a call to get the message from the queue. Congratulations! You've completed your first run of the promotion pipeline from dev to staging. Feel free to run the ibm-mq-promote-stage-prod pipeline more than once to get a feeling for how it works. You've used it to promote an instance of QM1 and MQ sample application from staging environment to prod environment in the cluster. You've explored how the promotion pipeline for staging to prod is structured as tasks and steps. You've examined it's pipeline run log to understand how this pipeline works and how tasks are implemented. Finally, you've examined how the GitOps apps repository is updated with the prod resources from a successful run of the pipeline. You've completed set up continuous deployment for the Queue Manager as well as the MQ sample application and deployed their instances to the prod namespace in the cluster. You've interacted with this queue manager using the command line as well as verified the application. In the next topic of this chapter we're going to see how we can automatically promote the changes across environments.","title":"Validate the Application"},{"location":"guides/cp4i/mq/cluster-config/gitops-config/","text":"Cloud Native MQ GitOps Configuration \u00b6 In the cluster configuration of this chapter, we installed the fundamental components for continuous integration and continuous deployment into our cluster. These included a sample GitOps repository, and ArgoCD. In this section we're going to customize and enable the GitOps repository, so that we can install the components highlighted in our MQ CICD process: We'll examine these components in more detail throughout this section of the tutorial; here's an initial overview of their function: ci-namespace provide an execution namespace for our pipelines tools-namespace provide an execution namespace for tools such as Artifactory dev-namespace provide an execution namespace for our deployed MQ applications and queue managers when running in the development environment. Later in the tutorial, we'll add staging and prod namespaces. ArgoCD applications manage the ci , tools and dev namespaces. You may have already noticed that the GitOps repository contains YAMLs that refer to the ArgoCD and namespaces in the diagram above. We're now going to use these YAMLs to configure the cluster resources using GitOps. Becoming comfortable with the concepts by practicing GitOps will help us in later chapters when we create MQ applications and queue managers using the same approach. In this topic, we're going to: Explore the sample GitOps repository in a little more detail Customize the GitOps repository for our cluster Connect ArgoCD to the customized GitOps repository Bootstrap the cluster Explore how the ci namespace is created Try out some dynamic changes to the cluster with ArgoCD Explore how ArgoCD manages configuration drift By the end of this topic we'll have a cluster up and running, having used GitOps to do it. We'll fully understand how ArgoCD manages cluster change and configuration drift. Pre-requisites \u00b6 Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed on your local machine the oc command that matches the version of your cluster. You have also installed npm , git and tree commands. You have customized the GitOps repository and ArgoCD. Please see the previous sections of this guide for information on how to do these tasks. Video Walkthrough \u00b6 This video demonstrates how to use ArgoCD and the GitOps repository to set up different infra related components. This is a video walkthrough and it takes you step by step through the below sections. The sample GitOps repository \u00b6 Let's understand how the GitOps repository works by using it to install the components we've highlighted in the above diagram. In the first part of this topic, let's look at the high level structure of the GitOps repository. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Explore the high level folder structure Use the following command to display the folder structure: tree . -d -L 2 We can see the different folders in the GitOps repository: . \u251c\u2500\u2500 0-bootstrap \u2502 \u251c\u2500\u2500 others \u2502 \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 doc \u2502 \u251c\u2500\u2500 diagrams \u2502 \u251c\u2500\u2500 experimental \u2502 \u251c\u2500\u2500 images \u2502 \u2514\u2500\u2500 scenarios \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 bom \u2514\u2500\u2500 setup \u2514\u2500\u2500 ocp47 The 0-bootstrap folder is the key folder that contains all the available profiles. Within this folders are Cluster Profiles that control which resources are deployed to the cluster. There are other folders, but these contain utility scripts; and some document related information. Let us examine the 0-bootstrap folder structure: tree ./0-bootstrap/ -d -L 2 We can see the different folders: ./0-bootstrap/ \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u2514\u2500\u2500 3-apps If we consider single-cluster profile, you can see the three layers under that covering infrastructure, services, and applications. Use single-cluster profile to configure the cluster Use the following command to display the folder structure: tree ./0-bootstrap/single-cluster/ -L 3 We can see the different folders: ./0-bootstrap/single-cluster/ \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 1-infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 2-services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3-apps \u2502 \u251c\u2500\u2500 3-apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 cp4a \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Notice that each high level folder has an argocd folder. Within these argocd folders are ArgoCD applications that control which resources are deployed to the cluster. Sample ArgoCD application We'll see later how these ArgoCD applications work in detail. For now, notice how each layer of the architecture ( infrastructure , service , application ) has a set of ArgoCD applications. Type the following command: tree 0 -bootstrap/single-cluster/1-infra/ It shows a list of ArgoCD applications that are used to manage Kubernetes infrastructure resources: 0-bootstrap/single-cluster/1-infra/ \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Notice the namespace- YAMLs; we'll see in a moment how these each define an ArgoCD application dedicated to managing a Kubernetes namespace in our reference architecture. A word on terminology As we get started with ArgoCD, it can be easy to confuse the term ArgoCD application with your application. That's because ArgoCD uses the term application to refer to a Kubernetes custom resource that was initially designed to automate application management. However, an ArgoCD application can automate the deployment of any Kubernetes resource within a cluster, such as namespaces or queue managers, as we'll see a little later. Let's now customize this repository to deploy the highlighted Kubernetes resources to our cluster. Customizing the GitOps repositories \u00b6 The cloned GitOps configuration repositories (specifically, multi-tenancy-gitops and multi-tenancy-gitops-apps ) needs to be customized for your cluster. The ArgoCD Application and AppProject resources and kustomization.yaml resources contains environment variables which will need to be updated to your specific GitHub organization and branch. Update multi-tenancy-gitops repository \u00b6 Have a look at the following kustomization.yaml as an example of what will need to be updated. cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS_INFRA} - op: add path: /spec/source/targetRevision value: ${GIT_GITOPS_INFRA_BRANCH} Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Run the customization script Let's customize the cloned multi-tenancy-gitops repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. This script has also made some customizations that we'll use much later in the tutorial; we'll refer to those at the relevant time. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 199 files changed, 290 insertions ( + ) , 290 deletions ( - ) Set your GitOps repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (48/48), 4.70 KiB | 1.57 MiB/s, done. Total 48 (delta 32), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 23 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git d95eca5..a900c39 master -> master Update multi-tenancy-gitops-apps repository \u00b6 Similar to the previous section, there are corresponding ArgoCD Applications resources that will need to be updated in the multi-tenancy-gitops-apps repository. Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Locate the multi-tenancy-gitops-apps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops-apps Run the customization script Let's customize the cloned multi-tenancy-gitops-apps repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops-apps repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 21 files changed, 42 insertions ( + ) , 42 deletions ( - ) Set your repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops-apps repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (42/42), 4.70 KiB | 1.57 MiB/s, done. Total 42 (delta 42), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 21 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git d95eca5..a900c39 master -> master Connect ArgoCD to the GitOps repository \u00b6 Let's now connect your customized GitOps repository to the instance of ArgoCD running in the cluster. Once connected, ArgoCD will use the contents of the repository to create necessary resources. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Review ArgoCD infrastructure folder Let's examine the 0-bootstrap/single-cluster/1-infra/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml We can see the contents of the kustomization.yaml : resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Notice that the resources that needs to be applied to the cluster are all inactivate and commented out. Let us enable the resources that are needed by un-commenting them. Deploy Kubernetes resources with kustomization.yaml Open 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the below resources: argocd/consolelink.yaml argocd/consolenotification.yaml argocd/namespace-ibm-common-services.yaml argocd/namespace-ci.yaml argocd/namespace-dev.yaml argocd/namespace-staging.yaml argocd/namespace-prod.yaml argocd/namespace-sealed-secrets.yaml argocd/namespace-tools.yaml You will have the following resources un-commented for infrastructure: resources : - argocd/consolelink.yaml - argocd/consolenotification.yaml - argocd/namespace-ibm-common-services.yaml - argocd/namespace-ci.yaml - argocd/namespace-dev.yaml - argocd/namespace-staging.yaml - argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml - argocd/namespace-sealed-secrets.yaml - argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for infrastructure\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a900c39..e3f696d master -> master Examine bootstrap.yaml residing in 0-bootstrap/single-cluster/ . The bootstrap.yaml file is used to create our first ArgoCD application called bootstrap-single-cluster . This initial ArgoCD application will create all the other ArgoCD applications that control the application, service, and infrastructure resources (such as the ci and dev namespaces) deployed to the cluster. Examine the YAML that defines the ArgoCD bootstrap application: cat 0 -bootstrap/single-cluster/bootstrap.yaml Notice also how this ArgoCD application has been customized to use the GitOps repository repoURL: https://github.com/prod-ref-guide/multi-tenancy-gitops.git . apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-single-cluster namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/single-cluster repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Most importantly, see how path: 0-bootstrap/single-cluster refers to the 0-bootstrap/single-cluster folder within this repository. This will result in the creation of individual ArgoCD applications to manage our cluster resources. Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us for now only deploy infra resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and comment out the 2-services/2-services.yaml and 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml # - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Commit and push changes to your git repository: git add . git commit -s -m \"Using only infra\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 456 bytes | 456.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git e3f696d..ea3b43f master -> master Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. The repository contains a bootstrap-single-cluster ArgoCD application that is watching this repository and using its contents to manage the cluster. When the bootstrap-single-cluster ArgoCD application is applied to the cluster, it will continuously ensure that all the activated resources are applied to the cluster. Apply the bootstrap YAML to the cluster: oc apply -f 0 -bootstrap/single-cluster/bootstrap.yaml Kubernetes will confirm that the bootstrap ArgoCD application has been created: application.argoproj.io/bootstrap-single-cluster created The bootstrap ArgoCD application will watch the 0-bootstrap/single-cluster folder in our GitOps repository on GitHub. In this way, as resources are added to the infrastructure , service and application folders, they will be deployed to the cluster automatically. This is therefore the only direct cluster operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Verify the bootstrap deployment Verify that the bootstrap ArgoCD application is running with the following command: oc get app/bootstrap-single-cluster -n openshift-gitops You should see that the bootstrap application was recently updated: NAME SYNC STATUS HEALTH STATUS bootstrap-single-cluster Synced Healthy HEALTH_STATUS may temporarily show Missing ; simply re-issue the command to confirm it moves to Healthy . Using the UI to view the newly deployed ArgoCD applications In the previous section of this chapter you logged on to the ArgoCD web console. Switch back to that console, refresh the page and you should see the bootstrap-single-cluster ArgoCD application together with many other ArgoCD applications: (You may need to select List view rather than the Tiles view.) We can see that eleven ArgoCD applications have been deployed to the cluster as a result of applying bootstrap.yaml . In the next section of the tutorial, we'll examine these applications to see how and why they were created, but for now let's focus on one of them -- the namespace-ci ArgoCD application. Examining the namespace-ci ArgoCD application resources Let's examine the Kubernetes resources applied to the cluster by the namespace-ci ArgoCD application. In the ArgoCD application list, click on namespace-ci : (You may need to clear filters to see this screenshot.) The directed graph shows that the namespace-ci ArgoCD app has created 4 Kubernetes resources; our ci namespace and three role bindings. Verify the namespace using the oc CLI We've seen the new namespace definition in the GitOps repository and visually in the ArgoCD UI. Let's also verify it via the command line: Type the following command: oc get namespace ci -o yaml This will list the full details of the ci namespace: apiVersion : v1 kind : Namespace metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"namespace-ci\"},\"name\":\"ci\"},\"spec\":{}} openshift.io/sa.scc.mcs : s0:c27,c9 openshift.io/sa.scc.supplemental-groups : 1000720000/10000 openshift.io/sa.scc.uid-range : 1000720000/10000 creationTimestamp : \"2021-08-31T15:27:32Z\" labels : app.kubernetes.io/instance : namespace-ci managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:labels : . : {} f:app.kubernetes.io/instance : {} f:status : f:phase : {} manager : argocd-application-controller operation : Update time : \"2021-08-31T15:27:32Z\" - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : f:openshift.io/sa.scc.mcs : {} f:openshift.io/sa.scc.supplemental-groups : {} f:openshift.io/sa.scc.uid-range : {} manager : cluster-policy-controller operation : Update time : \"2021-08-31T15:27:32Z\" name : ci resourceVersion : \"2255607\" selfLink : /api/v1/namespaces/ci uid : fff6b82b-6318-4828-83bb-ade4e8e3c0cf spec : finalizers : - kubernetes status : phase : Active Notice how manager: argocd-application-controller identifies that this namespace was created by ArgoCD. It's important to understand the sequence of actions. We simply deployed the bootstrap-single-cluster ArgoCD application, and it ultimately resulted in the creation of the namespace-ci ArgoCD application which created the ci namespace. We don't directly apply resources to the cluster once the bootstrap-single-cluster ArgoCD application has been applied; the cluster state is determined by an ArgoCD application YAML in the corresponding application , service or infrastructure related folders. It's these ArgoCD applications that create and manage the underlying Kubernetes resources using the GitOps repository as the source of truth. Understanding the namespace-ci ArgoCD application Let's examine the ArgoCD application namespace-ci to see how it created the ci namespace and three role bindings in the cluster. Issue the following command to examine its YAML: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-ci.yaml Notice that apiVersion and kind identify this as an ArgoCD application: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ci labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" spec : destination : namespace : ci server : https://kubernetes.default.svc project : infra source : path : namespaces/ci syncPolicy : automated : prune : true selfHeal : true Most importantly, see how this namespace-ci ArgoCD application monitors the folder path: namespaces/ci in https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git ; it then applies the contents of this folder to the cluster whenever its content changes. Notice the SyncPolicy of automated ; any changes to this folder will automatically be applied to the cluster; we do not need to perform a manual Sync operation from the ArgoCD UI. Examine the ci namespace YAML To examine this, navigate to multi-tenancy-gitops-infra repository you cloned previously. cd $HOME /git cd multi-tenancy-gitops-infra Let's examine the ci namespace YAML in the namespaces/ci folder: cat namespaces/ci/namespace.yaml It's a very simple YAML: apiVersion : v1 kind : Namespace metadata : name : ci spec : {} This is a YAML that we would normally apply to the cluster manually or via a script. However, when we use GitOps, we push the ArgoCD application that uses this YAML to GitHub, and it applies the ci namespace YAML to the cluster. This is the essence of GitOps; we declare what we want to appear in the cluster using Git and ArgoCD synchronizes the cluster with this declaration. Examine the ci rolebinding YAML Now that we've seen how the namespace was created, let's see how the three other rolebindings were created by the namespace-ci ArgoCD application. In the same namespace/ci folder as the ci namespace YAML, there is a rolebinding.yaml file. This file will be also applied to the cluster by the namespace-ci ArgoCD application which is continuously watching this folder. Examine this file with the following command: cat namespaces/ci/rolebinding.yaml This YAML is slightly more complex than the namespace YAML: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-dev namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:dev --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-staging namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:staging --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-prod namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:prod --- However its structure is quite straightforward; look carefully and you'll see that there are indeed three rolebindings defined in this YAML. Each of these rolebindings binds different roles to different service accounts associated with components that will run within the ci namespace, such as Tekton pipelines for example. We'll see later how these rolebindings are important; they limit the operations that can be performed by the ci namespace, creating a well governed cluster. This confirms why we saw four resources created by the namespace-ci ArgoCD application in the ArgoCD UI: one namespace and three rolebindings. Again, notice the pattern: a single ArgoCD application manages one or more Kubernetes resources in the cluster -- using one or more YAML files in which those resources are defined. The bootstrap-single-cluster ArgoCD application in more detail In the ArgoCD UI Applications view, click on the bootstrap-single-cluster application: You can see the bootstrap application creates two Kubernetes resources, the infra ArgoCD application and the infra ArgoCD project. An ArgoCD project is a mechanism by which we can group related resources; we keep all our ArgoCD applications that manage infrastructure in the infra project. Later, we'll create a services project for the ArgoCD applications that manage the services we want deployed to the cluster such as the MQ application and queue manager pipelines. The infra ArgoCD application Let's examine the infra ArgoCD application in more detail to see how it works. In the ArgoCD UI Applications view, click on the open application icon for the infra application: We can see that the infra ArgoCD application creates 9 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It's the infra ArgoCD application that watches the 0-bootstrap/single-cluster/1-infra/argocd folder for ArgoCD applications that apply infrastructure resources to our cluster. It was the infra application that created the namespace-ci ArgoCD application which manages the ci namespace that we've been exploring in this section of the tutorial. We'll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it's not necessary, as you'll get lots of practice as we proceed. ArgoCD change management and governance \u00b6 In this final section about managing infrastructure resources, let's explore how ArgoCD provides some advanced resource management features: Dynamic Monitored Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Customize the web console banner Examine the banner in the OpenShift web console: We're going to use GitOps to modify this banner dynamically. The banner YAML This banner properties are defined by the YAML in /0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml . This YAML is currently being used by the cntk-consolenotification ArgoCD application that was deployed earlier. We can examine the YAML with the following command: cat 0 -bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml which shows the banner properties are part of the ConsoleNotification custom resource: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : cntk-consolenotification labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" finalizers : - resources-finalizer.argocd.argoproj.io spec : syncPolicy : automated : prune : true selfHeal : true destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : path : consolenotification helm : values : | ocp-console-notification: ## The name of the ConsoleNotification resource in the cluster name: \"banner-env\" ## The background color that should be used for the banner backgroundColor: teal ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text: \"Cluster Description\" See how the banner at the top of the screen: * contains the text Cluster Description * is located at top of the screen * has the color Teal Modify the YAML for this banner Let's now change this YAML In your editor, modify this YAML and change the below fields as follows: ocp-console-notification : ## The name of the ConsoleNotification resource in the cluster name : \"banner-env\" ## The background color that should be used for the banner backgroundColor : red ## The color of the text that will appear in the banner color : \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location : BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text : \"Production reference guide\" It's clear that our intention is to modify the banner's backgroundColor:red and text: Production reference guide to the newly specified values. If you look at the diff : git diff you should see the following diff --git a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml index 30adf1a..596e821 100644 --- a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml +++ b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml @@ -26,10 +26,10 @@ spec: name: \"banner-env\" ## The background color that should be used for the banner - backgroundColor: teal + backgroundColor: red ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created - text: \"Cluster Description\" + text: \"Production reference guide\" Make the web console YAML change active Let's make these changes visible to the cntk-consolenotification ArgoCD application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Modify console banner\" git push origin $GIT_BRANCH You'll see the changes being pushed to GitHub: Enumerating objects: 13, done. Counting objects: 100% (13/13), done. Delta compression using up to 8 threads Compressing objects: 100% (7/7), done. Writing objects: 100% (7/7), 670 bytes | 670.00 KiB/s, done. Total 7 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a1e8292..b49dff5 master -> master Let's see what effect they have on the web console. A dynamic change to the web console You can either wait for ArgoCD to automatically sync the cntk-consolenotification application or manually Sync it yourself: Returning to the OpenShift web console, you'll notice changes. Notice the dynamic nature of these changes; we updated the console YAML, pushed our changes to our GitOps repository and everything else happened automatically. As a result, our OpenShift console has a new banner color and text. Governing changes to the dev namespace Let's now look at how ArgoCD monitors Kubernetes resources for configuration drift, and what happens if it detects an unexpected change to a monitored resource. Don't worry about the following command; it might seem drastic and even reckless, but as you'll see, everything will be OK. Let's delete the dev namespace from the cluster: oc get namespace dev oc delete namespace dev See how the active namespace: NAME STATUS AGE dev Active 2d18h is deleted: namespace \"dev\" deleted We can see that the dev namespace has been manually deleted from the cluster. GitOps repository as a source of truth If you switch back to the ArgoCD UI, you may see that ArgoCD has detected a configuration drift: a resource is Missing (the dev namespace) namespace-dev therefore is OutOfSync namespace-dev is therefore Syncing with the GitOps repository After a while we'll see that namespace-dev is Healthy and Synced : ArgoCD has detected a configuration drift, and resynched with GitOps repository, re-applying the dev namespace to the cluster. Note You may miss seeing the first screenshot if ArgoCD detects and corrects the missing dev namespace before you get a chance to switch to the ArgoCD UI. Don't worry, you can try this operator again! The restored dev namespace Issue the following command to determine the status of the dev namespace: oc get namespace dev which confirms that the dev namespace has been re-instated: NAME STATUS AGE dev Active 115s Note that it is a different instance of the dev namespace; as indicated by its AGE value. Notice the governed nature of these changes; GitOps is our source of truth about the resources deployed to the cluster. ArgoCD restores any resources that suffer from configuration drift to their GitOps-defined configuration. Congratulations! You've used ArgoCD and the GitOps repository to set up ci , tools and dev namespaces. You've seen how to create ArgoCD applications that watch their respective GitOps namespace folders for details of the namespace resources they should apply to the cluster. You've seen how you can dynamically change deployed resources by updating the resource definition in the GitOps repository. Finally, you've experience how ArgoCD keeps the cluster synchronized with the GitOps repository as a source of truth; any unexpected configuration drift will be corrected without intervention. In the next section of this tutorial, we're going to deploy some services into the infrastructure namespaces we've created in this topic. These services will include Artifactory and MQ operator.","title":"Add infrastructure"},{"location":"guides/cp4i/mq/cluster-config/gitops-config/#cloud-native-mq-gitops-configuration","text":"In the cluster configuration of this chapter, we installed the fundamental components for continuous integration and continuous deployment into our cluster. These included a sample GitOps repository, and ArgoCD. In this section we're going to customize and enable the GitOps repository, so that we can install the components highlighted in our MQ CICD process: We'll examine these components in more detail throughout this section of the tutorial; here's an initial overview of their function: ci-namespace provide an execution namespace for our pipelines tools-namespace provide an execution namespace for tools such as Artifactory dev-namespace provide an execution namespace for our deployed MQ applications and queue managers when running in the development environment. Later in the tutorial, we'll add staging and prod namespaces. ArgoCD applications manage the ci , tools and dev namespaces. You may have already noticed that the GitOps repository contains YAMLs that refer to the ArgoCD and namespaces in the diagram above. We're now going to use these YAMLs to configure the cluster resources using GitOps. Becoming comfortable with the concepts by practicing GitOps will help us in later chapters when we create MQ applications and queue managers using the same approach. In this topic, we're going to: Explore the sample GitOps repository in a little more detail Customize the GitOps repository for our cluster Connect ArgoCD to the customized GitOps repository Bootstrap the cluster Explore how the ci namespace is created Try out some dynamic changes to the cluster with ArgoCD Explore how ArgoCD manages configuration drift By the end of this topic we'll have a cluster up and running, having used GitOps to do it. We'll fully understand how ArgoCD manages cluster change and configuration drift.","title":"Cloud Native MQ GitOps Configuration"},{"location":"guides/cp4i/mq/cluster-config/gitops-config/#pre-requisites","text":"Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed on your local machine the oc command that matches the version of your cluster. You have also installed npm , git and tree commands. You have customized the GitOps repository and ArgoCD. Please see the previous sections of this guide for information on how to do these tasks.","title":"Pre-requisites"},{"location":"guides/cp4i/mq/cluster-config/gitops-config/#video-walkthrough","text":"This video demonstrates how to use ArgoCD and the GitOps repository to set up different infra related components. This is a video walkthrough and it takes you step by step through the below sections.","title":"Video Walkthrough"},{"location":"guides/cp4i/mq/cluster-config/gitops-config/#the-sample-gitops-repository","text":"Let's understand how the GitOps repository works by using it to install the components we've highlighted in the above diagram. In the first part of this topic, let's look at the high level structure of the GitOps repository. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Explore the high level folder structure Use the following command to display the folder structure: tree . -d -L 2 We can see the different folders in the GitOps repository: . \u251c\u2500\u2500 0-bootstrap \u2502 \u251c\u2500\u2500 others \u2502 \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 doc \u2502 \u251c\u2500\u2500 diagrams \u2502 \u251c\u2500\u2500 experimental \u2502 \u251c\u2500\u2500 images \u2502 \u2514\u2500\u2500 scenarios \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 bom \u2514\u2500\u2500 setup \u2514\u2500\u2500 ocp47 The 0-bootstrap folder is the key folder that contains all the available profiles. Within this folders are Cluster Profiles that control which resources are deployed to the cluster. There are other folders, but these contain utility scripts; and some document related information. Let us examine the 0-bootstrap folder structure: tree ./0-bootstrap/ -d -L 2 We can see the different folders: ./0-bootstrap/ \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u2514\u2500\u2500 3-apps If we consider single-cluster profile, you can see the three layers under that covering infrastructure, services, and applications. Use single-cluster profile to configure the cluster Use the following command to display the folder structure: tree ./0-bootstrap/single-cluster/ -L 3 We can see the different folders: ./0-bootstrap/single-cluster/ \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 1-infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 2-services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3-apps \u2502 \u251c\u2500\u2500 3-apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 cp4a \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Notice that each high level folder has an argocd folder. Within these argocd folders are ArgoCD applications that control which resources are deployed to the cluster. Sample ArgoCD application We'll see later how these ArgoCD applications work in detail. For now, notice how each layer of the architecture ( infrastructure , service , application ) has a set of ArgoCD applications. Type the following command: tree 0 -bootstrap/single-cluster/1-infra/ It shows a list of ArgoCD applications that are used to manage Kubernetes infrastructure resources: 0-bootstrap/single-cluster/1-infra/ \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Notice the namespace- YAMLs; we'll see in a moment how these each define an ArgoCD application dedicated to managing a Kubernetes namespace in our reference architecture. A word on terminology As we get started with ArgoCD, it can be easy to confuse the term ArgoCD application with your application. That's because ArgoCD uses the term application to refer to a Kubernetes custom resource that was initially designed to automate application management. However, an ArgoCD application can automate the deployment of any Kubernetes resource within a cluster, such as namespaces or queue managers, as we'll see a little later. Let's now customize this repository to deploy the highlighted Kubernetes resources to our cluster.","title":"The sample GitOps repository"},{"location":"guides/cp4i/mq/cluster-config/gitops-config/#customizing-the-gitops-repositories","text":"The cloned GitOps configuration repositories (specifically, multi-tenancy-gitops and multi-tenancy-gitops-apps ) needs to be customized for your cluster. The ArgoCD Application and AppProject resources and kustomization.yaml resources contains environment variables which will need to be updated to your specific GitHub organization and branch.","title":"Customizing the GitOps repositories"},{"location":"guides/cp4i/mq/cluster-config/gitops-config/#update-multi-tenancy-gitops-repository","text":"Have a look at the following kustomization.yaml as an example of what will need to be updated. cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS_INFRA} - op: add path: /spec/source/targetRevision value: ${GIT_GITOPS_INFRA_BRANCH} Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Run the customization script Let's customize the cloned multi-tenancy-gitops repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. This script has also made some customizations that we'll use much later in the tutorial; we'll refer to those at the relevant time. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 199 files changed, 290 insertions ( + ) , 290 deletions ( - ) Set your GitOps repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (48/48), 4.70 KiB | 1.57 MiB/s, done. Total 48 (delta 32), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 23 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git d95eca5..a900c39 master -> master","title":"Update multi-tenancy-gitops repository"},{"location":"guides/cp4i/mq/cluster-config/gitops-config/#update-multi-tenancy-gitops-apps-repository","text":"Similar to the previous section, there are corresponding ArgoCD Applications resources that will need to be updated in the multi-tenancy-gitops-apps repository. Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Locate the multi-tenancy-gitops-apps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops-apps Run the customization script Let's customize the cloned multi-tenancy-gitops-apps repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops-apps repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 21 files changed, 42 insertions ( + ) , 42 deletions ( - ) Set your repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops-apps repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (42/42), 4.70 KiB | 1.57 MiB/s, done. Total 42 (delta 42), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 21 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git d95eca5..a900c39 master -> master","title":"Update multi-tenancy-gitops-apps repository"},{"location":"guides/cp4i/mq/cluster-config/gitops-config/#connect-argocd-to-the-gitops-repository","text":"Let's now connect your customized GitOps repository to the instance of ArgoCD running in the cluster. Once connected, ArgoCD will use the contents of the repository to create necessary resources. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Review ArgoCD infrastructure folder Let's examine the 0-bootstrap/single-cluster/1-infra/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml We can see the contents of the kustomization.yaml : resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Notice that the resources that needs to be applied to the cluster are all inactivate and commented out. Let us enable the resources that are needed by un-commenting them. Deploy Kubernetes resources with kustomization.yaml Open 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the below resources: argocd/consolelink.yaml argocd/consolenotification.yaml argocd/namespace-ibm-common-services.yaml argocd/namespace-ci.yaml argocd/namespace-dev.yaml argocd/namespace-staging.yaml argocd/namespace-prod.yaml argocd/namespace-sealed-secrets.yaml argocd/namespace-tools.yaml You will have the following resources un-commented for infrastructure: resources : - argocd/consolelink.yaml - argocd/consolenotification.yaml - argocd/namespace-ibm-common-services.yaml - argocd/namespace-ci.yaml - argocd/namespace-dev.yaml - argocd/namespace-staging.yaml - argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml - argocd/namespace-sealed-secrets.yaml - argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for infrastructure\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a900c39..e3f696d master -> master Examine bootstrap.yaml residing in 0-bootstrap/single-cluster/ . The bootstrap.yaml file is used to create our first ArgoCD application called bootstrap-single-cluster . This initial ArgoCD application will create all the other ArgoCD applications that control the application, service, and infrastructure resources (such as the ci and dev namespaces) deployed to the cluster. Examine the YAML that defines the ArgoCD bootstrap application: cat 0 -bootstrap/single-cluster/bootstrap.yaml Notice also how this ArgoCD application has been customized to use the GitOps repository repoURL: https://github.com/prod-ref-guide/multi-tenancy-gitops.git . apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-single-cluster namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/single-cluster repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Most importantly, see how path: 0-bootstrap/single-cluster refers to the 0-bootstrap/single-cluster folder within this repository. This will result in the creation of individual ArgoCD applications to manage our cluster resources. Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us for now only deploy infra resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and comment out the 2-services/2-services.yaml and 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml # - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Commit and push changes to your git repository: git add . git commit -s -m \"Using only infra\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 456 bytes | 456.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git e3f696d..ea3b43f master -> master Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. The repository contains a bootstrap-single-cluster ArgoCD application that is watching this repository and using its contents to manage the cluster. When the bootstrap-single-cluster ArgoCD application is applied to the cluster, it will continuously ensure that all the activated resources are applied to the cluster. Apply the bootstrap YAML to the cluster: oc apply -f 0 -bootstrap/single-cluster/bootstrap.yaml Kubernetes will confirm that the bootstrap ArgoCD application has been created: application.argoproj.io/bootstrap-single-cluster created The bootstrap ArgoCD application will watch the 0-bootstrap/single-cluster folder in our GitOps repository on GitHub. In this way, as resources are added to the infrastructure , service and application folders, they will be deployed to the cluster automatically. This is therefore the only direct cluster operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Verify the bootstrap deployment Verify that the bootstrap ArgoCD application is running with the following command: oc get app/bootstrap-single-cluster -n openshift-gitops You should see that the bootstrap application was recently updated: NAME SYNC STATUS HEALTH STATUS bootstrap-single-cluster Synced Healthy HEALTH_STATUS may temporarily show Missing ; simply re-issue the command to confirm it moves to Healthy . Using the UI to view the newly deployed ArgoCD applications In the previous section of this chapter you logged on to the ArgoCD web console. Switch back to that console, refresh the page and you should see the bootstrap-single-cluster ArgoCD application together with many other ArgoCD applications: (You may need to select List view rather than the Tiles view.) We can see that eleven ArgoCD applications have been deployed to the cluster as a result of applying bootstrap.yaml . In the next section of the tutorial, we'll examine these applications to see how and why they were created, but for now let's focus on one of them -- the namespace-ci ArgoCD application. Examining the namespace-ci ArgoCD application resources Let's examine the Kubernetes resources applied to the cluster by the namespace-ci ArgoCD application. In the ArgoCD application list, click on namespace-ci : (You may need to clear filters to see this screenshot.) The directed graph shows that the namespace-ci ArgoCD app has created 4 Kubernetes resources; our ci namespace and three role bindings. Verify the namespace using the oc CLI We've seen the new namespace definition in the GitOps repository and visually in the ArgoCD UI. Let's also verify it via the command line: Type the following command: oc get namespace ci -o yaml This will list the full details of the ci namespace: apiVersion : v1 kind : Namespace metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"namespace-ci\"},\"name\":\"ci\"},\"spec\":{}} openshift.io/sa.scc.mcs : s0:c27,c9 openshift.io/sa.scc.supplemental-groups : 1000720000/10000 openshift.io/sa.scc.uid-range : 1000720000/10000 creationTimestamp : \"2021-08-31T15:27:32Z\" labels : app.kubernetes.io/instance : namespace-ci managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:labels : . : {} f:app.kubernetes.io/instance : {} f:status : f:phase : {} manager : argocd-application-controller operation : Update time : \"2021-08-31T15:27:32Z\" - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : f:openshift.io/sa.scc.mcs : {} f:openshift.io/sa.scc.supplemental-groups : {} f:openshift.io/sa.scc.uid-range : {} manager : cluster-policy-controller operation : Update time : \"2021-08-31T15:27:32Z\" name : ci resourceVersion : \"2255607\" selfLink : /api/v1/namespaces/ci uid : fff6b82b-6318-4828-83bb-ade4e8e3c0cf spec : finalizers : - kubernetes status : phase : Active Notice how manager: argocd-application-controller identifies that this namespace was created by ArgoCD. It's important to understand the sequence of actions. We simply deployed the bootstrap-single-cluster ArgoCD application, and it ultimately resulted in the creation of the namespace-ci ArgoCD application which created the ci namespace. We don't directly apply resources to the cluster once the bootstrap-single-cluster ArgoCD application has been applied; the cluster state is determined by an ArgoCD application YAML in the corresponding application , service or infrastructure related folders. It's these ArgoCD applications that create and manage the underlying Kubernetes resources using the GitOps repository as the source of truth. Understanding the namespace-ci ArgoCD application Let's examine the ArgoCD application namespace-ci to see how it created the ci namespace and three role bindings in the cluster. Issue the following command to examine its YAML: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-ci.yaml Notice that apiVersion and kind identify this as an ArgoCD application: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ci labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" spec : destination : namespace : ci server : https://kubernetes.default.svc project : infra source : path : namespaces/ci syncPolicy : automated : prune : true selfHeal : true Most importantly, see how this namespace-ci ArgoCD application monitors the folder path: namespaces/ci in https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git ; it then applies the contents of this folder to the cluster whenever its content changes. Notice the SyncPolicy of automated ; any changes to this folder will automatically be applied to the cluster; we do not need to perform a manual Sync operation from the ArgoCD UI. Examine the ci namespace YAML To examine this, navigate to multi-tenancy-gitops-infra repository you cloned previously. cd $HOME /git cd multi-tenancy-gitops-infra Let's examine the ci namespace YAML in the namespaces/ci folder: cat namespaces/ci/namespace.yaml It's a very simple YAML: apiVersion : v1 kind : Namespace metadata : name : ci spec : {} This is a YAML that we would normally apply to the cluster manually or via a script. However, when we use GitOps, we push the ArgoCD application that uses this YAML to GitHub, and it applies the ci namespace YAML to the cluster. This is the essence of GitOps; we declare what we want to appear in the cluster using Git and ArgoCD synchronizes the cluster with this declaration. Examine the ci rolebinding YAML Now that we've seen how the namespace was created, let's see how the three other rolebindings were created by the namespace-ci ArgoCD application. In the same namespace/ci folder as the ci namespace YAML, there is a rolebinding.yaml file. This file will be also applied to the cluster by the namespace-ci ArgoCD application which is continuously watching this folder. Examine this file with the following command: cat namespaces/ci/rolebinding.yaml This YAML is slightly more complex than the namespace YAML: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-dev namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:dev --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-staging namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:staging --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-prod namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:prod --- However its structure is quite straightforward; look carefully and you'll see that there are indeed three rolebindings defined in this YAML. Each of these rolebindings binds different roles to different service accounts associated with components that will run within the ci namespace, such as Tekton pipelines for example. We'll see later how these rolebindings are important; they limit the operations that can be performed by the ci namespace, creating a well governed cluster. This confirms why we saw four resources created by the namespace-ci ArgoCD application in the ArgoCD UI: one namespace and three rolebindings. Again, notice the pattern: a single ArgoCD application manages one or more Kubernetes resources in the cluster -- using one or more YAML files in which those resources are defined. The bootstrap-single-cluster ArgoCD application in more detail In the ArgoCD UI Applications view, click on the bootstrap-single-cluster application: You can see the bootstrap application creates two Kubernetes resources, the infra ArgoCD application and the infra ArgoCD project. An ArgoCD project is a mechanism by which we can group related resources; we keep all our ArgoCD applications that manage infrastructure in the infra project. Later, we'll create a services project for the ArgoCD applications that manage the services we want deployed to the cluster such as the MQ application and queue manager pipelines. The infra ArgoCD application Let's examine the infra ArgoCD application in more detail to see how it works. In the ArgoCD UI Applications view, click on the open application icon for the infra application: We can see that the infra ArgoCD application creates 9 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It's the infra ArgoCD application that watches the 0-bootstrap/single-cluster/1-infra/argocd folder for ArgoCD applications that apply infrastructure resources to our cluster. It was the infra application that created the namespace-ci ArgoCD application which manages the ci namespace that we've been exploring in this section of the tutorial. We'll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it's not necessary, as you'll get lots of practice as we proceed.","title":"Connect ArgoCD to the GitOps repository"},{"location":"guides/cp4i/mq/cluster-config/gitops-config/#argocd-change-management-and-governance","text":"In this final section about managing infrastructure resources, let's explore how ArgoCD provides some advanced resource management features: Dynamic Monitored Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Customize the web console banner Examine the banner in the OpenShift web console: We're going to use GitOps to modify this banner dynamically. The banner YAML This banner properties are defined by the YAML in /0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml . This YAML is currently being used by the cntk-consolenotification ArgoCD application that was deployed earlier. We can examine the YAML with the following command: cat 0 -bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml which shows the banner properties are part of the ConsoleNotification custom resource: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : cntk-consolenotification labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" finalizers : - resources-finalizer.argocd.argoproj.io spec : syncPolicy : automated : prune : true selfHeal : true destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : path : consolenotification helm : values : | ocp-console-notification: ## The name of the ConsoleNotification resource in the cluster name: \"banner-env\" ## The background color that should be used for the banner backgroundColor: teal ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text: \"Cluster Description\" See how the banner at the top of the screen: * contains the text Cluster Description * is located at top of the screen * has the color Teal Modify the YAML for this banner Let's now change this YAML In your editor, modify this YAML and change the below fields as follows: ocp-console-notification : ## The name of the ConsoleNotification resource in the cluster name : \"banner-env\" ## The background color that should be used for the banner backgroundColor : red ## The color of the text that will appear in the banner color : \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location : BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text : \"Production reference guide\" It's clear that our intention is to modify the banner's backgroundColor:red and text: Production reference guide to the newly specified values. If you look at the diff : git diff you should see the following diff --git a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml index 30adf1a..596e821 100644 --- a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml +++ b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml @@ -26,10 +26,10 @@ spec: name: \"banner-env\" ## The background color that should be used for the banner - backgroundColor: teal + backgroundColor: red ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created - text: \"Cluster Description\" + text: \"Production reference guide\" Make the web console YAML change active Let's make these changes visible to the cntk-consolenotification ArgoCD application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Modify console banner\" git push origin $GIT_BRANCH You'll see the changes being pushed to GitHub: Enumerating objects: 13, done. Counting objects: 100% (13/13), done. Delta compression using up to 8 threads Compressing objects: 100% (7/7), done. Writing objects: 100% (7/7), 670 bytes | 670.00 KiB/s, done. Total 7 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a1e8292..b49dff5 master -> master Let's see what effect they have on the web console. A dynamic change to the web console You can either wait for ArgoCD to automatically sync the cntk-consolenotification application or manually Sync it yourself: Returning to the OpenShift web console, you'll notice changes. Notice the dynamic nature of these changes; we updated the console YAML, pushed our changes to our GitOps repository and everything else happened automatically. As a result, our OpenShift console has a new banner color and text. Governing changes to the dev namespace Let's now look at how ArgoCD monitors Kubernetes resources for configuration drift, and what happens if it detects an unexpected change to a monitored resource. Don't worry about the following command; it might seem drastic and even reckless, but as you'll see, everything will be OK. Let's delete the dev namespace from the cluster: oc get namespace dev oc delete namespace dev See how the active namespace: NAME STATUS AGE dev Active 2d18h is deleted: namespace \"dev\" deleted We can see that the dev namespace has been manually deleted from the cluster. GitOps repository as a source of truth If you switch back to the ArgoCD UI, you may see that ArgoCD has detected a configuration drift: a resource is Missing (the dev namespace) namespace-dev therefore is OutOfSync namespace-dev is therefore Syncing with the GitOps repository After a while we'll see that namespace-dev is Healthy and Synced : ArgoCD has detected a configuration drift, and resynched with GitOps repository, re-applying the dev namespace to the cluster. Note You may miss seeing the first screenshot if ArgoCD detects and corrects the missing dev namespace before you get a chance to switch to the ArgoCD UI. Don't worry, you can try this operator again! The restored dev namespace Issue the following command to determine the status of the dev namespace: oc get namespace dev which confirms that the dev namespace has been re-instated: NAME STATUS AGE dev Active 115s Note that it is a different instance of the dev namespace; as indicated by its AGE value. Notice the governed nature of these changes; GitOps is our source of truth about the resources deployed to the cluster. ArgoCD restores any resources that suffer from configuration drift to their GitOps-defined configuration. Congratulations! You've used ArgoCD and the GitOps repository to set up ci , tools and dev namespaces. You've seen how to create ArgoCD applications that watch their respective GitOps namespace folders for details of the namespace resources they should apply to the cluster. You've seen how you can dynamically change deployed resources by updating the resource definition in the GitOps repository. Finally, you've experience how ArgoCD keeps the cluster synchronized with the GitOps repository as a source of truth; any unexpected configuration drift will be corrected without intervention. In the next section of this tutorial, we're going to deploy some services into the infrastructure namespaces we've created in this topic. These services will include Artifactory and MQ operator.","title":"ArgoCD change management and governance"},{"location":"guides/cp4i/mq/cluster-config/gitops-tekton-argocd/","text":"Configuring the cluster for GitOps \u00b6 Audience : Architects, Application developers, Administrators Overview \u00b6 In this topic, we're going to: Create a new GitHub organization for our tutorial Copy the sample repositories for this tutorial to our new organization Briefly review these repositories Install ArgoCD to help us set up the cluster Customize ArgoCD Login in to the ArgoCD UI By the end of this topic we'll have a all the basic components in place to perform GitOps in our cluster. Introduction \u00b6 Continuous integration and continuous deployment (CICD) are at the core of our MQ deployment . CICD ensures that any changes to source applications and queue managers are automatically built and tested before they are deployed, helping to ensure their correctness and the integrity of the cluster. MQ applications and queue managers are defined, configured and changed using a GitOps model. GitOps puts git repositories and git commands such as git push (to request a change) and git merge (to approve a change) at the heart of configuration management. A GitOps approach helps an organization implement best practices in version control and release governance based on a widely used open standard -- git . The following diagram outlines the major components in a GitOps MQ CICD process: Notice the clear separation of concerns: Tekton pipelines (OpenShift Pipelines) use MQ application and queue manager source repositories to build and store successfully tested Kubernetes artifacts in a Git config repository, image registry and Artifactory. While Kubernetes resources (e.g. pods, routes...) can be created as part of the pipeline run to test the source change, these resources only last for the duration of the pipeline run. It is the resultant Git, container image registry and Artifactory resources that are used to affect changes to the cluster in a subsequent, asynchronous processing step controlled by ArgoCD. ArgoCD applications (OpenShift GitOps) watch a Git config repository for changes built as a result of successful pipeline runs. This repository identifies the latest version of the application or queue manager using YAMLs stored in Git which reference the image repository and Artifactory. ArgoCD applies recently updated Kubernetes resources to the cluster, resulting in new or updated Kubernetes resources that represent the changed MQ applications and queue managers, such as pods, routes etc. In contrast to pipeline runs, ArgoCD changes are durable; they remain as defined unless and until they are explicitly changed or deleted in the GitOps repository. Moreover, if the cluster resources drift from their repository-defined values, ArgoCD will restore them to these values; only changes that are applied to the Git config repository affect the long term state of the cluster. The Git configuration repository , often referred to as the GitOps repository , is used to store the MQ application and queue manager YAML artifacts built by Tekton and watched by ArgoCD. We think of this repository as a transfer point between Continuous Integration and Continuous Deployment; the successful built and tested YAMLs are stored in the Git configuration repository by a Tekton pipeline, which is continuously being watched by an ArgoCD application which deploys the latest YAML which references information in Artifactory and the image registry. Often, a Tekton pipeline will perform its changes to a GitOps repository under a pull-request (PR) to provide an explicit approval mechanism for cluster changes. ArgoCD will only see the changes once a PR is merged, providing the formal sign-off which is often so important in higher environments such as production. In contrast, lower environments such as development, are often committed into directly and therefore immediately seen by ArgoCD and applied to the cluster. It's worth noting that although we show a single GitOps repository, there are in fact multiple repositories -- each corresponding to a different architectural layer in our cluster such as infrastructure , services and applications . As we'll see, we don't just use the GitOps repository to deploy MQ applications and queue managers, but every component in the cluster. When we look at our target architecture , every single component (Cert manager, SonarQube, JMeter...) will be deployed to the cluster using ArgoCD and a set of YAMLs from the appropriate specific GitOps repository. In this section of the tutorial, we're going to set up the GitOps repository, and install ArgoCD. Later, we'll customize our GitOps config repositories, and them in conjunction with ArgoCD it to install the rest of the components in the cluster, including MQ applications and queue managers. Pre-requisites \u00b6 Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed the oc command on your local machine. Its version must be compatible with the version of your cluster. Use these instructions to get the latest version of oc . Use oc version to confirm that you have Client Version: 4.6 or higher Server Version: 4.7 or higher You have also installed npm , git , tree and jq commands. See these instructions about how install these prerequisites. Video Walkthrough \u00b6 This video demonstrates how to create a Github Organization and set up all the necessary repositories for this tutorial. It also goes through how to install ArgoCD. This is a video walkthrough and it takes you step by step through the below sections. Downloading the sample GitOps repository \u00b6 In this topic section, we're going to create the GitOps repository for our cluster. We'll use a sample GitOps repository as a starting point; we'll make our own copy and review its contents. Later in the tutorial, we'll customize it for our organization and cluster. ( If you'd like to re-familiarize yourself with git, GitHub or managing repositories, you'll find the following short video and short read helpful. ) Create a new GitHub organization Let's start by creating our own GitHub organization. In your browser, navigate to https://github.com/ . In the upper-right corner, click on the profile photo, and then click Settings . In user settings sidebar, click Organizations . In the \"Organizations\" section, click New organization. Create a new organization. Create the GitOps repositories in the new GitHub organization Let's start by making our own copy of the GitOps repository. In your browser, navigate to https://github.com/cloud-native-toolkit/multi-tenancy-gitops : Click on Use this template at the top RHS of the screen and you'll be asked where you'd like to use this repository: Select your organization, name the repository and click Create repository from template . After a few seconds the repository will be present in your chosen account: You now have a copy of the sample GitOps repository in your GitHub account. Create the infrastructure, services and applications GitOps repositories from the templates into the organization your GitHub organization As shown above, follow similar steps and copy the the repository templates into your organization. GitOps Infrastructure Repository GitOps Services Repository GitOps Application Repository Your organization now will have the below repositories in it. Set up environment variable for GitHub Organization This tutorial uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Replace <git-organization> in the following command: export GIT_ORG = <git-organization> You can verify your $GIT_ORG as follows: echo $GIT_ORG Change to your working git folder Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . We've used this default in the following command, but you may have a different location on your local machine. Change to your working git folder: cd $HOME /git You may need to create this folder if it's not already created. Clone the GitOps repository We're going to work on a local clone of the four repositories. Clone the GitOps repositories to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops'... remote: Enumerating objects: 132, done. remote: Counting objects: 100% (132/132), done. remote: Compressing objects: 100% (54/54), done. remote: Total 132 (delta 72), reused 118 (delta 67), pack-reused 0 Receiving objects: 100% (132/132), 310.92 KiB | 3.24 MiB/s, done. Resolving deltas: 100% (72/72), done. Clone the GitOps Infra repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-infra.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-infra'... remote: Enumerating objects: 62, done. remote: Counting objects: 100% (62/62), done. remote: Compressing objects: 100% (44/44), done. remote: Total 62 (delta 7), reused 53 (delta 3), pack-reused 0 Unpacking objects: 100% (62/62), done. Clone the GitOps Services repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-services.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-services'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Clone the GitOps Applications repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-apps'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Info Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the folder containing the local copy of your new repository: cd multi-tenancy-gitops Review the GitOps folder structure Show the folder structure with the following command: tree . -L 1 This will list all the top level folders and files: . \u251c\u2500\u2500 0-bootstrap \u251c\u2500\u2500 README.md \u251c\u2500\u2500 doc \u251c\u2500\u2500 scripts \u2514\u2500\u2500 setup The main folders perform the following roles: 0-bootstrap contains different profiles that can be used to initialize the cluster ready. We can choose one of the available profiles. scripts and setup contain useful scripts that we'll use during this tutorial. Feel free to explore this structure. We'll examine it in much more detail throughout this tutorial, and you'll become very familiar with it by trying it out. Installing ArgoCD for GitOps \u00b6 Now that we've had an initial view of the GitOps repository, let's install ArgoCD to make use of it. We will install ArgoCD using the Red Hat OpenShift GitOps operator. We will also configure it with the appropriate access rights to limit the resources it can create in the cluster. In so doing, we ensure that ArgoCD acts like a well-governed administrator of the cluster, only doing what it needs to do. Also, we will customize our ArgoCD instance by adding necessary custom checks. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". On successful login, you'll see: Logged into \"https://c100-e.jp-tok.containers.cloud.ibm.com:30749\" as \"IAM#hperabat@in.ibm.com\" using the token provided. You have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\" . This shows some basic information about your user within this cluster. (This user is different to your GitHub user.) Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. The following command creates the subscription: oc apply -f setup/ocp47/ The response confirms that the below resources has been created: clusterrole.rbac.authorization.k8s.io/custom-argocd-cluster-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-cntk-argocd-application-controller created subscription.operators.coreos.com/openshift-gitops-operator created Wait for the ArgoCD installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/gitops-operator-controller-manager -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/gitops-operator condition met ArgoCD is now installed and ready to use. If you see something like: Error from server (NotFound): deployments.apps \"gitops-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Alternatively, you can also validate as follows: while ! oc wait crd applications.argoproj.io --timeout = -1s --for = condition = Established 2 >/dev/null ; do sleep 30 ; done while ! oc wait pod --timeout = -1s --for = condition = Ready -l '!job-name' -n openshift-gitops > /dev/null ; do sleep 30 ; done ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are also setup along with the subscription. oc get clusterrole custom-argocd-cluster-argocd-application-controller oc get clusterrolebinding openshift-gitops-argocd-application-controller oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller You'll see the resources are created in the cluster: $ oc get clusterrole custom-argocd-cluster-argocd-application-controller NAME CREATED AT custom-argocd-cluster-argocd-application-controller 2021-08-27T13:35:13Z $ oc get clusterrolebinding openshift-gitops-argocd-application-controller NAME ROLE AGE openshift-gitops-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m43s $ oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller NAME ROLE AGE openshift-gitops-cntk-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m45s The clusterrole custom-argocd-cluster-argocd-application-controller defines a specific set of specific resources that are required by the ArgoCD service account and access rights over them. The cluster role bindings openshift-gitops-argocd-application-controller and openshift-gitops-cntk-argocd-application-controller binds the ArgoCD service account to the cluster role above. In combination, these definitions limit ArgoCD to perform the minimum set of operations required to manage the cluster. This is important; it means that ArgoCD behaves as a well governed administrator of the cluster. We'll also see later in the tutorial how ArgoCD creates resource in the cluster. Delete default ArgoCD instance Let us now delete the default ArgoCD instance that is created earlier. oc delete gitopsservice cluster -n openshift-gitops || true You will see the resources being deleted as follows: warning: deleting cluster-scoped resources, not scoped to the provided namespace gitopsservice.pipelines.openshift.io \"cluster\" deleted We are deleting the existing default instance and planning to make use of a customized ArgoCD instance with additional capabilities. Create a custom ArgoCD instance This customized ArgoCD instance implements custom checks. By default ArgoCD provides some built-in health checks for validating standard kubernetes resources. However, these checks will not be sufficient to validate the custom resources that belong to IBM Cloud Pak for Integration. Argo CD supports custom health checks that are defined in Lua . If you want explore more about the ArgoCD Resource Health, check this link out. Let us create a custom ArgoCD instance as follows: oc apply -f setup/ocp47/argocd-instance/ -n openshift-gitops The response confirms that the below resources has been created: argocd.argoproj.io/openshift-gitops-cntk created Wait till the ArgoCD instance pod is up and running. Issue the below command to verify the same: while ! oc wait pod --timeout = -1s --for = condition = ContainersReady -l app.kubernetes.io/name = openshift-gitops-cntk-server -n openshift-gitops > /dev/null ; do sleep 30 ; done Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops | grep openshift-gitops-cntk-server | awk '{print \"https://\"$2}' This will list the route to the ArgoCD we've just installed, for example: https://openshift-gitops-cntk-server-openshift-gitops.ibmcloud-roks-xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/openshift-gitops-cntk-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Once the UI launches, you'll see: See how there are no ArgoCD applications active at the moment. In the next section of the tutorial, we'll configure ArgoCD to create the ArgoCD applications that will in turn spin up infrastructure , service , and application resources to apply to the cluster. Congratulations! You've created the GitOps repository for your cluster based on the sample MQ repository and examined its high level structure. You also installed ArgoCD. You created a specific clusterrole and clusterrolebinding for the ArgoCD service account to ensure that it manages the cluster in a well governed manner. Finally, you launched the UI for ArgoCD; you'll make extensive use of it during this tutorial. In the next topic of this chapter, we're going to customize the GitOps repository for your cluster and use Tekton and ArgoCD to create and manage the Kubernetes resources for our MQ applications and queue managers.","title":"Set up GitOps"},{"location":"guides/cp4i/mq/cluster-config/gitops-tekton-argocd/#configuring-the-cluster-for-gitops","text":"Audience : Architects, Application developers, Administrators","title":"Configuring the cluster for GitOps"},{"location":"guides/cp4i/mq/cluster-config/gitops-tekton-argocd/#overview","text":"In this topic, we're going to: Create a new GitHub organization for our tutorial Copy the sample repositories for this tutorial to our new organization Briefly review these repositories Install ArgoCD to help us set up the cluster Customize ArgoCD Login in to the ArgoCD UI By the end of this topic we'll have a all the basic components in place to perform GitOps in our cluster.","title":"Overview"},{"location":"guides/cp4i/mq/cluster-config/gitops-tekton-argocd/#introduction","text":"Continuous integration and continuous deployment (CICD) are at the core of our MQ deployment . CICD ensures that any changes to source applications and queue managers are automatically built and tested before they are deployed, helping to ensure their correctness and the integrity of the cluster. MQ applications and queue managers are defined, configured and changed using a GitOps model. GitOps puts git repositories and git commands such as git push (to request a change) and git merge (to approve a change) at the heart of configuration management. A GitOps approach helps an organization implement best practices in version control and release governance based on a widely used open standard -- git . The following diagram outlines the major components in a GitOps MQ CICD process: Notice the clear separation of concerns: Tekton pipelines (OpenShift Pipelines) use MQ application and queue manager source repositories to build and store successfully tested Kubernetes artifacts in a Git config repository, image registry and Artifactory. While Kubernetes resources (e.g. pods, routes...) can be created as part of the pipeline run to test the source change, these resources only last for the duration of the pipeline run. It is the resultant Git, container image registry and Artifactory resources that are used to affect changes to the cluster in a subsequent, asynchronous processing step controlled by ArgoCD. ArgoCD applications (OpenShift GitOps) watch a Git config repository for changes built as a result of successful pipeline runs. This repository identifies the latest version of the application or queue manager using YAMLs stored in Git which reference the image repository and Artifactory. ArgoCD applies recently updated Kubernetes resources to the cluster, resulting in new or updated Kubernetes resources that represent the changed MQ applications and queue managers, such as pods, routes etc. In contrast to pipeline runs, ArgoCD changes are durable; they remain as defined unless and until they are explicitly changed or deleted in the GitOps repository. Moreover, if the cluster resources drift from their repository-defined values, ArgoCD will restore them to these values; only changes that are applied to the Git config repository affect the long term state of the cluster. The Git configuration repository , often referred to as the GitOps repository , is used to store the MQ application and queue manager YAML artifacts built by Tekton and watched by ArgoCD. We think of this repository as a transfer point between Continuous Integration and Continuous Deployment; the successful built and tested YAMLs are stored in the Git configuration repository by a Tekton pipeline, which is continuously being watched by an ArgoCD application which deploys the latest YAML which references information in Artifactory and the image registry. Often, a Tekton pipeline will perform its changes to a GitOps repository under a pull-request (PR) to provide an explicit approval mechanism for cluster changes. ArgoCD will only see the changes once a PR is merged, providing the formal sign-off which is often so important in higher environments such as production. In contrast, lower environments such as development, are often committed into directly and therefore immediately seen by ArgoCD and applied to the cluster. It's worth noting that although we show a single GitOps repository, there are in fact multiple repositories -- each corresponding to a different architectural layer in our cluster such as infrastructure , services and applications . As we'll see, we don't just use the GitOps repository to deploy MQ applications and queue managers, but every component in the cluster. When we look at our target architecture , every single component (Cert manager, SonarQube, JMeter...) will be deployed to the cluster using ArgoCD and a set of YAMLs from the appropriate specific GitOps repository. In this section of the tutorial, we're going to set up the GitOps repository, and install ArgoCD. Later, we'll customize our GitOps config repositories, and them in conjunction with ArgoCD it to install the rest of the components in the cluster, including MQ applications and queue managers.","title":"Introduction"},{"location":"guides/cp4i/mq/cluster-config/gitops-tekton-argocd/#pre-requisites","text":"Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed the oc command on your local machine. Its version must be compatible with the version of your cluster. Use these instructions to get the latest version of oc . Use oc version to confirm that you have Client Version: 4.6 or higher Server Version: 4.7 or higher You have also installed npm , git , tree and jq commands. See these instructions about how install these prerequisites.","title":"Pre-requisites"},{"location":"guides/cp4i/mq/cluster-config/gitops-tekton-argocd/#video-walkthrough","text":"This video demonstrates how to create a Github Organization and set up all the necessary repositories for this tutorial. It also goes through how to install ArgoCD. This is a video walkthrough and it takes you step by step through the below sections.","title":"Video Walkthrough"},{"location":"guides/cp4i/mq/cluster-config/gitops-tekton-argocd/#downloading-the-sample-gitops-repository","text":"In this topic section, we're going to create the GitOps repository for our cluster. We'll use a sample GitOps repository as a starting point; we'll make our own copy and review its contents. Later in the tutorial, we'll customize it for our organization and cluster. ( If you'd like to re-familiarize yourself with git, GitHub or managing repositories, you'll find the following short video and short read helpful. ) Create a new GitHub organization Let's start by creating our own GitHub organization. In your browser, navigate to https://github.com/ . In the upper-right corner, click on the profile photo, and then click Settings . In user settings sidebar, click Organizations . In the \"Organizations\" section, click New organization. Create a new organization. Create the GitOps repositories in the new GitHub organization Let's start by making our own copy of the GitOps repository. In your browser, navigate to https://github.com/cloud-native-toolkit/multi-tenancy-gitops : Click on Use this template at the top RHS of the screen and you'll be asked where you'd like to use this repository: Select your organization, name the repository and click Create repository from template . After a few seconds the repository will be present in your chosen account: You now have a copy of the sample GitOps repository in your GitHub account. Create the infrastructure, services and applications GitOps repositories from the templates into the organization your GitHub organization As shown above, follow similar steps and copy the the repository templates into your organization. GitOps Infrastructure Repository GitOps Services Repository GitOps Application Repository Your organization now will have the below repositories in it. Set up environment variable for GitHub Organization This tutorial uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Replace <git-organization> in the following command: export GIT_ORG = <git-organization> You can verify your $GIT_ORG as follows: echo $GIT_ORG Change to your working git folder Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . We've used this default in the following command, but you may have a different location on your local machine. Change to your working git folder: cd $HOME /git You may need to create this folder if it's not already created. Clone the GitOps repository We're going to work on a local clone of the four repositories. Clone the GitOps repositories to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops'... remote: Enumerating objects: 132, done. remote: Counting objects: 100% (132/132), done. remote: Compressing objects: 100% (54/54), done. remote: Total 132 (delta 72), reused 118 (delta 67), pack-reused 0 Receiving objects: 100% (132/132), 310.92 KiB | 3.24 MiB/s, done. Resolving deltas: 100% (72/72), done. Clone the GitOps Infra repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-infra.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-infra'... remote: Enumerating objects: 62, done. remote: Counting objects: 100% (62/62), done. remote: Compressing objects: 100% (44/44), done. remote: Total 62 (delta 7), reused 53 (delta 3), pack-reused 0 Unpacking objects: 100% (62/62), done. Clone the GitOps Services repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-services.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-services'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Clone the GitOps Applications repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-apps'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Info Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the folder containing the local copy of your new repository: cd multi-tenancy-gitops Review the GitOps folder structure Show the folder structure with the following command: tree . -L 1 This will list all the top level folders and files: . \u251c\u2500\u2500 0-bootstrap \u251c\u2500\u2500 README.md \u251c\u2500\u2500 doc \u251c\u2500\u2500 scripts \u2514\u2500\u2500 setup The main folders perform the following roles: 0-bootstrap contains different profiles that can be used to initialize the cluster ready. We can choose one of the available profiles. scripts and setup contain useful scripts that we'll use during this tutorial. Feel free to explore this structure. We'll examine it in much more detail throughout this tutorial, and you'll become very familiar with it by trying it out.","title":"Downloading the sample GitOps repository"},{"location":"guides/cp4i/mq/cluster-config/gitops-tekton-argocd/#installing-argocd-for-gitops","text":"Now that we've had an initial view of the GitOps repository, let's install ArgoCD to make use of it. We will install ArgoCD using the Red Hat OpenShift GitOps operator. We will also configure it with the appropriate access rights to limit the resources it can create in the cluster. In so doing, we ensure that ArgoCD acts like a well-governed administrator of the cluster, only doing what it needs to do. Also, we will customize our ArgoCD instance by adding necessary custom checks. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". On successful login, you'll see: Logged into \"https://c100-e.jp-tok.containers.cloud.ibm.com:30749\" as \"IAM#hperabat@in.ibm.com\" using the token provided. You have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\" . This shows some basic information about your user within this cluster. (This user is different to your GitHub user.) Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. The following command creates the subscription: oc apply -f setup/ocp47/ The response confirms that the below resources has been created: clusterrole.rbac.authorization.k8s.io/custom-argocd-cluster-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-cntk-argocd-application-controller created subscription.operators.coreos.com/openshift-gitops-operator created Wait for the ArgoCD installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/gitops-operator-controller-manager -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/gitops-operator condition met ArgoCD is now installed and ready to use. If you see something like: Error from server (NotFound): deployments.apps \"gitops-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Alternatively, you can also validate as follows: while ! oc wait crd applications.argoproj.io --timeout = -1s --for = condition = Established 2 >/dev/null ; do sleep 30 ; done while ! oc wait pod --timeout = -1s --for = condition = Ready -l '!job-name' -n openshift-gitops > /dev/null ; do sleep 30 ; done ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are also setup along with the subscription. oc get clusterrole custom-argocd-cluster-argocd-application-controller oc get clusterrolebinding openshift-gitops-argocd-application-controller oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller You'll see the resources are created in the cluster: $ oc get clusterrole custom-argocd-cluster-argocd-application-controller NAME CREATED AT custom-argocd-cluster-argocd-application-controller 2021-08-27T13:35:13Z $ oc get clusterrolebinding openshift-gitops-argocd-application-controller NAME ROLE AGE openshift-gitops-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m43s $ oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller NAME ROLE AGE openshift-gitops-cntk-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m45s The clusterrole custom-argocd-cluster-argocd-application-controller defines a specific set of specific resources that are required by the ArgoCD service account and access rights over them. The cluster role bindings openshift-gitops-argocd-application-controller and openshift-gitops-cntk-argocd-application-controller binds the ArgoCD service account to the cluster role above. In combination, these definitions limit ArgoCD to perform the minimum set of operations required to manage the cluster. This is important; it means that ArgoCD behaves as a well governed administrator of the cluster. We'll also see later in the tutorial how ArgoCD creates resource in the cluster. Delete default ArgoCD instance Let us now delete the default ArgoCD instance that is created earlier. oc delete gitopsservice cluster -n openshift-gitops || true You will see the resources being deleted as follows: warning: deleting cluster-scoped resources, not scoped to the provided namespace gitopsservice.pipelines.openshift.io \"cluster\" deleted We are deleting the existing default instance and planning to make use of a customized ArgoCD instance with additional capabilities. Create a custom ArgoCD instance This customized ArgoCD instance implements custom checks. By default ArgoCD provides some built-in health checks for validating standard kubernetes resources. However, these checks will not be sufficient to validate the custom resources that belong to IBM Cloud Pak for Integration. Argo CD supports custom health checks that are defined in Lua . If you want explore more about the ArgoCD Resource Health, check this link out. Let us create a custom ArgoCD instance as follows: oc apply -f setup/ocp47/argocd-instance/ -n openshift-gitops The response confirms that the below resources has been created: argocd.argoproj.io/openshift-gitops-cntk created Wait till the ArgoCD instance pod is up and running. Issue the below command to verify the same: while ! oc wait pod --timeout = -1s --for = condition = ContainersReady -l app.kubernetes.io/name = openshift-gitops-cntk-server -n openshift-gitops > /dev/null ; do sleep 30 ; done Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops | grep openshift-gitops-cntk-server | awk '{print \"https://\"$2}' This will list the route to the ArgoCD we've just installed, for example: https://openshift-gitops-cntk-server-openshift-gitops.ibmcloud-roks-xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/openshift-gitops-cntk-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Once the UI launches, you'll see: See how there are no ArgoCD applications active at the moment. In the next section of the tutorial, we'll configure ArgoCD to create the ArgoCD applications that will in turn spin up infrastructure , service , and application resources to apply to the cluster. Congratulations! You've created the GitOps repository for your cluster based on the sample MQ repository and examined its high level structure. You also installed ArgoCD. You created a specific clusterrole and clusterrolebinding for the ArgoCD service account to ensure that it manages the cluster in a well governed manner. Finally, you launched the UI for ArgoCD; you'll make extensive use of it during this tutorial. In the next topic of this chapter, we're going to customize the GitOps repository for your cluster and use Tekton and ArgoCD to create and manage the Kubernetes resources for our MQ applications and queue managers.","title":"Installing ArgoCD for GitOps"},{"location":"guides/cp4i/mq/cluster-config/services/","text":"Installing services with ArgoCD \u00b6 Overview \u00b6 In the previous section of this chapter, we used GitOps to create the ArgoCD applications that installed and managed the ci , tools and dev namespaces in the cluster. In this section we're going complete the installation of all the necessary services required by our MQ CICD process: We'll examine these highlighted components in more detail throughout this section of the tutorial; here's an overview of their function. Tekton is used for Continuous Integration. Often, the Tekton pipeline will perform its changes under a pull-request (PR) to provide an explicit approval mechanism or through a push based on the requirements for cluster changes. Artifactory will provide a store for MQ application and queue manager build artifacts such as Helm charts. It is used in conjunction with the GitOps repository and image registry. Sonarqube is used by the MQ application pipeline for code quality and security scanning. It helps ensure the quality of a deployed application. ArgoCD applications will be created for each of these resources. Specifically, ArgoCD applications will keep the cluster synchronized with the application and queue manager YAML definitions stored in Artifactory and Git. Note how these application services are installed in the ci and tools namespaces we created in the previous topic. This section will reinforce our understanding of GitOps. We will then be ready to create MQ applications and queue managers that use the infrastructure we previously created and these services we are going to install further. In this topic, we're going to: Deploy services to the cluster using GitOps Explore the ArgoCD applications that manage these services Explore how more complex services work using Artifactory as an example Review how ArgoCD projects work See how infra and services ArgoCD applications manage the infrastructure and services layers in our architecture. By the end of this topic we'll have a fully configured cluster which is ready for us to deploy MQ applications and queue managers. Pre-requisites \u00b6 Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed on your local machine the oc command that matches the version of your cluster. You have installed npm , git and tree commands. You have completed the tutorial section to customize the GitOps repository, and install ArgoCD. You have completed the tutorial section to create the ci , tools and dev namespaces using GitOps. Please see the previous sections of this guide for information on how to do these tasks. Video Walkthrough \u00b6 This video demonstrates how to install Tekton. It also shows how to use the GitOps repository to set up different service related components. This is a video walkthrough and it takes you step by step through the below sections. Post cluster provisioning tasks \u00b6 Red Hat OpenShift cluster \u00b6 An OpenShift v4.7+ cluster is required. CLI tools \u00b6 Install the OpenShift CLI oc (version 4.7+) . The binary can be downloaded from the Help menu from the OpenShift Console. Download oc cli Log in from a terminal window. oc login --token = <token> --server = <server> IBM Entitlement Key \u00b6 The IBM Entitlement Key is required to pull IBM Cloud Pak specific container images from the IBM Entitled Registry. To get an entitlement key, Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. A Secret containing the entitlement key is created in the tools namespace. oc new-project tools || true oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io Installing Tekton for GitOps \u00b6 Tekton is made available to your Red Hat OpenShift cluster through the Red Hat OpenShift Pipelines operator. Hence, let's see how to get that operator installed on your cluster. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Install Tekton into the cluster We use the Red Hat Pipelines operator to install Tekton into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: - argocd/operators/openshift-pipelines.yaml Your kustomization.yaml for services should match the following: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Install tekton using Red Hat OpenShift Pipelines Operator\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 524 bytes | 524.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 85a4c46..61e15b0 master -> master Activate the services in the GitOps repo Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us only deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see openshift-pipelines ArgoCD application have been created as follows: Wait for the Tekton installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/openshift-pipelines-operator -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/openshift-pipelines-operator condition met Tekton is now installed and ready to use. We\u2019ll explore Tekton in later topics. If you see something like: Error from server (NotFound): deployments.apps \"openshift-pipelines-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Deploy services to the cluster \u00b6 We've just had our first successful GitOps experience, using an ArgoCD application to create the ci , tools and dev namespaces in our cluster at the infrastructure layer level as well as using ArgoCD to deploy the Red Hat OpenShift Pipelines operator at the services layer level. There are few more components you need to create for this IBM MQ tutorial: IBM MQ, IBM Platform navigator, IBM foundations, IBM automation foundations, IBM Catalog, Sealed secrets, Sonarqube and Artifactory. These components are part of the services layer in our architecture, and that requires us to access /0-bootstrap/single-cluster/2-services within our GitOps repository. Check services layer components As we saw earlier, the bootstrap-single-cluster application uses the contents of the /0-bootstrap/single-cluster/2-services folder to determine which Kubernetes resources are available to be deployed in the cluster. Issue the following command to see what could be deployed in the cluster: tree 0 -bootstrap/single-cluster/2-services/ -L 3 You should see a set of operators and instances of these available in the GitOps framework at the services layer: 0 -bootstrap/single-cluster/2-services/ \u251c\u2500\u2500 2 -services.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 artifactory.yaml \u2502 \u2502 \u251c\u2500\u2500 baas-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 cert-manager-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 chartmuseum.yaml \u2502 \u2502 \u251c\u2500\u2500 developer-dashboard.yaml \u2502 \u2502 \u251c\u2500\u2500 grafana-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-gateway-analytics-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-management-portal-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4sthreatmanagements-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cpd-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-foundational-services-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-platform-navigator-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-process-mining-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 oadp-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 openshift-service-mesh-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 pact-broker.yaml \u2502 \u2502 \u251c\u2500\u2500 sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 sonarqube.yaml \u2502 \u2502 \u251c\u2500\u2500 spp-instance.yaml \u2502 \u2502 \u2514\u2500\u2500 swaggereditor.yaml \u2502 \u2514\u2500\u2500 operators \u2502 \u251c\u2500\u2500 cert-manager.yaml \u2502 \u251c\u2500\u2500 elasticsearch.yaml \u2502 \u251c\u2500\u2500 grafana-operator.yaml \u2502 \u251c\u2500\u2500 ibm-ace-operator.yaml \u2502 \u251c\u2500\u2500 ibm-apic-operator.yaml \u2502 \u251c\u2500\u2500 ibm-aspera-operator.yaml \u2502 \u251c\u2500\u2500 ibm-assetrepository-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-core-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-operator.yaml \u2502 \u251c\u2500\u2500 ibm-catalogs.yaml \u2502 \u251c\u2500\u2500 ibm-cp4a-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4i-operators.yaml \u2502 \u251c\u2500\u2500 ibm-cp4s-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-platform-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-scheduling-operator.yaml \u2502 \u251c\u2500\u2500 ibm-datapower-operator.yaml \u2502 \u251c\u2500\u2500 ibm-db2u-operator.yaml \u2502 \u251c\u2500\u2500 ibm-eventstreams-operator.yaml \u2502 \u251c\u2500\u2500 ibm-foundations.yaml \u2502 \u251c\u2500\u2500 ibm-mq-operator.yaml \u2502 \u251c\u2500\u2500 ibm-opsdashboard-operator.yaml \u2502 \u251c\u2500\u2500 ibm-platform-navigator.yaml \u2502 \u251c\u2500\u2500 ibm-process-mining-operator.yaml \u2502 \u251c\u2500\u2500 jaeger.yaml \u2502 \u251c\u2500\u2500 kiali.yaml \u2502 \u251c\u2500\u2500 oadp-operator.yaml \u2502 \u251c\u2500\u2500 openshift-gitops.yaml \u2502 \u251c\u2500\u2500 openshift-pipelines.yaml \u2502 \u251c\u2500\u2500 openshift-service-mesh.yaml \u2502 \u251c\u2500\u2500 spp-catalog.yaml \u2502 \u2514\u2500\u2500 spp-operator.yaml \u2514\u2500\u2500 kustomization.yaml Review ArgoCD services folder Let\u2019s examine the 0-bootstrap/single-cluster/2-services/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/kustomization.yaml We can see the contents of the kustomization.yaml : resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Add the services to the cluster Note The IBM Platform Navigator instance requires a RWX storageclass and it is set to managed-nfs-storage by default in the ArgoCD Application 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-platform-navigator-instance.yaml . This storageclass is available for Red Hat OpenShift on IBM Cloud cluster provisioned from IBM Technology Zone with NFS storage selected. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: ## Cloud Pak for Integration - argocd/operators/ibm-mq-operator.yaml - argocd/operators/ibm-platform-navigator.yaml - argocd/instances/ibm-platform-navigator-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD - argocd/instances/artifactory.yaml - argocd/instances/sonarqube.yaml You will have the following resources deployed for services: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml - argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml - argocd/operators/ibm-platform-navigator.yaml - argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml - argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml - argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 564 bytes | 564.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git b49dff5..533602c master -> master The intention of this operation is to indicate that we'd like the resources declared in 0-bootstrap/single-cluster/2-services/kustomization.yaml to be deployed in the cluster. Like the infra ArgoCD application, the resources created by the services ArgoCD application will manage the Kubernetes relevant services resources applied to the cluster. Optional: Activate the services in the GitOps repo Run this step if you skip installing tekton in the previous section. If you did not skip that step go to Step 7 . Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see lots of new ArgoCD applications have been created to manage the services we have deployed by modifying kustomization.yaml under 0-bootstrap/single-cluster/2-services folder: See how most ArgoCD applications are Synced almost immediately, but some spend time in Progressing . That's because the Artifactory and Sonarqube ArgoCD applications are more complex -- they create more Kubernetes resources than other ArgoCD applications and therefore take longer to sync . After a few minutes you'll see that all ArgoCD applications become Healthy and Synced : Notice how many more Synched ArgoCD applications are now in the cluster; these are as a result of the newly added services layer in our architecture. The services application Let's examine the ArgoCD application that manage the services in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the services application: We can see that the services ArgoCD application creates 10 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It\u2019s the services ArgoCD application that watches the 0-bootstrap/single-cluster/2-services/argocd folder for ArgoCD applications that apply service resources to our cluster. It was the services application that created the artifactory ArgoCD application which manages the Artifactory instance that is used for application configuration management that we will be exploring later in this section of the tutorial. We\u2019ll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it\u2019s not necessary, as you\u2019ll get lots of practice as we proceed. Examine Artifactory resources in detail You can see the Kubernetes resources that the Artifactory ArgoCD application has created for Artifactory. Click on the Artifactory : See how many different Kubernetes resources were created when the Artifactory ArgoCD application YAML was applied to the cluster. That's why it took the Artifactory ArgoCD application a little time to sync. If you'd like, have a look at some of the other ArgoCD applications, such as Sonarqube and the Kubernetes resources they created. The services ArgoCD project As we've seen in the ArgoCD UI, the services ArgoCD application is responsible for creating the ArgoCD applications that manage the services within the cluster. Let's examine their definitions to see how they do this. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/2-services.yaml The following YAML may initially look a little intimidating; we'll discuss the major elements below: --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : services labels : gitops.tier.layer : services spec : sourceRepos : [] # Populated by kustomize patches in 2-services/kustomization.yaml destinations : - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : redhat-operators server : https://kubernetes.default.svc - namespace : openshift-operators server : https://kubernetes.default.svc - namespace : openshift-marketplace server : https://kubernetes.default.svc - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : # TODO: SCC needs to be moved to 1-infra, here for now for artifactory - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleLink - group : \"apps\" kind : statefulsets - group : \"apps\" kind : deployments - group : \"\" kind : services - group : \"\" kind : configmaps - group : \"\" kind : secrets - group : \"\" kind : serviceaccounts - group : \"batch\" kind : jobs - group : \"\" kind : roles - group : \"route.openshift.io\" kind : routes - group : \"\" kind : RoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRole - group : apiextensions.k8s.io kind : CustomResourceDefinition roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : services annotations : argocd.argoproj.io/sync-wave : \"200\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : services source : # repoURL and targetRevision populated by kustomize patches in 2-services/kustomization.yaml path : 0-bootstrap/single-cluster/2-services syncPolicy : automated : prune : true selfHeal : true Notice how this YAML defines three ArgoCD resources: a services project which manages all the necessary services that are needed by the applications. Notice how the destinations for the services project are limited to the ci , tools and dev namespaces -- as well as a few others that we'll use in the tutorial. These destinations restrict the namespaces where ArgoCD applications in the services project can manage resources. The same is true for clusterResourceWhiteList . It limits the Kubernetes resources that can be managed to configmaps , deployments and rolebindings amongst others. In summary, we see that the service project is used to group all the ArgoCD applications that will manage the services in our cluster. These ArgoCD applications can only perform specific actions on specific resource types in specific namespaces. See how ArgoCD is acting as a well-governed administrator. The similar structure of services and infra ArgoCD applications Even though we didn't closely examine the infra ArgoCD application YAML in the previous topic, it has has a very similar structure to the ArgoCD services applications we've just examined. Type the following command to list the ArgoCD infra app YAML. cat 0 -bootstrap/single-cluster/1-infra/1-infra.yaml Again, although this YAML might look a little intimidating, the overall structure is the same as for services : --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : infra labels : gitops.tier.layer : infra spec : sourceRepos : [] # Populated by kustomize patches in 1-infra/kustomization.yaml destinations : - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : - group : \"\" kind : Namespace - group : \"\" kind : RoleBinding - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleNotification - group : \"console.openshift.io\" kind : ConsoleLink roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : infra annotations : argocd.argoproj.io/sync-wave : \"100\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : # repoURL and targetRevision populated by kustomize patches in 1-infra/kustomization.yaml path : 0-bootstrap/single-cluster/1-infra syncPolicy : automated : prune : true selfHeal : true As with the 2-services.yaml , we can see An ArgoCD project called infra . ArgoCD applications defined in this project will be limited by the destinations: and clusterResourceWhitelist: specified in the YAML. An ArgoCD app called infra . This is the ArgoCD application that we used in the previous section of the tutorial. It watches the path: 0-bootstrap/single-cluster/1-infra folder for ArgoCD applications that it applied to the cluster. It was these applications that managed the ci , tools and dev namespaces for example. The installation of the operators will take approximately 30 - 45 minutes. Validate the deployment of IBM MQ and IBM Platform Navigator. Verify the IBM Platform Navigator has been deployed successfully. oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' Expected output = True The Platform Navigator console has been installed as part of the IBM Cloud Pak for Integration. We use the Platform Navigator to view our queue manager and its queues. The Spring application includes a Swagger interface which we can use to call methods on the Spring application. In the next few steps we will use these to interact with both the queue manager and Spring application. Access the IBM Platform Navigator console Retrieve the URL for the IBM Platform Navigator console: oc get route integration-navigator-pn -n tools -o jsonpath = '{.spec.host}' Copy the URL from the HOST/PORT column and paste into a browser, prefixed with https:// . Retrieve the admin password: oc get secret ibm-iam-bindinfo-platform-auth-idp-credentials -n dev -o json -n ibm-common-services | jq -r '.data.\"admin_password\"' | base64 -D Log in to the IBM Platform Navigator console with the admin credentials. Other important ArgoCD features \u00b6 In this final section of this chapter, let's explore ArgoCD features you may have noticed as you explored different YAML files in this chapter: SyncWave \u00b6 Using SyncWave to control deployment sequencing When ArgoCD uses a GitOps repository to apply ArgoCD applications to the cluster, it applies all ArgoCD applications concurrently. Even though Kubernetes use of eventual consistency means that resources which depend on each other can be deployed in any order, it often makes sense to help deploy certain resources in a certain order. This can avoid spurious error messages or wasted compute resources for example. Let's compare two ArgoCD YAML files. Firstly, let's examine the namespace-tools in the infrastructure layer: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-tools.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: tools labels: gitops.tier.layer: infra annotations: argocd.argoproj.io/sync-wave: \"100\" ... Then examine the artifactory in the services layer: cat 0 -bootstrap/single-cluster/2-services/argocd/instances/artifactory.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: artifactory annotations: argocd.argoproj.io/sync-wave: \"250\" labels: gitops.tier.group: cntk gitops.tier.layer: services finalizers: - resources-finalizer.argocd.argoproj.io ... Notice the use of sync-wave: \"100\" for tools and how it contrasts with sync-wave: \"250\" for artifactory . The lower 100 will be deployed before the higher sync-wave 250 because it only makes sense to deploy the pipelines into the tools namespace, after the tools namespace has been created. You are free to choose any number for sync-wave . In our deployment, we have chosen 100-199 for infrastructure, 200-299 for services and 300-399 for applications; it provides alignment with high level folder numbering such as 1-infra and so on. In our tutorial, we're going to incrementally add infrastructure, services and applications so that you can understand how everything fits together incrementally. This makes sync-wave less important than in a real-world system, where you might be making updates to each of these deployed layers simultaneously. In such cases the use of sync-wave provides a high degree of confidence in the effectiveness of a deployment change because all changes go through in the correct order. We'll make extensive use of sync-wave when we look at High Availability MQ deployments. Congratulations! You've installed tekton and configured the key services in the cluster to support continuous integration and continuous delivery. These have included services like Sonarqube and Artifactory. We installed them into the tools namespaces we created previously. This chapter is now complete. In the following chapters, we're going to use these components to deploy MQ applications and queue managers to the cluster.","title":"Add services"},{"location":"guides/cp4i/mq/cluster-config/services/#installing-services-with-argocd","text":"","title":"Installing services with ArgoCD"},{"location":"guides/cp4i/mq/cluster-config/services/#overview","text":"In the previous section of this chapter, we used GitOps to create the ArgoCD applications that installed and managed the ci , tools and dev namespaces in the cluster. In this section we're going complete the installation of all the necessary services required by our MQ CICD process: We'll examine these highlighted components in more detail throughout this section of the tutorial; here's an overview of their function. Tekton is used for Continuous Integration. Often, the Tekton pipeline will perform its changes under a pull-request (PR) to provide an explicit approval mechanism or through a push based on the requirements for cluster changes. Artifactory will provide a store for MQ application and queue manager build artifacts such as Helm charts. It is used in conjunction with the GitOps repository and image registry. Sonarqube is used by the MQ application pipeline for code quality and security scanning. It helps ensure the quality of a deployed application. ArgoCD applications will be created for each of these resources. Specifically, ArgoCD applications will keep the cluster synchronized with the application and queue manager YAML definitions stored in Artifactory and Git. Note how these application services are installed in the ci and tools namespaces we created in the previous topic. This section will reinforce our understanding of GitOps. We will then be ready to create MQ applications and queue managers that use the infrastructure we previously created and these services we are going to install further. In this topic, we're going to: Deploy services to the cluster using GitOps Explore the ArgoCD applications that manage these services Explore how more complex services work using Artifactory as an example Review how ArgoCD projects work See how infra and services ArgoCD applications manage the infrastructure and services layers in our architecture. By the end of this topic we'll have a fully configured cluster which is ready for us to deploy MQ applications and queue managers.","title":"Overview"},{"location":"guides/cp4i/mq/cluster-config/services/#pre-requisites","text":"Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed on your local machine the oc command that matches the version of your cluster. You have installed npm , git and tree commands. You have completed the tutorial section to customize the GitOps repository, and install ArgoCD. You have completed the tutorial section to create the ci , tools and dev namespaces using GitOps. Please see the previous sections of this guide for information on how to do these tasks.","title":"Pre-requisites"},{"location":"guides/cp4i/mq/cluster-config/services/#video-walkthrough","text":"This video demonstrates how to install Tekton. It also shows how to use the GitOps repository to set up different service related components. This is a video walkthrough and it takes you step by step through the below sections.","title":"Video Walkthrough"},{"location":"guides/cp4i/mq/cluster-config/services/#post-cluster-provisioning-tasks","text":"","title":"Post cluster provisioning tasks"},{"location":"guides/cp4i/mq/cluster-config/services/#red-hat-openshift-cluster","text":"An OpenShift v4.7+ cluster is required.","title":"Red Hat OpenShift cluster"},{"location":"guides/cp4i/mq/cluster-config/services/#cli-tools","text":"Install the OpenShift CLI oc (version 4.7+) . The binary can be downloaded from the Help menu from the OpenShift Console. Download oc cli Log in from a terminal window. oc login --token = <token> --server = <server>","title":"CLI tools"},{"location":"guides/cp4i/mq/cluster-config/services/#ibm-entitlement-key","text":"The IBM Entitlement Key is required to pull IBM Cloud Pak specific container images from the IBM Entitled Registry. To get an entitlement key, Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. A Secret containing the entitlement key is created in the tools namespace. oc new-project tools || true oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io","title":"IBM Entitlement Key"},{"location":"guides/cp4i/mq/cluster-config/services/#installing-tekton-for-gitops","text":"Tekton is made available to your Red Hat OpenShift cluster through the Red Hat OpenShift Pipelines operator. Hence, let's see how to get that operator installed on your cluster. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Install Tekton into the cluster We use the Red Hat Pipelines operator to install Tekton into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: - argocd/operators/openshift-pipelines.yaml Your kustomization.yaml for services should match the following: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Install tekton using Red Hat OpenShift Pipelines Operator\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 524 bytes | 524.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 85a4c46..61e15b0 master -> master Activate the services in the GitOps repo Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us only deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see openshift-pipelines ArgoCD application have been created as follows: Wait for the Tekton installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/openshift-pipelines-operator -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/openshift-pipelines-operator condition met Tekton is now installed and ready to use. We\u2019ll explore Tekton in later topics. If you see something like: Error from server (NotFound): deployments.apps \"openshift-pipelines-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected.","title":"Installing Tekton for GitOps"},{"location":"guides/cp4i/mq/cluster-config/services/#deploy-services-to-the-cluster","text":"We've just had our first successful GitOps experience, using an ArgoCD application to create the ci , tools and dev namespaces in our cluster at the infrastructure layer level as well as using ArgoCD to deploy the Red Hat OpenShift Pipelines operator at the services layer level. There are few more components you need to create for this IBM MQ tutorial: IBM MQ, IBM Platform navigator, IBM foundations, IBM automation foundations, IBM Catalog, Sealed secrets, Sonarqube and Artifactory. These components are part of the services layer in our architecture, and that requires us to access /0-bootstrap/single-cluster/2-services within our GitOps repository. Check services layer components As we saw earlier, the bootstrap-single-cluster application uses the contents of the /0-bootstrap/single-cluster/2-services folder to determine which Kubernetes resources are available to be deployed in the cluster. Issue the following command to see what could be deployed in the cluster: tree 0 -bootstrap/single-cluster/2-services/ -L 3 You should see a set of operators and instances of these available in the GitOps framework at the services layer: 0 -bootstrap/single-cluster/2-services/ \u251c\u2500\u2500 2 -services.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 artifactory.yaml \u2502 \u2502 \u251c\u2500\u2500 baas-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 cert-manager-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 chartmuseum.yaml \u2502 \u2502 \u251c\u2500\u2500 developer-dashboard.yaml \u2502 \u2502 \u251c\u2500\u2500 grafana-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-gateway-analytics-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-management-portal-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4sthreatmanagements-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cpd-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-foundational-services-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-platform-navigator-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-process-mining-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 oadp-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 openshift-service-mesh-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 pact-broker.yaml \u2502 \u2502 \u251c\u2500\u2500 sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 sonarqube.yaml \u2502 \u2502 \u251c\u2500\u2500 spp-instance.yaml \u2502 \u2502 \u2514\u2500\u2500 swaggereditor.yaml \u2502 \u2514\u2500\u2500 operators \u2502 \u251c\u2500\u2500 cert-manager.yaml \u2502 \u251c\u2500\u2500 elasticsearch.yaml \u2502 \u251c\u2500\u2500 grafana-operator.yaml \u2502 \u251c\u2500\u2500 ibm-ace-operator.yaml \u2502 \u251c\u2500\u2500 ibm-apic-operator.yaml \u2502 \u251c\u2500\u2500 ibm-aspera-operator.yaml \u2502 \u251c\u2500\u2500 ibm-assetrepository-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-core-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-operator.yaml \u2502 \u251c\u2500\u2500 ibm-catalogs.yaml \u2502 \u251c\u2500\u2500 ibm-cp4a-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4i-operators.yaml \u2502 \u251c\u2500\u2500 ibm-cp4s-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-platform-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-scheduling-operator.yaml \u2502 \u251c\u2500\u2500 ibm-datapower-operator.yaml \u2502 \u251c\u2500\u2500 ibm-db2u-operator.yaml \u2502 \u251c\u2500\u2500 ibm-eventstreams-operator.yaml \u2502 \u251c\u2500\u2500 ibm-foundations.yaml \u2502 \u251c\u2500\u2500 ibm-mq-operator.yaml \u2502 \u251c\u2500\u2500 ibm-opsdashboard-operator.yaml \u2502 \u251c\u2500\u2500 ibm-platform-navigator.yaml \u2502 \u251c\u2500\u2500 ibm-process-mining-operator.yaml \u2502 \u251c\u2500\u2500 jaeger.yaml \u2502 \u251c\u2500\u2500 kiali.yaml \u2502 \u251c\u2500\u2500 oadp-operator.yaml \u2502 \u251c\u2500\u2500 openshift-gitops.yaml \u2502 \u251c\u2500\u2500 openshift-pipelines.yaml \u2502 \u251c\u2500\u2500 openshift-service-mesh.yaml \u2502 \u251c\u2500\u2500 spp-catalog.yaml \u2502 \u2514\u2500\u2500 spp-operator.yaml \u2514\u2500\u2500 kustomization.yaml Review ArgoCD services folder Let\u2019s examine the 0-bootstrap/single-cluster/2-services/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/kustomization.yaml We can see the contents of the kustomization.yaml : resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Add the services to the cluster Note The IBM Platform Navigator instance requires a RWX storageclass and it is set to managed-nfs-storage by default in the ArgoCD Application 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-platform-navigator-instance.yaml . This storageclass is available for Red Hat OpenShift on IBM Cloud cluster provisioned from IBM Technology Zone with NFS storage selected. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: ## Cloud Pak for Integration - argocd/operators/ibm-mq-operator.yaml - argocd/operators/ibm-platform-navigator.yaml - argocd/instances/ibm-platform-navigator-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD - argocd/instances/artifactory.yaml - argocd/instances/sonarqube.yaml You will have the following resources deployed for services: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml - argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml - argocd/operators/ibm-platform-navigator.yaml - argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml - argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml - argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 564 bytes | 564.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git b49dff5..533602c master -> master The intention of this operation is to indicate that we'd like the resources declared in 0-bootstrap/single-cluster/2-services/kustomization.yaml to be deployed in the cluster. Like the infra ArgoCD application, the resources created by the services ArgoCD application will manage the Kubernetes relevant services resources applied to the cluster. Optional: Activate the services in the GitOps repo Run this step if you skip installing tekton in the previous section. If you did not skip that step go to Step 7 . Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see lots of new ArgoCD applications have been created to manage the services we have deployed by modifying kustomization.yaml under 0-bootstrap/single-cluster/2-services folder: See how most ArgoCD applications are Synced almost immediately, but some spend time in Progressing . That's because the Artifactory and Sonarqube ArgoCD applications are more complex -- they create more Kubernetes resources than other ArgoCD applications and therefore take longer to sync . After a few minutes you'll see that all ArgoCD applications become Healthy and Synced : Notice how many more Synched ArgoCD applications are now in the cluster; these are as a result of the newly added services layer in our architecture. The services application Let's examine the ArgoCD application that manage the services in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the services application: We can see that the services ArgoCD application creates 10 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It\u2019s the services ArgoCD application that watches the 0-bootstrap/single-cluster/2-services/argocd folder for ArgoCD applications that apply service resources to our cluster. It was the services application that created the artifactory ArgoCD application which manages the Artifactory instance that is used for application configuration management that we will be exploring later in this section of the tutorial. We\u2019ll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it\u2019s not necessary, as you\u2019ll get lots of practice as we proceed. Examine Artifactory resources in detail You can see the Kubernetes resources that the Artifactory ArgoCD application has created for Artifactory. Click on the Artifactory : See how many different Kubernetes resources were created when the Artifactory ArgoCD application YAML was applied to the cluster. That's why it took the Artifactory ArgoCD application a little time to sync. If you'd like, have a look at some of the other ArgoCD applications, such as Sonarqube and the Kubernetes resources they created. The services ArgoCD project As we've seen in the ArgoCD UI, the services ArgoCD application is responsible for creating the ArgoCD applications that manage the services within the cluster. Let's examine their definitions to see how they do this. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/2-services.yaml The following YAML may initially look a little intimidating; we'll discuss the major elements below: --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : services labels : gitops.tier.layer : services spec : sourceRepos : [] # Populated by kustomize patches in 2-services/kustomization.yaml destinations : - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : redhat-operators server : https://kubernetes.default.svc - namespace : openshift-operators server : https://kubernetes.default.svc - namespace : openshift-marketplace server : https://kubernetes.default.svc - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : # TODO: SCC needs to be moved to 1-infra, here for now for artifactory - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleLink - group : \"apps\" kind : statefulsets - group : \"apps\" kind : deployments - group : \"\" kind : services - group : \"\" kind : configmaps - group : \"\" kind : secrets - group : \"\" kind : serviceaccounts - group : \"batch\" kind : jobs - group : \"\" kind : roles - group : \"route.openshift.io\" kind : routes - group : \"\" kind : RoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRole - group : apiextensions.k8s.io kind : CustomResourceDefinition roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : services annotations : argocd.argoproj.io/sync-wave : \"200\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : services source : # repoURL and targetRevision populated by kustomize patches in 2-services/kustomization.yaml path : 0-bootstrap/single-cluster/2-services syncPolicy : automated : prune : true selfHeal : true Notice how this YAML defines three ArgoCD resources: a services project which manages all the necessary services that are needed by the applications. Notice how the destinations for the services project are limited to the ci , tools and dev namespaces -- as well as a few others that we'll use in the tutorial. These destinations restrict the namespaces where ArgoCD applications in the services project can manage resources. The same is true for clusterResourceWhiteList . It limits the Kubernetes resources that can be managed to configmaps , deployments and rolebindings amongst others. In summary, we see that the service project is used to group all the ArgoCD applications that will manage the services in our cluster. These ArgoCD applications can only perform specific actions on specific resource types in specific namespaces. See how ArgoCD is acting as a well-governed administrator. The similar structure of services and infra ArgoCD applications Even though we didn't closely examine the infra ArgoCD application YAML in the previous topic, it has has a very similar structure to the ArgoCD services applications we've just examined. Type the following command to list the ArgoCD infra app YAML. cat 0 -bootstrap/single-cluster/1-infra/1-infra.yaml Again, although this YAML might look a little intimidating, the overall structure is the same as for services : --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : infra labels : gitops.tier.layer : infra spec : sourceRepos : [] # Populated by kustomize patches in 1-infra/kustomization.yaml destinations : - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : - group : \"\" kind : Namespace - group : \"\" kind : RoleBinding - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleNotification - group : \"console.openshift.io\" kind : ConsoleLink roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : infra annotations : argocd.argoproj.io/sync-wave : \"100\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : # repoURL and targetRevision populated by kustomize patches in 1-infra/kustomization.yaml path : 0-bootstrap/single-cluster/1-infra syncPolicy : automated : prune : true selfHeal : true As with the 2-services.yaml , we can see An ArgoCD project called infra . ArgoCD applications defined in this project will be limited by the destinations: and clusterResourceWhitelist: specified in the YAML. An ArgoCD app called infra . This is the ArgoCD application that we used in the previous section of the tutorial. It watches the path: 0-bootstrap/single-cluster/1-infra folder for ArgoCD applications that it applied to the cluster. It was these applications that managed the ci , tools and dev namespaces for example. The installation of the operators will take approximately 30 - 45 minutes. Validate the deployment of IBM MQ and IBM Platform Navigator. Verify the IBM Platform Navigator has been deployed successfully. oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' Expected output = True The Platform Navigator console has been installed as part of the IBM Cloud Pak for Integration. We use the Platform Navigator to view our queue manager and its queues. The Spring application includes a Swagger interface which we can use to call methods on the Spring application. In the next few steps we will use these to interact with both the queue manager and Spring application. Access the IBM Platform Navigator console Retrieve the URL for the IBM Platform Navigator console: oc get route integration-navigator-pn -n tools -o jsonpath = '{.spec.host}' Copy the URL from the HOST/PORT column and paste into a browser, prefixed with https:// . Retrieve the admin password: oc get secret ibm-iam-bindinfo-platform-auth-idp-credentials -n dev -o json -n ibm-common-services | jq -r '.data.\"admin_password\"' | base64 -D Log in to the IBM Platform Navigator console with the admin credentials.","title":"Deploy services to the cluster"},{"location":"guides/cp4i/mq/cluster-config/services/#other-important-argocd-features","text":"In this final section of this chapter, let's explore ArgoCD features you may have noticed as you explored different YAML files in this chapter:","title":"Other important ArgoCD features"},{"location":"guides/cp4i/mq/cluster-config/services/#syncwave","text":"Using SyncWave to control deployment sequencing When ArgoCD uses a GitOps repository to apply ArgoCD applications to the cluster, it applies all ArgoCD applications concurrently. Even though Kubernetes use of eventual consistency means that resources which depend on each other can be deployed in any order, it often makes sense to help deploy certain resources in a certain order. This can avoid spurious error messages or wasted compute resources for example. Let's compare two ArgoCD YAML files. Firstly, let's examine the namespace-tools in the infrastructure layer: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-tools.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: tools labels: gitops.tier.layer: infra annotations: argocd.argoproj.io/sync-wave: \"100\" ... Then examine the artifactory in the services layer: cat 0 -bootstrap/single-cluster/2-services/argocd/instances/artifactory.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: artifactory annotations: argocd.argoproj.io/sync-wave: \"250\" labels: gitops.tier.group: cntk gitops.tier.layer: services finalizers: - resources-finalizer.argocd.argoproj.io ... Notice the use of sync-wave: \"100\" for tools and how it contrasts with sync-wave: \"250\" for artifactory . The lower 100 will be deployed before the higher sync-wave 250 because it only makes sense to deploy the pipelines into the tools namespace, after the tools namespace has been created. You are free to choose any number for sync-wave . In our deployment, we have chosen 100-199 for infrastructure, 200-299 for services and 300-399 for applications; it provides alignment with high level folder numbering such as 1-infra and so on. In our tutorial, we're going to incrementally add infrastructure, services and applications so that you can understand how everything fits together incrementally. This makes sync-wave less important than in a real-world system, where you might be making updates to each of these deployed layers simultaneously. In such cases the use of sync-wave provides a high degree of confidence in the effectiveness of a deployment change because all changes go through in the correct order. We'll make extensive use of sync-wave when we look at High Availability MQ deployments. Congratulations! You've installed tekton and configured the key services in the cluster to support continuous integration and continuous delivery. These have included services like Sonarqube and Artifactory. We installed them into the tools namespaces we created previously. This chapter is now complete. In the following chapters, we're going to use these components to deploy MQ applications and queue managers to the cluster.","title":"SyncWave"},{"location":"guides/cp4i/mq/cluster-create/aws-setup/","text":"Overview \u00b6 This sprint does not support AWS clusters Later sprints will support clusters provisioned on AWS","title":"Creating a cluster on AWS"},{"location":"guides/cp4i/mq/cluster-create/aws-setup/#overview","text":"This sprint does not support AWS clusters Later sprints will support clusters provisioned on AWS","title":"Overview"},{"location":"guides/cp4i/mq/cluster-create/azure-setup/","text":"Overview \u00b6 This sprint does not support Azure clusters Later sprints will support clusters on different platform","title":"Creating a cluster on Azure"},{"location":"guides/cp4i/mq/cluster-create/azure-setup/#overview","text":"This sprint does not support Azure clusters Later sprints will support clusters on different platform","title":"Overview"},{"location":"guides/cp4i/mq/cluster-create/ibm-setup/","text":"Creating a Red Hat OpenShift cluster \u00b6 Audience : Architects, Application developers, Administrators Overview \u00b6 In this topic we're going to: Introduce you to IBM Technology Zone Provision an OpenShift cluster By the end of this topic you'll have created an OpenShift cluster for the MQ tutorials. IBM Technology Zone \u00b6 IBM Technology Zone is a one-stop shop to get access to technical environments and software for demos, prototyping and deployment. We're going to use it to provision the OpenShift cluster used by our tutorial. Technology zone can be used to provision a Red Hat OpenShift cluster on different infrastructure including IBM Cloud, Azure, AWS and VMWare. There is also a degree of flexibility to configure the compute, storage and network for these infrastructures. The MQ tutorial will work on all of these platforms. It is also possible to provision a cluster via the command line using the instructions in the Hands-on section of this guide. Decide infrastructure \u00b6 Decide which infrastructure platform you are going to use for your Red Hat OpenShift cluster from the available options: Red Hat OpenShift on IBM Cloud Click on your chosen infrastructure link above or scroll to down to the appropriate section in this topic. OpenShift on IBM Cloud \u00b6 Navigate to the IBM Technology Zone In your browser, navigate to the IBM Technology Zone to create your cluster. You will need to sign-in with your IBM credentials if your browser has not authenticated. You will need to read and accept Terms and conditions to proceed to the IBM Technology Zone. Reserve a cluster You can reserve a cluster immediately on IBM Cloud. You'll see the following options: Click the Reserve now radio button. It takes between 30 and 60 minutes to provision a cluster. Moreover, once a cluster has been provisioned, by default, it will be deleted after 3 days -- unless you renew it. It can therefore be helpful to schedule the environment to be created later if you are not going to be using it for a while -- such as for a future demonstration or classroom session. Come back and explore the Schedule for later option another time. Complete reservation details - step 1 You will be asked to complete your reservations details in two steps. Firstly, when you make a cluster reservation, you can supply a set of details that describe: The cluster name The purpose of the reservation from a pre-defined set A description, which will appear for a Practice/Self-Education reservation Notes to help you identify this reservation In the following web page: Complete the details as follows: Type a helpful Name such as My cluster 1 Select Practice/Self-Education from the Purpose drop-down list Type a helpful Purpose Description such as Education Do not select Preferred Geography yet; we will do that in a moment Add some helpful Notes such as Learn how to build a cloud native production reference deployment using a CloudPak and OpenShift on IBM Cloud Complete reservation details - step 2 The second step in registration is to specify the physical details of the compute infrastructure for your cluster. When you make a cluster reservation, you can supply a set of details that describe: The cluster's geographical location The version of OpenShift Container Platform used by the cluster The number of CPUs in the cluster and their memory size The number of Kubernetes worker nodes in the cluster The NFS storage size for the cluster The End date and time for the cluster In the following web page: Complete the details as follows: Select your Preferred Geography such as London 5 , HongKong 2 or San Jose 04 Under End date and time , select a date and specify the time along with its time zone. Select Worker Node Count as 3 from the drop-down list Select Worker Node Flavor as 8CPU x 32GB from the drop-down list Select NFS Size as 500 GB from the drop-down list Select OpenShift Version as 4.8 from the drop-down-list Info By default, your cluster will be reserved for three days , after which time it will be deleted. This is more than enough time to complete this tutorial. If required, you can increase the cluster lifetime by configuring the End date and time . Don't do this unless you really need to do so; as you'll learn in the tutorial, the use of GitOps makes it quick and easy to provision a cluster when you need to do so. Create the cluster Check your reservation details and press Submit when you're ready to create the cluster: The cluster provisioning process will take between 30 and 60 minutes to complete on IBM Cloud. Provisioning email As the provisioning process starts, you will receive an email to confirm that the provisioning process has started: Checking the cluster provisioning status You can also view the status of your provisioning request on IBM Technology Zone. Click on My reservations to see: Note: The Status: field will be Ready when the cluster has been provisioned. The creation and expiry date-time for your cluster. You can return to the reservation at any time to get the details of your cluster. Cluster provisioned email You will receive another email once your cluster is ready: Your cluster is now provisioned and ready to use. Note the expiry date-time. Navigate to your cluster You can use the information in the notification email to access your cluster. Click on the Cluster URL link to login to IBM Cloud and locate your OpenShift cluster: Note: The Cluster ID that has been assigned by IBM Cloud. The Zone matches your requested geography. The OCP Version matches your requested version of OCP. The Node status identifies the 3 worker nodes in the Normal state. Feel free to explore this web page. Receiving cluster deletion email and extending your cluster's lifetime When your cluster is within 3 days of deletion, you will receive a daily email: Note: You can extend your cluster if required. This option may be limited to certain types of reservations. The My reservations page allows you to manage your cluster reservation Congratulations, you've now created your cluster. In the next topic, we're going to configure the cluster for GitOps, so that we can manage it quickly, easily and effectively.","title":"Create the cluster"},{"location":"guides/cp4i/mq/cluster-create/ibm-setup/#creating-a-red-hat-openshift-cluster","text":"Audience : Architects, Application developers, Administrators","title":"Creating a Red Hat OpenShift cluster"},{"location":"guides/cp4i/mq/cluster-create/ibm-setup/#overview","text":"In this topic we're going to: Introduce you to IBM Technology Zone Provision an OpenShift cluster By the end of this topic you'll have created an OpenShift cluster for the MQ tutorials.","title":"Overview"},{"location":"guides/cp4i/mq/cluster-create/ibm-setup/#ibm-technology-zone","text":"IBM Technology Zone is a one-stop shop to get access to technical environments and software for demos, prototyping and deployment. We're going to use it to provision the OpenShift cluster used by our tutorial. Technology zone can be used to provision a Red Hat OpenShift cluster on different infrastructure including IBM Cloud, Azure, AWS and VMWare. There is also a degree of flexibility to configure the compute, storage and network for these infrastructures. The MQ tutorial will work on all of these platforms. It is also possible to provision a cluster via the command line using the instructions in the Hands-on section of this guide.","title":"IBM Technology Zone"},{"location":"guides/cp4i/mq/cluster-create/ibm-setup/#decide-infrastructure","text":"Decide which infrastructure platform you are going to use for your Red Hat OpenShift cluster from the available options: Red Hat OpenShift on IBM Cloud Click on your chosen infrastructure link above or scroll to down to the appropriate section in this topic.","title":"Decide infrastructure"},{"location":"guides/cp4i/mq/cluster-create/ibm-setup/#openshift-on-ibm-cloud","text":"Navigate to the IBM Technology Zone In your browser, navigate to the IBM Technology Zone to create your cluster. You will need to sign-in with your IBM credentials if your browser has not authenticated. You will need to read and accept Terms and conditions to proceed to the IBM Technology Zone. Reserve a cluster You can reserve a cluster immediately on IBM Cloud. You'll see the following options: Click the Reserve now radio button. It takes between 30 and 60 minutes to provision a cluster. Moreover, once a cluster has been provisioned, by default, it will be deleted after 3 days -- unless you renew it. It can therefore be helpful to schedule the environment to be created later if you are not going to be using it for a while -- such as for a future demonstration or classroom session. Come back and explore the Schedule for later option another time. Complete reservation details - step 1 You will be asked to complete your reservations details in two steps. Firstly, when you make a cluster reservation, you can supply a set of details that describe: The cluster name The purpose of the reservation from a pre-defined set A description, which will appear for a Practice/Self-Education reservation Notes to help you identify this reservation In the following web page: Complete the details as follows: Type a helpful Name such as My cluster 1 Select Practice/Self-Education from the Purpose drop-down list Type a helpful Purpose Description such as Education Do not select Preferred Geography yet; we will do that in a moment Add some helpful Notes such as Learn how to build a cloud native production reference deployment using a CloudPak and OpenShift on IBM Cloud Complete reservation details - step 2 The second step in registration is to specify the physical details of the compute infrastructure for your cluster. When you make a cluster reservation, you can supply a set of details that describe: The cluster's geographical location The version of OpenShift Container Platform used by the cluster The number of CPUs in the cluster and their memory size The number of Kubernetes worker nodes in the cluster The NFS storage size for the cluster The End date and time for the cluster In the following web page: Complete the details as follows: Select your Preferred Geography such as London 5 , HongKong 2 or San Jose 04 Under End date and time , select a date and specify the time along with its time zone. Select Worker Node Count as 3 from the drop-down list Select Worker Node Flavor as 8CPU x 32GB from the drop-down list Select NFS Size as 500 GB from the drop-down list Select OpenShift Version as 4.8 from the drop-down-list Info By default, your cluster will be reserved for three days , after which time it will be deleted. This is more than enough time to complete this tutorial. If required, you can increase the cluster lifetime by configuring the End date and time . Don't do this unless you really need to do so; as you'll learn in the tutorial, the use of GitOps makes it quick and easy to provision a cluster when you need to do so. Create the cluster Check your reservation details and press Submit when you're ready to create the cluster: The cluster provisioning process will take between 30 and 60 minutes to complete on IBM Cloud. Provisioning email As the provisioning process starts, you will receive an email to confirm that the provisioning process has started: Checking the cluster provisioning status You can also view the status of your provisioning request on IBM Technology Zone. Click on My reservations to see: Note: The Status: field will be Ready when the cluster has been provisioned. The creation and expiry date-time for your cluster. You can return to the reservation at any time to get the details of your cluster. Cluster provisioned email You will receive another email once your cluster is ready: Your cluster is now provisioned and ready to use. Note the expiry date-time. Navigate to your cluster You can use the information in the notification email to access your cluster. Click on the Cluster URL link to login to IBM Cloud and locate your OpenShift cluster: Note: The Cluster ID that has been assigned by IBM Cloud. The Zone matches your requested geography. The OCP Version matches your requested version of OCP. The Node status identifies the 3 worker nodes in the Normal state. Feel free to explore this web page. Receiving cluster deletion email and extending your cluster's lifetime When your cluster is within 3 days of deletion, you will receive a daily email: Note: You can extend your cluster if required. This option may be limited to certain types of reservations. The My reservations page allows you to manage your cluster reservation Congratulations, you've now created your cluster. In the next topic, we're going to configure the cluster for GitOps, so that we can manage it quickly, easily and effectively.","title":"OpenShift on IBM Cloud"},{"location":"guides/cp4i/mq/cluster-create/vmware-setup/","text":"Overview \u00b6 This sprint does not support VMWare clusters Later sprints will allow clusters on different platform","title":"Creating a cluster on VMWare"},{"location":"guides/cp4i/mq/cluster-create/vmware-setup/#overview","text":"This sprint does not support VMWare clusters Later sprints will allow clusters on different platform","title":"Overview"},{"location":"guides/cp4i/mq/disaster-recovery/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/mq/disaster-recovery/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/mq/high-availability/ha1-ir-ap/","text":"MQ High Availability Architectures: In-Region Active Passive \u00b6 Overview \u00b6 Audience: Architects, Application developers, Administrators In this topic, we're going to: learn the essential elements of what provides NativeHA high-availability to the latest versions of IBM MQ understand the impact at the infrastructure, services, and application tiers which a high-availability requirement brings deploy this topology to your OpenShift cluster Architecture Overview \u00b6 The following components comprise the high-availability topology at a high level. As your desired topology evolves to reduce single points of failure, this list will become more robust in nature. Virtual Private Cloud: Distribute workloads across a minimum of three (3) sets of infrastructure service resources in 3 distinct availability zones (AZ). Red Hat OpenShift Container Platform: Minimum two (2) OpenShift Compute Nodes per AZ to achieve standard OpenShift SLOs (99.99% possible). IBM MQ - Native HA Queue Manager: Minimum one (1) OpenShift Node in each AZ for normal mode operations. Minimum two (2) OpenShift Nodes in each AZ during OpenShift Node maintenance. MQ Client Application: Stateless container-based client application Leverage anti-affinity deployment practices where possible Single Points of Failure \u00b6 These services and infrastructure components have been identified as potential single points of failure in this topology: Red Hat OpenShift Container Platform Queue Manager instance Regional Cloud Networking Regional Cloud Storage Regional Cloud Ingress/Egress Infrastructure Requirements \u00b6 This topology, as it is the simplest topology which still provides high-availability, it also requires the least amount of bespoke support from the underlying infrastructure. Block Storage with ReadWriteOnce support Multiple availability zones inside of a single region MQ Assets \u00b6 All of the high-availability topologies in this deployment guide leverage the NativeHA feature of IBM MQ for resiliency. As the topologies grow more complex, additional configurations and deployments are added to this least-common architectural denominator. The recently released NativeHA capability allows for multiple QueueManager pods to be running, with only one of them truly being \"active\" and receiving traffic from the OpenShift Route or Service. The running pods, managed by a StatefulSet on the underlying Kubernetes-based platform, actively share log and message data between the active and passive QueueManager instances. Once the active QueueManager instance becomes unavailable for any reason, the remaining passive pods take advantage of the RAFT algorithm to elect a new leader and within seconds failover to a different QueueManager instance, while minimizing service and message unavailability. For more details on the early release of NativeHA for IBM MQ 9.2, you can check out the official IBM MQ Docs . Our example here sets the following properties in the QueueManager YAML file and pushes the changes to your GitOps repo: availability : type : NativeHA tls : secretName : <inter-queuemanager-tls-cert-secret> cipherSpec : ANY_TLS12_OR_HIGHER storage : defaultClass : ibmc-vpc-block-metro-10iops-tier queueManager : enabled : true size : 2Gi type : persistent-claim The availability.type setting is required to be NativeHA to take advantage of this capability. The availability.tls setting allows for TLS-encrypted communication between QueueManager instances in the StatefulSet. The storage.defaultClass setting should point to a multi-AZ supported storage class the provides ReadWriteOnce storage capability for Block Storage. The storage.queueManager setting is required to be enabled and set to type persistent-claim for NativeHA fail-over capability to function. Client Application Recommendations \u00b6 Our sample client application is available under the Building MQ applications section. There are minimal additional configuration requirements to deploy the client application connected to this topology. To be addressed: Automatic client reconnection Connects to a single Route URL provided by the MQ QueueManager instance As only a single region is deployed, there is no consideration given for message sequencing needs beyond traditional single QueueManager requirements. Deploy the HA Topology \u00b6 Warning Needs to be updated (2021/09/08) Audience: Architects, Application developers, Administrators In this topic, we're going to: deploy an in-region, active-passive MQ topology to your OpenShift cluster consisting of a multi-AZ NativeHA-enabled Queue Manager and stateless, multi-AZ client application Pre-requisites \u00b6 In order to deploy a highly-available QueueManager and MQ client application, we require the following prerequisites to be configured and installed: Create the cluster on the infrastructure of choice, ensuring inclusion of multiple availability zones (AZ). Configure the cluster . This includes: ensuring an availability-zone-aware, block storage StorageClass is available on the cluster. Ensure you are logged into your cluster from the command line. Based upon these prerequisites, the sections below will not be covering each step as in-depth as previous sections have. If you require more explanation or understanding of certain topics or commands, please reference the relevant Configuring the cluster , Building queue managers , and Building MQ applications sections appropriately. Apply high-availability GitOps baseline \u00b6 This section will cover bootstrapping your cluster with everything that is required to deploy a highly-available MQ queue manager and relevant components. The artifacts used in this section are an extension of what was previously used in the Configuring the cluster , Building queue managers , and Building MQ applications sections, albeit in a more streamlined manner. Fork the sample GitOps repository We're going to use the same sample GitOps repository as the previous sections, however we will be utilizing a different branch. Fork the sample configuration repository at https://github.com/cloud-native-toolkit/multi-tenancy-gitops to your GitHub account, if not done previously. Set up environment variable for GitHub name This tutorial uses environment variables to save typing and reduce errors. Similar to the previous sections, you must define the following environment variables with values specific to your environment. Replace the placeholder values in the following commands with values specific to your environment. export GIT_USER = <git-user> export GIT_BRANCH = demo-mq-v2-ha export CLUSTER_DOMAIN = <xyz.clusterdomain.com> You can verify the environment variables are set correctly as follows: echo $GIT_USER echo $GIT_BRANCH echo $CLUSTER_DOMAIN Clone forked GitOps repository Similar to previous sections, we are going to work on a local clone of our GitOps repository. We'll push changes to our local copy back to GitHub at appropriate times so that they can be accessed by ArgoCD. Clone the forked Git config repository to your local machine: git clone https://github.com/ $GIT_USER /multi-tenancy-gitops.git Checkout demo-mq-v2-ha branch The sample repository contains multiple branches, each of which contains a different repository for different IBM Cloud Pak products. We're going to use a branch containing a sample GitOps configuration for highly-available Cloud Pak and MQ components. cd multi-tenancy-gitops git checkout $GIT_BRANCH Run the customization script Let's customize our cloned GitOps repository with the relevant values for our cluster. The sample GitOps repository provides a set-git-source.sh script to make this task easy. It uses the $GIT_USER , GIT_BRANCH , and CLUSTER_DOMAIN variables you defined in the previous steps to customize the repository with your GitHub organization: Run the script: ./scripts/set-git-source.sh Commit & push updates to forked gitops repository We've now customized our local clone of the GitOps config repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add, commit, and push all changes in the current folder to your GitHub repository: git add bootstrap.yaml 1 -infra 2 -services 3 -apps git commit -s -m \"GitOps customizations for organization and cluster\" git push origin $GIT_BRANCH The customized repository is now ready to be used by ArgoCD. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Install Tekton into the cluster We use the Red Hat Pipelines operator to install Tekton into the cluster. The following command creates the subscription: oc apply -f 2 -services/operators/openshift-pipelines/operator.yaml -n openshift-operators The response confirms that the subscription has been created: subscription.operators.coreos.com/openshift-pipelines-operator created It will take a little time for the operator to install; this is just the subscription. Refer to Installing Tekton and ArgoCD for GitOps ] for additional instructions on validating successful operator installation, if needed. Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The following command creates the subscription: oc apply -f 2 -services/operators/openshift-gitops/operator.yaml -n openshift-operators The response confirms that the subscription has been created: subscription.operators.coreos.com/openshift-gitops-operator created It will take a little time for the operator to install; this is just the subscription. Refer to Installing Tekton and ArgoCD for GitOps for additional instructions on validating successful operator installation, if needed. Configure an ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are in the 1-infra/clusterrole folder. We can apply them both to the cluster with the following command: oc apply -f 1 -infra/clusterrole Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. By applying the bootstrap.yaml , we're creating an ArgoCD application that is watching this repository and using its contents to manage the cluster. Apply the bootstrap YAML to the cluster: oc apply -f bootstrap.yaml -n openshift-gitops The bootstrap ArgoCD application will watch the 0-bootstrap/argocd/active folder in our GitOps repository on GitHub. This is the only manual operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops argocd-cluster-server -o jsonpath = \"{.spec.host}\" This will list the route to the ArgoCD we've just installed, for example: argocd-cluster-server-openshift-gitops.aod-cloud-native-1-d02cf90349a0fe46c9804e3ab1fe2643-0000.eu-gb.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console UI. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/argocd-cluster-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Verify ArgoCD Applications are synced, progressing, and healthy ArgoCD Applications with names and artifacts matching services-X-secrets will be in a Degraded state due to missing MQ secrets. This is expected and will be resolved by the steps performed in subsequent sections. Otherwise, all ArgoCD Applications and artifacts should display a Synced and Healthy state after a period of time, due to iterative waves of reconciliation. Review queue manager configuration \u00b6 Follow latest from https://pages.github.ibm.com/cloudpakbringup/mq-deployment-guide/qmgr-pipeline/topic1 Install kubeseal locally Create IBM Entitled Registry secret in ci namespace via SealedSecret pattern Ensure GitHub access token is created on your GitHub account Create GitHub access token secret in ci namespace via SealedSecret pattern Commit and push all changes to the forked gitops repository Ensure to omit the YAMLs that are created in the root repository directory Fork and clone https://github.com/cloud-native-toolkit/mq-infra Run the mq-infra-dev pipeline in the ci namespace, pointing to your forked mq-infra repository. Update values.yaml for NativeHA support as below... Pipeline should run automatically??? To be continued...??? Update the queue manager configuration From the main directory of the cloned GitOps repository source, change to the 2-services/instances/namespace-dev/mq-infra directory: Display the content of the values.yml by running: cat values.yml The configuration options to specifically enable this capability are the queuemanager.availability and queuemanager.storage options in the values.yaml file. Replace the contents of values.yaml with the following updates, ensuring to select an appropriate StorageClass for your infrastructure provider. global: {} queuemanager: availability: type: NativeHA tls: secretName: mq-server-cert cipherSpec: ANY_TLS12_OR_HIGHER storage: defaultClass: ocs-storagecluster-ceph-rbd queueManager: size: 2Gi type: persistent-claim Activate the QueueManager development instance mv 2-services/argocd/inactive/instances/ibm-mq-dev-instance.yaml 2-services/argocd/active/instances/ git commit -a -m \"Activating MQ HA dev instance\" git push Validate QueueManager pods start successfully (a total of 3). One will be Ready 1/1 Two will be Ready 0/1 Review client application configuration \u00b6 Follow latest from https://pages.github.ibm.com/cloudpakbringup/mq-deployment-guide/dev/app Fork and clone https://github.com/cloud-native-toolkit/mq-spring-app Set replicaCount: 3 in values.yaml . Commit & push changes to your forked mq-spring-app repo Create pipeline via oc pipeline --tekton . Activate app-level resources your forked gitops repo: mv 3-apps/argocd/inactive/dev-mq-spring-app.yaml 3-apps/argocd/active/ git commit -a -m \"Activating MQ Spring application\" git push Validate pipeline run & updated pod counts, spread across zones in the cluster Validate NativeHA high-availability using JMeter \u00b6 Jmeter scripts have been provided to facilitate the demonstration of MQ NativeHA high-availability. In this section, you will use Jmeter to send JSON requests to the Sample App which will then PUT messages on an MQ Queue, GET them back and respond back to JMeter. All messages will be processed by the active MQ pod while the other two pods remain in a passive mode. During the JMeter test, you will demo kill the active MQ pod which will result in a passive MQ pod becoming active and starting to accept messages. In a real production scenario, the application would likely retry sending the message to MQ a number of times before sending a failure to the caller, however, for demonstration purposes the application doesn't retry and sends a failure back to Jmeter immediately upon being unable to PUT the message to MQ. It is expected that it will take up to 5 seconds for an inactive passive pod to become the active pod and begin to accept messages. It is assumed that you have already: Configured MQ for NativeHA high-availability in active-passive mode (a StatefulSet of 3 pods with one active/running and two passive/waiting) Configured the Sample mq-spring-app Spring application Now we will run the Jmeter test Locate the JMeter script in the mq-spring-app repo Navigate to the root of the mq-spring-app cloned repository on your filesystem and issue the following command to verify the existence of the mq-spring-app.jmx file which you will use later. ls -la jmeter/mq-spring-app.jmx Download Jmeter to your machine Download and extract the JMeter zip from http://jmeter.apache.org/download_jmeter.cgi Launch JMeter from a terminal window <JMeter install folder>/bin/jmeter.sh Load the sample script From the JMeter UI, click File -> Open and then navigate to the mq-spring-app git repository> and select mq-spring-app.jmx file. Get the route URL for the sample application Use the following command to get the route URL for the sample application from the dev namespace oc get route mq-spring-app -n dev The response will be similar to the one shown below. The HOST/PORT value is the ROUTE URL value that you will need in the next step NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD mq-spring-app mq-spring-app-dev.gitops-mq-ha-6ccd7f378ae819553d37d5f2ee142bd6-0000.tor01.containers.appdomain.cloud mq-spring-app <all> edge None Update the cluster URL The Test Plan has been configured with a user-defined variable to make it easy to use in any environment. It is necessary to update the variable with the URL of the route of the sample application in your environment so that JMeter knows where to send the requests. In Jmeter, click on \"Test Plan\" and then update the user defined variable \u201cCLUSTER\u201d to the ROUTE URL that you collected in the previous step and save your change. Prepare your screen to execute the test In order to view the JMeter test while simulating an MQ pod failure, it is necessary to have both JMeter and the OpenShift administration console on your screen at the same time. In JMeter UI , select View Results in Table from the first Thread Group and select the Scroll Automatically option at the bottom of the screen. In OpenShift UI , select the Pods view of the qm-dev-ibm-mq StatefulSet . Now configure your screen as shown below: Locate the active MQ pod Locate the pod that is currently active by finding the one that is 1/1 in the Ready column in the OpenShift UI. In this example it is qm-dev-ibm-mq-1 Prepare to execute the test You are now ready to execute the test. In the upcoming steps you will: Start the JMeter Test Plan which will send messages to the sample Spring application and on to MQ. You will see green successful completions in the JMeter UI. Once you see some successful completions you will kill/delete the active MQ pod using the OpenShift UI. You will then see red failures in the JMeter UI until one of the other MQ pods becomes active and messages start to be processed successfully. The JMeter test will only run for 30 seconds, so you have to be relatively quick. Now let's run the test Start the JMeter Test In the JMeter UI, click the Green Start Arrow on the toolbar or click Run --> Start Simulate an outage Verify that you are seeing some successful responses in the JMeter UI and then delete the active MQ pod using the OpenShift UI Verify failures You should now see some failures in the JMeter UI until one of the inactive pods becomes active. In the screen shot below you can see some failures in the JMeter UI as there isn't an active MQ pod to process the messages. It can take up to 5 seconds for MQ to detect that the active pod is gone and to elect a new pod to process the messages, Verify success Once another MQ pod becomes active the failures in JMeter will stop and messages will return to success as shown below. As mentioned previously, in Production we would expect the application to handle retrying messages when a failure occurs and to only pass the failure back to the caller if a number of retries are unsuccessful. For this demonstration, it is easier to illustrate the failures in JMeter than it would be inside the application, so we've chosen to immediately reply to the caller with a failure if MQ is unavailable to process the messages.","title":"MQ High Availability Architectures: In-Region Active Passive"},{"location":"guides/cp4i/mq/high-availability/ha1-ir-ap/#mq-high-availability-architectures-in-region-active-passive","text":"","title":"MQ High Availability Architectures: In-Region Active Passive"},{"location":"guides/cp4i/mq/high-availability/ha1-ir-ap/#overview","text":"Audience: Architects, Application developers, Administrators In this topic, we're going to: learn the essential elements of what provides NativeHA high-availability to the latest versions of IBM MQ understand the impact at the infrastructure, services, and application tiers which a high-availability requirement brings deploy this topology to your OpenShift cluster","title":"Overview"},{"location":"guides/cp4i/mq/high-availability/ha1-ir-ap/#architecture-overview","text":"The following components comprise the high-availability topology at a high level. As your desired topology evolves to reduce single points of failure, this list will become more robust in nature. Virtual Private Cloud: Distribute workloads across a minimum of three (3) sets of infrastructure service resources in 3 distinct availability zones (AZ). Red Hat OpenShift Container Platform: Minimum two (2) OpenShift Compute Nodes per AZ to achieve standard OpenShift SLOs (99.99% possible). IBM MQ - Native HA Queue Manager: Minimum one (1) OpenShift Node in each AZ for normal mode operations. Minimum two (2) OpenShift Nodes in each AZ during OpenShift Node maintenance. MQ Client Application: Stateless container-based client application Leverage anti-affinity deployment practices where possible","title":"Architecture Overview"},{"location":"guides/cp4i/mq/high-availability/ha1-ir-ap/#single-points-of-failure","text":"These services and infrastructure components have been identified as potential single points of failure in this topology: Red Hat OpenShift Container Platform Queue Manager instance Regional Cloud Networking Regional Cloud Storage Regional Cloud Ingress/Egress","title":"Single Points of Failure"},{"location":"guides/cp4i/mq/high-availability/ha1-ir-ap/#infrastructure-requirements","text":"This topology, as it is the simplest topology which still provides high-availability, it also requires the least amount of bespoke support from the underlying infrastructure. Block Storage with ReadWriteOnce support Multiple availability zones inside of a single region","title":"Infrastructure Requirements"},{"location":"guides/cp4i/mq/high-availability/ha1-ir-ap/#mq-assets","text":"All of the high-availability topologies in this deployment guide leverage the NativeHA feature of IBM MQ for resiliency. As the topologies grow more complex, additional configurations and deployments are added to this least-common architectural denominator. The recently released NativeHA capability allows for multiple QueueManager pods to be running, with only one of them truly being \"active\" and receiving traffic from the OpenShift Route or Service. The running pods, managed by a StatefulSet on the underlying Kubernetes-based platform, actively share log and message data between the active and passive QueueManager instances. Once the active QueueManager instance becomes unavailable for any reason, the remaining passive pods take advantage of the RAFT algorithm to elect a new leader and within seconds failover to a different QueueManager instance, while minimizing service and message unavailability. For more details on the early release of NativeHA for IBM MQ 9.2, you can check out the official IBM MQ Docs . Our example here sets the following properties in the QueueManager YAML file and pushes the changes to your GitOps repo: availability : type : NativeHA tls : secretName : <inter-queuemanager-tls-cert-secret> cipherSpec : ANY_TLS12_OR_HIGHER storage : defaultClass : ibmc-vpc-block-metro-10iops-tier queueManager : enabled : true size : 2Gi type : persistent-claim The availability.type setting is required to be NativeHA to take advantage of this capability. The availability.tls setting allows for TLS-encrypted communication between QueueManager instances in the StatefulSet. The storage.defaultClass setting should point to a multi-AZ supported storage class the provides ReadWriteOnce storage capability for Block Storage. The storage.queueManager setting is required to be enabled and set to type persistent-claim for NativeHA fail-over capability to function.","title":"MQ Assets"},{"location":"guides/cp4i/mq/high-availability/ha1-ir-ap/#client-application-recommendations","text":"Our sample client application is available under the Building MQ applications section. There are minimal additional configuration requirements to deploy the client application connected to this topology. To be addressed: Automatic client reconnection Connects to a single Route URL provided by the MQ QueueManager instance As only a single region is deployed, there is no consideration given for message sequencing needs beyond traditional single QueueManager requirements.","title":"Client Application Recommendations"},{"location":"guides/cp4i/mq/high-availability/ha1-ir-ap/#deploy-the-ha-topology","text":"Warning Needs to be updated (2021/09/08) Audience: Architects, Application developers, Administrators In this topic, we're going to: deploy an in-region, active-passive MQ topology to your OpenShift cluster consisting of a multi-AZ NativeHA-enabled Queue Manager and stateless, multi-AZ client application","title":"Deploy the HA Topology"},{"location":"guides/cp4i/mq/high-availability/ha1-ir-ap/#pre-requisites","text":"In order to deploy a highly-available QueueManager and MQ client application, we require the following prerequisites to be configured and installed: Create the cluster on the infrastructure of choice, ensuring inclusion of multiple availability zones (AZ). Configure the cluster . This includes: ensuring an availability-zone-aware, block storage StorageClass is available on the cluster. Ensure you are logged into your cluster from the command line. Based upon these prerequisites, the sections below will not be covering each step as in-depth as previous sections have. If you require more explanation or understanding of certain topics or commands, please reference the relevant Configuring the cluster , Building queue managers , and Building MQ applications sections appropriately.","title":"Pre-requisites"},{"location":"guides/cp4i/mq/high-availability/ha1-ir-ap/#apply-high-availability-gitops-baseline","text":"This section will cover bootstrapping your cluster with everything that is required to deploy a highly-available MQ queue manager and relevant components. The artifacts used in this section are an extension of what was previously used in the Configuring the cluster , Building queue managers , and Building MQ applications sections, albeit in a more streamlined manner. Fork the sample GitOps repository We're going to use the same sample GitOps repository as the previous sections, however we will be utilizing a different branch. Fork the sample configuration repository at https://github.com/cloud-native-toolkit/multi-tenancy-gitops to your GitHub account, if not done previously. Set up environment variable for GitHub name This tutorial uses environment variables to save typing and reduce errors. Similar to the previous sections, you must define the following environment variables with values specific to your environment. Replace the placeholder values in the following commands with values specific to your environment. export GIT_USER = <git-user> export GIT_BRANCH = demo-mq-v2-ha export CLUSTER_DOMAIN = <xyz.clusterdomain.com> You can verify the environment variables are set correctly as follows: echo $GIT_USER echo $GIT_BRANCH echo $CLUSTER_DOMAIN Clone forked GitOps repository Similar to previous sections, we are going to work on a local clone of our GitOps repository. We'll push changes to our local copy back to GitHub at appropriate times so that they can be accessed by ArgoCD. Clone the forked Git config repository to your local machine: git clone https://github.com/ $GIT_USER /multi-tenancy-gitops.git Checkout demo-mq-v2-ha branch The sample repository contains multiple branches, each of which contains a different repository for different IBM Cloud Pak products. We're going to use a branch containing a sample GitOps configuration for highly-available Cloud Pak and MQ components. cd multi-tenancy-gitops git checkout $GIT_BRANCH Run the customization script Let's customize our cloned GitOps repository with the relevant values for our cluster. The sample GitOps repository provides a set-git-source.sh script to make this task easy. It uses the $GIT_USER , GIT_BRANCH , and CLUSTER_DOMAIN variables you defined in the previous steps to customize the repository with your GitHub organization: Run the script: ./scripts/set-git-source.sh Commit & push updates to forked gitops repository We've now customized our local clone of the GitOps config repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add, commit, and push all changes in the current folder to your GitHub repository: git add bootstrap.yaml 1 -infra 2 -services 3 -apps git commit -s -m \"GitOps customizations for organization and cluster\" git push origin $GIT_BRANCH The customized repository is now ready to be used by ArgoCD. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Install Tekton into the cluster We use the Red Hat Pipelines operator to install Tekton into the cluster. The following command creates the subscription: oc apply -f 2 -services/operators/openshift-pipelines/operator.yaml -n openshift-operators The response confirms that the subscription has been created: subscription.operators.coreos.com/openshift-pipelines-operator created It will take a little time for the operator to install; this is just the subscription. Refer to Installing Tekton and ArgoCD for GitOps ] for additional instructions on validating successful operator installation, if needed. Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The following command creates the subscription: oc apply -f 2 -services/operators/openshift-gitops/operator.yaml -n openshift-operators The response confirms that the subscription has been created: subscription.operators.coreos.com/openshift-gitops-operator created It will take a little time for the operator to install; this is just the subscription. Refer to Installing Tekton and ArgoCD for GitOps for additional instructions on validating successful operator installation, if needed. Configure an ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are in the 1-infra/clusterrole folder. We can apply them both to the cluster with the following command: oc apply -f 1 -infra/clusterrole Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. By applying the bootstrap.yaml , we're creating an ArgoCD application that is watching this repository and using its contents to manage the cluster. Apply the bootstrap YAML to the cluster: oc apply -f bootstrap.yaml -n openshift-gitops The bootstrap ArgoCD application will watch the 0-bootstrap/argocd/active folder in our GitOps repository on GitHub. This is the only manual operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops argocd-cluster-server -o jsonpath = \"{.spec.host}\" This will list the route to the ArgoCD we've just installed, for example: argocd-cluster-server-openshift-gitops.aod-cloud-native-1-d02cf90349a0fe46c9804e3ab1fe2643-0000.eu-gb.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console UI. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/argocd-cluster-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Verify ArgoCD Applications are synced, progressing, and healthy ArgoCD Applications with names and artifacts matching services-X-secrets will be in a Degraded state due to missing MQ secrets. This is expected and will be resolved by the steps performed in subsequent sections. Otherwise, all ArgoCD Applications and artifacts should display a Synced and Healthy state after a period of time, due to iterative waves of reconciliation.","title":"Apply high-availability GitOps baseline"},{"location":"guides/cp4i/mq/high-availability/ha1-ir-ap/#review-queue-manager-configuration","text":"Follow latest from https://pages.github.ibm.com/cloudpakbringup/mq-deployment-guide/qmgr-pipeline/topic1 Install kubeseal locally Create IBM Entitled Registry secret in ci namespace via SealedSecret pattern Ensure GitHub access token is created on your GitHub account Create GitHub access token secret in ci namespace via SealedSecret pattern Commit and push all changes to the forked gitops repository Ensure to omit the YAMLs that are created in the root repository directory Fork and clone https://github.com/cloud-native-toolkit/mq-infra Run the mq-infra-dev pipeline in the ci namespace, pointing to your forked mq-infra repository. Update values.yaml for NativeHA support as below... Pipeline should run automatically??? To be continued...??? Update the queue manager configuration From the main directory of the cloned GitOps repository source, change to the 2-services/instances/namespace-dev/mq-infra directory: Display the content of the values.yml by running: cat values.yml The configuration options to specifically enable this capability are the queuemanager.availability and queuemanager.storage options in the values.yaml file. Replace the contents of values.yaml with the following updates, ensuring to select an appropriate StorageClass for your infrastructure provider. global: {} queuemanager: availability: type: NativeHA tls: secretName: mq-server-cert cipherSpec: ANY_TLS12_OR_HIGHER storage: defaultClass: ocs-storagecluster-ceph-rbd queueManager: size: 2Gi type: persistent-claim Activate the QueueManager development instance mv 2-services/argocd/inactive/instances/ibm-mq-dev-instance.yaml 2-services/argocd/active/instances/ git commit -a -m \"Activating MQ HA dev instance\" git push Validate QueueManager pods start successfully (a total of 3). One will be Ready 1/1 Two will be Ready 0/1","title":"Review queue manager configuration"},{"location":"guides/cp4i/mq/high-availability/ha1-ir-ap/#review-client-application-configuration","text":"Follow latest from https://pages.github.ibm.com/cloudpakbringup/mq-deployment-guide/dev/app Fork and clone https://github.com/cloud-native-toolkit/mq-spring-app Set replicaCount: 3 in values.yaml . Commit & push changes to your forked mq-spring-app repo Create pipeline via oc pipeline --tekton . Activate app-level resources your forked gitops repo: mv 3-apps/argocd/inactive/dev-mq-spring-app.yaml 3-apps/argocd/active/ git commit -a -m \"Activating MQ Spring application\" git push Validate pipeline run & updated pod counts, spread across zones in the cluster","title":"Review client application configuration"},{"location":"guides/cp4i/mq/high-availability/ha1-ir-ap/#validate-nativeha-high-availability-using-jmeter","text":"Jmeter scripts have been provided to facilitate the demonstration of MQ NativeHA high-availability. In this section, you will use Jmeter to send JSON requests to the Sample App which will then PUT messages on an MQ Queue, GET them back and respond back to JMeter. All messages will be processed by the active MQ pod while the other two pods remain in a passive mode. During the JMeter test, you will demo kill the active MQ pod which will result in a passive MQ pod becoming active and starting to accept messages. In a real production scenario, the application would likely retry sending the message to MQ a number of times before sending a failure to the caller, however, for demonstration purposes the application doesn't retry and sends a failure back to Jmeter immediately upon being unable to PUT the message to MQ. It is expected that it will take up to 5 seconds for an inactive passive pod to become the active pod and begin to accept messages. It is assumed that you have already: Configured MQ for NativeHA high-availability in active-passive mode (a StatefulSet of 3 pods with one active/running and two passive/waiting) Configured the Sample mq-spring-app Spring application Now we will run the Jmeter test Locate the JMeter script in the mq-spring-app repo Navigate to the root of the mq-spring-app cloned repository on your filesystem and issue the following command to verify the existence of the mq-spring-app.jmx file which you will use later. ls -la jmeter/mq-spring-app.jmx Download Jmeter to your machine Download and extract the JMeter zip from http://jmeter.apache.org/download_jmeter.cgi Launch JMeter from a terminal window <JMeter install folder>/bin/jmeter.sh Load the sample script From the JMeter UI, click File -> Open and then navigate to the mq-spring-app git repository> and select mq-spring-app.jmx file. Get the route URL for the sample application Use the following command to get the route URL for the sample application from the dev namespace oc get route mq-spring-app -n dev The response will be similar to the one shown below. The HOST/PORT value is the ROUTE URL value that you will need in the next step NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD mq-spring-app mq-spring-app-dev.gitops-mq-ha-6ccd7f378ae819553d37d5f2ee142bd6-0000.tor01.containers.appdomain.cloud mq-spring-app <all> edge None Update the cluster URL The Test Plan has been configured with a user-defined variable to make it easy to use in any environment. It is necessary to update the variable with the URL of the route of the sample application in your environment so that JMeter knows where to send the requests. In Jmeter, click on \"Test Plan\" and then update the user defined variable \u201cCLUSTER\u201d to the ROUTE URL that you collected in the previous step and save your change. Prepare your screen to execute the test In order to view the JMeter test while simulating an MQ pod failure, it is necessary to have both JMeter and the OpenShift administration console on your screen at the same time. In JMeter UI , select View Results in Table from the first Thread Group and select the Scroll Automatically option at the bottom of the screen. In OpenShift UI , select the Pods view of the qm-dev-ibm-mq StatefulSet . Now configure your screen as shown below: Locate the active MQ pod Locate the pod that is currently active by finding the one that is 1/1 in the Ready column in the OpenShift UI. In this example it is qm-dev-ibm-mq-1 Prepare to execute the test You are now ready to execute the test. In the upcoming steps you will: Start the JMeter Test Plan which will send messages to the sample Spring application and on to MQ. You will see green successful completions in the JMeter UI. Once you see some successful completions you will kill/delete the active MQ pod using the OpenShift UI. You will then see red failures in the JMeter UI until one of the other MQ pods becomes active and messages start to be processed successfully. The JMeter test will only run for 30 seconds, so you have to be relatively quick. Now let's run the test Start the JMeter Test In the JMeter UI, click the Green Start Arrow on the toolbar or click Run --> Start Simulate an outage Verify that you are seeing some successful responses in the JMeter UI and then delete the active MQ pod using the OpenShift UI Verify failures You should now see some failures in the JMeter UI until one of the inactive pods becomes active. In the screen shot below you can see some failures in the JMeter UI as there isn't an active MQ pod to process the messages. It can take up to 5 seconds for MQ to detect that the active pod is gone and to elect a new pod to process the messages, Verify success Once another MQ pod becomes active the failures in JMeter will stop and messages will return to success as shown below. As mentioned previously, in Production we would expect the application to handle retrying messages when a failure occurs and to only pass the failure back to the caller if a number of retries are unsuccessful. For this demonstration, it is easier to illustrate the failures in JMeter than it would be inside the application, so we've chosen to immediately reply to the caller with a failure if MQ is unavailable to process the messages.","title":"Validate NativeHA high-availability using JMeter"},{"location":"guides/cp4i/mq/high-availability/ha2-cr-ap/","text":"MQ High Availability Architectures: Cross Region Active Passive \u00b6 Warning Work in Progress Overview \u00b6 Audience: Architects, Application developers, Administrators In this topic, we're going to: learn the essential elements of what provides NativeHA high-availability to the latest versions of IBM MQ understand the impact at the infrastructure, services, and application tiers which a multi-region, high-availability requirement brings discuss cross-region deployment characteristics and needs deploy this topology to your OpenShift clusters Architecture Overview \u00b6 NOTE: We have to discuss applications on the cluster talking to the GLB endpoint versus having local reference the QM URL The following components comprise the high-availability topology at a high level. As your desired topology evolves to reduce single points of failure, this list will become more robust in nature. Virtual Private Cloud: Distribute workloads across a minimum of three (3) sets of infrastructure service resources in 3 distinct availability zones (AZ). Red Hat OpenShift Container Platform: Minimum two (2) OpenShift Compute Nodes per AZ to achieve standard OpenShift SLOs (99.99% possible). IBM MQ - Native HA Queue Manager: Minimum one (1) OpenShift Node in each AZ for normal mode operations. Minimum two (2) OpenShift Nodes in each AZ during OpenShift Node maintenance. MQ Client Application: Stateless container-based client application Leverage anti-affinity deployment practices where possible Cross-Region Global Load Balancer TBD Discuss failover strategy (automated, manual, somewhere in between) Single Points of Failure \u00b6 These services and infrastructure components have been identified as potential single points of failure in this topology: Red Hat OpenShift Container Platform In-Region Networking Ingress/Egress Networking In-Region Storage Infrastructure Requirements \u00b6 This topology evolves this simplest high-availability scenario while adding minimal overhead to provide a nominal improvement in the overall availability. Block Storage with ReadWriteOnce support Multiple availability zones inside of a single region Multiple regions inside the same cloud provider A Cross-Region Global Load Balancer (ALB, NLB, or DNS still TBD) to handle traffic routing and failover MQ Assets \u00b6 All of the high-availability topologies in this deployment guide leverage the NativeHA feature of IBM MQ for resiliency. As the topologies grow more complex, additional configurations and deployments are added to this least-common architectural denominator. The recently released NativeHA capability allows for multiple QueueManager pods to be running, with only one of them truly being \"active\" and receiving traffic from the OpenShift Route or Service. The running pods, managed by a StatefulSet on the underlying Kubernetes-based platform, actively share log and message data between the active and passive QueueManager instances. Once the active QueueManager instance becomes unavailable for any reason, the remaining passive pods take advantage of the RAFT algorithm to elect a new leader and within seconds failover to a different QueueManager instance, while minimizing service and message unavailability. For more details on the early release of NativeHA for IBM MQ 9.2, you can check out the official IBM MQ Docs . Our example here sets the following properties in the QueueManager YAML file and pushes the changes to your GitOps repo: availability : type : NativeHA tls : secretName : <inter-queuemanager-tls-cert-secret> cipherSpec : ANY_TLS12_OR_HIGHER storage : defaultClass : ibmc-vpc-block-metro-10iops-tier queueManager : enabled : true size : 2Gi type : persistent-claim The availability.type setting is required to be NativeHA to take advantage of this capability. The availability.tls setting allows for TLS-encrypted communication between QueueManager instances in the StatefulSet. The storage.defaultClass setting should point to a multi-AZ supported storage class the provides ReadWriteOnce storage capability for Block Storage. The storage.queueManager setting is required to be enabled and set to type persistent-claim for NativeHA fail-over capability to function. Client Application Recommendations \u00b6 Our sample client application is available under the Building MQ applications section. There are minimal additional configuration requirements to deploy the client application connected to this topology. TBD Client application configuration details To be addressed: Automatic client reconnection Connects to a single URL (the global load balancer), which provides active-passive failover to the OpenShift Routes provided by the MQ QueueManager instances As we only have a single region deployed, there is no consideration given for message sequencing needs beyond traditional single QueueManager requirements. TBD","title":"MQ High Availability Architectures: Cross Region Active Passive"},{"location":"guides/cp4i/mq/high-availability/ha2-cr-ap/#mq-high-availability-architectures-cross-region-active-passive","text":"Warning Work in Progress","title":"MQ High Availability Architectures: Cross Region Active Passive"},{"location":"guides/cp4i/mq/high-availability/ha2-cr-ap/#overview","text":"Audience: Architects, Application developers, Administrators In this topic, we're going to: learn the essential elements of what provides NativeHA high-availability to the latest versions of IBM MQ understand the impact at the infrastructure, services, and application tiers which a multi-region, high-availability requirement brings discuss cross-region deployment characteristics and needs deploy this topology to your OpenShift clusters","title":"Overview"},{"location":"guides/cp4i/mq/high-availability/ha2-cr-ap/#architecture-overview","text":"NOTE: We have to discuss applications on the cluster talking to the GLB endpoint versus having local reference the QM URL The following components comprise the high-availability topology at a high level. As your desired topology evolves to reduce single points of failure, this list will become more robust in nature. Virtual Private Cloud: Distribute workloads across a minimum of three (3) sets of infrastructure service resources in 3 distinct availability zones (AZ). Red Hat OpenShift Container Platform: Minimum two (2) OpenShift Compute Nodes per AZ to achieve standard OpenShift SLOs (99.99% possible). IBM MQ - Native HA Queue Manager: Minimum one (1) OpenShift Node in each AZ for normal mode operations. Minimum two (2) OpenShift Nodes in each AZ during OpenShift Node maintenance. MQ Client Application: Stateless container-based client application Leverage anti-affinity deployment practices where possible Cross-Region Global Load Balancer TBD Discuss failover strategy (automated, manual, somewhere in between)","title":"Architecture Overview"},{"location":"guides/cp4i/mq/high-availability/ha2-cr-ap/#single-points-of-failure","text":"These services and infrastructure components have been identified as potential single points of failure in this topology: Red Hat OpenShift Container Platform In-Region Networking Ingress/Egress Networking In-Region Storage","title":"Single Points of Failure"},{"location":"guides/cp4i/mq/high-availability/ha2-cr-ap/#infrastructure-requirements","text":"This topology evolves this simplest high-availability scenario while adding minimal overhead to provide a nominal improvement in the overall availability. Block Storage with ReadWriteOnce support Multiple availability zones inside of a single region Multiple regions inside the same cloud provider A Cross-Region Global Load Balancer (ALB, NLB, or DNS still TBD) to handle traffic routing and failover","title":"Infrastructure Requirements"},{"location":"guides/cp4i/mq/high-availability/ha2-cr-ap/#mq-assets","text":"All of the high-availability topologies in this deployment guide leverage the NativeHA feature of IBM MQ for resiliency. As the topologies grow more complex, additional configurations and deployments are added to this least-common architectural denominator. The recently released NativeHA capability allows for multiple QueueManager pods to be running, with only one of them truly being \"active\" and receiving traffic from the OpenShift Route or Service. The running pods, managed by a StatefulSet on the underlying Kubernetes-based platform, actively share log and message data between the active and passive QueueManager instances. Once the active QueueManager instance becomes unavailable for any reason, the remaining passive pods take advantage of the RAFT algorithm to elect a new leader and within seconds failover to a different QueueManager instance, while minimizing service and message unavailability. For more details on the early release of NativeHA for IBM MQ 9.2, you can check out the official IBM MQ Docs . Our example here sets the following properties in the QueueManager YAML file and pushes the changes to your GitOps repo: availability : type : NativeHA tls : secretName : <inter-queuemanager-tls-cert-secret> cipherSpec : ANY_TLS12_OR_HIGHER storage : defaultClass : ibmc-vpc-block-metro-10iops-tier queueManager : enabled : true size : 2Gi type : persistent-claim The availability.type setting is required to be NativeHA to take advantage of this capability. The availability.tls setting allows for TLS-encrypted communication between QueueManager instances in the StatefulSet. The storage.defaultClass setting should point to a multi-AZ supported storage class the provides ReadWriteOnce storage capability for Block Storage. The storage.queueManager setting is required to be enabled and set to type persistent-claim for NativeHA fail-over capability to function.","title":"MQ Assets"},{"location":"guides/cp4i/mq/high-availability/ha2-cr-ap/#client-application-recommendations","text":"Our sample client application is available under the Building MQ applications section. There are minimal additional configuration requirements to deploy the client application connected to this topology. TBD Client application configuration details To be addressed: Automatic client reconnection Connects to a single URL (the global load balancer), which provides active-passive failover to the OpenShift Routes provided by the MQ QueueManager instances As we only have a single region deployed, there is no consideration given for message sequencing needs beyond traditional single QueueManager requirements. TBD","title":"Client Application Recommendations"},{"location":"guides/cp4i/mq/high-availability/ha3-ir-aa/","text":"MQ High Availability Architectures: In-Region Active Active \u00b6 Warning Work in Progress Overview \u00b6 Audience: Architects, Application developers, Administrators In this topic, we're going to: review the essential elements of what provides NativeHA high-availability to the latest versions of IBM MQ understand the impact at the infrastructure, services, and application tiers which a multi-instance, high-availability requirement brings discuss in-region, multi-instance deployment characteristics and needs deploy this topology to your OpenShift cluster Architecture Overview \u00b6 The following components comprise the high-availability topology at a high level. As your desired topology evolves to reduce single points of failure, this list will become more robust in nature. Virtual Private Cloud: Distribute workloads across a minimum of three (3) sets of infrastructure service resources in 3 distinct availability zones (AZ). Red Hat OpenShift Container Platform: Minimum two (2) OpenShift Compute Nodes per AZ to achieve standard OpenShift SLOs (99.99% possible). IBM MQ - Native HA Queue Manager: Requires a minimum of one (1) OpenShift Node in each AZ for normal mode operations. Requires a minimum of two (2) OpenShift Nodes in each AZ during OpenShift Node maintenance. Requires a Uniform Cluster with at least two unique Queue Manager instances as members. MQ Client Application: Stateless container-based client application. Leverage anti-affinity deployment practices, where possible. Utilizing CCDT (client channel definition table) connection patterns. Single Points of Failure \u00b6 These services and infrastructure components have been identified as potential single points of failure in this topology: Red Hat OpenShift Container Platform Regional Cloud Networking Regional Cloud Storage Regional Cloud Ingress/Egress The Queue Manager has been removed from this list (first established in the In-Region Active Passive - Single Points of Failure section), as there are now multiple Queue Manager instances deployed in unison as a Uniform Cluster. Infrastructure Requirements \u00b6 This topology, as it extends the simplest topology which still provides high-availability, it also requires the least amount of bespoke support from the underlying infrastructure. Block Storage with ReadWriteOnce support Multiple availability zones inside of a single region MQ Assets \u00b6 Warning TODO Uniform clusters (mqsc/ini) All of the high-availability topologies in this deployment guide leverage the NativeHA feature of IBM MQ for resiliency. As the topologies grow more complex, additional configurations and deployments are added to this least-common architectural denominator. The recently released NativeHA capability allows for multiple QueueManager pods to be running, with only one of them truly being \"active\" and receiving traffic from the OpenShift Route or Service. The running pods, managed by a StatefulSet on the underlying Kubernetes-based platform, actively share log and message data between the active and passive QueueManager instances. Once the active QueueManager instance becomes unavailable for any reason, the remaining passive pods take advantage of the RAFT algorithm to elect a new leader and within seconds failover to a different QueueManager instance, while minimizing service and message unavailability. For more details on the early release of NativeHA for IBM MQ 9.2, you can check out the official IBM MQ Docs . Our example here sets the following properties in the QueueManager YAML file and pushes the changes to your GitOps repo: availability : type : NativeHA tls : secretName : <inter-queuemanager-tls-cert-secret> cipherSpec : ANY_TLS12_OR_HIGHER storage : defaultClass : ibmc-vpc-block-metro-10iops-tier queueManager : enabled : true size : 2Gi type : persistent-claim The availability.type setting is required to be NativeHA to take advantage of this capability. The availability.tls setting allows for TLS-encrypted communication between QueueManager instances in the StatefulSet. The storage.defaultClass setting should point to a multi-AZ supported storage class the provides ReadWriteOnce storage capability for Block Storage. The storage.queueManager setting is required to be enabled and set to type persistent-claim for NativeHA fail-over capability to function. Client Application Recommendations \u00b6 Warning TODO CCDT Our sample client application is available under the Building MQ applications section. There are minimal additional configuration requirements to deploy the client application connected to this topology. To be addressed: Automatic client reconnection Connects to a single Route URL provided by the MQ QueueManager instance As only a single region is deployed, there is no consideration given for message sequencing needs beyond traditional single QueueManager requirements.","title":"MQ High Availability Architectures: In-Region Active Active"},{"location":"guides/cp4i/mq/high-availability/ha3-ir-aa/#mq-high-availability-architectures-in-region-active-active","text":"Warning Work in Progress","title":"MQ High Availability Architectures: In-Region Active Active"},{"location":"guides/cp4i/mq/high-availability/ha3-ir-aa/#overview","text":"Audience: Architects, Application developers, Administrators In this topic, we're going to: review the essential elements of what provides NativeHA high-availability to the latest versions of IBM MQ understand the impact at the infrastructure, services, and application tiers which a multi-instance, high-availability requirement brings discuss in-region, multi-instance deployment characteristics and needs deploy this topology to your OpenShift cluster","title":"Overview"},{"location":"guides/cp4i/mq/high-availability/ha3-ir-aa/#architecture-overview","text":"The following components comprise the high-availability topology at a high level. As your desired topology evolves to reduce single points of failure, this list will become more robust in nature. Virtual Private Cloud: Distribute workloads across a minimum of three (3) sets of infrastructure service resources in 3 distinct availability zones (AZ). Red Hat OpenShift Container Platform: Minimum two (2) OpenShift Compute Nodes per AZ to achieve standard OpenShift SLOs (99.99% possible). IBM MQ - Native HA Queue Manager: Requires a minimum of one (1) OpenShift Node in each AZ for normal mode operations. Requires a minimum of two (2) OpenShift Nodes in each AZ during OpenShift Node maintenance. Requires a Uniform Cluster with at least two unique Queue Manager instances as members. MQ Client Application: Stateless container-based client application. Leverage anti-affinity deployment practices, where possible. Utilizing CCDT (client channel definition table) connection patterns.","title":"Architecture Overview"},{"location":"guides/cp4i/mq/high-availability/ha3-ir-aa/#single-points-of-failure","text":"These services and infrastructure components have been identified as potential single points of failure in this topology: Red Hat OpenShift Container Platform Regional Cloud Networking Regional Cloud Storage Regional Cloud Ingress/Egress The Queue Manager has been removed from this list (first established in the In-Region Active Passive - Single Points of Failure section), as there are now multiple Queue Manager instances deployed in unison as a Uniform Cluster.","title":"Single Points of Failure"},{"location":"guides/cp4i/mq/high-availability/ha3-ir-aa/#infrastructure-requirements","text":"This topology, as it extends the simplest topology which still provides high-availability, it also requires the least amount of bespoke support from the underlying infrastructure. Block Storage with ReadWriteOnce support Multiple availability zones inside of a single region","title":"Infrastructure Requirements"},{"location":"guides/cp4i/mq/high-availability/ha3-ir-aa/#mq-assets","text":"Warning TODO Uniform clusters (mqsc/ini) All of the high-availability topologies in this deployment guide leverage the NativeHA feature of IBM MQ for resiliency. As the topologies grow more complex, additional configurations and deployments are added to this least-common architectural denominator. The recently released NativeHA capability allows for multiple QueueManager pods to be running, with only one of them truly being \"active\" and receiving traffic from the OpenShift Route or Service. The running pods, managed by a StatefulSet on the underlying Kubernetes-based platform, actively share log and message data between the active and passive QueueManager instances. Once the active QueueManager instance becomes unavailable for any reason, the remaining passive pods take advantage of the RAFT algorithm to elect a new leader and within seconds failover to a different QueueManager instance, while minimizing service and message unavailability. For more details on the early release of NativeHA for IBM MQ 9.2, you can check out the official IBM MQ Docs . Our example here sets the following properties in the QueueManager YAML file and pushes the changes to your GitOps repo: availability : type : NativeHA tls : secretName : <inter-queuemanager-tls-cert-secret> cipherSpec : ANY_TLS12_OR_HIGHER storage : defaultClass : ibmc-vpc-block-metro-10iops-tier queueManager : enabled : true size : 2Gi type : persistent-claim The availability.type setting is required to be NativeHA to take advantage of this capability. The availability.tls setting allows for TLS-encrypted communication between QueueManager instances in the StatefulSet. The storage.defaultClass setting should point to a multi-AZ supported storage class the provides ReadWriteOnce storage capability for Block Storage. The storage.queueManager setting is required to be enabled and set to type persistent-claim for NativeHA fail-over capability to function.","title":"MQ Assets"},{"location":"guides/cp4i/mq/high-availability/ha3-ir-aa/#client-application-recommendations","text":"Warning TODO CCDT Our sample client application is available under the Building MQ applications section. There are minimal additional configuration requirements to deploy the client application connected to this topology. To be addressed: Automatic client reconnection Connects to a single Route URL provided by the MQ QueueManager instance As only a single region is deployed, there is no consideration given for message sequencing needs beyond traditional single QueueManager requirements.","title":"Client Application Recommendations"},{"location":"guides/cp4i/mq/high-availability/overview/","text":"MQ High Availability Architectures \u00b6 Overview \u00b6 Per the IBM Cloud Architecture Center , the terms high availability and disaster recovery are often used interchangeably. However, they are two distinct concepts: High availability (HA) describes the ability of an application to withstand all planned and unplanned outages (a planned outage could be performing a system upgrade) and to provide continuous processing for business-critical applications. Disaster recovery (DR) involves a set of policies, tools, and procedures for returning a system, an application, or an entire data center to full operation after a catastrophic interruption. It includes procedures for copying and storing an installed system's essential data in a secure location, and for recovering that data to restore normalcy of operation. High availability is about avoiding single points of failure and ensuring that the application will continue to process requests. Disaster recovery is about policies and procedures for restoring a system or application to its normal operating condition after the system or application suffered a catastrophic failure or loss of availability of the entire data center. For MQ, the high-availability concept can be broken down into: Service availability: Ensure that from the application point of view, it is always able to access the service and put or get a message from a queue. Note that the logical queue can be supported by a single logical queue manager or by a cluster of queue managers. Message availability: While the MQ service might be available, it is possible that messages be marooned on a Queue Manager that is offline. So while the MQ service might still be available, it is possible for a subset of messages to be unaccessible while the Queue Managers are recovering. These perspective are useful to understand the impact of an outage and plan the topology accordingly. Key Concepts \u00b6 IBM MQ, being a stateful service, requires careful planning to ensure that the selected topologies actually provide the correct service and message availability. Here we will discuss the technical specifications and single points of failure for each architecture, how they are addressed, and how each architecture solves the HA service level objectives for a broad set of use cases. Generally, high-availability for MQ is achieved through: Service resiliency Service redundancy Client connectivity resiliency Service Resiliency \u00b6 Key points: Single \u201clogical\u201d Queue Manager OpenShift Route direct traffic to active Queue Manager Availability of MQ service ensured by platform Service recovered on different worker node or zone but same region All messages unavailable during recovery Service Redundancy \u00b6 Key points: Two or more \u201clogical\u201d Queue Manager Sharing similar configuration OpenShift Route per Queue Manager Availability of MQ service ensured by MQ cluster and platform Service remains available during recovery Some messages unavailable during recovery Client connectivity resiliency \u00b6 Key points: Decouple application from MQ topology knowledge CCDT can be pulled remotely through URL Load balancing only viable when Queue Managers are \u201cidentical\u201d Architecture Overview \u00b6 In-Region Active-Passive \u00b6 Cross-Region Active-Passive \u00b6 In-Region Active-Active \u00b6 Cross-Region Active-Active \u00b6 Architecture Comparison \u00b6 Table of comparison \u00b6 IMPORTANT TO NOTE: Values are relative to each other, not scalar values. HA1: In-Region Active-Passive HA2: Cross-Region Active-Passive HA3: In-Region Active-Active HA4: Cross-Region Active-Active COST $ $$$$ $$ $$$$ RPO Medium Medium/Low(*) Low Low RTO Localized issue Low Medium Near-Zero Near-Zero RTO Regional issue High Medium(**) High Near-Zero SPOFs Client connectivity Queue Manager OCP Cluster Regional Cloud Client connectivity Queue Manager Client connectivity OCP Cluster Regional Cloud Client connectivity MTTR (lower is better) Localized issue Low Medium(**) Near-Zero Near-Zero MTTR (lower is better) Regional issue High Medium(**) High Near-Zero MTBF (higher is better)(***) Low Medium Medium High (*) Largely dependent on the continuous data replication and the lag that can exists between the two sites (**) As the data replication is happening at the infrastructure level, there is a requirement for an additional mechanism to cause the service to be activated on the passive region. (***) This is defined as the complete system \"failure\" for MTBF - since we are adding more components to the system, expecting them to fail, but leveraging redundancy, etc. Definitions \u00b6 Recovery Point Objective (RPO) \u00b6 Recovery Point Objective (RPO) is the data loss tolerance measured in time (hours, minutes, seconds). In the context of MQ, this represents the average queue depth. In systems where rate of get is always superior to rate of put, this is not an issue. This needs to take into account peak times and for periods where puts are higher than the get - the delta between the two rates can be used to calculate the number of \"marooned\" messages. Note: Transmit queues need to be considered in this calculation. Maintaining low RPO is largely dependent on: Time required for client to detect failed a failure Balancing workload across multiple instances to reduce average queue depth per instance Recovery Time Objective (RTO) \u00b6 Recovery Time Objective (RTO) is the amount of time a system will need to recover from a component failure. In the context of MQ, recovery time will be influenced by: Service redundancy - having more than one logical queue manager active MQ instance recovery time - Time required for a passive instance of a Queue Manager to take over Time required for client to detect failure and reconnect, either to the same logical queue manager a different queue manager Single point of failure (SPOF) \u00b6 Single points of failure (SPOF) are components that are subject to take a system down should they stop working Mean-time-to-recovery (MTTR) \u00b6 Mean-time-to-recovery (MTTR) is the average time required for the system or a component to recover from a failure. In the context of a Queue Manager, this means having a passive Queue Manager detect a failure of the active one, initiate the takeover and become active. From a client point of view, it means the time required to re-establish connectivity to \"a\" queue manager (could be the same logical queue manager or a different one). Mean-time-between-failure (MTBF) \u00b6 Mean-time-between-failure (MTBF) is the average time between failures. This is calculated from a system's perspective and it is understood that resiliency and redundancy within a system can contribute to increased time between failures. So from an MQ perspective, it is useful to consider MTBF from a client point of view. A client that loses connection but is able to immediately reconnect may not perceive the failure of a component (ie the Queue Manager) .","title":"High Availability"},{"location":"guides/cp4i/mq/high-availability/overview/#mq-high-availability-architectures","text":"","title":"MQ High Availability Architectures"},{"location":"guides/cp4i/mq/high-availability/overview/#overview","text":"Per the IBM Cloud Architecture Center , the terms high availability and disaster recovery are often used interchangeably. However, they are two distinct concepts: High availability (HA) describes the ability of an application to withstand all planned and unplanned outages (a planned outage could be performing a system upgrade) and to provide continuous processing for business-critical applications. Disaster recovery (DR) involves a set of policies, tools, and procedures for returning a system, an application, or an entire data center to full operation after a catastrophic interruption. It includes procedures for copying and storing an installed system's essential data in a secure location, and for recovering that data to restore normalcy of operation. High availability is about avoiding single points of failure and ensuring that the application will continue to process requests. Disaster recovery is about policies and procedures for restoring a system or application to its normal operating condition after the system or application suffered a catastrophic failure or loss of availability of the entire data center. For MQ, the high-availability concept can be broken down into: Service availability: Ensure that from the application point of view, it is always able to access the service and put or get a message from a queue. Note that the logical queue can be supported by a single logical queue manager or by a cluster of queue managers. Message availability: While the MQ service might be available, it is possible that messages be marooned on a Queue Manager that is offline. So while the MQ service might still be available, it is possible for a subset of messages to be unaccessible while the Queue Managers are recovering. These perspective are useful to understand the impact of an outage and plan the topology accordingly.","title":"Overview"},{"location":"guides/cp4i/mq/high-availability/overview/#key-concepts","text":"IBM MQ, being a stateful service, requires careful planning to ensure that the selected topologies actually provide the correct service and message availability. Here we will discuss the technical specifications and single points of failure for each architecture, how they are addressed, and how each architecture solves the HA service level objectives for a broad set of use cases. Generally, high-availability for MQ is achieved through: Service resiliency Service redundancy Client connectivity resiliency","title":"Key Concepts"},{"location":"guides/cp4i/mq/high-availability/overview/#service-resiliency","text":"Key points: Single \u201clogical\u201d Queue Manager OpenShift Route direct traffic to active Queue Manager Availability of MQ service ensured by platform Service recovered on different worker node or zone but same region All messages unavailable during recovery","title":"Service Resiliency"},{"location":"guides/cp4i/mq/high-availability/overview/#service-redundancy","text":"Key points: Two or more \u201clogical\u201d Queue Manager Sharing similar configuration OpenShift Route per Queue Manager Availability of MQ service ensured by MQ cluster and platform Service remains available during recovery Some messages unavailable during recovery","title":"Service Redundancy"},{"location":"guides/cp4i/mq/high-availability/overview/#client-connectivity-resiliency","text":"Key points: Decouple application from MQ topology knowledge CCDT can be pulled remotely through URL Load balancing only viable when Queue Managers are \u201cidentical\u201d","title":"Client connectivity resiliency"},{"location":"guides/cp4i/mq/high-availability/overview/#architecture-overview","text":"","title":"Architecture Overview"},{"location":"guides/cp4i/mq/high-availability/overview/#in-region-active-passive","text":"","title":"In-Region Active-Passive"},{"location":"guides/cp4i/mq/high-availability/overview/#cross-region-active-passive","text":"","title":"Cross-Region Active-Passive"},{"location":"guides/cp4i/mq/high-availability/overview/#in-region-active-active","text":"","title":"In-Region Active-Active"},{"location":"guides/cp4i/mq/high-availability/overview/#cross-region-active-active","text":"","title":"Cross-Region Active-Active"},{"location":"guides/cp4i/mq/high-availability/overview/#architecture-comparison","text":"","title":"Architecture Comparison"},{"location":"guides/cp4i/mq/high-availability/overview/#table-of-comparison","text":"IMPORTANT TO NOTE: Values are relative to each other, not scalar values. HA1: In-Region Active-Passive HA2: Cross-Region Active-Passive HA3: In-Region Active-Active HA4: Cross-Region Active-Active COST $ $$$$ $$ $$$$ RPO Medium Medium/Low(*) Low Low RTO Localized issue Low Medium Near-Zero Near-Zero RTO Regional issue High Medium(**) High Near-Zero SPOFs Client connectivity Queue Manager OCP Cluster Regional Cloud Client connectivity Queue Manager Client connectivity OCP Cluster Regional Cloud Client connectivity MTTR (lower is better) Localized issue Low Medium(**) Near-Zero Near-Zero MTTR (lower is better) Regional issue High Medium(**) High Near-Zero MTBF (higher is better)(***) Low Medium Medium High (*) Largely dependent on the continuous data replication and the lag that can exists between the two sites (**) As the data replication is happening at the infrastructure level, there is a requirement for an additional mechanism to cause the service to be activated on the passive region. (***) This is defined as the complete system \"failure\" for MTBF - since we are adding more components to the system, expecting them to fail, but leveraging redundancy, etc.","title":"Table of comparison"},{"location":"guides/cp4i/mq/high-availability/overview/#definitions","text":"","title":"Definitions"},{"location":"guides/cp4i/mq/monitoring/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/mq/monitoring/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/mq/overview/architecture/","text":"Target architecture \u00b6 Audience : Architects, Application developers, Administrators Overview \u00b6 In this topic we're going to: Examine a system context diagram for an MQ deployment Identify the components used to build a cloud native deployment Describe the role of each component Explore a typical architecture overview diagram that includes these components Describe the GitOps model for MQ applications and queue managers By the end of this topic you'll understand the architectural design of a typical cloud native MQ system, its primary components and their function. System context \u00b6 A system context diagram helps us understand how our system interacts with its different users and other systems. We can see the different entities that interact with a typical MQ deployment. These include users as well as MQ applications and MQ-enabled systems. We'll be developing the MQ deployment at the centre of the diagram. We can see that it connects MQ applications and MQ enabled systems. Its users are developers, MQ admins, SREs, Kubernetes administrators, architects and business analysts. We'll learn more about these users later in the tutorial. Component diagram \u00b6 The following diagram shows the technical components used in a typical MQ production deployment. You'll notice that there are many different components. Don't worry -- the tutorial will introduce them gradually as they are required. The diagram organizes the components according to when they are introduced in system development (earlier or later) and whether they are a relatively high level application-oriented component, or a relatively low level system- oriented component. For example, GitHub is a system component that is fundamental to how we structure a cloud native MQ deployment. In contrast, an MQ application is a high level component, and requires other components to be deployed prior to it. We'll start the tutorial with with the basic components required to get a working queue manager and application and running. We'll then introduce components that support more advanced use cases. Your knowledge of these components will grow over time as you complete the tutorials. Let's briefly introduce these components below to get a flavor of what they do. MQ application \u00b6 These applications are the focus of most developer activity. They use MQ APIs such as the MQI, JMS or REST to communicate with other applications connected to an MQ network. These applications provide APIs to the mobile and web applications that deliver user interfaces to business users. MQ-enabled system \u00b6 A system such as Salesforce, SAP or CICS that can interact with an MQ network is often described as MQ-enabled . Systems of Record like these either support MQ natively or have MQ adapters, allowing them to interact with applications that wish to consume their services using an MQ API. In a typical MQ network, MQ applications communicate with each other and MQ enabled systems by exchanging messages. These messages represent a request for work to be done, or a reply confirming that work has completed. Queue manager \u00b6 A queue manager provides queueing services via one of the many MQ APIs. A queue manager hosts the queues that store the messages produced and consumed by connected applications and systems. Queue managers can be connected together via network channels to allow messages to flow between disparate systems and applications on different platforms including on-premise and cloud systems. ArgoCD \u00b6 ArgoCD is used for the continuous deployment of software components to the Kubernetes cluster. ArgoCD watches a Git repository for new or changed Kubernetes resource definitions, and applies them to a cluster. In this way, ArgoCD ensures that the component configuration stored in GitHub always reflects the state of the cluster. For example, we will use ArgoCD to deploy and maintain queue managers, MQ applications and the other cloud native components in our architecture. ArgoCD also has the added benefit of being able to monitor resources that it has deployed to ensure that if they drift from their desired values, they will be automatically restored to those values by ArgoCD. Tekton \u00b6 Tekton is used to automate manual tasks using the concept of a pipeline. A pipeline comprises a set of tasks that are executed in a specified order in order to accomplish a specific objective. We use pipelines as part of the continuous integration process to build, test and deliver queue managers and MQ applications ready for deployment by ArgoCD. We also use pipelines to run performance tests, and to promote queue managers and applications from dev to stage and production environments. Open LDAP \u00b6 Most existing MQ on-premise deployments use LDAP for access control. Our architecture provides an OpenLDAP to help migration of existing MQ estates. While LDAP is an excellent technology, most cloud native systems have started to use certificates for authentication, identification and authorization. Indeed, MQ has added the ability to use the identify from certificate for access control. This removes the need for LDAP, and removes a single point of failure. Many customers will want to exploit the Cert manager components to help move to certificate based authorization. Kibana \u00b6 Kibana is a tool that helps users visualize and search component trace logs (not to be confused with MQ transaction logs). As we know, IBM MQ generates a rich set of trace log information, and Kibana can be used to visualize and analyze these data. It does this by working with FluentD as the log collector and Elastic search as the log store, in the EFK stack provided as a built in part of OpenShift. Grafana \u00b6 Grafana is a metrics visualization tool. Whereas Kibana views the MQ logs, Grafana is used to view the many different kinds of metrics that MQ collects such as queue depth or how long a messages stays on a queue. It works in conjunction with Prometheus that collects the different metrics from the queue manager, which are then visualized using Grafana views. JMeter \u00b6 JMeter is a performance load testing measurement tool. It is typically used with Web applications, but also has support for JMS, alongside an extensive range of plugins. It can be used to performance test both MQ applications and queue managers. Sealed secrets \u00b6 Very often a component has a Kubernetes secret associated with it. Inside the secret might be a private key to access the IBM entitled container registry for example. For obvious reasons, we don't want to store the secret in GitHub with the rest of our queue manager configuration. A sealed secret solves this problem by introducing a new kind of Kubernetes resource. A sealed secret is created from a regular secret, and can be safely stored in a Git repository. A deployment time, the sealed secret controller will recreate the secret in its original form so that it can be access by components with the appropriate RBAC authority. Image Registry \u00b6 OpenShift contains a registry for storing container images. These images will be used to create the MQ queue manager containers within the cluster. These images are based on the MQ images shipped by IBM. They are built and stored by Tekton pipelines as part of the CICD process. Tekton pipelines and ArgoCD also retrieve the latest best images from the image registry to ensure that what's being tested or deployed in higher environments is the same as what's tested in development environments. We often refer to uploading images as pushing and downloading images as pulling . Cert manager \u00b6 Managing certificates is a difficult process; certificate creation requires a Certificate Authority (CA), certificates expire after a period of time, and private keys can sometimes be compromised -- requiring a certificate to be revoked and a new one issued. Cert manager makes all these processes relatively straightforward by introducing new Kubernetes resources for certificate issuers and certificates. These resource types radically simplify the management of certificates: their creation, expiry and revocation. Moreover, Cert manager makes it feasible to adopt mutual TLS (mTLS) as an authorization strategy in MQ removing the need for LDAP, and aligning MQ with modern trends in decentralized access control which don't have a single point of failure. Artifactory \u00b6 The successful completion of the build and test phases of a CICD pipeline result in the creation of a newly versioned set of artifacts used to deploy MQ applications, queue managers and related components to a Kubernetes cluster. These might include Helm charts for example that contain a queue manager's configuration together with related config maps. Artifactory provides a repository for these build outputs, where they can be versioned, saved and accessed when required. Note that although a container image is also built by a Tekton pipeline, it is typically held the image registry rather than Artifactory. SonarQube \u00b6 SonarQube is a code quality tool used to perform static analysis of code to detect bugs, code smells, and security vulnerabilities in MQ and other applications. SonarQube is used within the CICD process to generate reports on code quality including duplicate code, coding standards, unit tests, code coverage, code complexity, comments, bugs, and security vulnerabilities. Prometheus \u00b6 Prometheus is used in conjunction with Grafana. It stores MQ metrics retrieved from the queue manager as a set of tuples in a time-series , which allows it to be subsequently used to create Grafana views to assist with monitoring MQ queue managers. Kustomize \u00b6 Kubernetes resources such as queue managers and applications, have their operational properties defined using YAMLs. As these resources move through environments such as dev stage and prod, Kustomize provides a natural way to adapt ( customize !) these YAMLs to these environments. For example, we might want to change the CPU or memory available to a queue manager in a production environment compared to a development environment. Because Kustomize is built into the kubectl and oc commands via the -k option, it makes configuration management both easy and natural. GitHub \u00b6 This popular version control system is based on git and stores the MQ application and queue manager source configuration as well as the other Kubernetes resources. By keeping our MQ application and queue manager source configuration in Git, and using that to build, test and deploy our MQ applications and queue managers to the Kubernetes cluster, we have a single source of truth -- what's in Git is running in the cluster. Moreover, by using Git operations such as pull , push and merge to make changes, we can exploit the extensive governance and change control provided by Git when managing our MQ estate. Kubernetes Cluster \u00b6 This is the \"operating system\" used to orchestrate our MQ application, queue manager and related component containers. Kubernetes is portable across on-premise and cloud systems and allows us to easily scale our workloads across these environments as required. You'll be learning a lot more about Kubernetes in this tutorial; it's the foundation upon which everything else is built. Architecture Overview Diagram \u00b6 Let's look at an example of how these components come together in an example Architecture Overview Diagram (AOD) . This shows a typical MQ deployment where many of the components we've mentioned come together in a typical MQ topology. We'll be building out this deployment throughout the tutorial. It's a really good idea to draw an AOD when you're designing a new MQ deployment on OpenShift. It helps you understand and communicate what the system is going to look like to its users. There are lots of other UML diagrams that complement an AOD; indeed, and AOD is often a composite of UML component, interaction, use case and deployment diagrams, emphasizing particularly relevant aspects of the design. GitOps model \u00b6 In our AOD, we've emphasized the two main components that are essential to a production-ready MQ cloud native deployment: A Kubernetes cluster containing: MQ applications MQ queue managers Cloud native components such as Tekton, ArgoCD, Kibana and Grafana. GitHub as a source of truth for the cluster runtime containing: MQ application source MQ application configuration Queue manager configuration Configuration information for the cloud native components running in the cluster Notice the set of users who interact with these components: Developer Administrator Architect Business analyst MQ application user Site reliability engineer Kubernetes admin In this tutorial, we'll see how these users work within this environment. All users will follow the GitOps model . In this model, Git holds the entire specification for the system -- hardware, cluster, build, test and deploy components, queue managers and applications. Whenever the system needs to be changed, whether by a developer, administrator, SRE or architect, they use Git and git operations such as Pull requests to make a change. A change must pass a full set of tests (stored in git) to ensure the change is correct. If successful, it is merged into the current system either automatically or after approval, if required. In the GitOps model, Git is at the heart of every operational change performed to every component of the system; you'll learn more about this model throughout the tutorial by using it. Congratulations! Now that we know what we're trying to build, we can start building it. In the next topic, we're going to create our cluster.","title":"Target architecture"},{"location":"guides/cp4i/mq/overview/architecture/#target-architecture","text":"Audience : Architects, Application developers, Administrators","title":"Target architecture"},{"location":"guides/cp4i/mq/overview/architecture/#overview","text":"In this topic we're going to: Examine a system context diagram for an MQ deployment Identify the components used to build a cloud native deployment Describe the role of each component Explore a typical architecture overview diagram that includes these components Describe the GitOps model for MQ applications and queue managers By the end of this topic you'll understand the architectural design of a typical cloud native MQ system, its primary components and their function.","title":"Overview"},{"location":"guides/cp4i/mq/overview/architecture/#system-context","text":"A system context diagram helps us understand how our system interacts with its different users and other systems. We can see the different entities that interact with a typical MQ deployment. These include users as well as MQ applications and MQ-enabled systems. We'll be developing the MQ deployment at the centre of the diagram. We can see that it connects MQ applications and MQ enabled systems. Its users are developers, MQ admins, SREs, Kubernetes administrators, architects and business analysts. We'll learn more about these users later in the tutorial.","title":"System context"},{"location":"guides/cp4i/mq/overview/architecture/#component-diagram","text":"The following diagram shows the technical components used in a typical MQ production deployment. You'll notice that there are many different components. Don't worry -- the tutorial will introduce them gradually as they are required. The diagram organizes the components according to when they are introduced in system development (earlier or later) and whether they are a relatively high level application-oriented component, or a relatively low level system- oriented component. For example, GitHub is a system component that is fundamental to how we structure a cloud native MQ deployment. In contrast, an MQ application is a high level component, and requires other components to be deployed prior to it. We'll start the tutorial with with the basic components required to get a working queue manager and application and running. We'll then introduce components that support more advanced use cases. Your knowledge of these components will grow over time as you complete the tutorials. Let's briefly introduce these components below to get a flavor of what they do.","title":"Component diagram"},{"location":"guides/cp4i/mq/overview/architecture/#mq-application","text":"These applications are the focus of most developer activity. They use MQ APIs such as the MQI, JMS or REST to communicate with other applications connected to an MQ network. These applications provide APIs to the mobile and web applications that deliver user interfaces to business users.","title":"MQ application"},{"location":"guides/cp4i/mq/overview/architecture/#mq-enabled-system","text":"A system such as Salesforce, SAP or CICS that can interact with an MQ network is often described as MQ-enabled . Systems of Record like these either support MQ natively or have MQ adapters, allowing them to interact with applications that wish to consume their services using an MQ API. In a typical MQ network, MQ applications communicate with each other and MQ enabled systems by exchanging messages. These messages represent a request for work to be done, or a reply confirming that work has completed.","title":"MQ-enabled system"},{"location":"guides/cp4i/mq/overview/architecture/#queue-manager","text":"A queue manager provides queueing services via one of the many MQ APIs. A queue manager hosts the queues that store the messages produced and consumed by connected applications and systems. Queue managers can be connected together via network channels to allow messages to flow between disparate systems and applications on different platforms including on-premise and cloud systems.","title":"Queue manager"},{"location":"guides/cp4i/mq/overview/architecture/#argocd","text":"ArgoCD is used for the continuous deployment of software components to the Kubernetes cluster. ArgoCD watches a Git repository for new or changed Kubernetes resource definitions, and applies them to a cluster. In this way, ArgoCD ensures that the component configuration stored in GitHub always reflects the state of the cluster. For example, we will use ArgoCD to deploy and maintain queue managers, MQ applications and the other cloud native components in our architecture. ArgoCD also has the added benefit of being able to monitor resources that it has deployed to ensure that if they drift from their desired values, they will be automatically restored to those values by ArgoCD.","title":"ArgoCD"},{"location":"guides/cp4i/mq/overview/architecture/#tekton","text":"Tekton is used to automate manual tasks using the concept of a pipeline. A pipeline comprises a set of tasks that are executed in a specified order in order to accomplish a specific objective. We use pipelines as part of the continuous integration process to build, test and deliver queue managers and MQ applications ready for deployment by ArgoCD. We also use pipelines to run performance tests, and to promote queue managers and applications from dev to stage and production environments.","title":"Tekton"},{"location":"guides/cp4i/mq/overview/architecture/#open-ldap","text":"Most existing MQ on-premise deployments use LDAP for access control. Our architecture provides an OpenLDAP to help migration of existing MQ estates. While LDAP is an excellent technology, most cloud native systems have started to use certificates for authentication, identification and authorization. Indeed, MQ has added the ability to use the identify from certificate for access control. This removes the need for LDAP, and removes a single point of failure. Many customers will want to exploit the Cert manager components to help move to certificate based authorization.","title":"Open LDAP"},{"location":"guides/cp4i/mq/overview/architecture/#kibana","text":"Kibana is a tool that helps users visualize and search component trace logs (not to be confused with MQ transaction logs). As we know, IBM MQ generates a rich set of trace log information, and Kibana can be used to visualize and analyze these data. It does this by working with FluentD as the log collector and Elastic search as the log store, in the EFK stack provided as a built in part of OpenShift.","title":"Kibana"},{"location":"guides/cp4i/mq/overview/architecture/#grafana","text":"Grafana is a metrics visualization tool. Whereas Kibana views the MQ logs, Grafana is used to view the many different kinds of metrics that MQ collects such as queue depth or how long a messages stays on a queue. It works in conjunction with Prometheus that collects the different metrics from the queue manager, which are then visualized using Grafana views.","title":"Grafana"},{"location":"guides/cp4i/mq/overview/architecture/#jmeter","text":"JMeter is a performance load testing measurement tool. It is typically used with Web applications, but also has support for JMS, alongside an extensive range of plugins. It can be used to performance test both MQ applications and queue managers.","title":"JMeter"},{"location":"guides/cp4i/mq/overview/architecture/#sealed-secrets","text":"Very often a component has a Kubernetes secret associated with it. Inside the secret might be a private key to access the IBM entitled container registry for example. For obvious reasons, we don't want to store the secret in GitHub with the rest of our queue manager configuration. A sealed secret solves this problem by introducing a new kind of Kubernetes resource. A sealed secret is created from a regular secret, and can be safely stored in a Git repository. A deployment time, the sealed secret controller will recreate the secret in its original form so that it can be access by components with the appropriate RBAC authority.","title":"Sealed secrets"},{"location":"guides/cp4i/mq/overview/architecture/#image-registry","text":"OpenShift contains a registry for storing container images. These images will be used to create the MQ queue manager containers within the cluster. These images are based on the MQ images shipped by IBM. They are built and stored by Tekton pipelines as part of the CICD process. Tekton pipelines and ArgoCD also retrieve the latest best images from the image registry to ensure that what's being tested or deployed in higher environments is the same as what's tested in development environments. We often refer to uploading images as pushing and downloading images as pulling .","title":"Image Registry"},{"location":"guides/cp4i/mq/overview/architecture/#cert-manager","text":"Managing certificates is a difficult process; certificate creation requires a Certificate Authority (CA), certificates expire after a period of time, and private keys can sometimes be compromised -- requiring a certificate to be revoked and a new one issued. Cert manager makes all these processes relatively straightforward by introducing new Kubernetes resources for certificate issuers and certificates. These resource types radically simplify the management of certificates: their creation, expiry and revocation. Moreover, Cert manager makes it feasible to adopt mutual TLS (mTLS) as an authorization strategy in MQ removing the need for LDAP, and aligning MQ with modern trends in decentralized access control which don't have a single point of failure.","title":"Cert manager"},{"location":"guides/cp4i/mq/overview/architecture/#artifactory","text":"The successful completion of the build and test phases of a CICD pipeline result in the creation of a newly versioned set of artifacts used to deploy MQ applications, queue managers and related components to a Kubernetes cluster. These might include Helm charts for example that contain a queue manager's configuration together with related config maps. Artifactory provides a repository for these build outputs, where they can be versioned, saved and accessed when required. Note that although a container image is also built by a Tekton pipeline, it is typically held the image registry rather than Artifactory.","title":"Artifactory"},{"location":"guides/cp4i/mq/overview/architecture/#sonarqube","text":"SonarQube is a code quality tool used to perform static analysis of code to detect bugs, code smells, and security vulnerabilities in MQ and other applications. SonarQube is used within the CICD process to generate reports on code quality including duplicate code, coding standards, unit tests, code coverage, code complexity, comments, bugs, and security vulnerabilities.","title":"SonarQube"},{"location":"guides/cp4i/mq/overview/architecture/#prometheus","text":"Prometheus is used in conjunction with Grafana. It stores MQ metrics retrieved from the queue manager as a set of tuples in a time-series , which allows it to be subsequently used to create Grafana views to assist with monitoring MQ queue managers.","title":"Prometheus"},{"location":"guides/cp4i/mq/overview/architecture/#kustomize","text":"Kubernetes resources such as queue managers and applications, have their operational properties defined using YAMLs. As these resources move through environments such as dev stage and prod, Kustomize provides a natural way to adapt ( customize !) these YAMLs to these environments. For example, we might want to change the CPU or memory available to a queue manager in a production environment compared to a development environment. Because Kustomize is built into the kubectl and oc commands via the -k option, it makes configuration management both easy and natural.","title":"Kustomize"},{"location":"guides/cp4i/mq/overview/architecture/#github","text":"This popular version control system is based on git and stores the MQ application and queue manager source configuration as well as the other Kubernetes resources. By keeping our MQ application and queue manager source configuration in Git, and using that to build, test and deploy our MQ applications and queue managers to the Kubernetes cluster, we have a single source of truth -- what's in Git is running in the cluster. Moreover, by using Git operations such as pull , push and merge to make changes, we can exploit the extensive governance and change control provided by Git when managing our MQ estate.","title":"GitHub"},{"location":"guides/cp4i/mq/overview/architecture/#kubernetes-cluster","text":"This is the \"operating system\" used to orchestrate our MQ application, queue manager and related component containers. Kubernetes is portable across on-premise and cloud systems and allows us to easily scale our workloads across these environments as required. You'll be learning a lot more about Kubernetes in this tutorial; it's the foundation upon which everything else is built.","title":"Kubernetes Cluster"},{"location":"guides/cp4i/mq/overview/architecture/#architecture-overview-diagram","text":"Let's look at an example of how these components come together in an example Architecture Overview Diagram (AOD) . This shows a typical MQ deployment where many of the components we've mentioned come together in a typical MQ topology. We'll be building out this deployment throughout the tutorial. It's a really good idea to draw an AOD when you're designing a new MQ deployment on OpenShift. It helps you understand and communicate what the system is going to look like to its users. There are lots of other UML diagrams that complement an AOD; indeed, and AOD is often a composite of UML component, interaction, use case and deployment diagrams, emphasizing particularly relevant aspects of the design.","title":"Architecture Overview Diagram"},{"location":"guides/cp4i/mq/overview/architecture/#gitops-model","text":"In our AOD, we've emphasized the two main components that are essential to a production-ready MQ cloud native deployment: A Kubernetes cluster containing: MQ applications MQ queue managers Cloud native components such as Tekton, ArgoCD, Kibana and Grafana. GitHub as a source of truth for the cluster runtime containing: MQ application source MQ application configuration Queue manager configuration Configuration information for the cloud native components running in the cluster Notice the set of users who interact with these components: Developer Administrator Architect Business analyst MQ application user Site reliability engineer Kubernetes admin In this tutorial, we'll see how these users work within this environment. All users will follow the GitOps model . In this model, Git holds the entire specification for the system -- hardware, cluster, build, test and deploy components, queue managers and applications. Whenever the system needs to be changed, whether by a developer, administrator, SRE or architect, they use Git and git operations such as Pull requests to make a change. A change must pass a full set of tests (stored in git) to ensure the change is correct. If successful, it is merged into the current system either automatically or after approval, if required. In the GitOps model, Git is at the heart of every operational change performed to every component of the system; you'll learn more about this model throughout the tutorial by using it. Congratulations! Now that we know what we're trying to build, we can start building it. In the next topic, we're going to create our cluster.","title":"GitOps model"},{"location":"guides/cp4i/mq/overview/overview/","text":"Cloud Native MQ \u00b6 Overview \u00b6 Audience : Architects, Application developers, Administrators In this introduction to Cloud Native MQ, we're going to: Examine a system context diagram for an MQ deployment Identify the different categories of concern for production operations Understand how the IBM CloudPak for Integration is used to build production deployments Highlight the benefits of a cloud native approach to MQ deployment System context \u00b6 Examine the following system context diagram: We can see the different entities that interact with a typical MQ deployment. These include physical users as well as applications and systems. This guide is divided into chapters that help you build, step-by-step, a working MQ deployment, based on a cloud native approach. The guide addresses all the major aspects and best practices for: Installation Deployment Configuration management Operational monitoring High Availability Security Disaster recovery Performance The guide makes extensive use of the IBM Cloud Pak for Integration (CP4I) and other cloud native technologies. We'll use the the containers, operators, microservices, immutable infrastructure and declarative APIs provided by the CloudPak to to create a best-practice, production-ready MQ deployment. Alongside the CloudPak, we'll also learn how CNCF technologies such as Kubernetes, Operators, Tekton, ArgoCD, or Prometheus integrate with the CloudPak in a production environment. Cloud native systems like MQ are loosely coupled, making them resilient, manageable, and observable. A cloud native approach also brings robust DevOps automation which allows MQ developers, administrators and architects to make high quality changes with predictability and minimal toil as frequently as their organization requires. Now that we've identified the entities that interact with a cloud native MQ deployment, let's examine the architecture of an MQ deployment in more detail.","title":"Cloud Native MQ"},{"location":"guides/cp4i/mq/overview/overview/#cloud-native-mq","text":"","title":"Cloud Native MQ"},{"location":"guides/cp4i/mq/overview/overview/#overview","text":"Audience : Architects, Application developers, Administrators In this introduction to Cloud Native MQ, we're going to: Examine a system context diagram for an MQ deployment Identify the different categories of concern for production operations Understand how the IBM CloudPak for Integration is used to build production deployments Highlight the benefits of a cloud native approach to MQ deployment","title":"Overview"},{"location":"guides/cp4i/mq/overview/overview/#system-context","text":"Examine the following system context diagram: We can see the different entities that interact with a typical MQ deployment. These include physical users as well as applications and systems. This guide is divided into chapters that help you build, step-by-step, a working MQ deployment, based on a cloud native approach. The guide addresses all the major aspects and best practices for: Installation Deployment Configuration management Operational monitoring High Availability Security Disaster recovery Performance The guide makes extensive use of the IBM Cloud Pak for Integration (CP4I) and other cloud native technologies. We'll use the the containers, operators, microservices, immutable infrastructure and declarative APIs provided by the CloudPak to to create a best-practice, production-ready MQ deployment. Alongside the CloudPak, we'll also learn how CNCF technologies such as Kubernetes, Operators, Tekton, ArgoCD, or Prometheus integrate with the CloudPak in a production environment. Cloud native systems like MQ are loosely coupled, making them resilient, manageable, and observable. A cloud native approach also brings robust DevOps automation which allows MQ developers, administrators and architects to make high quality changes with predictability and minimal toil as frequently as their organization requires. Now that we've identified the entities that interact with a cloud native MQ deployment, let's examine the architecture of an MQ deployment in more detail.","title":"System context"},{"location":"guides/cp4i/mq/performance/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/mq/performance/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic1/","text":"Building MQ Queue Managers \u00b6 Overview \u00b6 In the previous chapter of this guide, we set up and configured GitOps for our cluster. We then used GitOps to install and manage Kubernetes infrastructure and services resources. In this chapter, we're going to use these resources to build, deploy and run an MQ queue manager. This will require us to install some more components into our cluster. Look at the following diagram: We'll examine these highlighted components in more detail throughout this section of the tutorial; here's an overview of their function, from left to right: The queue manager pipeline requires a secret to access the IBM Entitled Registry. This registry hosts the container images used by IBM Operators. The secret contains your IBM entitlement key, allowing you to easily access the IBM container software you have licensed. The QM1 Git repo contains the source definition for queue manager QM1, including its MQSC resources (e.g. queue and channel definitions) and qm.ini file that contains other configuration information (e.g. log file sizes). The queue manager pipeline requires a configmap and secret to connect to GitHub. They contain the repository URL and security token used by the pipeline to access the QM1 Git repository. IBM Catalogs is a CatalogSource that adds the IBM custom provider to the Operator Hub. Once added, IBM Operators, such as the MQ operator, or CloudPak for Integration, or CloudPak for Data can be installed into the cluster. The IBM MQ operator is installed from the IBM Catalogs source. It adds a new queuemanager custom resource definition to the cluster. This allows Tekton pipelines and ArgoCD applications to manage queue managers using a simple YAML interface. In this topic, we're going to: Fork, clone and explore a sample queue manager repository, QM1 Configure the cluster connection to the IBM Entitled Registry Install the IBM Catalogs and IBM MQ operator using GitOps Configure the pipeline connection to the QM1 repository Review the MQ queue manager pipeline for queue manager QM1 By the end of this topic we'll have a fully functioning MQ GitOps CICD pipeline that we can use to build and change queue managers. Pre-requisites \u00b6 Before attempting this topic, you should have successfully completed the previous chapter . The sample queue manager repository \u00b6 In the diagram above , we can see that the result of the CICD process is to build, test and deploy the queue manager QM1 that runs in the dev namespace. The CICD process uses the QM1 Git repository as its primary input source; every committed change to this repository will result in a pipeline run which, if successful, will update the deployed version of QM1 in the cluster. This is GitOps in practice, the upstream source of truth for QM1 is the Git repository. In this section, we're going to create our copy of the QM1 repository. We'll examine its contents to see how queue manager QM1 and its properties are defined. In a subsequent section, we'll use the Tekton queue manager pipeline and a dedicated ArgoCD queue manager application to build, test and deploy QM1 to our cluster. It's wise to use a new terminal window for this chapter. It will help us switch between the queue manager repository and GitOps repository as we examine the different steps in the CICD process. Fork the sample queue manager repository The sample queue manager repository gets us going quickly. Once you've seen how it works, you can use it as a starting point for your projects. Fork the sample repository https://github.com/cloud-native-toolkit/mq-infra to your custom GitHub Organization. Ensure you've set up the $GIT_ORG environment variable This chapter also uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Open a new terminal window . Replace <git-org> in the following command with your GitHub user name: export GIT_ORG = <git-org> You can verify your $GIT_ORG as follows: echo $GIT_ORG which will show your GitHub Organization, for example: prod-ref-guide We'll use this environment variable extensively in scripts and bash commands, so it's important to ensure it's correct. Change to your working git folder As in the previous chapter, we're going to clone a local copy of our sample repository. Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . Change to your working git folder: cd $HOME /git Clone the MQ infra repository We're going to work on a local clone of our GitOps repository. We'll push changes to our local copy back to GitHub at appropriate times so that they can be accessed by the queue manager pipeline to build and test the most recently committed version of QM1 . Clone the forked Git config repository to your local machine: git clone https://github.com/ $GIT_ORG /mq-infra A local copy will be made on your machine as follows: cloning into `mq-infra` warning: redirecting to https://github.com/prod-ref-guide/mq-infra/ remote: Enumerating objects: 99, done. remote: Counting objects: 100% (24/24), done. remote: Compressing objects: 100% (9/9), done. remote: Total 99 (delta 19), reused 15 (delta 15), pack-reused 75 Receiving objects: 100% (99/99), 12.93 KiB | 150.00 KiB/s, done. Resolving deltas: 100% (42/42), done. !!! note Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the mq-infra folder containing your local clone of the queue manager repository: cd mq-infra Review the GitOps folder structure The folder structure of this repository defines all the properties for queue manager QM1 . The queue manager pipeline reads the repository contents to build and test a queue manager with exactly these properties, ready for deployment by ArgoCD. Let's examine the structure in a little more detail. Show the folder structure with the following command: tree . Notice that this is quite a simple folder structure. You may recognize some of the files immediately: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 README.md \u2514\u2500\u2500 chart \u2514\u2500\u2500 base \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 config.mqsc \u251c\u2500\u2500 security \u2502 \u2514\u2500\u2500 config.mqsc \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 configmap.yaml \u2502 \u2514\u2500\u2500 qm-template.yaml \u2514\u2500\u2500 values.yaml The key folders and files are as follows: The Dockerfile file contains the base Dockerfile that will be used to build the container for QM1 . The contents of the chart folder are used to create a Helm application for QM1 . This Helm application is used to manage the deployment of a set of Kubernetes resources such as an MQ custom resource and config map. The templates folder contains the Kubernetes resource definitions that will be created for this chart. We can see a queue manager custom resource and a config map for example. The values.yaml contains a set of default values used by these templates to customize the Kubernetes resources. For example, the qm-template gets the CPU limit for QM1 from values.yaml . The config folder is ignored by Helm; but will be used by the pipeline. It contains the introductory queue manager MQSC definitions that will be placed in the config map used by QM1 . The security folder is also ignored by Helm; but will be used by the pipeline. It contains the more advanced queue manager MQSC definitions that will be placed in the config map used by QM1 , when security is required. The values.yaml file held in the source repository ensures that QM1 always has a set good defaults for deployment. We'll see how the mq-infra-dev pipeline uses these defaults to populate the Helm chart that is used by ArgoCD to deploy QM1 . However, we'll also see how these values can be overridden -- using GitOps -- when required. Feel free to explore the Helm chart; we'll examine it in much more detail throughout this chapter. Alternatively, feel free to learn more about Helm charts before you proceed. Install the kubeseal operator into the cluster \u00b6 Change to the Application GitOps directory Let's ensure we're in the correct folder. Again, we've used typical defaults: cd $HOME /git cd multi-tenancy-gitops-apps Verify that your $GIT_ORG and $GIT_BRANCH environment variables are set: echo $GIT_ORG echo $GIT_BRANCH For example: ( base ) anthonyodowd/git/multi-tenancy-gitops-apps echo $GIT_ORG prod-ref-guide ( base ) anthonyodowd/git/multi-tenancy-gitops-apps echo $GIT_BRANCH master If either is not set, set them as follows. export GIT_ORG = <replace_with_gitops_apps_repo_organization> export GIT_BRANCH = <replace_with_gitops_apps_repo_branch> Login to the cluster In the terminal window, log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Install the kubeseal CLI Now that we've installed the sealed secret operator and instance, we can create a sealed secret. We install the Kubeseal CLI on our local machine to do this. brew install kubeseal This make take a minute or so to install: ==> Downloading https://homebrew.bintray.com/bottles/kubeseal-0.14.1.big_sur.bottle.tar.gz ... ==> Installing kubeseal ==> Pouring kubeseal-0.14.1.big_sur.bottle.tar.gz /usr/local/Cellar/kubeseal/0.14.1: 5 files, 31.9MB The above installation is for MacOS. Use your favorite package manager to install the CLI on Linux distributions. Configure access to IBM Entitled Registry and Artifactory \u00b6 We need access credentials to interact with IBM Entitled Registry, Artifactory and MQ Git repositories. We store these credentials in Kubernetes secrets. However, because of the sensitive nature of its contents, we don't want to store these secret in our Git repository. Instead, we are using sealed secrets . To create a sealed secret, we use the YAML for a regular secret to create a sealed secret which is a strongly encrypted form of the secret. We store this in our GitOps repository. When ArgoCD deploys the sealed secret, the sealed secret operator will create a regular, un-encrypted, secret in the ci namespace where it can be accessed when required. The key point is that the deployed un-encrypted secret is never stored in the GitOps repository; only the sealed secret which is encrypted is stored in Git. Once created, the secret can only be accessed by users who have access to the ci namespace, such as the Tekton queue manager pipeline. Configure connection to the IBM Entitled Registry \u00b6 To install and use the relevant IBM MQ containers in your cluster, an entitlement key is required to retrieve them from the IBM Entitled Registry. In this section, we retrieve your key, and store it in a secret in the cluster where it can be used to install containers at the appropriate time. For this section, we return to the terminal window we used in previous chapter, Configuring the cluster , rather than the one we've just used to clone the QM1 source repository. Open a new terminal window for the multi-tenancy-gitops repository if necessary. Discover your IBM Entitlement Key Your IBM entitlement key is used to access IBM software. By accessing the IBM Entitled Registry using this key, you can easily access the IBM container software you have licensed. Access your IBM entitlement key . Copy the key and store it in the $IBM_ENTITLEMENT_KEY environment variable where it will be used by subsequent commands in this tutorial: export IBM_ENTITLEMENT_KEY = <ibm-reg-key> You should keep this key private from other users. Create the YAML for the IBM entitlement key and seal the IBM entitlement key secret Let us first run the below script and look into the details later. ./mq/environments/ci/secrets/ibm-entitled-registry-credentials-secret.sh This script performs the below actions. It initially creates the YAML for a regular secret using the oc create secret command with the --dry-run option; it creates the YAML, but doesn't apply it to the cluster. Then, it seals the IBM entitlement key secret. We can examine the sealed secret YAML using the following command: cat ibm-entitled-registry-credentials-secret.yaml See how a sealed secret is very similar to a regular secret in structure: apiVersion : bitnami.com/v1alpha1 kind : SealedSecret metadata : creationTimestamp : null name : ibm-entitled-registry-credentials namespace : ci spec : encryptedData : IBM_ENTITLED_REGISTRY_PASSWORD : AgALF7SxZrv+cWxZzplncnNImnWBl0vSjQy3whhwl3pch2eVgwVBoN94AH2XUNRlY+pVXKUAAFwDexUgLwpkdvfFo5ZTZR33sIEiCl3JdV0IXTJmpLqMeSUXMgcJ2RUL81frGwPMGy9/Tmv41k/Eaapyj9FTj0n8IlKiNkHqwHzkOsEF1ueSkoLETlrR0rOV8DHy7VPzPQ0RylX66Q7vvB43xI/Aq3AfpNu+jthH+qRv8D7Mx1mVmqXVxaOz/TBhQyk5Z2P3/JPu1oQK3Xrrr3i5tb8vUHQLh/1lCa0hY30HrCDXwTphcvDnXkKGsyw4pmf6Mkzr6SbPahEOG6Mmz8InYYbJQOLXPwRmrluGv+IRe+1nqh5JAaSrJHC38mKXUFsRaLEODf/DCtNz9NNJkSlLM2OSBzGsFL2v1NrhA6envrScljjv8MqcSFkzR9bPNuXu/8ANHM3C4mcvHAY1/+uRkOxLmEaEgBab1zu3ellSnNH3zNi9YjhXVw1R4505kjM2r5O6Oe7UdxtI50axFdxcU6JpfFT++kEsiVJZyqqBzVKjFDVww8ReH9U9BK0ibDfmhO+7IFNNODVCcXHicf/pRjh+Y0Z/OY/IVbpdenS9YL/vZIVdVXKwf/oZzMiIAvoZ93KgIY0pzqZitXo7jNuAIYlMX6X0vHrx0TK6nJh4OOOhUwGDnsSg4GaYRs8GZPw= IBM_ENTITLED_REGISTRY_USER : AgCVxkhumBCyp3nGLuOhFKo4o03NwB7audQ2DJruzO7uscFTBcktzeDrMKmdwOfZkqyhb8zNpnxyZwZzdqP4TgTTLVr71c1YqYo+tMd+UEYO1+QdCKGi7+IzJYusMKSgLcNshuAZfhKqLybGyOOK6qv+EmIjJ5GuPKgZgKsMEvlSHswp4pWYEfC+sokwWV8t3APRb1RR3B0Lx40dLduJ8+QL4i4Qcu267vCqa6HK5423b/ryjF+FBAlpUELiK9BX2OQ7BtT/en/zbDW1tj0hdsOu8BD1JvTmY/ueRTF+CQ+dU5dPQCvGIZCDQEAqhr0jjMp9eL+JQMiBEjFlR6AYf1IFNk1SkaD68f8mSLy+eLn3qfc365shIBQtuGF9x/bpLgMbuHofEgO3/qvXy4WxNxqwsGVlNl3bYd6JrLf+SnOR+NmC69k/cRJVs/6wjod/lJrDHsWofc07paADcSRVJ1aDv5E77ibuJSRlw8Zuyu4S445/8/m0x3Fgc9WW69G7pBAYRyN7w83jaGMf/2pJVcqUPu06jFeCwvGOXwRbK/A0f76gG/CWzM3zMSzr5CqyNaiAJN7GFdv3xuFkCpKpSsRu7jYP6PSS+0G0YJ03IAXj9smxmLKe0ZCEYxBetzmhiMychAZTbUSLtXDscbdYZlkcZct/E9lqOM3hl8FuPNDchS1VRDVbcsQUohx5oqtwJSukgQ== template : metadata : creationTimestamp : null name : ibm-entitled-registry-credentials namespace : ci type : Opaque However, notice the following differences: apiVersion: and kind: identify this as a sealed secret. .dockerconfigjson has been encrypted using the public key of the sealed secret controller, meaning that it can only be decrypted by this controller. This sealed secret is safe to store in our GitOps repository, because it can only be decrypted by the sealed secret controller that was used to encrypt it. We haven't applied this sealed secret to the cluster yet -- we create the YAML file so that we can use the GitOps process to deploy it. Install the entitlement key into the cluster We deploy this sealed secret into the ci namespace in the cluster. To do this, we need to place the sealed secret in the GitOps Application repository, within the folder mq/environments/ci/secrets . mv ibm-entitled-registry-credentials-secret.yaml mq/environments/ci/secrets/ Pushing these changes to our GitOps repository will apply them to the cluster: git add mq/environments/ci/secrets git commit -s -m \"Deploy the IBM entitled registry sealed secret\" git push origin $GIT_BRANCH Configure connection to access Artifactory \u00b6 The pipelines we install later in this tutorial runs in the ci namespace store their build output in Artifactory. To do this they need a secret that it initially created in the tools namespace when Artifactory is installed. Create the YAML for the artifactory access secret and seal the artifactory access secret Issue the following command: ./mq/environments/ci/secrets/artifactory-access-secret.sh This script performs the below actions. It initially creates the YAML for a regular secret by using the artifactory-access secret from the tools namespace; it creates the YAML, but doesn't apply it to the cluster. Then, it seals the artifactory-access secret. We can examine the sealed secret YAML using the following command: cat artifactory-access-secret.yaml See how a sealed secret is very similar to a regular secret in structure: apiVersion : bitnami.com/v1alpha1 kind : SealedSecret metadata : creationTimestamp : null name : artifactory-access namespace : ci spec : encryptedData : ARTIFACTORY_ENCRYPT : AgCklDmp+y4VmnErpKOLGEijzgc0SiZjpJQg46BGfb78MnkY3ZEyH752YDnNp9/JBvsSPL20agoQabXYczVJP/JogWPbhPbZoDMubTjPq7Lf63CSkF1WFsMyYrVpPApWrsLL3YTeh4cXYqwCowZUfLWrTIAKduCkI2L2Fo25HSNY3ovBnxbzofHK6u2CAc7BBzAJ7O3T4mK1tKY67CCwFYkTXqfG5ECIF67HKjLhnj7rJGQ+bbjYgQ1RYR04adYGBBYgIQvWzXBQgz/8XuWs+Utk0rtWuAyUfEFehJfOXc/4iOSoctnyaZtodlFqnyEgajSUE/HHzy6QJwdVwls/necjbBzvYNO3OpQg/Vb3h+owTikAzzleNTMtAV6628AFV7f+20UnAn7xtoLMEjZ6pHu12DVfmjQ8mG9zNqo5/M2czWFQPLpmmf3KAaEbGjAQHV8tBvDRaaR389BS5w95QOrigJG5w+reKA5bosGdCQintPol+cVafIYxsqJDDI40qPPIlVOX/wlqHKvzD6UacXIFywcjjYGNIATo9Zg3PQfnTjnCzTs46oNswF7N7rUnXrg4KOcUDv18QuTR3brzIs+ccWdDLS+ScnRqxoOQp0x+D+vT/8EsdF0+MrfBPc95eLCJu1OjUULhVOVv/pCOaZCULrhdSqYlCf4BjhNSEtTrvU9lUg4bBVm4X/Q7nyjZ5GLqs5QSSfLKkaRE49M4BAamPRGXxNQcur9TFl44mIEev/Tkap0YAg== ARTIFACTORY_PASSWORD : AgBBkfKFEYXPYxHCY5ggrpSynRT+10boLkD5+udB4YXFU7MPqFoXe8z+NPnDDKEhsVBWNhUyqp2M7RjyC0BUFMDgGevMOCNOdg4pM+0vuf/pGjPVHSqUK48qoRX3gs8QBdTXRBnfTfqd59GTFRhdMQ7tp5ztZ8TawlAnzWbdfWSdxya+fKSQxVG1WDamnpcRMI5zHJkAyxz4689yXE5eS/ughkNA35MB2BZdrAaM1qVJ/9ZaV5nnwYeZTTGp6i5s8bZoqz0a/rlYbdbtHYvRoCPxnbAelwCu6bkvkXc424fA9m4h5JPWxSfF4TMCLG7eKngJ7DJZ3RRRc1aFIQb8BMfFmA+z/AHsup/KcLSrjWcTWaekNdf6At+iHDr5tXhQjyYedtlulwhgH+dUa5bh6jisdqcf0lnjJLSaLswZvFtWuXj3MbB4aQ1jlHpp39I/EVpHKTYbUcVZWZJAf9AlhJ+ml16FLYHzjBDXCdv34Km3RcKQW4xo8aolgbI2X8Ph6EFA3Pwv6qGAD/r65YkTsgssdgLCgxw/4dKnZbbfdoCqPq0paeERPWk/Ss6y8g86vMNPiVlMDbVaHYY6Fi4tmCSmxTFzNT2idox4t7PaHAuDtFwwyMQbMNlNPKxvjFyaEqjdZ4l2UEmpjyZHcNv3Tg4c+OL5OLmPJ29kB2/AzbS0khtgZggflTZfvlCECVio15iOVL7dIOCk+CMWPN878RfpWZCCow== ARTIFACTORY_URL : AgB7K7Ul7JJwH4D/UlWxEtVQ6R9FAtZzhiYPxgSHIAITt9QIdQ3rYvrQG4H4mLzMkQY+2jd6CBYDRhHy5H3gGLi1wr4W71BQ5c0mwA5R9XL/IKbbJhnqXCKF07LAbiHqFucd0Uk2k68u9SxLmxNr4f6o+GuBfaDSJ2QeM4xqT1h/vmdR/NtErMg0xaewiB37w8FwjTpHRH9EpfODWXToiBNCKVF10WA7D4WyLYK7P227b06rUnckJ6jnTbz+JASlTnHngNkhflOsNZrjFFrSbSmH+i6PITM8QGBaPlMoqIFeI3rIlHOuQ2wQ/7fSZWu3pZZ/EWMkh0WpmlrEAXwpoL1mzc6ZMrqWWNZJeJ5WoywiJ2skTx/sC7GbWGN4u/kEwt5TY4g761Yz+9ctUcpdVpFyjTxSjWk21rr9G6NP3i58J3oj7zKakyWXcV11zhidhrtOHCHdgg0COy1Wrx3QoQgD6EdLXJ6KbHcXjcSQd9vbxp39g4Zw3MyPKB4S5ceZXNGcrzpxP2PNE8e4SqHCEpqMjI137X1W7XA1U2jddYjAHYnH9f8azG99b8hRheCUJsx8Gvc+EViaCc1pXXQ/ttrRVixgWUy3VU/5FpM5zEdYKFvTjp5sZ0VGYtJZl72eU4Y43GQ0UwQRFAT0511zleViqT6dzT2POeaSybR8BNyZuI4I3OHQEX/BAgvTXpzSSIjoS9oB+bOjqdOPh0YPB5BDch+Us1oDZPrOO9awO5qFdZnF24DO3Ra9+w== ARTIFACTORY_USER : AgCXJcAoBaSttkNHeVt3h1sXk8DRlYqXKM8Ah5uLIp7cGA2y5gVSJw6wh5aLaI5zdjeC6LpZG7pV9gjEEN130DFP2E3eAMZ99gB3CI/eLHEK7SfIfoCJjhpTcxJqrrfBttreCjTcXLoNjlOZPJJzM7Qsb12VBu2vwMJP3RyqOr0SPPZ7iVSDj0ZdPOzjslj/71Es0ooYze8u5AOHSa4gwYiPyuRy0jXTsPUBmSdxvL66qPzstiyaMqDkleE1j71t2KM7a9JmDGisJWw+bmlsZTAz9Zu1HDfl8uj3KdtZUhthmjIhssVK5obeLArMFx4UQgnhI1ZM1W8E2N+TuKmPWDvkLivIxpxCxy2vokbe3iZDLsi+NSkhDsfqdUq46bvb1OcTw6qoPkjWDALOFc1Jh8d5x6WQK/sPNicOptVt47OU2R5QhAS3+GsvFaeQqvry/MPus2Vna+E+okbiTAlcJbXo5Z8US0yTb+NmtdeuY5bVzDD0ptbRAgqDV1Lt/cMP9gNRJmKHf6lFa2H5MU0nBpWi5SMiwKtVSAoUvhJGU+WwijF32XrrOaySIHfL4OfFULnChEzFjgrEthgryVIirJ9lSDeoL7ziV2pJBMiVx98UiFEBGYiRupxV9InDxR7t/kyXYhJWrVXmakl06LSbRl+9Ist8jj7LIzK6AwZCJt1DyUvMun+ekGJWVhKETl1ulJKRMXUDgg== template : data : null metadata : creationTimestamp : null managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:data : . : {} f:ARTIFACTORY_ENCRYPT : {} f:ARTIFACTORY_PASSWORD : {} f:ARTIFACTORY_URL : {} f:ARTIFACTORY_USER : {} f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:type : {} manager : kubectl-client-side-apply operation : Update time : \"2021-09-15T12:40:59Z\" name : artifactory-access namespace : ci type : Opaque This sealed secret is safe to store in our GitOps repository, because it can only be decrypted by the sealed secret controller that was used to encrypt it. We haven't applied this sealed secret to the cluster yet -- we create the YAML file so that we can use the GitOps process to deploy it. Install the artifactory access secret into the cluster We deploy this sealed secret into the ci namespace in the cluster. To do this, we need to place the sealed secret in the GitOps Application repository, within the folder mq/environments/ci/secrets . mv artifactory-access-secret.yaml mq/environments/ci/secrets/ Pushing these changes to our GitOps repository will apply them to the cluster: git add mq/environments/ci/secrets git commit -s -m \"Deploy the artifactory access sealed secret\" git push origin $GIT_BRANCH Configure the pipeline for QM1 source repository \u00b6 Now that we have the ability to create queue managers in our cluster, let's configure the queue manager Tekton pipeline we are going deploy later in this tutorial to use the source repository for QM1 we created earlier as well as the GitOps repository where its built configuration will be stored before deployment by ArgoCD. To run the pipeline, we need to create a configmap and secret that contain the Github repository location, and GitHub access token. Once we've deployed these to the cluster, we can run the pipeline. Change to the Application GitOps directory Let's ensure we're in the correct folder. Again, we've used typical defaults: cd $HOME /git cd multi-tenancy-gitops-apps Verify that your $GIT_ORG and $GIT_BRANCH environment variables are set: echo $GIT_ORG echo $GIT_BRANCH For example: ( base ) anthonyodowd/git/multi-tenancy-gitops-apps echo $GIT_ORG prod-ref-guide ( base ) anthonyodowd/git/multi-tenancy-gitops-apps echo $GIT_BRANCH master If either is not set, use the instructions below. export GIT_ORG = <replace_with_gitops_apps_repo_organization> export GIT_BRANCH = <replace_with_gitops_apps_repo_branch> The sample configmap The GitOps repository contains a template for the configmap used by the pipeline to access GitHub. Issue the following command to view the template of the configmap YAML: cat mq/environments/ci/configmaps/gitops-repo-configmap.yaml_template apiVersion : v1 kind : ConfigMap metadata : labels : app : multi-tenancy-gitops group : pipeline type : git name : gitops-repo data : branch : ${GIT_BRANCH} host : github.com org : ${GIT_ORG} owner : ${GIT_ORG} parentdir : . protocol : https repo : multi-tenancy-gitops-apps url : https://github.com/${GIT_ORG}/multi-tenancy-gitops-apps.git Many of these YAML nodes will be customized by the script we run now. cd mq/environments/ci/configmaps/ ./gitops-repo-configmap.sh cd ../../../../ Once, this script is run successfully, you should see a new file named gitops-repo-configmap.yaml . Issue the following command to view the configmap YAML we just created: cat mq/environments/ci/configmaps/gitops-repo-configmap.yaml apiVersion : v1 kind : ConfigMap metadata : labels : app : multi-tenancy-gitops group : pipeline type : git name : gitops-repo data : branch : master host : github.com org : hp-gitops-test owner : hp-gitops-test parentdir : . protocol : https repo : multi-tenancy-gitops-apps url : https://github.com/hp-gitops-test/multi-tenancy-gitops-apps.git For example, branch: , host: , org: and owner: , these values will be used by the queue manager pipeline to update the GitOps repository after a successful pipeline run. Create GitHub access token As well as the location of the GitHub repositories it uses, the pipeline will also need a GitHub personal access token to identify it as a valid user to GitHub. A Personal Access Token is used in place of a user password to authenticate with GitHub. This token provides specific access to all repositories owned by a particular GitHub user or organization. When GitHub is accessed using this token, the only permitted operations are those defined by the specific token. Navigate to your GitHub Developer Settings to generate a new token. Click on Generate new token and make the following selections: Name the token CI pipeline Select public_repo to enable git clone Select write:repo_hook so the pipeline can create a web hook After you click Generate token to create the access token with these permissions, the GitHub UI will let you see the generated token once, but never again. Therefore, save the token somewhere safe -- so that you can use it later if required. In the meantime, let's store the token in the $GIT_TOKEN environment variable. export GIT_TOKEN = <personal access token> Also, store the git user information in the $GIT_USER environment variable. export GIT_USER = <git user name> Create a regular secret to hold the GitHub access credentials and seal the credentials As before, we're going to create a sealed secret to store the personal access token previously created. To do this, we first create the regular secret YAML, and then seal it with kubeseal . The secret will contain two literals, username and password , which contain our GitHub username and personal access token. We will store this secret in the ci namespace. Make sure you set the GIT_USER and GIT_TOKEN environment as shown above. Create the regular secret YAML with the following command: ./mq/environments/ci/secrets/git-credentials-secret.sh This script performs the below actions: It initially creates the YAML for a regular secret by using the git username and access token provided; it creates the YAML, but doesn't apply it to the cluster. Then, it seals the github credentials secret. We can examine the sealed secret YAML using the following command: cat git-credentials-secret.yaml Notice the kind: SealedSecret in the YAML: apiVersion : bitnami.com/v1alpha1 kind : SealedSecret metadata : creationTimestamp : null name : git-credentials namespace : ci spec : encryptedData : password : AgCufC++OG16tYVA5cs7v8uE9ed/ihpkdYYXSlrtmdlQfwjHzttj97X32tq4NaXzF64GIVCzr2vnweDCYIC+sd0SSs9b//KpdNgiZ2AHmJBdhhnmGs0PHHEgh16QmDlxV3on+Ewe6uM5MmbV/bpmRHu9L3spF+kcRdTfdpsQegpz7+QlR9Owvgt0LbZYiNwGzkwnczRtRhG8DZC6FodH95t7KoXN+fjupGJFS2GQo55e0QbhgdgPDLHYrZo8pQyvQVG+PsK3bn6OiJbB9nLsY4xV+r38Qg6BrVGW+kY2oqAHT0BR+4KljIWdSVMXoG9zU7HNU1H6R1nFB+8wblHF9ySaKxObIPhjxMEjD3ieOg7WjuySWDtIL2PF5OGuY8B2m/totM1CKWCJZ+RT0X8y98NjqR03QPD7AKz4JgL8Cerkq4JojroANNyIDl9iiwjEd7on7d2CmzLSWX2STXw0LuqfjMoId3K2l/THpwpbhpnX2ZDJHiSUtJ+mepjFYyJrj4hCm4dXGKABVE7rilQvLaS5paZPx3uUC7eZqqdo+pz3dd/+o4R+PCQTCTKEzEnrGZCKISff6pMMgDHx7RsotGDbyAEqrpK2MuwxzIV/lxeQEA6qZeSS65sndOHTZlgMCGeRazkybDxc4PMcuEqbXM2e9cDwzf7cqyCL+czCjYPF0PRA5kaYBgTZiLUUfnUlVtttCStBdUOKiQ54wMj8mjFCkfnbMnhgFCrylLhmGiq5Oq4nUPQ4GQh/ username : AgCc4ywhZ7MDcrKNbGNKeItPvg343PwycBGMiTESVtNgWGdcDOhMUzx2T1Het+JLFbfCQDV4Ewh1Jv/CC+4yjF87YBUu6vu+dTFM05M59mEj9PgG89d/Xpf/H2St1z8ZoqQyYKIF90W3ji4HvMzQv/RHC3dECTFQI5Jh2YmA7U0XyQjWb+YxSQtRFDM/E2CTXoYZD5ZK+J8emWzCk0F5/1bdUt+Q/IHoJg5LsPgjZEVnv7Wd+8Gn0UKQE/BYqfInU6rrS1Sc5ogQvWnd73LeN2gViqriwiTgjA2LlNK4JWkimYifDgt0ZEOdaJ7XjyFznq42vYL+KbY1oeL1dKMuh1bFv0m9796CHsYjYahLknXbN1FztGfh1gswhP97mPrp0Fq5x0qF8ZbTFz/GeH8TzN4VlsaKzvApxQDBV/FHCZQ0n5xLlSLpuDHKDXgEbD+Hc8Nfz18aUgBC/Tgm4qcObjg1Px7ASbEG5m4uxZb30Qp2R4tW7NFIzAd1EbQSaN+cPrsmlcRNoE6lU2bpWrW9e1HnUQiZzZp1/W8iqBkpb7Dl9bR8XtfxB3CK03rj72dBSOJseQruNXn7tfhMf5vgxwEppC3rSR1eTPuXFdiI2tbLvMawNtSASiQB23MVrqXdjV0gXCM9cf3XJA8mIzppc8iE3T7z1WmkLhlAR66gkByrOyemKgw4q9T+aSSpE2HQtemwp8SbbUWpCPcF template : data : null metadata : annotations : tekton.dev/git-0 : https://github.com creationTimestamp : null name : git-credentials namespace : ci type : kubernetes.io/basic-auth Also notice how the username and password are not just obfuscated, they are fully encrypted. (If you'd like, you can use the base64 -D command from above -- it will not reveal the original username.) Install the sealed secret into the cluster Again, we deploy this sealed secret into the ci namespace in the cluster. To do this, we need to place the sealed secret in the GitOps Application repository, within the folder mq/environments/ci/secrets . mv git-credentials-secret.yaml mq/environments/ci/secrets/ Pushing these changes to our GitOps repository will apply them to the cluster: git add mq/environments/ci/secrets mq/environments/ci/configmaps/ git commit -s -m \"Deploy the github credentials secret and configmap\" git push origin $GIT_BRANCH Deploy applications to the cluster \u00b6 Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Deploy the applications layer in the GitOps repo Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy apps resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the changes will be applied to the cluster.: git add . git commit -s -m \"Intial boostrap setup for applications\" git push origin $GIT_BRANCH The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the changes: Select resources to deploy Access the 0-bootstrap/single-cluster/3-apps/kustomization.yaml : cat 0 -bootstrap/single-cluster/3-apps/kustomization.yaml Open 0-bootstrap/single-cluster/3-apps/kustomization.yaml and uncomment the following: resources : #- argocd/ace/cicd.yaml #- argocd/ace/dev.yaml #- argocd/ace/stage.yaml #- argocd/ace/prod.yaml - argocd/mq/cicd.yaml - argocd/mq/dev.yaml #- argocd/mq/stage.yaml #- argocd/mq/prod.yaml #- argocd/apic/cicd.yaml #- argocd/apic/dev.yaml #- argocd/apic/stage.yaml #- argocd/apic/prod.yaml #- argocd/apic/single-cluster.yaml #- argocd/apic/multi-cluster-app.yaml #- argocd/apic/multi-cluster-ops.yaml #- argocd/bookinfo/cicd.yaml #- argocd/bookinfo/dev.yaml #- argocd/bookinfo/stage.yaml #- argocd/bookinfo/prod.yaml #- argocd/soapserver/soapserver.yaml #- argocd/cp4a/cp4a.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications\" patch : |- - op : add path : /spec/source/repoURL value : https://github.com/gitops-workflow-demo/multi-tenancy-gitops-apps.git - op : add path : /spec/source/targetRevision value : master Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying apps\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The applications argocd application Let's examine the ArgoCD application that manage the applications in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the applications application: The apps-mq-rest-ci-1 argocd application Let's examine the ArgoCD application that manage the applications in ci namespace . In the ArgoCD UI Applications view, click on the icon for the apps-mq-rest-ci-1 application: You will see all the necessary configurations. If you observe carefully, you will see all the sealed secrets and configmaps we created earlier under this application. Examine the fully configured queue manager dev pipeline We've now fully configured the queue manager pipeline for the dev namespace. In the next section of this chapter we're going to run this pipeline to build QM1 and deploy it to the dev namespace. Let's have a quick look at the pipeline in the OpenShift console. From OpenShift console, select Pipelines > Pipelines . Then select Project:ci . You'll see the below pipelines; Select mq-infra-dev pipeline: We'll explore this pipeline more fully in the next topic of this chapter. Congratulations! You've now got a fully functioning MQ queue manager pipeline. In the next topic of this chapter, we're going to use this pipeline to deploy a fully tested queue manager QM1 to the dev namespace. We'll explore the pipeline, tasks and steps in more detail to see exactly how they work.","title":"Configuring the pipeline"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic1/#building-mq-queue-managers","text":"","title":"Building MQ Queue Managers"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic1/#overview","text":"In the previous chapter of this guide, we set up and configured GitOps for our cluster. We then used GitOps to install and manage Kubernetes infrastructure and services resources. In this chapter, we're going to use these resources to build, deploy and run an MQ queue manager. This will require us to install some more components into our cluster. Look at the following diagram: We'll examine these highlighted components in more detail throughout this section of the tutorial; here's an overview of their function, from left to right: The queue manager pipeline requires a secret to access the IBM Entitled Registry. This registry hosts the container images used by IBM Operators. The secret contains your IBM entitlement key, allowing you to easily access the IBM container software you have licensed. The QM1 Git repo contains the source definition for queue manager QM1, including its MQSC resources (e.g. queue and channel definitions) and qm.ini file that contains other configuration information (e.g. log file sizes). The queue manager pipeline requires a configmap and secret to connect to GitHub. They contain the repository URL and security token used by the pipeline to access the QM1 Git repository. IBM Catalogs is a CatalogSource that adds the IBM custom provider to the Operator Hub. Once added, IBM Operators, such as the MQ operator, or CloudPak for Integration, or CloudPak for Data can be installed into the cluster. The IBM MQ operator is installed from the IBM Catalogs source. It adds a new queuemanager custom resource definition to the cluster. This allows Tekton pipelines and ArgoCD applications to manage queue managers using a simple YAML interface. In this topic, we're going to: Fork, clone and explore a sample queue manager repository, QM1 Configure the cluster connection to the IBM Entitled Registry Install the IBM Catalogs and IBM MQ operator using GitOps Configure the pipeline connection to the QM1 repository Review the MQ queue manager pipeline for queue manager QM1 By the end of this topic we'll have a fully functioning MQ GitOps CICD pipeline that we can use to build and change queue managers.","title":"Overview"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic1/#pre-requisites","text":"Before attempting this topic, you should have successfully completed the previous chapter .","title":"Pre-requisites"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic1/#the-sample-queue-manager-repository","text":"In the diagram above , we can see that the result of the CICD process is to build, test and deploy the queue manager QM1 that runs in the dev namespace. The CICD process uses the QM1 Git repository as its primary input source; every committed change to this repository will result in a pipeline run which, if successful, will update the deployed version of QM1 in the cluster. This is GitOps in practice, the upstream source of truth for QM1 is the Git repository. In this section, we're going to create our copy of the QM1 repository. We'll examine its contents to see how queue manager QM1 and its properties are defined. In a subsequent section, we'll use the Tekton queue manager pipeline and a dedicated ArgoCD queue manager application to build, test and deploy QM1 to our cluster. It's wise to use a new terminal window for this chapter. It will help us switch between the queue manager repository and GitOps repository as we examine the different steps in the CICD process. Fork the sample queue manager repository The sample queue manager repository gets us going quickly. Once you've seen how it works, you can use it as a starting point for your projects. Fork the sample repository https://github.com/cloud-native-toolkit/mq-infra to your custom GitHub Organization. Ensure you've set up the $GIT_ORG environment variable This chapter also uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Open a new terminal window . Replace <git-org> in the following command with your GitHub user name: export GIT_ORG = <git-org> You can verify your $GIT_ORG as follows: echo $GIT_ORG which will show your GitHub Organization, for example: prod-ref-guide We'll use this environment variable extensively in scripts and bash commands, so it's important to ensure it's correct. Change to your working git folder As in the previous chapter, we're going to clone a local copy of our sample repository. Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . Change to your working git folder: cd $HOME /git Clone the MQ infra repository We're going to work on a local clone of our GitOps repository. We'll push changes to our local copy back to GitHub at appropriate times so that they can be accessed by the queue manager pipeline to build and test the most recently committed version of QM1 . Clone the forked Git config repository to your local machine: git clone https://github.com/ $GIT_ORG /mq-infra A local copy will be made on your machine as follows: cloning into `mq-infra` warning: redirecting to https://github.com/prod-ref-guide/mq-infra/ remote: Enumerating objects: 99, done. remote: Counting objects: 100% (24/24), done. remote: Compressing objects: 100% (9/9), done. remote: Total 99 (delta 19), reused 15 (delta 15), pack-reused 75 Receiving objects: 100% (99/99), 12.93 KiB | 150.00 KiB/s, done. Resolving deltas: 100% (42/42), done. !!! note Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the mq-infra folder containing your local clone of the queue manager repository: cd mq-infra Review the GitOps folder structure The folder structure of this repository defines all the properties for queue manager QM1 . The queue manager pipeline reads the repository contents to build and test a queue manager with exactly these properties, ready for deployment by ArgoCD. Let's examine the structure in a little more detail. Show the folder structure with the following command: tree . Notice that this is quite a simple folder structure. You may recognize some of the files immediately: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 README.md \u2514\u2500\u2500 chart \u2514\u2500\u2500 base \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 config.mqsc \u251c\u2500\u2500 security \u2502 \u2514\u2500\u2500 config.mqsc \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 configmap.yaml \u2502 \u2514\u2500\u2500 qm-template.yaml \u2514\u2500\u2500 values.yaml The key folders and files are as follows: The Dockerfile file contains the base Dockerfile that will be used to build the container for QM1 . The contents of the chart folder are used to create a Helm application for QM1 . This Helm application is used to manage the deployment of a set of Kubernetes resources such as an MQ custom resource and config map. The templates folder contains the Kubernetes resource definitions that will be created for this chart. We can see a queue manager custom resource and a config map for example. The values.yaml contains a set of default values used by these templates to customize the Kubernetes resources. For example, the qm-template gets the CPU limit for QM1 from values.yaml . The config folder is ignored by Helm; but will be used by the pipeline. It contains the introductory queue manager MQSC definitions that will be placed in the config map used by QM1 . The security folder is also ignored by Helm; but will be used by the pipeline. It contains the more advanced queue manager MQSC definitions that will be placed in the config map used by QM1 , when security is required. The values.yaml file held in the source repository ensures that QM1 always has a set good defaults for deployment. We'll see how the mq-infra-dev pipeline uses these defaults to populate the Helm chart that is used by ArgoCD to deploy QM1 . However, we'll also see how these values can be overridden -- using GitOps -- when required. Feel free to explore the Helm chart; we'll examine it in much more detail throughout this chapter. Alternatively, feel free to learn more about Helm charts before you proceed.","title":"The sample queue manager repository"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic1/#install-the-kubeseal-operator-into-the-cluster","text":"Change to the Application GitOps directory Let's ensure we're in the correct folder. Again, we've used typical defaults: cd $HOME /git cd multi-tenancy-gitops-apps Verify that your $GIT_ORG and $GIT_BRANCH environment variables are set: echo $GIT_ORG echo $GIT_BRANCH For example: ( base ) anthonyodowd/git/multi-tenancy-gitops-apps echo $GIT_ORG prod-ref-guide ( base ) anthonyodowd/git/multi-tenancy-gitops-apps echo $GIT_BRANCH master If either is not set, set them as follows. export GIT_ORG = <replace_with_gitops_apps_repo_organization> export GIT_BRANCH = <replace_with_gitops_apps_repo_branch> Login to the cluster In the terminal window, log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Install the kubeseal CLI Now that we've installed the sealed secret operator and instance, we can create a sealed secret. We install the Kubeseal CLI on our local machine to do this. brew install kubeseal This make take a minute or so to install: ==> Downloading https://homebrew.bintray.com/bottles/kubeseal-0.14.1.big_sur.bottle.tar.gz ... ==> Installing kubeseal ==> Pouring kubeseal-0.14.1.big_sur.bottle.tar.gz /usr/local/Cellar/kubeseal/0.14.1: 5 files, 31.9MB The above installation is for MacOS. Use your favorite package manager to install the CLI on Linux distributions.","title":"Install the kubeseal operator into the cluster"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic1/#configure-access-to-ibm-entitled-registry-and-artifactory","text":"We need access credentials to interact with IBM Entitled Registry, Artifactory and MQ Git repositories. We store these credentials in Kubernetes secrets. However, because of the sensitive nature of its contents, we don't want to store these secret in our Git repository. Instead, we are using sealed secrets . To create a sealed secret, we use the YAML for a regular secret to create a sealed secret which is a strongly encrypted form of the secret. We store this in our GitOps repository. When ArgoCD deploys the sealed secret, the sealed secret operator will create a regular, un-encrypted, secret in the ci namespace where it can be accessed when required. The key point is that the deployed un-encrypted secret is never stored in the GitOps repository; only the sealed secret which is encrypted is stored in Git. Once created, the secret can only be accessed by users who have access to the ci namespace, such as the Tekton queue manager pipeline.","title":"Configure access to IBM Entitled Registry and Artifactory"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic1/#configure-connection-to-the-ibm-entitled-registry","text":"To install and use the relevant IBM MQ containers in your cluster, an entitlement key is required to retrieve them from the IBM Entitled Registry. In this section, we retrieve your key, and store it in a secret in the cluster where it can be used to install containers at the appropriate time. For this section, we return to the terminal window we used in previous chapter, Configuring the cluster , rather than the one we've just used to clone the QM1 source repository. Open a new terminal window for the multi-tenancy-gitops repository if necessary. Discover your IBM Entitlement Key Your IBM entitlement key is used to access IBM software. By accessing the IBM Entitled Registry using this key, you can easily access the IBM container software you have licensed. Access your IBM entitlement key . Copy the key and store it in the $IBM_ENTITLEMENT_KEY environment variable where it will be used by subsequent commands in this tutorial: export IBM_ENTITLEMENT_KEY = <ibm-reg-key> You should keep this key private from other users. Create the YAML for the IBM entitlement key and seal the IBM entitlement key secret Let us first run the below script and look into the details later. ./mq/environments/ci/secrets/ibm-entitled-registry-credentials-secret.sh This script performs the below actions. It initially creates the YAML for a regular secret using the oc create secret command with the --dry-run option; it creates the YAML, but doesn't apply it to the cluster. Then, it seals the IBM entitlement key secret. We can examine the sealed secret YAML using the following command: cat ibm-entitled-registry-credentials-secret.yaml See how a sealed secret is very similar to a regular secret in structure: apiVersion : bitnami.com/v1alpha1 kind : SealedSecret metadata : creationTimestamp : null name : ibm-entitled-registry-credentials namespace : ci spec : encryptedData : IBM_ENTITLED_REGISTRY_PASSWORD : AgALF7SxZrv+cWxZzplncnNImnWBl0vSjQy3whhwl3pch2eVgwVBoN94AH2XUNRlY+pVXKUAAFwDexUgLwpkdvfFo5ZTZR33sIEiCl3JdV0IXTJmpLqMeSUXMgcJ2RUL81frGwPMGy9/Tmv41k/Eaapyj9FTj0n8IlKiNkHqwHzkOsEF1ueSkoLETlrR0rOV8DHy7VPzPQ0RylX66Q7vvB43xI/Aq3AfpNu+jthH+qRv8D7Mx1mVmqXVxaOz/TBhQyk5Z2P3/JPu1oQK3Xrrr3i5tb8vUHQLh/1lCa0hY30HrCDXwTphcvDnXkKGsyw4pmf6Mkzr6SbPahEOG6Mmz8InYYbJQOLXPwRmrluGv+IRe+1nqh5JAaSrJHC38mKXUFsRaLEODf/DCtNz9NNJkSlLM2OSBzGsFL2v1NrhA6envrScljjv8MqcSFkzR9bPNuXu/8ANHM3C4mcvHAY1/+uRkOxLmEaEgBab1zu3ellSnNH3zNi9YjhXVw1R4505kjM2r5O6Oe7UdxtI50axFdxcU6JpfFT++kEsiVJZyqqBzVKjFDVww8ReH9U9BK0ibDfmhO+7IFNNODVCcXHicf/pRjh+Y0Z/OY/IVbpdenS9YL/vZIVdVXKwf/oZzMiIAvoZ93KgIY0pzqZitXo7jNuAIYlMX6X0vHrx0TK6nJh4OOOhUwGDnsSg4GaYRs8GZPw= IBM_ENTITLED_REGISTRY_USER : AgCVxkhumBCyp3nGLuOhFKo4o03NwB7audQ2DJruzO7uscFTBcktzeDrMKmdwOfZkqyhb8zNpnxyZwZzdqP4TgTTLVr71c1YqYo+tMd+UEYO1+QdCKGi7+IzJYusMKSgLcNshuAZfhKqLybGyOOK6qv+EmIjJ5GuPKgZgKsMEvlSHswp4pWYEfC+sokwWV8t3APRb1RR3B0Lx40dLduJ8+QL4i4Qcu267vCqa6HK5423b/ryjF+FBAlpUELiK9BX2OQ7BtT/en/zbDW1tj0hdsOu8BD1JvTmY/ueRTF+CQ+dU5dPQCvGIZCDQEAqhr0jjMp9eL+JQMiBEjFlR6AYf1IFNk1SkaD68f8mSLy+eLn3qfc365shIBQtuGF9x/bpLgMbuHofEgO3/qvXy4WxNxqwsGVlNl3bYd6JrLf+SnOR+NmC69k/cRJVs/6wjod/lJrDHsWofc07paADcSRVJ1aDv5E77ibuJSRlw8Zuyu4S445/8/m0x3Fgc9WW69G7pBAYRyN7w83jaGMf/2pJVcqUPu06jFeCwvGOXwRbK/A0f76gG/CWzM3zMSzr5CqyNaiAJN7GFdv3xuFkCpKpSsRu7jYP6PSS+0G0YJ03IAXj9smxmLKe0ZCEYxBetzmhiMychAZTbUSLtXDscbdYZlkcZct/E9lqOM3hl8FuPNDchS1VRDVbcsQUohx5oqtwJSukgQ== template : metadata : creationTimestamp : null name : ibm-entitled-registry-credentials namespace : ci type : Opaque However, notice the following differences: apiVersion: and kind: identify this as a sealed secret. .dockerconfigjson has been encrypted using the public key of the sealed secret controller, meaning that it can only be decrypted by this controller. This sealed secret is safe to store in our GitOps repository, because it can only be decrypted by the sealed secret controller that was used to encrypt it. We haven't applied this sealed secret to the cluster yet -- we create the YAML file so that we can use the GitOps process to deploy it. Install the entitlement key into the cluster We deploy this sealed secret into the ci namespace in the cluster. To do this, we need to place the sealed secret in the GitOps Application repository, within the folder mq/environments/ci/secrets . mv ibm-entitled-registry-credentials-secret.yaml mq/environments/ci/secrets/ Pushing these changes to our GitOps repository will apply them to the cluster: git add mq/environments/ci/secrets git commit -s -m \"Deploy the IBM entitled registry sealed secret\" git push origin $GIT_BRANCH","title":"Configure connection to the IBM Entitled Registry"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic1/#configure-connection-to-access-artifactory","text":"The pipelines we install later in this tutorial runs in the ci namespace store their build output in Artifactory. To do this they need a secret that it initially created in the tools namespace when Artifactory is installed. Create the YAML for the artifactory access secret and seal the artifactory access secret Issue the following command: ./mq/environments/ci/secrets/artifactory-access-secret.sh This script performs the below actions. It initially creates the YAML for a regular secret by using the artifactory-access secret from the tools namespace; it creates the YAML, but doesn't apply it to the cluster. Then, it seals the artifactory-access secret. We can examine the sealed secret YAML using the following command: cat artifactory-access-secret.yaml See how a sealed secret is very similar to a regular secret in structure: apiVersion : bitnami.com/v1alpha1 kind : SealedSecret metadata : creationTimestamp : null name : artifactory-access namespace : ci spec : encryptedData : ARTIFACTORY_ENCRYPT : AgCklDmp+y4VmnErpKOLGEijzgc0SiZjpJQg46BGfb78MnkY3ZEyH752YDnNp9/JBvsSPL20agoQabXYczVJP/JogWPbhPbZoDMubTjPq7Lf63CSkF1WFsMyYrVpPApWrsLL3YTeh4cXYqwCowZUfLWrTIAKduCkI2L2Fo25HSNY3ovBnxbzofHK6u2CAc7BBzAJ7O3T4mK1tKY67CCwFYkTXqfG5ECIF67HKjLhnj7rJGQ+bbjYgQ1RYR04adYGBBYgIQvWzXBQgz/8XuWs+Utk0rtWuAyUfEFehJfOXc/4iOSoctnyaZtodlFqnyEgajSUE/HHzy6QJwdVwls/necjbBzvYNO3OpQg/Vb3h+owTikAzzleNTMtAV6628AFV7f+20UnAn7xtoLMEjZ6pHu12DVfmjQ8mG9zNqo5/M2czWFQPLpmmf3KAaEbGjAQHV8tBvDRaaR389BS5w95QOrigJG5w+reKA5bosGdCQintPol+cVafIYxsqJDDI40qPPIlVOX/wlqHKvzD6UacXIFywcjjYGNIATo9Zg3PQfnTjnCzTs46oNswF7N7rUnXrg4KOcUDv18QuTR3brzIs+ccWdDLS+ScnRqxoOQp0x+D+vT/8EsdF0+MrfBPc95eLCJu1OjUULhVOVv/pCOaZCULrhdSqYlCf4BjhNSEtTrvU9lUg4bBVm4X/Q7nyjZ5GLqs5QSSfLKkaRE49M4BAamPRGXxNQcur9TFl44mIEev/Tkap0YAg== ARTIFACTORY_PASSWORD : AgBBkfKFEYXPYxHCY5ggrpSynRT+10boLkD5+udB4YXFU7MPqFoXe8z+NPnDDKEhsVBWNhUyqp2M7RjyC0BUFMDgGevMOCNOdg4pM+0vuf/pGjPVHSqUK48qoRX3gs8QBdTXRBnfTfqd59GTFRhdMQ7tp5ztZ8TawlAnzWbdfWSdxya+fKSQxVG1WDamnpcRMI5zHJkAyxz4689yXE5eS/ughkNA35MB2BZdrAaM1qVJ/9ZaV5nnwYeZTTGp6i5s8bZoqz0a/rlYbdbtHYvRoCPxnbAelwCu6bkvkXc424fA9m4h5JPWxSfF4TMCLG7eKngJ7DJZ3RRRc1aFIQb8BMfFmA+z/AHsup/KcLSrjWcTWaekNdf6At+iHDr5tXhQjyYedtlulwhgH+dUa5bh6jisdqcf0lnjJLSaLswZvFtWuXj3MbB4aQ1jlHpp39I/EVpHKTYbUcVZWZJAf9AlhJ+ml16FLYHzjBDXCdv34Km3RcKQW4xo8aolgbI2X8Ph6EFA3Pwv6qGAD/r65YkTsgssdgLCgxw/4dKnZbbfdoCqPq0paeERPWk/Ss6y8g86vMNPiVlMDbVaHYY6Fi4tmCSmxTFzNT2idox4t7PaHAuDtFwwyMQbMNlNPKxvjFyaEqjdZ4l2UEmpjyZHcNv3Tg4c+OL5OLmPJ29kB2/AzbS0khtgZggflTZfvlCECVio15iOVL7dIOCk+CMWPN878RfpWZCCow== ARTIFACTORY_URL : AgB7K7Ul7JJwH4D/UlWxEtVQ6R9FAtZzhiYPxgSHIAITt9QIdQ3rYvrQG4H4mLzMkQY+2jd6CBYDRhHy5H3gGLi1wr4W71BQ5c0mwA5R9XL/IKbbJhnqXCKF07LAbiHqFucd0Uk2k68u9SxLmxNr4f6o+GuBfaDSJ2QeM4xqT1h/vmdR/NtErMg0xaewiB37w8FwjTpHRH9EpfODWXToiBNCKVF10WA7D4WyLYK7P227b06rUnckJ6jnTbz+JASlTnHngNkhflOsNZrjFFrSbSmH+i6PITM8QGBaPlMoqIFeI3rIlHOuQ2wQ/7fSZWu3pZZ/EWMkh0WpmlrEAXwpoL1mzc6ZMrqWWNZJeJ5WoywiJ2skTx/sC7GbWGN4u/kEwt5TY4g761Yz+9ctUcpdVpFyjTxSjWk21rr9G6NP3i58J3oj7zKakyWXcV11zhidhrtOHCHdgg0COy1Wrx3QoQgD6EdLXJ6KbHcXjcSQd9vbxp39g4Zw3MyPKB4S5ceZXNGcrzpxP2PNE8e4SqHCEpqMjI137X1W7XA1U2jddYjAHYnH9f8azG99b8hRheCUJsx8Gvc+EViaCc1pXXQ/ttrRVixgWUy3VU/5FpM5zEdYKFvTjp5sZ0VGYtJZl72eU4Y43GQ0UwQRFAT0511zleViqT6dzT2POeaSybR8BNyZuI4I3OHQEX/BAgvTXpzSSIjoS9oB+bOjqdOPh0YPB5BDch+Us1oDZPrOO9awO5qFdZnF24DO3Ra9+w== ARTIFACTORY_USER : AgCXJcAoBaSttkNHeVt3h1sXk8DRlYqXKM8Ah5uLIp7cGA2y5gVSJw6wh5aLaI5zdjeC6LpZG7pV9gjEEN130DFP2E3eAMZ99gB3CI/eLHEK7SfIfoCJjhpTcxJqrrfBttreCjTcXLoNjlOZPJJzM7Qsb12VBu2vwMJP3RyqOr0SPPZ7iVSDj0ZdPOzjslj/71Es0ooYze8u5AOHSa4gwYiPyuRy0jXTsPUBmSdxvL66qPzstiyaMqDkleE1j71t2KM7a9JmDGisJWw+bmlsZTAz9Zu1HDfl8uj3KdtZUhthmjIhssVK5obeLArMFx4UQgnhI1ZM1W8E2N+TuKmPWDvkLivIxpxCxy2vokbe3iZDLsi+NSkhDsfqdUq46bvb1OcTw6qoPkjWDALOFc1Jh8d5x6WQK/sPNicOptVt47OU2R5QhAS3+GsvFaeQqvry/MPus2Vna+E+okbiTAlcJbXo5Z8US0yTb+NmtdeuY5bVzDD0ptbRAgqDV1Lt/cMP9gNRJmKHf6lFa2H5MU0nBpWi5SMiwKtVSAoUvhJGU+WwijF32XrrOaySIHfL4OfFULnChEzFjgrEthgryVIirJ9lSDeoL7ziV2pJBMiVx98UiFEBGYiRupxV9InDxR7t/kyXYhJWrVXmakl06LSbRl+9Ist8jj7LIzK6AwZCJt1DyUvMun+ekGJWVhKETl1ulJKRMXUDgg== template : data : null metadata : creationTimestamp : null managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:data : . : {} f:ARTIFACTORY_ENCRYPT : {} f:ARTIFACTORY_PASSWORD : {} f:ARTIFACTORY_URL : {} f:ARTIFACTORY_USER : {} f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:type : {} manager : kubectl-client-side-apply operation : Update time : \"2021-09-15T12:40:59Z\" name : artifactory-access namespace : ci type : Opaque This sealed secret is safe to store in our GitOps repository, because it can only be decrypted by the sealed secret controller that was used to encrypt it. We haven't applied this sealed secret to the cluster yet -- we create the YAML file so that we can use the GitOps process to deploy it. Install the artifactory access secret into the cluster We deploy this sealed secret into the ci namespace in the cluster. To do this, we need to place the sealed secret in the GitOps Application repository, within the folder mq/environments/ci/secrets . mv artifactory-access-secret.yaml mq/environments/ci/secrets/ Pushing these changes to our GitOps repository will apply them to the cluster: git add mq/environments/ci/secrets git commit -s -m \"Deploy the artifactory access sealed secret\" git push origin $GIT_BRANCH","title":"Configure connection to access Artifactory"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic1/#configure-the-pipeline-for-qm1-source-repository","text":"Now that we have the ability to create queue managers in our cluster, let's configure the queue manager Tekton pipeline we are going deploy later in this tutorial to use the source repository for QM1 we created earlier as well as the GitOps repository where its built configuration will be stored before deployment by ArgoCD. To run the pipeline, we need to create a configmap and secret that contain the Github repository location, and GitHub access token. Once we've deployed these to the cluster, we can run the pipeline. Change to the Application GitOps directory Let's ensure we're in the correct folder. Again, we've used typical defaults: cd $HOME /git cd multi-tenancy-gitops-apps Verify that your $GIT_ORG and $GIT_BRANCH environment variables are set: echo $GIT_ORG echo $GIT_BRANCH For example: ( base ) anthonyodowd/git/multi-tenancy-gitops-apps echo $GIT_ORG prod-ref-guide ( base ) anthonyodowd/git/multi-tenancy-gitops-apps echo $GIT_BRANCH master If either is not set, use the instructions below. export GIT_ORG = <replace_with_gitops_apps_repo_organization> export GIT_BRANCH = <replace_with_gitops_apps_repo_branch> The sample configmap The GitOps repository contains a template for the configmap used by the pipeline to access GitHub. Issue the following command to view the template of the configmap YAML: cat mq/environments/ci/configmaps/gitops-repo-configmap.yaml_template apiVersion : v1 kind : ConfigMap metadata : labels : app : multi-tenancy-gitops group : pipeline type : git name : gitops-repo data : branch : ${GIT_BRANCH} host : github.com org : ${GIT_ORG} owner : ${GIT_ORG} parentdir : . protocol : https repo : multi-tenancy-gitops-apps url : https://github.com/${GIT_ORG}/multi-tenancy-gitops-apps.git Many of these YAML nodes will be customized by the script we run now. cd mq/environments/ci/configmaps/ ./gitops-repo-configmap.sh cd ../../../../ Once, this script is run successfully, you should see a new file named gitops-repo-configmap.yaml . Issue the following command to view the configmap YAML we just created: cat mq/environments/ci/configmaps/gitops-repo-configmap.yaml apiVersion : v1 kind : ConfigMap metadata : labels : app : multi-tenancy-gitops group : pipeline type : git name : gitops-repo data : branch : master host : github.com org : hp-gitops-test owner : hp-gitops-test parentdir : . protocol : https repo : multi-tenancy-gitops-apps url : https://github.com/hp-gitops-test/multi-tenancy-gitops-apps.git For example, branch: , host: , org: and owner: , these values will be used by the queue manager pipeline to update the GitOps repository after a successful pipeline run. Create GitHub access token As well as the location of the GitHub repositories it uses, the pipeline will also need a GitHub personal access token to identify it as a valid user to GitHub. A Personal Access Token is used in place of a user password to authenticate with GitHub. This token provides specific access to all repositories owned by a particular GitHub user or organization. When GitHub is accessed using this token, the only permitted operations are those defined by the specific token. Navigate to your GitHub Developer Settings to generate a new token. Click on Generate new token and make the following selections: Name the token CI pipeline Select public_repo to enable git clone Select write:repo_hook so the pipeline can create a web hook After you click Generate token to create the access token with these permissions, the GitHub UI will let you see the generated token once, but never again. Therefore, save the token somewhere safe -- so that you can use it later if required. In the meantime, let's store the token in the $GIT_TOKEN environment variable. export GIT_TOKEN = <personal access token> Also, store the git user information in the $GIT_USER environment variable. export GIT_USER = <git user name> Create a regular secret to hold the GitHub access credentials and seal the credentials As before, we're going to create a sealed secret to store the personal access token previously created. To do this, we first create the regular secret YAML, and then seal it with kubeseal . The secret will contain two literals, username and password , which contain our GitHub username and personal access token. We will store this secret in the ci namespace. Make sure you set the GIT_USER and GIT_TOKEN environment as shown above. Create the regular secret YAML with the following command: ./mq/environments/ci/secrets/git-credentials-secret.sh This script performs the below actions: It initially creates the YAML for a regular secret by using the git username and access token provided; it creates the YAML, but doesn't apply it to the cluster. Then, it seals the github credentials secret. We can examine the sealed secret YAML using the following command: cat git-credentials-secret.yaml Notice the kind: SealedSecret in the YAML: apiVersion : bitnami.com/v1alpha1 kind : SealedSecret metadata : creationTimestamp : null name : git-credentials namespace : ci spec : encryptedData : password : AgCufC++OG16tYVA5cs7v8uE9ed/ihpkdYYXSlrtmdlQfwjHzttj97X32tq4NaXzF64GIVCzr2vnweDCYIC+sd0SSs9b//KpdNgiZ2AHmJBdhhnmGs0PHHEgh16QmDlxV3on+Ewe6uM5MmbV/bpmRHu9L3spF+kcRdTfdpsQegpz7+QlR9Owvgt0LbZYiNwGzkwnczRtRhG8DZC6FodH95t7KoXN+fjupGJFS2GQo55e0QbhgdgPDLHYrZo8pQyvQVG+PsK3bn6OiJbB9nLsY4xV+r38Qg6BrVGW+kY2oqAHT0BR+4KljIWdSVMXoG9zU7HNU1H6R1nFB+8wblHF9ySaKxObIPhjxMEjD3ieOg7WjuySWDtIL2PF5OGuY8B2m/totM1CKWCJZ+RT0X8y98NjqR03QPD7AKz4JgL8Cerkq4JojroANNyIDl9iiwjEd7on7d2CmzLSWX2STXw0LuqfjMoId3K2l/THpwpbhpnX2ZDJHiSUtJ+mepjFYyJrj4hCm4dXGKABVE7rilQvLaS5paZPx3uUC7eZqqdo+pz3dd/+o4R+PCQTCTKEzEnrGZCKISff6pMMgDHx7RsotGDbyAEqrpK2MuwxzIV/lxeQEA6qZeSS65sndOHTZlgMCGeRazkybDxc4PMcuEqbXM2e9cDwzf7cqyCL+czCjYPF0PRA5kaYBgTZiLUUfnUlVtttCStBdUOKiQ54wMj8mjFCkfnbMnhgFCrylLhmGiq5Oq4nUPQ4GQh/ username : AgCc4ywhZ7MDcrKNbGNKeItPvg343PwycBGMiTESVtNgWGdcDOhMUzx2T1Het+JLFbfCQDV4Ewh1Jv/CC+4yjF87YBUu6vu+dTFM05M59mEj9PgG89d/Xpf/H2St1z8ZoqQyYKIF90W3ji4HvMzQv/RHC3dECTFQI5Jh2YmA7U0XyQjWb+YxSQtRFDM/E2CTXoYZD5ZK+J8emWzCk0F5/1bdUt+Q/IHoJg5LsPgjZEVnv7Wd+8Gn0UKQE/BYqfInU6rrS1Sc5ogQvWnd73LeN2gViqriwiTgjA2LlNK4JWkimYifDgt0ZEOdaJ7XjyFznq42vYL+KbY1oeL1dKMuh1bFv0m9796CHsYjYahLknXbN1FztGfh1gswhP97mPrp0Fq5x0qF8ZbTFz/GeH8TzN4VlsaKzvApxQDBV/FHCZQ0n5xLlSLpuDHKDXgEbD+Hc8Nfz18aUgBC/Tgm4qcObjg1Px7ASbEG5m4uxZb30Qp2R4tW7NFIzAd1EbQSaN+cPrsmlcRNoE6lU2bpWrW9e1HnUQiZzZp1/W8iqBkpb7Dl9bR8XtfxB3CK03rj72dBSOJseQruNXn7tfhMf5vgxwEppC3rSR1eTPuXFdiI2tbLvMawNtSASiQB23MVrqXdjV0gXCM9cf3XJA8mIzppc8iE3T7z1WmkLhlAR66gkByrOyemKgw4q9T+aSSpE2HQtemwp8SbbUWpCPcF template : data : null metadata : annotations : tekton.dev/git-0 : https://github.com creationTimestamp : null name : git-credentials namespace : ci type : kubernetes.io/basic-auth Also notice how the username and password are not just obfuscated, they are fully encrypted. (If you'd like, you can use the base64 -D command from above -- it will not reveal the original username.) Install the sealed secret into the cluster Again, we deploy this sealed secret into the ci namespace in the cluster. To do this, we need to place the sealed secret in the GitOps Application repository, within the folder mq/environments/ci/secrets . mv git-credentials-secret.yaml mq/environments/ci/secrets/ Pushing these changes to our GitOps repository will apply them to the cluster: git add mq/environments/ci/secrets mq/environments/ci/configmaps/ git commit -s -m \"Deploy the github credentials secret and configmap\" git push origin $GIT_BRANCH","title":"Configure the pipeline for QM1 source repository"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic1/#deploy-applications-to-the-cluster","text":"Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Deploy the applications layer in the GitOps repo Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy apps resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the changes will be applied to the cluster.: git add . git commit -s -m \"Intial boostrap setup for applications\" git push origin $GIT_BRANCH The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the changes: Select resources to deploy Access the 0-bootstrap/single-cluster/3-apps/kustomization.yaml : cat 0 -bootstrap/single-cluster/3-apps/kustomization.yaml Open 0-bootstrap/single-cluster/3-apps/kustomization.yaml and uncomment the following: resources : #- argocd/ace/cicd.yaml #- argocd/ace/dev.yaml #- argocd/ace/stage.yaml #- argocd/ace/prod.yaml - argocd/mq/cicd.yaml - argocd/mq/dev.yaml #- argocd/mq/stage.yaml #- argocd/mq/prod.yaml #- argocd/apic/cicd.yaml #- argocd/apic/dev.yaml #- argocd/apic/stage.yaml #- argocd/apic/prod.yaml #- argocd/apic/single-cluster.yaml #- argocd/apic/multi-cluster-app.yaml #- argocd/apic/multi-cluster-ops.yaml #- argocd/bookinfo/cicd.yaml #- argocd/bookinfo/dev.yaml #- argocd/bookinfo/stage.yaml #- argocd/bookinfo/prod.yaml #- argocd/soapserver/soapserver.yaml #- argocd/cp4a/cp4a.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications\" patch : |- - op : add path : /spec/source/repoURL value : https://github.com/gitops-workflow-demo/multi-tenancy-gitops-apps.git - op : add path : /spec/source/targetRevision value : master Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying apps\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The applications argocd application Let's examine the ArgoCD application that manage the applications in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the applications application: The apps-mq-rest-ci-1 argocd application Let's examine the ArgoCD application that manage the applications in ci namespace . In the ArgoCD UI Applications view, click on the icon for the apps-mq-rest-ci-1 application: You will see all the necessary configurations. If you observe carefully, you will see all the sealed secrets and configmaps we created earlier under this application. Examine the fully configured queue manager dev pipeline We've now fully configured the queue manager pipeline for the dev namespace. In the next section of this chapter we're going to run this pipeline to build QM1 and deploy it to the dev namespace. Let's have a quick look at the pipeline in the OpenShift console. From OpenShift console, select Pipelines > Pipelines . Then select Project:ci . You'll see the below pipelines; Select mq-infra-dev pipeline: We'll explore this pipeline more fully in the next topic of this chapter. Congratulations! You've now got a fully functioning MQ queue manager pipeline. In the next topic of this chapter, we're going to use this pipeline to deploy a fully tested queue manager QM1 to the dev namespace. We'll explore the pipeline, tasks and steps in more detail to see exactly how they work.","title":"Deploy applications to the cluster"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic2/","text":"Running the Pipeline \u00b6 Overview \u00b6 In the previous topic of this chapter, we installed and customized the queue manager repository for QM1 and configured the queue manager pipeline to access it. In this topic, we will perform our first full iteration of continuous integration for QM1 . We'll run the queue manager pipeline to build and test the queue manager QM1 . The successful pipeline run will store a new Helm chart representing QM1 in the the GitOps repository, ready for deployment. Look at the following diagram: We've highlighted the components we're going to explore in this topic: The QM1 Git repo that contains the source definition for queue manager QM1 . It includes the queue manager qm.ini and MQSC files, as well as its default Helm configuration. The queue manager pipeline that will use the QM1 repository as input. It will perform a set of tasks to build and test QM1 . If successful, the pipeline stores updated artifacts in the Image registry, GitOps apps repository and Artifactory as required. The Image registry that will store the newly built version of the queue manager image. The GitOps apps repository that stores the latest good build and test of QM1 represented as a Helm chart. This chart will be subsequently used by ArgoCD to deploy QM1 to the cluster. The Artifactory repository that stores a base Helm chart containing all the default values for QM1 , including its name, container image for example. These can be overridden by the GitOps Helm chart when required. In this topic, we're going to: Run the queue manager pipeline to build and test QM1 . Explore the queue manager pipeline. Explore the Artifactory Helm chart. Explore the GitOps apps repository Helm chart. By the end of this topic we'll have a fully built and tested queue manager QM1 ready to deploy to the cluster. Pre-requisites \u00b6 Before attempting this topic, you should have successfully completed the previous topic . Run the queue manager pipeline \u00b6 As we can see in the diagram above, the queue manager pipeline is responsible for building and testing QM1 from a source repository. If successful, it stores updated artifacts in Artifactory and the GitOps apps repository, which are used by ArgoCD to deploy the updated QM1 to the cluster. It's usually the case a pipeline runs automatically whenever a source repository changes. However, our first pipeline run is manual so that you can be in control, making it easier to understand what's happening. The queue manager pipeline performs a set of operations called tasks . Each task is responsible for a specific function such as building a queue manager image, or unit testing a queue manager. In turn, each task comprises a set of individual steps . For example, the build task might comprise three steps: Clone a source repository git, Build an image with buildah and Store an image in an image registry. If you'd like to know more about Tekton pipelines, here's an introduction . You will also find this video very helpful. Come back to these later if you'd like to keep going now. Locate the ci pipelines in the web console Let's find the queue manager pipeline using the OpenShift web console. Navigate to Pipelines->Pipelines in the left hand pane, and select Project: ci , to show all pipelines in the ci namespace: You can see all the pipeline that we installed into the ci namespace in the previous chapter. We'll use different pipelines for different activities throughout the tutorial. For this topic, we're going to use the mq-infra-dev pipeline. When applied to the QM1 source repository, this pipeline will build and test a new instance of QM1 ready for deployment to the dev namespace. The mq-infra-dev queue manager pipeline Select the mq-infra-dev pipeline from the list of all pipelines: Like all pipelines, mq-infra-dev is composed from a set of tasks : setup build smoke-tests-mq tag-release image-release img-scan helm-release gitops The task name often provides a strong hint of each task's specific function. We'll examine some of these tasks in detail as the pipeline runs. The oc command as an alternative to the Web console As well as using the OpenShift web console, you can also interact with pipeline using the oc or tekton commands. Ensure you're logged in to cluster, and issue the following command to list the mq-infra-dev pipeline details: oc get pipeline mq-infra-dev -n ci which shows a brief summary of the pipeline: NAME AGE mq-infra-dev 46h You can get more detail by adding the -o yaml option; we'll do that later. We use the command line and the web console in the tutorial so that you become familiar with both. As a general rule, you can do the same things in the command line as you can with the web console. The web console tends to be better for simple interactive tasks; the command line tends to be better for scripting and bulk tasks. Configure your first pipeline run Every time we run the mq-infra-dev pipeline, we create a new pipeline run . We can think of a pipeline run as an instance of a pipeline. Pipeline and PipelineRun are new custom resources that were added to the cluster when we installed Tekton. In pipeline details view above, you'll see an Actions button. Select Start to configure a pipeline run. You'll be presented with the following dialog: The supplied arguments allow the user of the pipeline to configure its behavior. For example, a user can use this pipeline with different queue manager source repositories. Configure the run as follows: Set git-url to your fork of the mq-infra repository Set git-revision to master . (Later in the tutorial, we will use a new branch.) Set scan-image: false (temporary fix while issues with UBI are resolved) Hit Start to start the pipeline run. You can also use the command line to run a pipeline; we'll explore that option later. Watch a pipeline run executing The pipeline run has now started. Notice how the view changes to Pipeline Run details : We're now looking at the live output from a pipeline run, rather than the pipeline used to create the run. Notice that the pipeline run name mq-infra-dev-44f3v5 is based on the pipeline name -- with a unique suffix. Every new pipeline run will have a unique name. See also how the first setup task is running, while the remaining tasks are waiting to start. Hover over setup or build task and you will see the steps that comprise it. Watch pipeline steps complete As the pipeline run proceeds, notice the build step complete and the smoke-tests-mq step start: This pipeline will take about 15 minutes to complete. While it's doing that, let's explore its tasks in a little more detail. List the pipeline run from the command line You can also explore the pipeline run from the command line. Issue the following command, replacing mq-infra-dev-44f3v5 with your pipeline run name: oc get pipelinerun mq-infra-dev-44f3v5 -n ci -o yaml which will show the details of your pipeline run: apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : ... spec : ... startTime : \"2021-07-27T07:34:06Z\" taskRuns : mq-infra-dev-44f3v5-build-gw5bm : pipelineTaskName : build status : completionTime : \"2021-07-27T07:35:51Z\" conditions : - lastTransitionTime : \"2021-07-27T07:35:51Z\" message : All Steps have completed executing reason : Succeeded status : \"True\" type : Succeeded podName : mq-infra-dev-44f3v5-build-gw5bm-pod-w8chs startTime : \"2021-07-27T07:34:35Z\" steps : - container : step-git-clone imageID : quay.io/ibmgaragecloud/alpine-git@sha256:70f7b70d9324314642b324ac79655862656255dd71b4d18357e5a22f76215ade name : git-clone terminated : containerID : cri-o://256775d430edcc5c4e87fa57d3029b9776ea6dcff7c7e80ae306c1aee49aab06 exitCode : 0 finishedAt : \"2021-07-27T07:34:57Z\" reason : Completed startedAt : \"2021-07-27T07:34:56Z\" - container : step-build imageID : quay.io/buildah/stable@sha256:04803d2144a2df5bf3aa2875f130e2b6cfc6ee45003125dc4df13f05f0898f9a name : build terminated : containerID : cri-o://043c8661b91bf02e3f726a9ab9f5bc445f9b7bf5c354070e0d432f5914b55810 exitCode : 0 finishedAt : \"2021-07-27T07:35:50Z\" reason : Completed startedAt : \"2021-07-27T07:34:57Z\" taskSpec : params : ... stepTemplate : name : \"\" resources : {} volumeMounts : - mountPath : $(params.source-dir) name : source steps : - env : - name : GIT_PASSWORD valueFrom : secretKeyRef : key : password name : git-credentials optional : true - name : GIT_USERNAME valueFrom : secretKeyRef : key : username name : git-credentials optional : true image : quay.io/ibmgaragecloud/alpine-git name : git-clone resources : {} script : | set +x if [[ -n \"${GIT_USERNAME}\" ]] && [[ -n \"${GIT_PASSWORD}\" ]]; then git clone \"$(echo $(params.git-url) | awk -F '://' '{print $1}')://${GIT_USERNAME}:${GIT_PASSWORD}@$(echo $(params.git-url) | awk -F '://' '{print $2}')\" $(params.source-dir) else set -x git clone $(params.git-url) $(params.source-dir) fi set -x cd $(params.source-dir) git checkout $(params.git-revision) - env : The full output is very large; we've abbreviated it significantly. This output contains the same information displayed as we see in the web console. Indeed the web console view is graphical representation of this information. Here's just a few of the interesting things that you can see: spec.startTime: identifies when the pipeline run started pipelineTaskName: build holds lots of interesting information on the build task. For example, we can see the build start and completion times and that it succeeded . steps identifies lots of interesting information on each of the steps within the build task. For example we can see the git-clone start and completion times, and that it also succeeded. There's no need to try to understand all this output right now; we're going to do that during this topic. Moreover, we'll make extensive use of the web console because it's easier to see what's happening. However, as we do this, you might like to link the console display back to this PipelineRun output. Let's continue exploring while the pipeline run completes. Explore the pipeline \u00b6 Let's look more closely at how the mq-infra-dev pipeline is structured as the pipeline run proceeds. Let's also examine the tasks and steps that make up the pipeline, and how they progressively build and test a queue manager, resulting in the production of a Helm chart ready for deployment. The mq-infra-dev pipeline run may still be running, so you'll be looking at a live pipeline run. Don't worry if the run has already completed, all the information remains available because its held in a PipelineRun custom resource for that run. Pipeline, Task, Step Let's start with the pipeline, its tasks and steps. Hover over the smoke-tests-mq task: See how our pipeline is made up of a set of tasks such as setup , build or gitops . These run in the order defined by the pipeline. Our pipeline is linear, though Tekton supports more sophisticated pipeline graphs if necessary. See how each task comprises a set of steps such as git-clone or setup . These run in the order defined by the task. The pipeline run logs When a pipeline runs, all its output is captured in a set of logs, one for each task. Click on the setup task to show its logs: ( Alternatively, you can select the Logs tab from the UI, and then select the tasks on the left pane within the pipeline run view. ) See how the setup task has its output in a dedicated log. You can select any log for any task that has completed or is executing. When a pipeline run completes, all its logs remain available, which can help diagnosing problems for example. Exploring an example task output: build It's easy to examine a task log; you simply select the relevant task and scroll the log up or down. For active tasks the log will be dynamically updated. Click on the build task: This console window shows the output generated by build task. As the task script proceeds, its output is captured; that's what we can see in this window. Notice that the build task output is from the first step in the build task. This step is called STEP-GIT-CLONE . Note how the step names are capitalized in the web console output. Let's look at another task and its steps more closely. Exploring a task step in detail : STEP-BUILD A task is built of multiple steps. Let's explore the build task and its step-build step. Select the build task and scroll through its logs to see its second step , STEP-BUILD : See how the step-build output is captured in the same log as the previous step git-clone . Each step runs in a separate container -- if you re-examine the PipelineRun YAML, you'll see those containers. When you scroll through the build task output, you're seeing what's happening in those containers. Read some of the log output to get a feeling for what's happening. Later, we'll see how step-build and step-git-clone are coded. The pipeline definition Up to this point, we've examined the pipeline run and the logs it generates. Let's now look at how a pipeline is defined. Issue the following command to show the mq-infra-dev pipeline: oc describe pipeline mq-infra-dev -n ci which shows the pipeline YAML in considerable detail: Name : mq-infra-dev Namespace : ci Labels : app.kubernetes.io/instance=ci-pipelines Annotations : app.openshift.io/runtime : mq API Version : tekton.dev/v1beta1 Kind : Pipeline Metadata : Creation Timestamp : 2021-07-26T11:46:57Z Generation : 1 Managed Fields : API Version : tekton.dev/v1beta1 Fields Type : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : f:app.openshift.io/runtime : f:kubectl.kubernetes.io/last-applied-configuration : f:labels : . : f:app.kubernetes.io/instance : f:spec : . : f:params : f:tasks : Manager : argocd-application-controller Operation : Update Time : 2021-07-26T11:46:56Z Resource Version : 2163788 Self Link : /apis/tekton.dev/v1beta1/namespaces/ci/pipelines/mq-infra-dev UID : 9bc6eaaa-a217-4bef-81fb-5ff30d5952e1 Spec : Params : Description : The url for the git repository Name : git-url Type : string Default : master Description : The git revision (branch, tag, or sha) that should be built Name : git-revision Type : string Default : true Description : Enable the pipeline to scan the image for vulnerabilities Name : scan-image Type : string Default : namespace-dev Description : environment Name : environment Type : string Default : 2-services/instances Description : Service Name : app-type Type : string Default : false Description : Enable security for queueManager Name : security Type : string Default : false Description : Enable ha for queueManager Name : ha Type : string Tasks : Name : setup Params : Name : git-url Value : $(params.git-url) Name : git-revision Value : $(params.git-revision) Name : scan-image Value : $(params.scan-image) Task Ref : Kind : Task Name : ibm-setup-v2-6-13 Name : build Params : Name : git-url Value : $(tasks.setup.results.git-url) Name : git-revision Value : $(tasks.setup.results.git-revision) Name : source-dir Value : $(tasks.setup.results.source-dir) Name : image-server Value : $(tasks.setup.results.image-server) Name : image-namespace Value : $(tasks.setup.results.image-namespace) Name : image-repository Value : $(tasks.setup.results.image-repository) Name : image-tag Value : $(tasks.setup.results.image-tag) Run After : setup Task Ref : Kind : Task Name : ibm-build-tag-push-v2-6-13 Name : smoke-tests-mq Params : Name : git-url Value : $(tasks.setup.results.git-url) Name : git-revision Value : $(tasks.setup.results.git-revision) Name : source-dir Value : $(tasks.setup.results.source-dir) Name : image-server Value : $(tasks.setup.results.image-server) Name : image-namespace Value : $(tasks.setup.results.image-namespace) Name : image-repository Value : $(tasks.setup.results.image-repository) Name : image-tag Value : $(tasks.setup.results.image-tag) Name : app-namespace Value : $(tasks.setup.results.app-namespace) Name : app-name Value : $(tasks.setup.results.app-name) Name : deploy-ingress-type Value : $(tasks.setup.results.deploy-ingress-type) Name : tools-image Value : $(tasks.setup.results.tools-image) Name : security Value : $(params.security) Name : ha Value : $(params.ha) Run After : build Task Ref : Kind : Task Name : ibm-smoke-tests-mq Name : tag-release Params : Name : git-url Value : $(tasks.setup.results.git-url) Name : git-revision Value : $(tasks.setup.results.git-revision) Name : source-dir Value : $(tasks.setup.results.source-dir) Name : js-image Value : $(tasks.setup.results.js-image) Run After : smoke-tests-mq Task Ref : Kind : Task Name : ibm-tag-release-v2-6-13 Name : img-release Params : Name : image-from Value : $(tasks.setup.results.image-url) Name : image-to Value : $(tasks.setup.results.image-release):$(tasks.tag-release.results.tag) Run After : tag-release Task Ref : Kind : Task Name : ibm-img-release-v2-6-13 Name : img-scan Params : Name : image-url Value : $(tasks.img-release.results.image-url) Name : scan-trivy Value : $(tasks.setup.results.scan-trivy) Name : scan-ibm Value : $(tasks.setup.results.scan-ibm) Run After : img-release Task Ref : Kind : Task Name : ibm-img-scan-v2-6-13 Name : helm-release Params : Name : git-url Value : $(tasks.setup.results.git-url) Name : git-revision Value : $(tasks.setup.results.git-revision) Name : source-dir Value : $(tasks.setup.results.source-dir) Name : image-url Value : $(tasks.img-release.results.image-url) Name : app-name Value : $(tasks.setup.results.app-name) Name : deploy-ingress-type Value : $(tasks.setup.results.deploy-ingress-type) Name : tools-image Value : $(tasks.setup.results.tools-image) Run After : img-scan Task Ref : Kind : Task Name : ibm-helm-release-v2-6-13 Name : gitops Params : Name : app-name Value : $(tasks.setup.results.app-name) Name : version Value : $(tasks.tag-release.results.tag) Name : helm-url Value : $(tasks.helm-release.results.helm-url) Name : tools-image Value : $(tasks.setup.results.tools-image) Name : environment Value : $(params.environment) Name : app-type Value : $(params.app-type) Run After : helm-release Task Ref : Kind : Task Name : ibm-gitops Events : <none> Don't be intimidated by this output -- it's actually just a more detailed source view of the information shown for the mq-infra-dev pipeline in the web console. Locate the following key structures in the Pipeline YAML: API Version: tekton.dev/v1beta1 and Kind: Pipeline identify this as a Tekton pipeline. Spec: Params identifies the pipeline input parameters Tasks: is a list of the tasks in this pipeline, each of which has A Name: naming the task A set of Params: identifying the task input parameters An optional Run After: value indicating when the task is run A Task Ref: identifying the actual task code to be run using the task's input Notice that there's no code in the pipeline definition; instead, the definition specifies the required inputs to the pipeline, as well as the set of required tasks and their order of execution. The code executed by each task is identified by Task Ref: , rather than in the pipeline; the pipeline definition merely defines the order of task execution and how parameters are marshaled between tasks. Let's now examine the pipeline definition in a little more detail. The pipeline input Spec: Params When we configure a pipeline run, the arguments map precisely to Spec: Params in the pipeline YAML file. Below, we've just shown the Spec: Params: for the mq-infra-dev pipeline: spec : params : - name : git-url description : The url for the git repository - name : git-revision description : The git revision (branch, tag, or sha) that should be built default : master - name : scan-image description : Enable the pipeline to scan the image for vulnerabilities default : \"false\" - name : environment description : environment default : dev - name : app-path description : Path in gitops repo default : mq/environments - name : security description : Enable security for queueManager default : \"false\" - name : ha description : Enable ha for queueManager default : \"false\" - name : git-pr description : Enable the pipeline to do a PR for the gitops repo default : \"false\" Spend a few moments mapping each of these parameters maps to those on the Start Pipeline input dialog where you specified the pipeline run arguments. For example, map Names: , Description: and Default: to the different fields in the dialog. Parameters for the first setup task We can see that the first task in the pipeline is called setup . Let's examine its YAML to see how it gets its input parameters: tasks : - name : setup taskRef : name : ibm-setup-v2-6-13 params : - name : git-url value : $(params.git-url) - name : git-revision value : $(params.git-revision) - name : scan-image value : $(params.scan-image) See how the setup task derives its git-url parameter using the pipeline input parameter value $(params.git-url) . See how git-revision and scan-image work in a similar way. The first task in a pipeline typically works like this -- its parameters are mapped from the pipeline's input parameters. Notice also that some pipeline input parameters are not referred to by the setup task; they will be used by subsequent tasks using the appropriate $(params.) value. Passing arguments between tasks As each task completes, the pipeline proceeds. When a new task starts it often requires one or more results generated by a previous task. We can see a good example of this in the build task: - name : build taskRef : name : ibm-build-tag-push-v2-6-13 runAfter : - setup params : - name : git-url value : \"$(tasks.setup.results.git-url)\" - name : git-revision value : \"$(tasks.setup.results.git-revision)\" - name : source-dir value : \"$(tasks.setup.results.source-dir)\" - name : image-server value : \"$(tasks.setup.results.image-server)\" - name : image-namespace value : \"$(tasks.setup.results.image-namespace)\" - name : image-repository value : \"$(tasks.setup.results.image-repository)\" - name : image-tag value : \"$(tasks.setup.results.image-tag)\" See how the build task specifies that the image-tag parameter should use value generated by the setup task using the syntax: $(tasks.setup.results.image-tag) . Also notice how the build tasks uses Run After: to specify that it should execute after the setup task. This follows the Kubernetes idiom of resources being declarative -- the order of execution is defined by RunAfter: rather than the order in which tasks appear in the YAML. Again, notice that the build task doesn't contain the code that the task executes. This is contained in Task Ref: which identifies ibm-build-tag-push-v2-6-13 as the task to execute using the specified parameters. It's the code in ibm-build-tag-push-v2-6-13 which generates the log output for the task; we'll examine it later. The way pipelines tasks are designed makes them highly reusable. As we'll see later, tasks are written with defined inputs and outputs such that they can be re-used by different pipelines. Pipelines focus on organizing the order of task execution and how parameters are marshaled into and between tasks; it's the tasks that do the actual work. Locating the pipeline and tasks source YAMLs Finally, let's locate the source for the mq-infra-dev pipeline and the tasks within it. It is located in the following folder: cd $HOME /git cd multi-tenancy-gitops-apps tree mq/environments/ci/pipelines/ We can see the other pipelines for the ci namespace in this folder: mq/environments/ci/pipelines/ \u251c\u2500\u2500 ibm-test-pipeline-for-dev.yaml \u251c\u2500\u2500 ibm-test-pipeline-for-stage.yaml \u251c\u2500\u2500 java-maven-dev-pipeline.yaml \u251c\u2500\u2500 mq-pipeline-dev.yaml \u2514\u2500\u2500 mq-spring-app-dev-pipeline.yaml These map to the MQ-related pipelines we saw in the Pipelines->Pipelines view in the web console. Exploring the mq-infra-dev source YAML View the source for the mq-pipeline-dev.yaml pipeline with the command: cat mq/environments/ci/pipelines/mq-pipeline-dev.yaml which shows the source YAML for the mq-infra-dev pipeline: apiVersion : tekton.dev/v1beta1 kind : Pipeline metadata : name : mq-infra-dev annotations : app.openshift.io/runtime : mq spec : params : - name : git-url description : The url for the git repository - name : git-revision description : The git revision (branch, tag, or sha) that should be built default : master - name : scan-image description : Enable the pipeline to scan the image for vulnerabilities default : \"false\" - name : environment description : environment default : dev - name : app-path description : Path in gitops repo default : mq/environments - name : security description : Enable security for queueManager default : \"false\" - name : ha description : Enable ha for queueManager default : \"false\" - name : git-pr description : Enable the pipeline to do a PR for the gitops repo default : \"false\" tasks : - name : setup taskRef : name : ibm-setup-v2-6-13 params : - name : git-url value : $(params.git-url) - name : git-revision value : $(params.git-revision) - name : scan-image value : $(params.scan-image) - name : build taskRef : name : ibm-build-tag-push-v2-6-13 runAfter : - setup params : - name : git-url value : \"$(tasks.setup.results.git-url)\" - name : git-revision value : \"$(tasks.setup.results.git-revision)\" - name : source-dir value : \"$(tasks.setup.results.source-dir)\" - name : image-server value : \"$(tasks.setup.results.image-server)\" - name : image-namespace value : \"$(tasks.setup.results.image-namespace)\" - name : image-repository value : \"$(tasks.setup.results.image-repository)\" - name : image-tag value : \"$(tasks.setup.results.image-tag)\" - name : smoke-tests-mq taskRef : name : ibm-smoke-tests-mq runAfter : - build params : - name : git-url value : \"$(tasks.setup.results.git-url)\" - name : git-revision value : \"$(tasks.setup.results.git-revision)\" - name : source-dir value : \"$(tasks.setup.results.source-dir)\" - name : image-server value : \"$(tasks.setup.results.image-server)\" - name : image-namespace value : \"$(tasks.setup.results.image-namespace)\" - name : image-repository value : \"$(tasks.setup.results.image-repository)\" - name : image-tag value : \"$(tasks.setup.results.image-tag)\" - name : app-namespace value : \"$(tasks.setup.results.app-namespace)\" - name : app-name value : \"$(tasks.setup.results.app-name)\" - name : deploy-ingress-type value : \"$(tasks.setup.results.deploy-ingress-type)\" - name : tools-image value : \"$(tasks.setup.results.tools-image)\" - name : security value : \"$(params.security)\" - name : ha value : \"$(params.ha)\" - name : tag-release taskRef : name : ibm-tag-release-v2-6-13 runAfter : - smoke-tests-mq params : - name : git-url value : \"$(tasks.setup.results.git-url)\" - name : git-revision value : \"$(tasks.setup.results.git-revision)\" - name : source-dir value : \"$(tasks.setup.results.source-dir)\" - name : js-image value : \"$(tasks.setup.results.js-image)\" - name : img-release taskRef : name : ibm-img-release-v2-6-13 runAfter : - tag-release params : - name : image-from value : \"$(tasks.setup.results.image-url)\" - name : image-to value : \"$(tasks.setup.results.image-release):$(tasks.tag-release.results.tag)\" - name : img-scan taskRef : name : ibm-img-scan-v2-6-13 runAfter : - img-release params : - name : image-url value : $(tasks.img-release.results.image-url) - name : scan-trivy value : $(tasks.setup.results.scan-trivy) - name : scan-ibm value : $(tasks.setup.results.scan-ibm) - name : helm-release taskRef : name : ibm-helm-release-v2-6-13 runAfter : - img-scan params : - name : git-url value : \"$(tasks.setup.results.git-url)\" - name : git-revision value : \"$(tasks.setup.results.git-revision)\" - name : source-dir value : \"$(tasks.setup.results.source-dir)\" - name : image-url value : \"$(tasks.img-release.results.image-url)\" - name : app-name value : \"$(tasks.setup.results.app-name)\" - name : deploy-ingress-type value : \"$(tasks.setup.results.deploy-ingress-type)\" - name : tools-image value : \"$(tasks.setup.results.tools-image)\" - name : gitops taskRef : name : ibm-gitops runAfter : - helm-release params : - name : app-name value : \"$(tasks.setup.results.app-name)\" - name : version value : \"$(tasks.tag-release.results.tag)\" - name : helm-url value : $(tasks.helm-release.results.helm-url) - name : tools-image value : \"$(tasks.setup.results.tools-image)\" - name : app-path value : \"$(params.app-path)\" - name : dest-environment value : \"$(params.environment)\" - name : git-pr value : \"$(params.git-pr)\" This YAML is slightly different to the output of the oc get pipeline command, because extra information is added during deployment such as Creation Timestamp: . Finding the ArgoCD application that manages pipelines You can see how the mq-infra-dev and other pipeline YAMLs were deployed by examining the ArgoCD application that watches the folder containing the ci namespace pipelines. Issue the following command: cat mq/config/argocd/ci/ci-app-rest.yaml which shows the apps-mq-rest-ci-1 ArgoCD application that watches for updates: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : apps-mq-rest-ci-1 annotations : argocd.argoproj.io/sync-wave : \"300\" finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : ci server : https://kubernetes.default.svc project : applications source : path : mq/environments/ci repoURL : https://github.com/hp-gitops-test/multi-tenancy-gitops-apps targetRevision : master syncPolicy : automated : prune : true selfHeal : true See how this apps-mq-rest-ci-1 watches: path: mq/environments/ci . As we know, this folder contains the YAML for the mq-infra-dev and other pipelines under pipelines folder. When this ArgoCD application was made active in the cluster, it installed all the pipelines along with other resources in this folder. Later in the tutorial, we'll examine how tasks are coded to perform their tasks, but for now, you should have a good feeling for how pipelines are structured and how they work. By now, the pipeline run should have completed successfully. In the next section, we're going to examine the Helm charts that it created. Understanding the QM1 Helm chart \u00b6 The successful completion of the mq-infra-dev pipeline run which used the QM1 source repository as input has resulted in three new objects being produced: A versioned container image for QM1 stored in the cluster image registry. A versioned Helm chart for QM1 stored in Artifactory. This chart contains a full default configuration for QM1 based largely on the Helm chart in the source repository. A versioned Helm chart for QM1 stored in the GitOps repository. This chart is created the first time the pipeline runs. It has a much sparser structure, and points to the full Helm chart stored in Artifactory. The following diagram represents this structure and process: This diagram shows how: The Helm chart stored in Artifactory points to the container image used by QM1 in the Image registry. This Helm chart contains the default configuration for the queuemanager custom resource YAML for QM1 derived from it source repository. This YAML refers to the container that was built during the pipeline run. This Helm chart stored in the GitOps repository merely refers to the full Helm chart stored in Artifactory. This Helm chart will be used by ArgoCD to deploy QM1 to the cluster. In this topic we're going to examine these Helm charts in more detail. We'll see how they are structured and how they are used. We'll see how, in combination, these Helm charts provide a well governed deployment of QM1 to the cluster. The source repository for the Helm chart Return to the terminal window you're using for the mq-infra source repository. ( Rather than the terminal window you're using for the multi-tenancy-gitops GitOps repository. ) In the previous topic, we forked and cloned the mq-infra repository. This contains the source Helm chart for queue manager QM1 . Let's quickly recap this Helm chart. Ensure you're in the correct directory: cd $HOME /git/mq-infra The source Helm chart Let's see how the source Helm chart for QM1 is structured. Issue the following command: tree chart/base to show the Helm folder structure and detail: chart/base \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 config.mqsc \u251c\u2500\u2500 security \u2502 \u2514\u2500\u2500 config.mqsc \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 configmap.yaml \u2502 \u2514\u2500\u2500 qm-template.yaml \u2514\u2500\u2500 values.yaml The key folders and files are as follows: The templates folder contains the templates that will be used to generate the queuemanager custom resource and configmap for QM1 . The values.yaml file contains a set of values that are used by these templates. For example, qm-template gets the queue manager name and CPU/memory limits from values.yaml . The config folder contains the MQSC definitions that will be placed in the config map used by QM1 when the queue manager is not configured for security. The security folder contains the MQSC definitions that will be placed in the config map used by QM1 when the queue manager is configured for security. Feel free to learn more about Helm charts before you proceed. Exploring Helm charts in Artifactory Every successful mq-infra-dev pipeline run stores a new version of the QM1 Helm chart in Artifactory. The Artifactory Helm chart is almost a verbatim copy of the source Helm chart. However, the pipeline adds vitally important data that is generated when the pipeline in run, and cannot be known ahead of time. This includes the name of the successfully built and tested container image, as well as chart version information. These calculated values are stored in the values.yaml file in Artifactory alongside the defaults from the source repository. You can access Artifactory via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n tools | grep artifactory | awk '{print \"https://\"$2}' This will list the route to the Artifactory, for example: https://artifactory-tools.xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the Artifactory UI. ( You can safely ignore any browser certificate warnings. ) Grab the user and password using the below commands. echo \"User: \" ; oc get secret/artifactory-access -n tools -o json | jq -r '.data.\"ARTIFACTORY_USER\"' | base64 -D echo \"Password: \" ; oc get secret/artifactory-access -n tools -o json | jq -r '.data.ARTIFACTORY_PASSWORD' | base64 -D Locating the Artifactory Helm chart We can locate the Helm chart produced by the pipeline in the Artifactory UI. On the left hand pane select Artifactory->Artifacts . Within this view, select generic-local->ci->mq-infra-0.0.1.tgz->mq-infra . Select the values.yaml file and View Source tab: Notice: The chart folder structure folder is exactly the same as the source repository. The chart is stored in a binary zipped form, indicated by the .tgz file extension. The chart has a version 0.0.1 -- each pipeline run will increment this value. The image: specified in the QM1 custom resource YAML has been updated to specify the exact image and version that should be used from the image registry. Let's have a more detailed look at the Helm chart for QM1 stored in Artifactory. Examine the Artifactory Helm chart Chart.yaml We'll start with the Chart.yaml file; it's the starting point for the QM1 Helm chart. Its contents are almost exactly the same as those in the QM1 source repository, but with one important difference. As we'll see, the chart version stored in Artifactory must be generated dynamically by the pipeline run. In the Artifactory UI, select the Chart.yaml file and View Source tab: apiVersion : v2 appVersion : 1.16.0 description : A Helm chart for Kubernetes name : mq-infra type : application version : 0.0.1 This file contains the high level information about the chart. The apiVersion is the Helm API version for the chart. We're using Helm 3, which requires v2 . The name field specifies the name of the chart. We've used base because the GitOps Helm chart will will be based on this one, inheriting most of its properties. This name could be more helpful -- QM1 base chart for example. The description could be more helpful -- Helm chart for QM1 , for example. The type field identifies whether the chart is a Helm application or Helm library. The QM1 chart is an application type -- a collection of templates (a queue manager and configmap) that are packaged and versioned for deployment. The version field specifies the chart version. Every time the mq-infra-dev pipeline run is successful, the version is incremented. The appVersion field is purely informational in Helm. The mq-infra-dev pipeline should keep appVersion the same as the version , but it's currently fixed at 1.16.0 . (This isn't helpful and will be addressed.) In summary, the most important aspect of Chart.yaml is its version . This version is stored within the chart, as well forming the chart name: mq-infra-0.0.1.tgz . Every time the mq-infra-dev pipeline run is successful, a new, versioned instance of the QM1 Helm chart is produced and stored in Artifactory. Examine the Artifactory Helm chart values.yaml The values.yaml file is created by the designer of the Helm chart to allow users to control its behavior. Its contents are used by the queue manager and configmap templates to generate deployable queuemanager and configmap YAMLs. These YAMLs are customized according by their corresponding templates using the values specified in values.yaml . To re-iterate, the values.yaml file has almost the same contents as the QM1 source repository, but with a few important differences. For example, the queue manager container image name stored in Artifactory must be generated dynamically by the pipeline run. In the Artifactory UI, select the values.yaml file and View Source tab: name : qm-dev version : 9.2.3.0-r1 web : enabled : true license : accept : true license : L-RJON-BN7PN3 halicense : L-RJON-BYRMYW use : NonProduction image : repository : replace tag : replace pullPolicy : Always queuemanager : debug : false imagePullPolicy : IfNotPresent livenessProbe : failureThreshold : 1 initialDelaySeconds : 90 periodSeconds : 10 successThreshold : 1 timeoutSeconds : 5 logFormat : Basic metrics : enabled : true name : QM1 readinessProbe : failureThreshold : 1 initialDelaySeconds : 10 periodSeconds : 5 successThreshold : 1 timeoutSeconds : 3 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : \"1\" memory : 1Gi storage : persistedData : enabled : false queueManager : type : ephemeral hatype : persistent-claim size : 2Gi recoveryLogs : enabled : false availability : type : SingleInstance hatype : NativeHA mqsc : configmap : mqsc-configmap name : config.mqsc ini : configmap : name : securityContext : initVolumeAsRoot : false template : pod : containers : - name : qmgr # env: # - name: MQSNOAUT # value: \"yes\" configmap : path : config pathWithSecurity : security cert : secretName : mq-server-cert terminationGracePeriodSeconds : 30 tracing : agent : {} collector : {} enabled : false namespace : \"\" security : false ha : false Notice how: Most of these values exactly correspond to the same literal values in the queuemanager custom resource YAML. For example, license: , resources and readinessProbe: are simple mappings to those found in a queuemanager custom resource YAML. Other values are not literal mappings; they are used by the queue manager or configmap templates in more sophisticated ways. For example security: false is used in combination pathWithSecurity: to configure a different MQSC file for QM1 depending on the security requirements. The logic to accomplish this processing is held within the template files. image: identifies the name and version of the queue manager container image to retrieve from the container registry. This value was set by the pipeline run, and corresponds to the image that was successfully smoke tested. A short digression on Helm templating As we've just seen with our security example, it's often helpful for the values.yaml file to contain a YAML node such as security: that is more than a literal equivalent to the YAML node found in a Kubernetes resource. We're going to take a short detour to explain how this works by exploring the templates/configmap.yaml file in Artifactory. In the Artifactory UI, select the /templates/configmap.yaml file and View Source tab: apiVersion : v1 kind : ConfigMap metadata : name : {{ .Values.mqsc.configmap }} data : {{ - if eq .Values.security true }} {{ - $path : = printf \"%s/*\" .Values.configmap.pathWithSecurity }} {{ (.Files.Glob $path).AsConfig | indent 2 }} {{ else }} {{ - $path : = printf \"%s/*\" .Values.configmap.path }} {{ (.Files.Glob $path).AsConfig | indent 2 }} {{ end }} This is a Helm template, and it uses the Go templating language. Learn more about Helm templates if you'd like; for now it's enough to have this basic understanding. In the example above, notice how: configmap name is derived from the values.yaml file using {{.Values.mqsc.configmap}} . .Values.security is used to control the configmap that is generated. You can also view the queue manager template. In the Artifactory UI, select the /templates/qm-template.yaml file and View Source tab: apiVersion : mq.ibm.com/v1beta1 kind : QueueManager metadata : name : {{ .Values.name }} annotations : argocd.argoproj.io/sync-wave : \"300\" helm.sh/hook-weight : \"300\" spec : license : accept : {{ .Values.license.accept }} {{ if eq .Values.ha true }} license : {{ .Values.license.halicense }} {{ else }} license : {{ .Values.license.license }} {{ end }} use : {{ .Values.license.use }} queueManager : {{ - toYaml .Values.queuemanager | nindent 4 }} {{ if eq .Values.ha true }} availability : type : {{ .Values.availability.hatype }} tls : secretName : {{ .Values.cert.secretName }} cipherSpec : ANY_TLS12_OR_HIGHER {{ else }} availability : type : {{ .Values.availability.type }} {{ end }} image : \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\" imagePullPolicy : {{ .Values.image.pullPolicy }} mqsc : - configMap : name : {{ .Values.mqsc.configmap }} items : - {{ .Values.mqsc.name }} {{ if eq .Values.ha true }} storage : queueManager : size : {{ .Values.storage.queueManager.size }} type : {{ .Values.storage.queueManager.hatype }} {{ else }} storage : persistedData : enabled : {{ .Values.storage.persistedData.enabled }} queueManager : type : {{ .Values.storage.queueManager.type }} recoveryLogs : enabled : {{ .Values.storage.recoveryLogs.enabled }} {{ end }} securityContext : initVolumeAsRoot : {{ .Values.securityContext.initVolumeAsRoot }} template : pod : containers : - name : qmgr {{ - if eq .Values.security false }} env : - name : MQSNOAUT value : \"yes\" {{ - end }} terminationGracePeriodSeconds : {{ .Values.terminationGracePeriodSeconds }} tracing : {{ - toYaml .Values.tracing | nindent 4 }} version : {{ .Values.version }} web : enabled : {{ .Values.web.enabled }} {{ - if eq .Values.security true }} pki : keys : - name : certificate secret : items : - tls.key - tls.crt secretName : {{ .Values.cert.secretName }} trust : - name : ca secret : items : - ca.crt secretName : {{ .Values.cert.secretName }} {{ - end }} Spend a few moments examining this template, to see how it uses the values.yaml file. Re-merging local clone to view Helm chart in GitOps repository The mq-infra-dev pipeline run updated the GitOps repository with the Helm chart. This means that our local clone of the GitOps repository is one commit behind GitHub. To allow us to view the Helm chart locally, we must re-merge our local branch with GitHub. Return to the terminal window you're using for the multi-tenancy-gitops-apps GitOps apps repository. ( Rather than the terminal window you're using for the mq-infra source repository. ) Issue the following commands to merge the local branch: git fetch origin git merge origin/ $GIT_BRANCH which shows our local branch being updated: Updating 0c0e5b9..8cf318d Fast-forward mq/environments/dev/mq-infra/Chart.yaml | 4 ++++ mq/environments/dev/mq-infra/requirements.yaml | 4 ++++ mq/environments/dev/mq-infra/values.yaml | 3 +++ 3 files changed, 11 insertions(+) create mode 100644 mq/environments/dev/mq-infra/Chart.yaml create mode 100644 mq/environments/dev/mq-infra/requirements.yaml create mode 100644 mq/environments/dev/mq-infra/values.yaml Notice how these files correspond to the new Helm chart created in the GitOps repository by the mq-infra-dev pipeline run. Explore the Helm chart in the GitOps repository Let's examine the newly produced Helm chart in the GitOps repository; it was created by the pipeline run. Issue the following command: tree mq/environments/dev/mq-infra/ which shows the newly produced Helm chart: mq/environments/dev/mq-infra/ \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml Notice that: The chart for QM1 was created in the mq/environments/ folder to reflect the fact this queue manager is part of the applications layer. The chart was created in the dev subfolder to reflect the fact that it's going to be deployed to the dev namespace. The chart was created in a new folder /mq-infra . This folder is dedicated to QM1 . A different queue manager would have a different folder. Examine the GitOps Helm chart requirements.yaml We need a Helm chart in the GitOps repository for a few reasons: ArgoCD requires a Git repository to watch for updates Git is our source of truth. Although Artifactory contains the base Helm chart, we link to it from Git. We can override the Artifactory Helm chart with a GitOps repository values.yaml file. For example, we might want to change the CPU or memory used by QM1 . This should not require a re-run of the mq-infra-dev pipeline because it not that QM1 has changed, but its environment . ( An example of a change that should require a pipeline run is an updated MQSC or qm.ini file. ) Helm makes it easy for one Helm chart to build upon an existing Helm chart using a requirements.yaml file. Let's see how the GitOps chart builds upon the Helm chart in Artifactory. Issue the following command: cat mq/environments/dev/mq-infra/requirements.yaml to see the requirements.yaml file: dependencies : - name : mq-infra version : 0.0.1 repository : http://artifactory-artifactory.tools:8082/artifactory/generic-local/ci See how this Helm chart identifies the dependent Helm chart in the Artifactory repository. This information effectively provides a link to the mq-infra-0.0.1.tgz zipped Helm chart we explored earlier. Examine the GitOps Helm chart values.yaml Recall that the values.yaml file in the QM1 source repository provides sensible defaults for QM1 . As we saw, the values.yaml file in Artifactory is largely a copy of this file. The values.yaml file in the GitOps repository performs a very different role; it allows these defaults to be overridden. Issue the following command: cat mq/environments/dev/mq-infra/values.yaml to see the GitOps values.yaml file: global : {} mq-infra : replicaCount : 1 See how sparsely populated this file is; it really just provides overrides and extensions to the mq-infra chart stored in Artifactory. The example provided of replicaCount is not a good example because its copied from the MQ application chart. Imagine a production scenario where we wanted to increase the CPU and memory requirements of the deployed QM1 . We could update the GitOps repository values.yaml file as follows: global : {} mq-infra : resources : limits : cpu : \"2\" memory : 2Gi requests : cpu : \"2\" memory : 2Gi This would result in the already deployed QM1 being re-deployed with these new values, but the base Helm chart in Artifactory wouldn't change. To support this practice, the mq-infra-dev pipeline will create an initial version of the GitOps value.yaml file, but subsequently pipeline run will not change it, as they assume that it is being used for GitOps operational overrides such as our CPU/memory example. Examine the GitOps Helm chart Chart.yaml Finally, we examine the GitOps Chart.yaml file. Issue the following command: cat mq/environments/dev/mq-infra/Chart.yaml to see the Helm chart details: apiVersion : v2 version : 0.1.0 name : mq-infra description : Chart to configure ArgoCD with the mq-infra project and its applications Notice that this chart is similar in structure to the Helm chart in Artifactory. However, it's important to realize that its version is independently maintained. For example, if we modified the GitOps values.yaml file to override CPU and memory, the GitOps Helm chart version would be incremented, but the Artifactory base Helm chart version would remain unchanged. Congratulations! You've completed your first run of the queue manager pipeline. Feel free to run the mq-infra-dev pipeline more than once to get a feeling for how it works. You've used it to build and test an instance of QM1 ready for deployment to the cluster. You've explored how the queue manager pipeline is structured as tasks and steps. You've examined a pipeline run log to understand how a pipeline works and how tasks are implemented. Finally, you've examined the Helm chart that resulted from a successful run of the pipeline. In the next topic of this chapter we're going to deploy this Helm chart to the cluster to instantiate QM1 in the dev namespace.","title":"Running the pipeline"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic2/#running-the-pipeline","text":"","title":"Running the Pipeline"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic2/#overview","text":"In the previous topic of this chapter, we installed and customized the queue manager repository for QM1 and configured the queue manager pipeline to access it. In this topic, we will perform our first full iteration of continuous integration for QM1 . We'll run the queue manager pipeline to build and test the queue manager QM1 . The successful pipeline run will store a new Helm chart representing QM1 in the the GitOps repository, ready for deployment. Look at the following diagram: We've highlighted the components we're going to explore in this topic: The QM1 Git repo that contains the source definition for queue manager QM1 . It includes the queue manager qm.ini and MQSC files, as well as its default Helm configuration. The queue manager pipeline that will use the QM1 repository as input. It will perform a set of tasks to build and test QM1 . If successful, the pipeline stores updated artifacts in the Image registry, GitOps apps repository and Artifactory as required. The Image registry that will store the newly built version of the queue manager image. The GitOps apps repository that stores the latest good build and test of QM1 represented as a Helm chart. This chart will be subsequently used by ArgoCD to deploy QM1 to the cluster. The Artifactory repository that stores a base Helm chart containing all the default values for QM1 , including its name, container image for example. These can be overridden by the GitOps Helm chart when required. In this topic, we're going to: Run the queue manager pipeline to build and test QM1 . Explore the queue manager pipeline. Explore the Artifactory Helm chart. Explore the GitOps apps repository Helm chart. By the end of this topic we'll have a fully built and tested queue manager QM1 ready to deploy to the cluster.","title":"Overview"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic2/#pre-requisites","text":"Before attempting this topic, you should have successfully completed the previous topic .","title":"Pre-requisites"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic2/#run-the-queue-manager-pipeline","text":"As we can see in the diagram above, the queue manager pipeline is responsible for building and testing QM1 from a source repository. If successful, it stores updated artifacts in Artifactory and the GitOps apps repository, which are used by ArgoCD to deploy the updated QM1 to the cluster. It's usually the case a pipeline runs automatically whenever a source repository changes. However, our first pipeline run is manual so that you can be in control, making it easier to understand what's happening. The queue manager pipeline performs a set of operations called tasks . Each task is responsible for a specific function such as building a queue manager image, or unit testing a queue manager. In turn, each task comprises a set of individual steps . For example, the build task might comprise three steps: Clone a source repository git, Build an image with buildah and Store an image in an image registry. If you'd like to know more about Tekton pipelines, here's an introduction . You will also find this video very helpful. Come back to these later if you'd like to keep going now. Locate the ci pipelines in the web console Let's find the queue manager pipeline using the OpenShift web console. Navigate to Pipelines->Pipelines in the left hand pane, and select Project: ci , to show all pipelines in the ci namespace: You can see all the pipeline that we installed into the ci namespace in the previous chapter. We'll use different pipelines for different activities throughout the tutorial. For this topic, we're going to use the mq-infra-dev pipeline. When applied to the QM1 source repository, this pipeline will build and test a new instance of QM1 ready for deployment to the dev namespace. The mq-infra-dev queue manager pipeline Select the mq-infra-dev pipeline from the list of all pipelines: Like all pipelines, mq-infra-dev is composed from a set of tasks : setup build smoke-tests-mq tag-release image-release img-scan helm-release gitops The task name often provides a strong hint of each task's specific function. We'll examine some of these tasks in detail as the pipeline runs. The oc command as an alternative to the Web console As well as using the OpenShift web console, you can also interact with pipeline using the oc or tekton commands. Ensure you're logged in to cluster, and issue the following command to list the mq-infra-dev pipeline details: oc get pipeline mq-infra-dev -n ci which shows a brief summary of the pipeline: NAME AGE mq-infra-dev 46h You can get more detail by adding the -o yaml option; we'll do that later. We use the command line and the web console in the tutorial so that you become familiar with both. As a general rule, you can do the same things in the command line as you can with the web console. The web console tends to be better for simple interactive tasks; the command line tends to be better for scripting and bulk tasks. Configure your first pipeline run Every time we run the mq-infra-dev pipeline, we create a new pipeline run . We can think of a pipeline run as an instance of a pipeline. Pipeline and PipelineRun are new custom resources that were added to the cluster when we installed Tekton. In pipeline details view above, you'll see an Actions button. Select Start to configure a pipeline run. You'll be presented with the following dialog: The supplied arguments allow the user of the pipeline to configure its behavior. For example, a user can use this pipeline with different queue manager source repositories. Configure the run as follows: Set git-url to your fork of the mq-infra repository Set git-revision to master . (Later in the tutorial, we will use a new branch.) Set scan-image: false (temporary fix while issues with UBI are resolved) Hit Start to start the pipeline run. You can also use the command line to run a pipeline; we'll explore that option later. Watch a pipeline run executing The pipeline run has now started. Notice how the view changes to Pipeline Run details : We're now looking at the live output from a pipeline run, rather than the pipeline used to create the run. Notice that the pipeline run name mq-infra-dev-44f3v5 is based on the pipeline name -- with a unique suffix. Every new pipeline run will have a unique name. See also how the first setup task is running, while the remaining tasks are waiting to start. Hover over setup or build task and you will see the steps that comprise it. Watch pipeline steps complete As the pipeline run proceeds, notice the build step complete and the smoke-tests-mq step start: This pipeline will take about 15 minutes to complete. While it's doing that, let's explore its tasks in a little more detail. List the pipeline run from the command line You can also explore the pipeline run from the command line. Issue the following command, replacing mq-infra-dev-44f3v5 with your pipeline run name: oc get pipelinerun mq-infra-dev-44f3v5 -n ci -o yaml which will show the details of your pipeline run: apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : ... spec : ... startTime : \"2021-07-27T07:34:06Z\" taskRuns : mq-infra-dev-44f3v5-build-gw5bm : pipelineTaskName : build status : completionTime : \"2021-07-27T07:35:51Z\" conditions : - lastTransitionTime : \"2021-07-27T07:35:51Z\" message : All Steps have completed executing reason : Succeeded status : \"True\" type : Succeeded podName : mq-infra-dev-44f3v5-build-gw5bm-pod-w8chs startTime : \"2021-07-27T07:34:35Z\" steps : - container : step-git-clone imageID : quay.io/ibmgaragecloud/alpine-git@sha256:70f7b70d9324314642b324ac79655862656255dd71b4d18357e5a22f76215ade name : git-clone terminated : containerID : cri-o://256775d430edcc5c4e87fa57d3029b9776ea6dcff7c7e80ae306c1aee49aab06 exitCode : 0 finishedAt : \"2021-07-27T07:34:57Z\" reason : Completed startedAt : \"2021-07-27T07:34:56Z\" - container : step-build imageID : quay.io/buildah/stable@sha256:04803d2144a2df5bf3aa2875f130e2b6cfc6ee45003125dc4df13f05f0898f9a name : build terminated : containerID : cri-o://043c8661b91bf02e3f726a9ab9f5bc445f9b7bf5c354070e0d432f5914b55810 exitCode : 0 finishedAt : \"2021-07-27T07:35:50Z\" reason : Completed startedAt : \"2021-07-27T07:34:57Z\" taskSpec : params : ... stepTemplate : name : \"\" resources : {} volumeMounts : - mountPath : $(params.source-dir) name : source steps : - env : - name : GIT_PASSWORD valueFrom : secretKeyRef : key : password name : git-credentials optional : true - name : GIT_USERNAME valueFrom : secretKeyRef : key : username name : git-credentials optional : true image : quay.io/ibmgaragecloud/alpine-git name : git-clone resources : {} script : | set +x if [[ -n \"${GIT_USERNAME}\" ]] && [[ -n \"${GIT_PASSWORD}\" ]]; then git clone \"$(echo $(params.git-url) | awk -F '://' '{print $1}')://${GIT_USERNAME}:${GIT_PASSWORD}@$(echo $(params.git-url) | awk -F '://' '{print $2}')\" $(params.source-dir) else set -x git clone $(params.git-url) $(params.source-dir) fi set -x cd $(params.source-dir) git checkout $(params.git-revision) - env : The full output is very large; we've abbreviated it significantly. This output contains the same information displayed as we see in the web console. Indeed the web console view is graphical representation of this information. Here's just a few of the interesting things that you can see: spec.startTime: identifies when the pipeline run started pipelineTaskName: build holds lots of interesting information on the build task. For example, we can see the build start and completion times and that it succeeded . steps identifies lots of interesting information on each of the steps within the build task. For example we can see the git-clone start and completion times, and that it also succeeded. There's no need to try to understand all this output right now; we're going to do that during this topic. Moreover, we'll make extensive use of the web console because it's easier to see what's happening. However, as we do this, you might like to link the console display back to this PipelineRun output. Let's continue exploring while the pipeline run completes.","title":"Run the queue manager pipeline"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic2/#explore-the-pipeline","text":"Let's look more closely at how the mq-infra-dev pipeline is structured as the pipeline run proceeds. Let's also examine the tasks and steps that make up the pipeline, and how they progressively build and test a queue manager, resulting in the production of a Helm chart ready for deployment. The mq-infra-dev pipeline run may still be running, so you'll be looking at a live pipeline run. Don't worry if the run has already completed, all the information remains available because its held in a PipelineRun custom resource for that run. Pipeline, Task, Step Let's start with the pipeline, its tasks and steps. Hover over the smoke-tests-mq task: See how our pipeline is made up of a set of tasks such as setup , build or gitops . These run in the order defined by the pipeline. Our pipeline is linear, though Tekton supports more sophisticated pipeline graphs if necessary. See how each task comprises a set of steps such as git-clone or setup . These run in the order defined by the task. The pipeline run logs When a pipeline runs, all its output is captured in a set of logs, one for each task. Click on the setup task to show its logs: ( Alternatively, you can select the Logs tab from the UI, and then select the tasks on the left pane within the pipeline run view. ) See how the setup task has its output in a dedicated log. You can select any log for any task that has completed or is executing. When a pipeline run completes, all its logs remain available, which can help diagnosing problems for example. Exploring an example task output: build It's easy to examine a task log; you simply select the relevant task and scroll the log up or down. For active tasks the log will be dynamically updated. Click on the build task: This console window shows the output generated by build task. As the task script proceeds, its output is captured; that's what we can see in this window. Notice that the build task output is from the first step in the build task. This step is called STEP-GIT-CLONE . Note how the step names are capitalized in the web console output. Let's look at another task and its steps more closely. Exploring a task step in detail : STEP-BUILD A task is built of multiple steps. Let's explore the build task and its step-build step. Select the build task and scroll through its logs to see its second step , STEP-BUILD : See how the step-build output is captured in the same log as the previous step git-clone . Each step runs in a separate container -- if you re-examine the PipelineRun YAML, you'll see those containers. When you scroll through the build task output, you're seeing what's happening in those containers. Read some of the log output to get a feeling for what's happening. Later, we'll see how step-build and step-git-clone are coded. The pipeline definition Up to this point, we've examined the pipeline run and the logs it generates. Let's now look at how a pipeline is defined. Issue the following command to show the mq-infra-dev pipeline: oc describe pipeline mq-infra-dev -n ci which shows the pipeline YAML in considerable detail: Name : mq-infra-dev Namespace : ci Labels : app.kubernetes.io/instance=ci-pipelines Annotations : app.openshift.io/runtime : mq API Version : tekton.dev/v1beta1 Kind : Pipeline Metadata : Creation Timestamp : 2021-07-26T11:46:57Z Generation : 1 Managed Fields : API Version : tekton.dev/v1beta1 Fields Type : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : f:app.openshift.io/runtime : f:kubectl.kubernetes.io/last-applied-configuration : f:labels : . : f:app.kubernetes.io/instance : f:spec : . : f:params : f:tasks : Manager : argocd-application-controller Operation : Update Time : 2021-07-26T11:46:56Z Resource Version : 2163788 Self Link : /apis/tekton.dev/v1beta1/namespaces/ci/pipelines/mq-infra-dev UID : 9bc6eaaa-a217-4bef-81fb-5ff30d5952e1 Spec : Params : Description : The url for the git repository Name : git-url Type : string Default : master Description : The git revision (branch, tag, or sha) that should be built Name : git-revision Type : string Default : true Description : Enable the pipeline to scan the image for vulnerabilities Name : scan-image Type : string Default : namespace-dev Description : environment Name : environment Type : string Default : 2-services/instances Description : Service Name : app-type Type : string Default : false Description : Enable security for queueManager Name : security Type : string Default : false Description : Enable ha for queueManager Name : ha Type : string Tasks : Name : setup Params : Name : git-url Value : $(params.git-url) Name : git-revision Value : $(params.git-revision) Name : scan-image Value : $(params.scan-image) Task Ref : Kind : Task Name : ibm-setup-v2-6-13 Name : build Params : Name : git-url Value : $(tasks.setup.results.git-url) Name : git-revision Value : $(tasks.setup.results.git-revision) Name : source-dir Value : $(tasks.setup.results.source-dir) Name : image-server Value : $(tasks.setup.results.image-server) Name : image-namespace Value : $(tasks.setup.results.image-namespace) Name : image-repository Value : $(tasks.setup.results.image-repository) Name : image-tag Value : $(tasks.setup.results.image-tag) Run After : setup Task Ref : Kind : Task Name : ibm-build-tag-push-v2-6-13 Name : smoke-tests-mq Params : Name : git-url Value : $(tasks.setup.results.git-url) Name : git-revision Value : $(tasks.setup.results.git-revision) Name : source-dir Value : $(tasks.setup.results.source-dir) Name : image-server Value : $(tasks.setup.results.image-server) Name : image-namespace Value : $(tasks.setup.results.image-namespace) Name : image-repository Value : $(tasks.setup.results.image-repository) Name : image-tag Value : $(tasks.setup.results.image-tag) Name : app-namespace Value : $(tasks.setup.results.app-namespace) Name : app-name Value : $(tasks.setup.results.app-name) Name : deploy-ingress-type Value : $(tasks.setup.results.deploy-ingress-type) Name : tools-image Value : $(tasks.setup.results.tools-image) Name : security Value : $(params.security) Name : ha Value : $(params.ha) Run After : build Task Ref : Kind : Task Name : ibm-smoke-tests-mq Name : tag-release Params : Name : git-url Value : $(tasks.setup.results.git-url) Name : git-revision Value : $(tasks.setup.results.git-revision) Name : source-dir Value : $(tasks.setup.results.source-dir) Name : js-image Value : $(tasks.setup.results.js-image) Run After : smoke-tests-mq Task Ref : Kind : Task Name : ibm-tag-release-v2-6-13 Name : img-release Params : Name : image-from Value : $(tasks.setup.results.image-url) Name : image-to Value : $(tasks.setup.results.image-release):$(tasks.tag-release.results.tag) Run After : tag-release Task Ref : Kind : Task Name : ibm-img-release-v2-6-13 Name : img-scan Params : Name : image-url Value : $(tasks.img-release.results.image-url) Name : scan-trivy Value : $(tasks.setup.results.scan-trivy) Name : scan-ibm Value : $(tasks.setup.results.scan-ibm) Run After : img-release Task Ref : Kind : Task Name : ibm-img-scan-v2-6-13 Name : helm-release Params : Name : git-url Value : $(tasks.setup.results.git-url) Name : git-revision Value : $(tasks.setup.results.git-revision) Name : source-dir Value : $(tasks.setup.results.source-dir) Name : image-url Value : $(tasks.img-release.results.image-url) Name : app-name Value : $(tasks.setup.results.app-name) Name : deploy-ingress-type Value : $(tasks.setup.results.deploy-ingress-type) Name : tools-image Value : $(tasks.setup.results.tools-image) Run After : img-scan Task Ref : Kind : Task Name : ibm-helm-release-v2-6-13 Name : gitops Params : Name : app-name Value : $(tasks.setup.results.app-name) Name : version Value : $(tasks.tag-release.results.tag) Name : helm-url Value : $(tasks.helm-release.results.helm-url) Name : tools-image Value : $(tasks.setup.results.tools-image) Name : environment Value : $(params.environment) Name : app-type Value : $(params.app-type) Run After : helm-release Task Ref : Kind : Task Name : ibm-gitops Events : <none> Don't be intimidated by this output -- it's actually just a more detailed source view of the information shown for the mq-infra-dev pipeline in the web console. Locate the following key structures in the Pipeline YAML: API Version: tekton.dev/v1beta1 and Kind: Pipeline identify this as a Tekton pipeline. Spec: Params identifies the pipeline input parameters Tasks: is a list of the tasks in this pipeline, each of which has A Name: naming the task A set of Params: identifying the task input parameters An optional Run After: value indicating when the task is run A Task Ref: identifying the actual task code to be run using the task's input Notice that there's no code in the pipeline definition; instead, the definition specifies the required inputs to the pipeline, as well as the set of required tasks and their order of execution. The code executed by each task is identified by Task Ref: , rather than in the pipeline; the pipeline definition merely defines the order of task execution and how parameters are marshaled between tasks. Let's now examine the pipeline definition in a little more detail. The pipeline input Spec: Params When we configure a pipeline run, the arguments map precisely to Spec: Params in the pipeline YAML file. Below, we've just shown the Spec: Params: for the mq-infra-dev pipeline: spec : params : - name : git-url description : The url for the git repository - name : git-revision description : The git revision (branch, tag, or sha) that should be built default : master - name : scan-image description : Enable the pipeline to scan the image for vulnerabilities default : \"false\" - name : environment description : environment default : dev - name : app-path description : Path in gitops repo default : mq/environments - name : security description : Enable security for queueManager default : \"false\" - name : ha description : Enable ha for queueManager default : \"false\" - name : git-pr description : Enable the pipeline to do a PR for the gitops repo default : \"false\" Spend a few moments mapping each of these parameters maps to those on the Start Pipeline input dialog where you specified the pipeline run arguments. For example, map Names: , Description: and Default: to the different fields in the dialog. Parameters for the first setup task We can see that the first task in the pipeline is called setup . Let's examine its YAML to see how it gets its input parameters: tasks : - name : setup taskRef : name : ibm-setup-v2-6-13 params : - name : git-url value : $(params.git-url) - name : git-revision value : $(params.git-revision) - name : scan-image value : $(params.scan-image) See how the setup task derives its git-url parameter using the pipeline input parameter value $(params.git-url) . See how git-revision and scan-image work in a similar way. The first task in a pipeline typically works like this -- its parameters are mapped from the pipeline's input parameters. Notice also that some pipeline input parameters are not referred to by the setup task; they will be used by subsequent tasks using the appropriate $(params.) value. Passing arguments between tasks As each task completes, the pipeline proceeds. When a new task starts it often requires one or more results generated by a previous task. We can see a good example of this in the build task: - name : build taskRef : name : ibm-build-tag-push-v2-6-13 runAfter : - setup params : - name : git-url value : \"$(tasks.setup.results.git-url)\" - name : git-revision value : \"$(tasks.setup.results.git-revision)\" - name : source-dir value : \"$(tasks.setup.results.source-dir)\" - name : image-server value : \"$(tasks.setup.results.image-server)\" - name : image-namespace value : \"$(tasks.setup.results.image-namespace)\" - name : image-repository value : \"$(tasks.setup.results.image-repository)\" - name : image-tag value : \"$(tasks.setup.results.image-tag)\" See how the build task specifies that the image-tag parameter should use value generated by the setup task using the syntax: $(tasks.setup.results.image-tag) . Also notice how the build tasks uses Run After: to specify that it should execute after the setup task. This follows the Kubernetes idiom of resources being declarative -- the order of execution is defined by RunAfter: rather than the order in which tasks appear in the YAML. Again, notice that the build task doesn't contain the code that the task executes. This is contained in Task Ref: which identifies ibm-build-tag-push-v2-6-13 as the task to execute using the specified parameters. It's the code in ibm-build-tag-push-v2-6-13 which generates the log output for the task; we'll examine it later. The way pipelines tasks are designed makes them highly reusable. As we'll see later, tasks are written with defined inputs and outputs such that they can be re-used by different pipelines. Pipelines focus on organizing the order of task execution and how parameters are marshaled into and between tasks; it's the tasks that do the actual work. Locating the pipeline and tasks source YAMLs Finally, let's locate the source for the mq-infra-dev pipeline and the tasks within it. It is located in the following folder: cd $HOME /git cd multi-tenancy-gitops-apps tree mq/environments/ci/pipelines/ We can see the other pipelines for the ci namespace in this folder: mq/environments/ci/pipelines/ \u251c\u2500\u2500 ibm-test-pipeline-for-dev.yaml \u251c\u2500\u2500 ibm-test-pipeline-for-stage.yaml \u251c\u2500\u2500 java-maven-dev-pipeline.yaml \u251c\u2500\u2500 mq-pipeline-dev.yaml \u2514\u2500\u2500 mq-spring-app-dev-pipeline.yaml These map to the MQ-related pipelines we saw in the Pipelines->Pipelines view in the web console. Exploring the mq-infra-dev source YAML View the source for the mq-pipeline-dev.yaml pipeline with the command: cat mq/environments/ci/pipelines/mq-pipeline-dev.yaml which shows the source YAML for the mq-infra-dev pipeline: apiVersion : tekton.dev/v1beta1 kind : Pipeline metadata : name : mq-infra-dev annotations : app.openshift.io/runtime : mq spec : params : - name : git-url description : The url for the git repository - name : git-revision description : The git revision (branch, tag, or sha) that should be built default : master - name : scan-image description : Enable the pipeline to scan the image for vulnerabilities default : \"false\" - name : environment description : environment default : dev - name : app-path description : Path in gitops repo default : mq/environments - name : security description : Enable security for queueManager default : \"false\" - name : ha description : Enable ha for queueManager default : \"false\" - name : git-pr description : Enable the pipeline to do a PR for the gitops repo default : \"false\" tasks : - name : setup taskRef : name : ibm-setup-v2-6-13 params : - name : git-url value : $(params.git-url) - name : git-revision value : $(params.git-revision) - name : scan-image value : $(params.scan-image) - name : build taskRef : name : ibm-build-tag-push-v2-6-13 runAfter : - setup params : - name : git-url value : \"$(tasks.setup.results.git-url)\" - name : git-revision value : \"$(tasks.setup.results.git-revision)\" - name : source-dir value : \"$(tasks.setup.results.source-dir)\" - name : image-server value : \"$(tasks.setup.results.image-server)\" - name : image-namespace value : \"$(tasks.setup.results.image-namespace)\" - name : image-repository value : \"$(tasks.setup.results.image-repository)\" - name : image-tag value : \"$(tasks.setup.results.image-tag)\" - name : smoke-tests-mq taskRef : name : ibm-smoke-tests-mq runAfter : - build params : - name : git-url value : \"$(tasks.setup.results.git-url)\" - name : git-revision value : \"$(tasks.setup.results.git-revision)\" - name : source-dir value : \"$(tasks.setup.results.source-dir)\" - name : image-server value : \"$(tasks.setup.results.image-server)\" - name : image-namespace value : \"$(tasks.setup.results.image-namespace)\" - name : image-repository value : \"$(tasks.setup.results.image-repository)\" - name : image-tag value : \"$(tasks.setup.results.image-tag)\" - name : app-namespace value : \"$(tasks.setup.results.app-namespace)\" - name : app-name value : \"$(tasks.setup.results.app-name)\" - name : deploy-ingress-type value : \"$(tasks.setup.results.deploy-ingress-type)\" - name : tools-image value : \"$(tasks.setup.results.tools-image)\" - name : security value : \"$(params.security)\" - name : ha value : \"$(params.ha)\" - name : tag-release taskRef : name : ibm-tag-release-v2-6-13 runAfter : - smoke-tests-mq params : - name : git-url value : \"$(tasks.setup.results.git-url)\" - name : git-revision value : \"$(tasks.setup.results.git-revision)\" - name : source-dir value : \"$(tasks.setup.results.source-dir)\" - name : js-image value : \"$(tasks.setup.results.js-image)\" - name : img-release taskRef : name : ibm-img-release-v2-6-13 runAfter : - tag-release params : - name : image-from value : \"$(tasks.setup.results.image-url)\" - name : image-to value : \"$(tasks.setup.results.image-release):$(tasks.tag-release.results.tag)\" - name : img-scan taskRef : name : ibm-img-scan-v2-6-13 runAfter : - img-release params : - name : image-url value : $(tasks.img-release.results.image-url) - name : scan-trivy value : $(tasks.setup.results.scan-trivy) - name : scan-ibm value : $(tasks.setup.results.scan-ibm) - name : helm-release taskRef : name : ibm-helm-release-v2-6-13 runAfter : - img-scan params : - name : git-url value : \"$(tasks.setup.results.git-url)\" - name : git-revision value : \"$(tasks.setup.results.git-revision)\" - name : source-dir value : \"$(tasks.setup.results.source-dir)\" - name : image-url value : \"$(tasks.img-release.results.image-url)\" - name : app-name value : \"$(tasks.setup.results.app-name)\" - name : deploy-ingress-type value : \"$(tasks.setup.results.deploy-ingress-type)\" - name : tools-image value : \"$(tasks.setup.results.tools-image)\" - name : gitops taskRef : name : ibm-gitops runAfter : - helm-release params : - name : app-name value : \"$(tasks.setup.results.app-name)\" - name : version value : \"$(tasks.tag-release.results.tag)\" - name : helm-url value : $(tasks.helm-release.results.helm-url) - name : tools-image value : \"$(tasks.setup.results.tools-image)\" - name : app-path value : \"$(params.app-path)\" - name : dest-environment value : \"$(params.environment)\" - name : git-pr value : \"$(params.git-pr)\" This YAML is slightly different to the output of the oc get pipeline command, because extra information is added during deployment such as Creation Timestamp: . Finding the ArgoCD application that manages pipelines You can see how the mq-infra-dev and other pipeline YAMLs were deployed by examining the ArgoCD application that watches the folder containing the ci namespace pipelines. Issue the following command: cat mq/config/argocd/ci/ci-app-rest.yaml which shows the apps-mq-rest-ci-1 ArgoCD application that watches for updates: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : apps-mq-rest-ci-1 annotations : argocd.argoproj.io/sync-wave : \"300\" finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : ci server : https://kubernetes.default.svc project : applications source : path : mq/environments/ci repoURL : https://github.com/hp-gitops-test/multi-tenancy-gitops-apps targetRevision : master syncPolicy : automated : prune : true selfHeal : true See how this apps-mq-rest-ci-1 watches: path: mq/environments/ci . As we know, this folder contains the YAML for the mq-infra-dev and other pipelines under pipelines folder. When this ArgoCD application was made active in the cluster, it installed all the pipelines along with other resources in this folder. Later in the tutorial, we'll examine how tasks are coded to perform their tasks, but for now, you should have a good feeling for how pipelines are structured and how they work. By now, the pipeline run should have completed successfully. In the next section, we're going to examine the Helm charts that it created.","title":"Explore the pipeline"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic2/#understanding-the-qm1-helm-chart","text":"The successful completion of the mq-infra-dev pipeline run which used the QM1 source repository as input has resulted in three new objects being produced: A versioned container image for QM1 stored in the cluster image registry. A versioned Helm chart for QM1 stored in Artifactory. This chart contains a full default configuration for QM1 based largely on the Helm chart in the source repository. A versioned Helm chart for QM1 stored in the GitOps repository. This chart is created the first time the pipeline runs. It has a much sparser structure, and points to the full Helm chart stored in Artifactory. The following diagram represents this structure and process: This diagram shows how: The Helm chart stored in Artifactory points to the container image used by QM1 in the Image registry. This Helm chart contains the default configuration for the queuemanager custom resource YAML for QM1 derived from it source repository. This YAML refers to the container that was built during the pipeline run. This Helm chart stored in the GitOps repository merely refers to the full Helm chart stored in Artifactory. This Helm chart will be used by ArgoCD to deploy QM1 to the cluster. In this topic we're going to examine these Helm charts in more detail. We'll see how they are structured and how they are used. We'll see how, in combination, these Helm charts provide a well governed deployment of QM1 to the cluster. The source repository for the Helm chart Return to the terminal window you're using for the mq-infra source repository. ( Rather than the terminal window you're using for the multi-tenancy-gitops GitOps repository. ) In the previous topic, we forked and cloned the mq-infra repository. This contains the source Helm chart for queue manager QM1 . Let's quickly recap this Helm chart. Ensure you're in the correct directory: cd $HOME /git/mq-infra The source Helm chart Let's see how the source Helm chart for QM1 is structured. Issue the following command: tree chart/base to show the Helm folder structure and detail: chart/base \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 config.mqsc \u251c\u2500\u2500 security \u2502 \u2514\u2500\u2500 config.mqsc \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 configmap.yaml \u2502 \u2514\u2500\u2500 qm-template.yaml \u2514\u2500\u2500 values.yaml The key folders and files are as follows: The templates folder contains the templates that will be used to generate the queuemanager custom resource and configmap for QM1 . The values.yaml file contains a set of values that are used by these templates. For example, qm-template gets the queue manager name and CPU/memory limits from values.yaml . The config folder contains the MQSC definitions that will be placed in the config map used by QM1 when the queue manager is not configured for security. The security folder contains the MQSC definitions that will be placed in the config map used by QM1 when the queue manager is configured for security. Feel free to learn more about Helm charts before you proceed. Exploring Helm charts in Artifactory Every successful mq-infra-dev pipeline run stores a new version of the QM1 Helm chart in Artifactory. The Artifactory Helm chart is almost a verbatim copy of the source Helm chart. However, the pipeline adds vitally important data that is generated when the pipeline in run, and cannot be known ahead of time. This includes the name of the successfully built and tested container image, as well as chart version information. These calculated values are stored in the values.yaml file in Artifactory alongside the defaults from the source repository. You can access Artifactory via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n tools | grep artifactory | awk '{print \"https://\"$2}' This will list the route to the Artifactory, for example: https://artifactory-tools.xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the Artifactory UI. ( You can safely ignore any browser certificate warnings. ) Grab the user and password using the below commands. echo \"User: \" ; oc get secret/artifactory-access -n tools -o json | jq -r '.data.\"ARTIFACTORY_USER\"' | base64 -D echo \"Password: \" ; oc get secret/artifactory-access -n tools -o json | jq -r '.data.ARTIFACTORY_PASSWORD' | base64 -D Locating the Artifactory Helm chart We can locate the Helm chart produced by the pipeline in the Artifactory UI. On the left hand pane select Artifactory->Artifacts . Within this view, select generic-local->ci->mq-infra-0.0.1.tgz->mq-infra . Select the values.yaml file and View Source tab: Notice: The chart folder structure folder is exactly the same as the source repository. The chart is stored in a binary zipped form, indicated by the .tgz file extension. The chart has a version 0.0.1 -- each pipeline run will increment this value. The image: specified in the QM1 custom resource YAML has been updated to specify the exact image and version that should be used from the image registry. Let's have a more detailed look at the Helm chart for QM1 stored in Artifactory. Examine the Artifactory Helm chart Chart.yaml We'll start with the Chart.yaml file; it's the starting point for the QM1 Helm chart. Its contents are almost exactly the same as those in the QM1 source repository, but with one important difference. As we'll see, the chart version stored in Artifactory must be generated dynamically by the pipeline run. In the Artifactory UI, select the Chart.yaml file and View Source tab: apiVersion : v2 appVersion : 1.16.0 description : A Helm chart for Kubernetes name : mq-infra type : application version : 0.0.1 This file contains the high level information about the chart. The apiVersion is the Helm API version for the chart. We're using Helm 3, which requires v2 . The name field specifies the name of the chart. We've used base because the GitOps Helm chart will will be based on this one, inheriting most of its properties. This name could be more helpful -- QM1 base chart for example. The description could be more helpful -- Helm chart for QM1 , for example. The type field identifies whether the chart is a Helm application or Helm library. The QM1 chart is an application type -- a collection of templates (a queue manager and configmap) that are packaged and versioned for deployment. The version field specifies the chart version. Every time the mq-infra-dev pipeline run is successful, the version is incremented. The appVersion field is purely informational in Helm. The mq-infra-dev pipeline should keep appVersion the same as the version , but it's currently fixed at 1.16.0 . (This isn't helpful and will be addressed.) In summary, the most important aspect of Chart.yaml is its version . This version is stored within the chart, as well forming the chart name: mq-infra-0.0.1.tgz . Every time the mq-infra-dev pipeline run is successful, a new, versioned instance of the QM1 Helm chart is produced and stored in Artifactory. Examine the Artifactory Helm chart values.yaml The values.yaml file is created by the designer of the Helm chart to allow users to control its behavior. Its contents are used by the queue manager and configmap templates to generate deployable queuemanager and configmap YAMLs. These YAMLs are customized according by their corresponding templates using the values specified in values.yaml . To re-iterate, the values.yaml file has almost the same contents as the QM1 source repository, but with a few important differences. For example, the queue manager container image name stored in Artifactory must be generated dynamically by the pipeline run. In the Artifactory UI, select the values.yaml file and View Source tab: name : qm-dev version : 9.2.3.0-r1 web : enabled : true license : accept : true license : L-RJON-BN7PN3 halicense : L-RJON-BYRMYW use : NonProduction image : repository : replace tag : replace pullPolicy : Always queuemanager : debug : false imagePullPolicy : IfNotPresent livenessProbe : failureThreshold : 1 initialDelaySeconds : 90 periodSeconds : 10 successThreshold : 1 timeoutSeconds : 5 logFormat : Basic metrics : enabled : true name : QM1 readinessProbe : failureThreshold : 1 initialDelaySeconds : 10 periodSeconds : 5 successThreshold : 1 timeoutSeconds : 3 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : \"1\" memory : 1Gi storage : persistedData : enabled : false queueManager : type : ephemeral hatype : persistent-claim size : 2Gi recoveryLogs : enabled : false availability : type : SingleInstance hatype : NativeHA mqsc : configmap : mqsc-configmap name : config.mqsc ini : configmap : name : securityContext : initVolumeAsRoot : false template : pod : containers : - name : qmgr # env: # - name: MQSNOAUT # value: \"yes\" configmap : path : config pathWithSecurity : security cert : secretName : mq-server-cert terminationGracePeriodSeconds : 30 tracing : agent : {} collector : {} enabled : false namespace : \"\" security : false ha : false Notice how: Most of these values exactly correspond to the same literal values in the queuemanager custom resource YAML. For example, license: , resources and readinessProbe: are simple mappings to those found in a queuemanager custom resource YAML. Other values are not literal mappings; they are used by the queue manager or configmap templates in more sophisticated ways. For example security: false is used in combination pathWithSecurity: to configure a different MQSC file for QM1 depending on the security requirements. The logic to accomplish this processing is held within the template files. image: identifies the name and version of the queue manager container image to retrieve from the container registry. This value was set by the pipeline run, and corresponds to the image that was successfully smoke tested. A short digression on Helm templating As we've just seen with our security example, it's often helpful for the values.yaml file to contain a YAML node such as security: that is more than a literal equivalent to the YAML node found in a Kubernetes resource. We're going to take a short detour to explain how this works by exploring the templates/configmap.yaml file in Artifactory. In the Artifactory UI, select the /templates/configmap.yaml file and View Source tab: apiVersion : v1 kind : ConfigMap metadata : name : {{ .Values.mqsc.configmap }} data : {{ - if eq .Values.security true }} {{ - $path : = printf \"%s/*\" .Values.configmap.pathWithSecurity }} {{ (.Files.Glob $path).AsConfig | indent 2 }} {{ else }} {{ - $path : = printf \"%s/*\" .Values.configmap.path }} {{ (.Files.Glob $path).AsConfig | indent 2 }} {{ end }} This is a Helm template, and it uses the Go templating language. Learn more about Helm templates if you'd like; for now it's enough to have this basic understanding. In the example above, notice how: configmap name is derived from the values.yaml file using {{.Values.mqsc.configmap}} . .Values.security is used to control the configmap that is generated. You can also view the queue manager template. In the Artifactory UI, select the /templates/qm-template.yaml file and View Source tab: apiVersion : mq.ibm.com/v1beta1 kind : QueueManager metadata : name : {{ .Values.name }} annotations : argocd.argoproj.io/sync-wave : \"300\" helm.sh/hook-weight : \"300\" spec : license : accept : {{ .Values.license.accept }} {{ if eq .Values.ha true }} license : {{ .Values.license.halicense }} {{ else }} license : {{ .Values.license.license }} {{ end }} use : {{ .Values.license.use }} queueManager : {{ - toYaml .Values.queuemanager | nindent 4 }} {{ if eq .Values.ha true }} availability : type : {{ .Values.availability.hatype }} tls : secretName : {{ .Values.cert.secretName }} cipherSpec : ANY_TLS12_OR_HIGHER {{ else }} availability : type : {{ .Values.availability.type }} {{ end }} image : \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\" imagePullPolicy : {{ .Values.image.pullPolicy }} mqsc : - configMap : name : {{ .Values.mqsc.configmap }} items : - {{ .Values.mqsc.name }} {{ if eq .Values.ha true }} storage : queueManager : size : {{ .Values.storage.queueManager.size }} type : {{ .Values.storage.queueManager.hatype }} {{ else }} storage : persistedData : enabled : {{ .Values.storage.persistedData.enabled }} queueManager : type : {{ .Values.storage.queueManager.type }} recoveryLogs : enabled : {{ .Values.storage.recoveryLogs.enabled }} {{ end }} securityContext : initVolumeAsRoot : {{ .Values.securityContext.initVolumeAsRoot }} template : pod : containers : - name : qmgr {{ - if eq .Values.security false }} env : - name : MQSNOAUT value : \"yes\" {{ - end }} terminationGracePeriodSeconds : {{ .Values.terminationGracePeriodSeconds }} tracing : {{ - toYaml .Values.tracing | nindent 4 }} version : {{ .Values.version }} web : enabled : {{ .Values.web.enabled }} {{ - if eq .Values.security true }} pki : keys : - name : certificate secret : items : - tls.key - tls.crt secretName : {{ .Values.cert.secretName }} trust : - name : ca secret : items : - ca.crt secretName : {{ .Values.cert.secretName }} {{ - end }} Spend a few moments examining this template, to see how it uses the values.yaml file. Re-merging local clone to view Helm chart in GitOps repository The mq-infra-dev pipeline run updated the GitOps repository with the Helm chart. This means that our local clone of the GitOps repository is one commit behind GitHub. To allow us to view the Helm chart locally, we must re-merge our local branch with GitHub. Return to the terminal window you're using for the multi-tenancy-gitops-apps GitOps apps repository. ( Rather than the terminal window you're using for the mq-infra source repository. ) Issue the following commands to merge the local branch: git fetch origin git merge origin/ $GIT_BRANCH which shows our local branch being updated: Updating 0c0e5b9..8cf318d Fast-forward mq/environments/dev/mq-infra/Chart.yaml | 4 ++++ mq/environments/dev/mq-infra/requirements.yaml | 4 ++++ mq/environments/dev/mq-infra/values.yaml | 3 +++ 3 files changed, 11 insertions(+) create mode 100644 mq/environments/dev/mq-infra/Chart.yaml create mode 100644 mq/environments/dev/mq-infra/requirements.yaml create mode 100644 mq/environments/dev/mq-infra/values.yaml Notice how these files correspond to the new Helm chart created in the GitOps repository by the mq-infra-dev pipeline run. Explore the Helm chart in the GitOps repository Let's examine the newly produced Helm chart in the GitOps repository; it was created by the pipeline run. Issue the following command: tree mq/environments/dev/mq-infra/ which shows the newly produced Helm chart: mq/environments/dev/mq-infra/ \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml Notice that: The chart for QM1 was created in the mq/environments/ folder to reflect the fact this queue manager is part of the applications layer. The chart was created in the dev subfolder to reflect the fact that it's going to be deployed to the dev namespace. The chart was created in a new folder /mq-infra . This folder is dedicated to QM1 . A different queue manager would have a different folder. Examine the GitOps Helm chart requirements.yaml We need a Helm chart in the GitOps repository for a few reasons: ArgoCD requires a Git repository to watch for updates Git is our source of truth. Although Artifactory contains the base Helm chart, we link to it from Git. We can override the Artifactory Helm chart with a GitOps repository values.yaml file. For example, we might want to change the CPU or memory used by QM1 . This should not require a re-run of the mq-infra-dev pipeline because it not that QM1 has changed, but its environment . ( An example of a change that should require a pipeline run is an updated MQSC or qm.ini file. ) Helm makes it easy for one Helm chart to build upon an existing Helm chart using a requirements.yaml file. Let's see how the GitOps chart builds upon the Helm chart in Artifactory. Issue the following command: cat mq/environments/dev/mq-infra/requirements.yaml to see the requirements.yaml file: dependencies : - name : mq-infra version : 0.0.1 repository : http://artifactory-artifactory.tools:8082/artifactory/generic-local/ci See how this Helm chart identifies the dependent Helm chart in the Artifactory repository. This information effectively provides a link to the mq-infra-0.0.1.tgz zipped Helm chart we explored earlier. Examine the GitOps Helm chart values.yaml Recall that the values.yaml file in the QM1 source repository provides sensible defaults for QM1 . As we saw, the values.yaml file in Artifactory is largely a copy of this file. The values.yaml file in the GitOps repository performs a very different role; it allows these defaults to be overridden. Issue the following command: cat mq/environments/dev/mq-infra/values.yaml to see the GitOps values.yaml file: global : {} mq-infra : replicaCount : 1 See how sparsely populated this file is; it really just provides overrides and extensions to the mq-infra chart stored in Artifactory. The example provided of replicaCount is not a good example because its copied from the MQ application chart. Imagine a production scenario where we wanted to increase the CPU and memory requirements of the deployed QM1 . We could update the GitOps repository values.yaml file as follows: global : {} mq-infra : resources : limits : cpu : \"2\" memory : 2Gi requests : cpu : \"2\" memory : 2Gi This would result in the already deployed QM1 being re-deployed with these new values, but the base Helm chart in Artifactory wouldn't change. To support this practice, the mq-infra-dev pipeline will create an initial version of the GitOps value.yaml file, but subsequently pipeline run will not change it, as they assume that it is being used for GitOps operational overrides such as our CPU/memory example. Examine the GitOps Helm chart Chart.yaml Finally, we examine the GitOps Chart.yaml file. Issue the following command: cat mq/environments/dev/mq-infra/Chart.yaml to see the Helm chart details: apiVersion : v2 version : 0.1.0 name : mq-infra description : Chart to configure ArgoCD with the mq-infra project and its applications Notice that this chart is similar in structure to the Helm chart in Artifactory. However, it's important to realize that its version is independently maintained. For example, if we modified the GitOps values.yaml file to override CPU and memory, the GitOps Helm chart version would be incremented, but the Artifactory base Helm chart version would remain unchanged. Congratulations! You've completed your first run of the queue manager pipeline. Feel free to run the mq-infra-dev pipeline more than once to get a feeling for how it works. You've used it to build and test an instance of QM1 ready for deployment to the cluster. You've explored how the queue manager pipeline is structured as tasks and steps. You've examined a pipeline run log to understand how a pipeline works and how tasks are implemented. Finally, you've examined the Helm chart that resulted from a successful run of the pipeline. In the next topic of this chapter we're going to deploy this Helm chart to the cluster to instantiate QM1 in the dev namespace.","title":"Understanding the QM1 Helm chart"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic3/","text":"Deploying and using the queue manager \u00b6 Overview \u00b6 In the previous topic of this chapter, we ran the queue pipeline using the source repository for QM1 . The pipeline successfully built and tested the queue manager as well as creating versioned resources in the image registry, Artifactory and the GitOps repository. In this topic, you will set up continuous deployment for QM1 that will use the resources created by the pipeline run. We'll examine the activated ArgoCD application that will watch the GitOps folder containing the Helm chart for QM1 and use it and its dependent resources to deploy a running queue manager to the dev namespace in the cluster. Look at the following diagram: We've highlighted the components we're going to explore in this topic: The GitOps repository is used by ArgoCD applications to determine what should be active in the cluster. It contains the latest good deployment configuration for QM1 accessed via its GitOps Helm chart. An ArgoCD application specific for QM1 will monitor a GitOps folder where its Helm chart is held. Whenever a this folder is updated, this ArgoCD application will apply these updates to the cluster, resulting in a new deployment of QM1 . The instance of QM1 running in the cluster is active development queue manager ready for use by MQ applications under development. In this topic, we're going to: Examine the activated ArgoCD application that deploys the QM1 configuration to the cluster. Explore the Kubernetes resources that have been deployed to the cluster for QM1 . Interact with the deployed queue manager QM1 . By the end of this topic we'll have a fully functioning queue manager deployed to the cluster with which we will have interacted. Pre-requisites \u00b6 Before attempting this topic, you should have successfully completed the previous topic . ArgoCD application for QM1 \u00b6 We're now going to move from the integration of QM1 to its deployment . The result of our previously successful mq-infra-dev pipeline run was to create a Helm chart in a GitOps folder for QM1 . This Helm chart brings together all the necessary Kubernetes components for the deployment of QM1 . We're now going to examine the activated ArgoCD application that uses this Helm chart to manage the deployment of QM1 to the dev namespace. The ArgoCD application for QM1 QM1 has its deployment to the cluster managed by a dedicated ArgoCD application called dev-mq-infra-instance . This follows the separation of concerns pattern where one ArgoCD application manages a set of related Kubernetes resources deployed to a cluster; in this case, all those resources associated with queue manager QM1 in the dev namespace. Make sure you are within the GitOps application repository folder: cd $HOME /git cd multi-tenancy-gitops-apps Issue the following command to show the ArgoCD application details: cat mq/config/argocd/dev/dev-mq-infra-instance.yaml which shows a YAML file typical of those we've seen before: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : dev-mq-infra-instance annotations : argocd.argoproj.io/sync-wave : \"300\" finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : dev server : https://kubernetes.default.svc project : applications source : path : mq/environments/dev/mq-infra repoURL : https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps targetRevision : master helm : valueFiles : - values.yaml syncPolicy : automated : prune : true selfHeal : true syncOptions : - Replace=true See how the Helm chart we explored in the previous section of this topic is referenced by path: mq/environments/dev/mq-infra : mq/environments/dev/mq-infra \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml The ArgoCD application applies this Helm chart to the cluster to instantiate QM1 as a set of cluster resources. Look at active QM1 ArgoCD application Let's examine QM1 and its Kubernetes resources using the ArgoCD UI. In the ArgoCD UI search the Applications view with the keyword mq-infra : ( You may need to launch the ArgoCD UI again. Refer to these instructions . ) We can now see the below ArgoCD Application: A new dev-mq-infra-instance ArgoCD application that is managing QM1 resources deployed to the cluster. View the new QM1 Kubernetes resources We can look at the deployed instance of QM1 and its dependent kubernetes resources. Click on the dev-mq-infra-instance ArgoCD application: We can see that QM1 is comprised of many Kubernetes resources. We'll explore them in more detail later, but for now notice: The QM1 queue manager comprises two main Kubernetes components: the mqsc-configmap configmap resource that holds the MQSC definitions applied to QM1 when it started. the qm-dev queuemanager custom resource that represents the running queue manager. The qm-dev custom resource has a rich structure comprising: a qm-dev-ibm-mq-0 pod where the QM1 binaries are running a qm-dev-ibm-mq-qm route to the queue manager a qm-dev-ibm-mq-qm-web route to the MQ web console a qm-dev-ibm-mq service a qm-dev-ibm-mq-metrics metrics service a qm-dev-ibm-mq service account a qm-dev-ibm-mq stateful set many other resources including secrets, configmaps, endpoints and more These queue manager components are managed by the IBM MQ operator which takes the QM1 YAML and creates (and maintains) these Kubernetes resources. Feel free to click on these components to get an idea of their contents. It's not important to understand this structure in detail -- but it helps to appreciate what's going on when we deploy QM1 . If you're used to running MQ outside of Kubernetes, you may find it interesting to map these Kubernetes resources to MQ components with which you're familiar. Explore the deployed Kubernetes resources for QM1 \u00b6 Let's spend a few moments exploring the two Kubernetes resources that have been deployed: the queuemanager custom resource named qm-dev , and its MQSC configuration stored in the configmap named mqsc-configmap . Exploring the deployed queue manager using the command line We will explore the qm-dev queuemanager resource from the oc command line. The extensible nature of Kubernetes allows us to examine the new queuemanager custom resource added by the IBM MQ operator. Issue the following command: oc describe queuemanager qm-dev -n dev Notice how API Version: mq.ibm.com/v1beta1 and Kind: QueueManager identify this as an IBM MQ queue manager. Name : qm-dev Namespace : dev Labels : app.kubernetes.io/instance=ibm-mq-dev-instance Annotations : argocd.argoproj.io/sync-wave : 300 helm.sh/hook-weight : 300 API Version : mq.ibm.com/v1beta1 Kind : QueueManager Metadata : Creation Timestamp : 2021-07-29T17:20:26Z Finalizers : finalizer.queuemanagers.mq.ibm.com Generation : 2 Managed Fields : ... Spec : License : Accept : true License : L-RJON-BN7PN3 Metric : VirtualProcessorCore Use : NonProduction Queue Manager : Availability : Type : SingleInstance Debug : false Image : image-registry.openshift-image-registry.svc:5000/ci/mq-infra:0.0.1 Image Pull Policy : Always Liveness Probe : Failure Threshold : 1 Initial Delay Seconds : 90 Period Seconds : 10 Success Threshold : 1 Timeout Seconds : 5 Log Format : Basic Metrics : Enabled : true Mqsc : Config Map : Items : config.mqsc Name : mqsc-configmap Name : QM1 Readiness Probe : Failure Threshold : 1 Initial Delay Seconds : 10 Period Seconds : 5 Success Threshold : 1 Timeout Seconds : 3 Resources : Limits : Cpu : 1 Memory : 1Gi Requests : Cpu : 1 Memory : 1Gi Route : Enabled : true Storage : Persisted Data : Enabled : false Queue Manager : Type : ephemeral Recovery Logs : Enabled : false Security Context : Init Volume As Root : false Template : Pod : Containers : Env : Name : MQSNOAUT Value : yes Name : qmgr Resources : Termination Grace Period Seconds : 30 Tracing : Agent : Collector : Enabled : false Namespace : Version : 9.2.2.0-r1 Web : Enabled : true Status : Admin Ui URL : https://qm-dev-ibm-mq-web-dev.odowda-cloudmq-1-d02cf90349a0fe46c9804e3ab1fe2643-0000.eu-gb.containers.appdomain.cloud/ibmmq/console Conditions : Endpoints : Name : ui Type : UI Uri : https://qm-dev-ibm-mq-web-dev.odowda-cloudmq-1-d02cf90349a0fe46c9804e3ab1fe2643-0000.eu-gb.containers.appdomain.cloud/ibmmq/console Name : QM1 Phase : Running Versions : Available : Channels : Versions : Name : 9.2.2.0-r1 Reconciled : 9.2.2.0-r1 Events : <none> (We've abbreviated it slightly to exclude the Managed Fields: YAML detail.) Here are a few things to note: Spec.Queue Manager.Name identities the name of the queue manager as QM1 Name: identifies the name of the Kubernetes resource as qm-dev . Note how it incorporates the fact that the queue manager is running in the dev namespace. (This name should really be qm1-dev !) Spec.Queue Manager.Image: identifies the image stored in the image registry that this queue manager is running. Spec.Queue Manager.Mqsc: identifies the name of the config map mqsc-config map. Notice how we cannot see the internal structure of the queue manager. Instead we can see the YAML that was built from the QM1 Helm chart and applied to the cluster. For more details on this configuration, see the API reference for the QueueManager (mq.ibm.com/v1beta1) . Exploring the MQSC configuration for QM1 Let's now look at the MQSC configuration for this queue manager. As we've seen, it's contained in the mqsc-configmap configmap. Issue the following command oc get configmap mqsc-configmap -n dev -o yaml to show the MQSC commands that are applied to the queue manager when it starts: apiVersion : v1 data : config.mqsc : | DEFINE QLOCAL(IBM.DEMO.Q) BOQNAME(IBM.DEMO.Q.BOQ) BOTHRESH(3) REPLACE DEFINE QLOCAL(IBM.DEMO.Q.BOQ) REPLACE * Use a different dead letter queue, for undeliverable messages DEFINE QLOCAL('DEV.DEAD.LETTER.QUEUE') REPLACE ALTER QMGR DEADQ('DEV.DEAD.LETTER.QUEUE') DEFINE CHANNEL('IBM.APP.SVRCONN') CHLTYPE(SVRCONN) ALTER QMGR CHLAUTH (DISABLED) REFRESH SECURITY TYPE(CONNAUTH) kind : ConfigMap ... Recall that this MQSC configuration was built by the Helm configmap template when the security: false value is set in values.yaml . Now that we have a fully operational queue manager, we can try it out. Try out the deployed queue manager QM1 \u00b6 Now that we have a deployed instance of QM1 running in the dev namespaces, we can use it. In this section, we're going to use both the MQ web console and oc command line to interact with QM1 . Locating the MQ web console You can interact graphically with all the MQ queue managers in a cluster using the MQ web console. Access to the console is achieved via an OCP route. Issue the following command: oc get qmgr qm-dev -n dev -o = jsonpath = '{.status.adminUiUrl}' to show the web console route, for example: https://cpd-tools.xxxxx.containers.appdomain.cloud/integration/messaging/dev/qm-dev-ibm-mq Copy the URL into your browser to launch the MQ web console. (You can safely ignore any browser certificate warnings.) Authenticating to the MQ web console All IBM Cloud Pak products provide the option to use OpenID Connect ( OIDC ) for secure authentication and authorization. If you'd like to know more about why OpenID Connect is important, watch this short video . When QM1 was deployed, it registered with the OpenID Connect provider for the cluster. For example, if your cluster is provisioned using IBM Cloud, it will use the IBM Cloud OIDC provider to provide both secure authentication and secure authorization. As the console launches, you will be presented with two authentication options. Select IBM provided credentials (admin only) to log in with your IBM Cloud OIDC credentials. Login as admin and retrieve the password as follows: oc extract -n ibm-common-services secrets/platform-auth-idp-credentials --keys = admin_username,admin_password --to = - (*You may have to login to your OIDC provider, such as IBM Cloud, if your browser has not recently logged in.) We'll discuss MQ security in more detail in the security chapter Using the MQ web console Once you've logged in to your OIDC provider, you'll be presented with the MQ web console home page for QM1 . This console will allow you to perform most MQ operations such as querying queues and channels, or putting and getting messages. The web console will look like this: We're going to use the console to view a queue that was originally defined in the QM1 source repository. Then we're going to put a message to this queue. Viewing the IBM.DEMO.Q queue on QM1 In the MQ console you can interact with any queue manager that is connected to this console. We're going to interact with QM1 . Click on the Manage tab, or select Manage QM1 tile: You can see a list of all the queues defined on QM1 . Notice the IBM.DEMO.Q for example. See how Maximum depth is 0/5000 ; there are no messages in this queue. See how you can select different operations for each queue such as Create message or Clear queue . Put a message to IBM.DEMO.Q Let's put a message to a the empty queue IBM.DEMO.Q . Select Create message for queue IBM.DEMO.Q to see the Add Message dialogue box: Type Test message 1 in the Application data box. Press Create to put the message to the queue. You'll see a confirmation message: Notice that the queue depth for IBM.DEMO.Q is now 1/5000 : Viewing the MQ pod in OpenShift web console When we interact with QM1 via the MQ web console, we're interacting with a regular MQ queue manager running in a container managed by Kubernetes. You may have noticed the pod qm-dev-ibm-mq-0 when we explored the Kubernetes resources that comprised QM1 using the ArgoCD UI. Issue the following command: oc get pods -n dev to see the queue manager pod for QM1 : NAME READY STATUS RESTARTS AGE qm-dev-ibm-mq-0 1/1 Running 0 2d15h We can see a single pod qm-dev-ibm-mq-0 in the READY state. Each queuemanager custom resource has a single pod. Within this pod is a single container inside which the queue manager QM1 is running. We'll examine this structure in a little more detail in the next section. Connect to the queue manager pod Let's connect to this container, so that we can explore the queue manager QM1 . Issue the following command: oc exec -n dev qm-dev-ibm-mq-0 -it -- /bin/bash to connect you to the pod default container, where you'll see the bash prompt: bash-4.4$ Your terminal is now connected to the container running the queue manager. Use MQ command line The container we've connected to has a fully running instance of the QM1 queue manager configured according to the YAML generated by the Helm chart. We can use regular MQ commands to verify that this is a regular queue manager. Issue the following command: dspmq -o all to see a full list of the queue managers running in the container: QMNAME(QM1) ... STATUS(Running) DEFAULT(yes) STANDBY(Permitted) INSTNAME(Installation1) INSTPATH(/opt/mqm) INSTVER(9.2.0.0) We can see that QM1 is running, it's the default queue manager, where it's installed in the container file system, and it's version. See how a container is actually a full execution environment; in principle, we could have multiple queue managers executing within this container, although this is not good practice. Feel free to issue other MQ commands. Display queue manager properties We can also run an MQSC command to display the queue manager properties. These properties were defined in the MQSC file. Issue the following command: runmqsc QM1 <<< \"dis qmgr\" to see the full set of queue manger properties for QM1 : 5724-H72 (C) Copyright IBM Corp. 1994, 2020. Starting MQSC for queue manager QM1. 1 : dis qmgr AMQ8408I: Display Queue Manager details. QMNAME(QM1) ACCTCONO(DISABLED) ACCTINT(1800) ACCTMQI(OFF) ACCTQ(OFF) ACTIVREC(MSG) ACTVCONO(DISABLED) ACTVTRC(OFF) ADVCAP(ENABLED) ALTDATE(2021-07-30) ALTTIME(17.17.48) AMQPCAP(NO) AUTHOREV(DISABLED) CCSID(819) CERTLABL( ) CERTVPOL(ANY) CHAD(DISABLED) CHADEV(DISABLED) CHADEXIT( ) CHLEV(DISABLED) CHLAUTH(DISABLED) CLWLDATA( ) CLWLEXIT( ) CLWLLEN(100) CLWLMRUC(999999999) CLWLUSEQ(LOCAL) CMDEV(DISABLED) CMDLEVEL(920) COMMANDQ(SYSTEM.ADMIN.COMMAND.QUEUE) CONFIGEV(DISABLED) CONNAUTH(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) CRDATE(2021-07-30) CRTIME(17.17.44) CUSTOM( ) DEADQ(DEV.DEAD.LETTER.QUEUE) DEFCLXQ(SCTQ) DEFXMITQ( ) DESCR( ) DISTL(YES) IMGINTVL(60) IMGLOGLN(OFF) IMGRCOVO(YES) IMGRCOVQ(YES) IMGSCHED(MANUAL) INHIBTEV(DISABLED) IPADDRV(IPV4) LOCALEV(DISABLED) LOGGEREV(DISABLED) MARKINT(5000) MAXHANDS(256) MAXMSGL(4194304) MAXPROPL(NOLIMIT) MAXPRTY(9) MAXUMSGS(10000) MONACLS(QMGR) MONCHL(OFF) MONQ(OFF) PARENT( ) PERFMEV(DISABLED) PLATFORM(UNIX) PSMODE(ENABLED) PSCLUS(ENABLED) PSNPMSG(DISCARD) PSNPRES(NORMAL) PSRTYCNT(5) PSSYNCPT(IFPER) QMID(QM1_2021-07-30_17.17.44) REMOTEEV(DISABLED) REPOS( ) REPOSNL( ) REVDNS(ENABLED) ROUTEREC(MSG) SCHINIT(QMGR) SCMDSERV(QMGR) SPLCAP(ENABLED) SSLCRLNL( ) SSLCRYP( ) SSLEV(DISABLED) SSLFIPS(NO) SSLKEYR(/run/runmqserver/tls/key) SSLRKEYC(0) STATACLS(QMGR) STATCHL(OFF) STATINT(1800) STATMQI(OFF) STATQ(OFF) STRSTPEV(ENABLED) SUITEB(NONE) SYNCPT TREELIFE(1800) TRIGINT(999999999) VERSION(09020000) XRCAP(NO) One MQSC command read. No commands have a syntax error. All valid MQSC commands were processed. Note how CHLAUTH(DISABLED) has been set. This was set in the MQSC file configuration for QM1 in via the mqsc-configmap configmap. Feel free to run other runmqsc commands to explore the queue manager properties. Exit from queue manager container When we're finished exploring the queue manager container, we can exit it. Issue the following command: exit to return to your local machine's command prompt: exit .../git/multi-tenancy-gitops-apps Now that you've tried out the deployed queue manager, let's look at the deployed Kubernetes resources for QM1 in a little more detail. Congratulations! You've completed set up continuous deployment for the queue manager QM1 and deployed an instance of it to the dev namespace in the cluster. You've interacted with this queue manager using the command line as well as the MQ web console. You've also explored the key resources Kubernetes resources deployed to the cluster for QM1 . In the next and final topic of this chapter we're going to make a change to the queue manager, and verify that the change is continuously integrated and deployed to the cluster.","title":"Deploying and using"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic3/#deploying-and-using-the-queue-manager","text":"","title":"Deploying and using the queue manager"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic3/#overview","text":"In the previous topic of this chapter, we ran the queue pipeline using the source repository for QM1 . The pipeline successfully built and tested the queue manager as well as creating versioned resources in the image registry, Artifactory and the GitOps repository. In this topic, you will set up continuous deployment for QM1 that will use the resources created by the pipeline run. We'll examine the activated ArgoCD application that will watch the GitOps folder containing the Helm chart for QM1 and use it and its dependent resources to deploy a running queue manager to the dev namespace in the cluster. Look at the following diagram: We've highlighted the components we're going to explore in this topic: The GitOps repository is used by ArgoCD applications to determine what should be active in the cluster. It contains the latest good deployment configuration for QM1 accessed via its GitOps Helm chart. An ArgoCD application specific for QM1 will monitor a GitOps folder where its Helm chart is held. Whenever a this folder is updated, this ArgoCD application will apply these updates to the cluster, resulting in a new deployment of QM1 . The instance of QM1 running in the cluster is active development queue manager ready for use by MQ applications under development. In this topic, we're going to: Examine the activated ArgoCD application that deploys the QM1 configuration to the cluster. Explore the Kubernetes resources that have been deployed to the cluster for QM1 . Interact with the deployed queue manager QM1 . By the end of this topic we'll have a fully functioning queue manager deployed to the cluster with which we will have interacted.","title":"Overview"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic3/#pre-requisites","text":"Before attempting this topic, you should have successfully completed the previous topic .","title":"Pre-requisites"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic3/#argocd-application-for-qm1","text":"We're now going to move from the integration of QM1 to its deployment . The result of our previously successful mq-infra-dev pipeline run was to create a Helm chart in a GitOps folder for QM1 . This Helm chart brings together all the necessary Kubernetes components for the deployment of QM1 . We're now going to examine the activated ArgoCD application that uses this Helm chart to manage the deployment of QM1 to the dev namespace. The ArgoCD application for QM1 QM1 has its deployment to the cluster managed by a dedicated ArgoCD application called dev-mq-infra-instance . This follows the separation of concerns pattern where one ArgoCD application manages a set of related Kubernetes resources deployed to a cluster; in this case, all those resources associated with queue manager QM1 in the dev namespace. Make sure you are within the GitOps application repository folder: cd $HOME /git cd multi-tenancy-gitops-apps Issue the following command to show the ArgoCD application details: cat mq/config/argocd/dev/dev-mq-infra-instance.yaml which shows a YAML file typical of those we've seen before: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : dev-mq-infra-instance annotations : argocd.argoproj.io/sync-wave : \"300\" finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : dev server : https://kubernetes.default.svc project : applications source : path : mq/environments/dev/mq-infra repoURL : https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps targetRevision : master helm : valueFiles : - values.yaml syncPolicy : automated : prune : true selfHeal : true syncOptions : - Replace=true See how the Helm chart we explored in the previous section of this topic is referenced by path: mq/environments/dev/mq-infra : mq/environments/dev/mq-infra \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 requirements.yaml \u2514\u2500\u2500 values.yaml The ArgoCD application applies this Helm chart to the cluster to instantiate QM1 as a set of cluster resources. Look at active QM1 ArgoCD application Let's examine QM1 and its Kubernetes resources using the ArgoCD UI. In the ArgoCD UI search the Applications view with the keyword mq-infra : ( You may need to launch the ArgoCD UI again. Refer to these instructions . ) We can now see the below ArgoCD Application: A new dev-mq-infra-instance ArgoCD application that is managing QM1 resources deployed to the cluster. View the new QM1 Kubernetes resources We can look at the deployed instance of QM1 and its dependent kubernetes resources. Click on the dev-mq-infra-instance ArgoCD application: We can see that QM1 is comprised of many Kubernetes resources. We'll explore them in more detail later, but for now notice: The QM1 queue manager comprises two main Kubernetes components: the mqsc-configmap configmap resource that holds the MQSC definitions applied to QM1 when it started. the qm-dev queuemanager custom resource that represents the running queue manager. The qm-dev custom resource has a rich structure comprising: a qm-dev-ibm-mq-0 pod where the QM1 binaries are running a qm-dev-ibm-mq-qm route to the queue manager a qm-dev-ibm-mq-qm-web route to the MQ web console a qm-dev-ibm-mq service a qm-dev-ibm-mq-metrics metrics service a qm-dev-ibm-mq service account a qm-dev-ibm-mq stateful set many other resources including secrets, configmaps, endpoints and more These queue manager components are managed by the IBM MQ operator which takes the QM1 YAML and creates (and maintains) these Kubernetes resources. Feel free to click on these components to get an idea of their contents. It's not important to understand this structure in detail -- but it helps to appreciate what's going on when we deploy QM1 . If you're used to running MQ outside of Kubernetes, you may find it interesting to map these Kubernetes resources to MQ components with which you're familiar.","title":"ArgoCD application for QM1"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic3/#explore-the-deployed-kubernetes-resources-for-qm1","text":"Let's spend a few moments exploring the two Kubernetes resources that have been deployed: the queuemanager custom resource named qm-dev , and its MQSC configuration stored in the configmap named mqsc-configmap . Exploring the deployed queue manager using the command line We will explore the qm-dev queuemanager resource from the oc command line. The extensible nature of Kubernetes allows us to examine the new queuemanager custom resource added by the IBM MQ operator. Issue the following command: oc describe queuemanager qm-dev -n dev Notice how API Version: mq.ibm.com/v1beta1 and Kind: QueueManager identify this as an IBM MQ queue manager. Name : qm-dev Namespace : dev Labels : app.kubernetes.io/instance=ibm-mq-dev-instance Annotations : argocd.argoproj.io/sync-wave : 300 helm.sh/hook-weight : 300 API Version : mq.ibm.com/v1beta1 Kind : QueueManager Metadata : Creation Timestamp : 2021-07-29T17:20:26Z Finalizers : finalizer.queuemanagers.mq.ibm.com Generation : 2 Managed Fields : ... Spec : License : Accept : true License : L-RJON-BN7PN3 Metric : VirtualProcessorCore Use : NonProduction Queue Manager : Availability : Type : SingleInstance Debug : false Image : image-registry.openshift-image-registry.svc:5000/ci/mq-infra:0.0.1 Image Pull Policy : Always Liveness Probe : Failure Threshold : 1 Initial Delay Seconds : 90 Period Seconds : 10 Success Threshold : 1 Timeout Seconds : 5 Log Format : Basic Metrics : Enabled : true Mqsc : Config Map : Items : config.mqsc Name : mqsc-configmap Name : QM1 Readiness Probe : Failure Threshold : 1 Initial Delay Seconds : 10 Period Seconds : 5 Success Threshold : 1 Timeout Seconds : 3 Resources : Limits : Cpu : 1 Memory : 1Gi Requests : Cpu : 1 Memory : 1Gi Route : Enabled : true Storage : Persisted Data : Enabled : false Queue Manager : Type : ephemeral Recovery Logs : Enabled : false Security Context : Init Volume As Root : false Template : Pod : Containers : Env : Name : MQSNOAUT Value : yes Name : qmgr Resources : Termination Grace Period Seconds : 30 Tracing : Agent : Collector : Enabled : false Namespace : Version : 9.2.2.0-r1 Web : Enabled : true Status : Admin Ui URL : https://qm-dev-ibm-mq-web-dev.odowda-cloudmq-1-d02cf90349a0fe46c9804e3ab1fe2643-0000.eu-gb.containers.appdomain.cloud/ibmmq/console Conditions : Endpoints : Name : ui Type : UI Uri : https://qm-dev-ibm-mq-web-dev.odowda-cloudmq-1-d02cf90349a0fe46c9804e3ab1fe2643-0000.eu-gb.containers.appdomain.cloud/ibmmq/console Name : QM1 Phase : Running Versions : Available : Channels : Versions : Name : 9.2.2.0-r1 Reconciled : 9.2.2.0-r1 Events : <none> (We've abbreviated it slightly to exclude the Managed Fields: YAML detail.) Here are a few things to note: Spec.Queue Manager.Name identities the name of the queue manager as QM1 Name: identifies the name of the Kubernetes resource as qm-dev . Note how it incorporates the fact that the queue manager is running in the dev namespace. (This name should really be qm1-dev !) Spec.Queue Manager.Image: identifies the image stored in the image registry that this queue manager is running. Spec.Queue Manager.Mqsc: identifies the name of the config map mqsc-config map. Notice how we cannot see the internal structure of the queue manager. Instead we can see the YAML that was built from the QM1 Helm chart and applied to the cluster. For more details on this configuration, see the API reference for the QueueManager (mq.ibm.com/v1beta1) . Exploring the MQSC configuration for QM1 Let's now look at the MQSC configuration for this queue manager. As we've seen, it's contained in the mqsc-configmap configmap. Issue the following command oc get configmap mqsc-configmap -n dev -o yaml to show the MQSC commands that are applied to the queue manager when it starts: apiVersion : v1 data : config.mqsc : | DEFINE QLOCAL(IBM.DEMO.Q) BOQNAME(IBM.DEMO.Q.BOQ) BOTHRESH(3) REPLACE DEFINE QLOCAL(IBM.DEMO.Q.BOQ) REPLACE * Use a different dead letter queue, for undeliverable messages DEFINE QLOCAL('DEV.DEAD.LETTER.QUEUE') REPLACE ALTER QMGR DEADQ('DEV.DEAD.LETTER.QUEUE') DEFINE CHANNEL('IBM.APP.SVRCONN') CHLTYPE(SVRCONN) ALTER QMGR CHLAUTH (DISABLED) REFRESH SECURITY TYPE(CONNAUTH) kind : ConfigMap ... Recall that this MQSC configuration was built by the Helm configmap template when the security: false value is set in values.yaml . Now that we have a fully operational queue manager, we can try it out.","title":"Explore the deployed Kubernetes resources for QM1"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic3/#try-out-the-deployed-queue-manager-qm1","text":"Now that we have a deployed instance of QM1 running in the dev namespaces, we can use it. In this section, we're going to use both the MQ web console and oc command line to interact with QM1 . Locating the MQ web console You can interact graphically with all the MQ queue managers in a cluster using the MQ web console. Access to the console is achieved via an OCP route. Issue the following command: oc get qmgr qm-dev -n dev -o = jsonpath = '{.status.adminUiUrl}' to show the web console route, for example: https://cpd-tools.xxxxx.containers.appdomain.cloud/integration/messaging/dev/qm-dev-ibm-mq Copy the URL into your browser to launch the MQ web console. (You can safely ignore any browser certificate warnings.) Authenticating to the MQ web console All IBM Cloud Pak products provide the option to use OpenID Connect ( OIDC ) for secure authentication and authorization. If you'd like to know more about why OpenID Connect is important, watch this short video . When QM1 was deployed, it registered with the OpenID Connect provider for the cluster. For example, if your cluster is provisioned using IBM Cloud, it will use the IBM Cloud OIDC provider to provide both secure authentication and secure authorization. As the console launches, you will be presented with two authentication options. Select IBM provided credentials (admin only) to log in with your IBM Cloud OIDC credentials. Login as admin and retrieve the password as follows: oc extract -n ibm-common-services secrets/platform-auth-idp-credentials --keys = admin_username,admin_password --to = - (*You may have to login to your OIDC provider, such as IBM Cloud, if your browser has not recently logged in.) We'll discuss MQ security in more detail in the security chapter Using the MQ web console Once you've logged in to your OIDC provider, you'll be presented with the MQ web console home page for QM1 . This console will allow you to perform most MQ operations such as querying queues and channels, or putting and getting messages. The web console will look like this: We're going to use the console to view a queue that was originally defined in the QM1 source repository. Then we're going to put a message to this queue. Viewing the IBM.DEMO.Q queue on QM1 In the MQ console you can interact with any queue manager that is connected to this console. We're going to interact with QM1 . Click on the Manage tab, or select Manage QM1 tile: You can see a list of all the queues defined on QM1 . Notice the IBM.DEMO.Q for example. See how Maximum depth is 0/5000 ; there are no messages in this queue. See how you can select different operations for each queue such as Create message or Clear queue . Put a message to IBM.DEMO.Q Let's put a message to a the empty queue IBM.DEMO.Q . Select Create message for queue IBM.DEMO.Q to see the Add Message dialogue box: Type Test message 1 in the Application data box. Press Create to put the message to the queue. You'll see a confirmation message: Notice that the queue depth for IBM.DEMO.Q is now 1/5000 : Viewing the MQ pod in OpenShift web console When we interact with QM1 via the MQ web console, we're interacting with a regular MQ queue manager running in a container managed by Kubernetes. You may have noticed the pod qm-dev-ibm-mq-0 when we explored the Kubernetes resources that comprised QM1 using the ArgoCD UI. Issue the following command: oc get pods -n dev to see the queue manager pod for QM1 : NAME READY STATUS RESTARTS AGE qm-dev-ibm-mq-0 1/1 Running 0 2d15h We can see a single pod qm-dev-ibm-mq-0 in the READY state. Each queuemanager custom resource has a single pod. Within this pod is a single container inside which the queue manager QM1 is running. We'll examine this structure in a little more detail in the next section. Connect to the queue manager pod Let's connect to this container, so that we can explore the queue manager QM1 . Issue the following command: oc exec -n dev qm-dev-ibm-mq-0 -it -- /bin/bash to connect you to the pod default container, where you'll see the bash prompt: bash-4.4$ Your terminal is now connected to the container running the queue manager. Use MQ command line The container we've connected to has a fully running instance of the QM1 queue manager configured according to the YAML generated by the Helm chart. We can use regular MQ commands to verify that this is a regular queue manager. Issue the following command: dspmq -o all to see a full list of the queue managers running in the container: QMNAME(QM1) ... STATUS(Running) DEFAULT(yes) STANDBY(Permitted) INSTNAME(Installation1) INSTPATH(/opt/mqm) INSTVER(9.2.0.0) We can see that QM1 is running, it's the default queue manager, where it's installed in the container file system, and it's version. See how a container is actually a full execution environment; in principle, we could have multiple queue managers executing within this container, although this is not good practice. Feel free to issue other MQ commands. Display queue manager properties We can also run an MQSC command to display the queue manager properties. These properties were defined in the MQSC file. Issue the following command: runmqsc QM1 <<< \"dis qmgr\" to see the full set of queue manger properties for QM1 : 5724-H72 (C) Copyright IBM Corp. 1994, 2020. Starting MQSC for queue manager QM1. 1 : dis qmgr AMQ8408I: Display Queue Manager details. QMNAME(QM1) ACCTCONO(DISABLED) ACCTINT(1800) ACCTMQI(OFF) ACCTQ(OFF) ACTIVREC(MSG) ACTVCONO(DISABLED) ACTVTRC(OFF) ADVCAP(ENABLED) ALTDATE(2021-07-30) ALTTIME(17.17.48) AMQPCAP(NO) AUTHOREV(DISABLED) CCSID(819) CERTLABL( ) CERTVPOL(ANY) CHAD(DISABLED) CHADEV(DISABLED) CHADEXIT( ) CHLEV(DISABLED) CHLAUTH(DISABLED) CLWLDATA( ) CLWLEXIT( ) CLWLLEN(100) CLWLMRUC(999999999) CLWLUSEQ(LOCAL) CMDEV(DISABLED) CMDLEVEL(920) COMMANDQ(SYSTEM.ADMIN.COMMAND.QUEUE) CONFIGEV(DISABLED) CONNAUTH(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) CRDATE(2021-07-30) CRTIME(17.17.44) CUSTOM( ) DEADQ(DEV.DEAD.LETTER.QUEUE) DEFCLXQ(SCTQ) DEFXMITQ( ) DESCR( ) DISTL(YES) IMGINTVL(60) IMGLOGLN(OFF) IMGRCOVO(YES) IMGRCOVQ(YES) IMGSCHED(MANUAL) INHIBTEV(DISABLED) IPADDRV(IPV4) LOCALEV(DISABLED) LOGGEREV(DISABLED) MARKINT(5000) MAXHANDS(256) MAXMSGL(4194304) MAXPROPL(NOLIMIT) MAXPRTY(9) MAXUMSGS(10000) MONACLS(QMGR) MONCHL(OFF) MONQ(OFF) PARENT( ) PERFMEV(DISABLED) PLATFORM(UNIX) PSMODE(ENABLED) PSCLUS(ENABLED) PSNPMSG(DISCARD) PSNPRES(NORMAL) PSRTYCNT(5) PSSYNCPT(IFPER) QMID(QM1_2021-07-30_17.17.44) REMOTEEV(DISABLED) REPOS( ) REPOSNL( ) REVDNS(ENABLED) ROUTEREC(MSG) SCHINIT(QMGR) SCMDSERV(QMGR) SPLCAP(ENABLED) SSLCRLNL( ) SSLCRYP( ) SSLEV(DISABLED) SSLFIPS(NO) SSLKEYR(/run/runmqserver/tls/key) SSLRKEYC(0) STATACLS(QMGR) STATCHL(OFF) STATINT(1800) STATMQI(OFF) STATQ(OFF) STRSTPEV(ENABLED) SUITEB(NONE) SYNCPT TREELIFE(1800) TRIGINT(999999999) VERSION(09020000) XRCAP(NO) One MQSC command read. No commands have a syntax error. All valid MQSC commands were processed. Note how CHLAUTH(DISABLED) has been set. This was set in the MQSC file configuration for QM1 in via the mqsc-configmap configmap. Feel free to run other runmqsc commands to explore the queue manager properties. Exit from queue manager container When we're finished exploring the queue manager container, we can exit it. Issue the following command: exit to return to your local machine's command prompt: exit .../git/multi-tenancy-gitops-apps Now that you've tried out the deployed queue manager, let's look at the deployed Kubernetes resources for QM1 in a little more detail. Congratulations! You've completed set up continuous deployment for the queue manager QM1 and deployed an instance of it to the dev namespace in the cluster. You've interacted with this queue manager using the command line as well as the MQ web console. You've also explored the key resources Kubernetes resources deployed to the cluster for QM1 . In the next and final topic of this chapter we're going to make a change to the queue manager, and verify that the change is continuously integrated and deployed to the cluster.","title":"Try out the deployed queue manager QM1"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic4/","text":"Continuous updates \u00b6 Overview \u00b6 In the previous topic of this chapter, we ran the queue manager pipeline to build a queue manager ready for deployment. We then deployed it using ArgoCD. Finally, we tested it using the MQ web console and MQ command line. In this topic, we're going to make this integration and deployment processes fully automatic. A change to the queue manager source repository will automatically result in a pipeline run for it. If the pipeline is successful, a new Helm chart will be built and automatically deployed by ArgoCD into the dev namespace. This deployment will update the previously active instance of QM1 . To recap, the whole process will be automatic from end-to-end. Look at the following diagram: We've already seen most of the highlighted components. Notice that we've added a webhook for the source repository. This web hook will be triggered every time the QM1 repository changes and the notification will cause the queue manager pipeline to run automatically. As before, a successful pipeline run will result in a new image being stored in the Image registry, and new Helm charts in Artifactory and the GitOps repository. The already active ArgoCD application will apply the GiOps Helm chart to the cluster resulting in an updated instance of QM1 . In this topic, we're going to: Set up a webhook for queue manager source repository. Make changes to the queue manager source code and push the updates to the source repository. Confirm the new pipeline run and ArgoCD deployment. Verify the change made to QM1 . By the end of this topic we'll have a fully functioning CICD set up in place to deploy the latest changes made to QueueManager to the cluster in an automated fashion. Pre-requisites \u00b6 Before attempting this topic, you should have successfully completed the previous topic . Set up the webhook and its processing \u00b6 A webhook is useful because it allows a system like GitHub to inform another component -- such as our queue manager pipeline -- that a particular repository owned by a user or organization has changed. Importantly, a webhook provides a loosely coupled interaction; GitHub and our pipeline are unaware of each other -- their interaction is mediated by the webhook and some other components that we'll now set up. In this topic section we're going to set up the components we need for our webhook: A GitHub event containing information about the repository change is generated by our webhook and sent to an event listener . In Kubernetes, this event listener comprises a normal route and service front-ending a Tekton eventlistener custom resource. The event listener is driven by the webhook, and runs the pipeline using a Tekton trigger . A trigger has two components, a triggertemplate that specifies the pipeline to run, and a triggerbinding that allows the trigger event data to be presented to the pipeline run in the order it expects. A webhook design looks a little more complex than directly linking GitHub to our pipeline, but as we'll see, this design is much more flexible. This short article provides a nice overview on how to use webhooks with OpenShift. Your GitOps apps repository contains a sample event listener. Let's customize this sample with the relevant values for our cluster. We'll then deploy the event listener resources and trigger resources to the cluster using GitOps. Select the GitOps terminal window Return to the terminal window you're using for the multi-tenancy-gitops-apps GitOps repository. We're going to make use of a script in the sample GitOps apps repository that makes use of the $GIT_ORG and $GIT_BRANCH_QM1 environment variables you've seen earlier. Previously we used them in the mq-infra terminal window; now we're going to use this in the multi-tenancy-gitops-apps terminal window. Again, make sure in you're in the multi-tenancy-gitops-apps terminal window. Issue the following command to ensure you're in the correct starting folder: cd $HOME /git/multi-tenancy-gitops-apps Set up the $GIT_ORG and $GIT_BRANCH_QM1 environment variables Let's set up the environment variables that are used by the customization script. Replace <git-org> in the following command with your GitHub user name: export GIT_ORG = <git-org> Then create the $GIT_BRANCH_QM1 environment variable: export GIT_BRANCH_QM1 = qm1- $GIT_ORG Verify that your $GIT_USER and $GIT_BRANCH environment variables are set: echo $GIT_ORG echo $GIT_BRANCH_QM1 For example: (base) anthonyodowd/git/multi-tenancy-gitops echo GIT_ORG prod-ref-guide (base) anthonyodowd/git/multi-tenancy-gitops echo GIT_BRANCH_QM1 qm1-prod-ref-guide The sample event listener The GitOps repository contains a template for the event listener. Issue the following command to view the template of the event listener YAML: cat mq/environments/ci/eventlisteners/cntk-event-listener.yaml_template apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - name : mq-infra-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_QM1}' && body.repository.full_name == '${GIT_ORG}/mq-infra'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-infra-dev # - name: mq-spring-app-dev # interceptors: # - ref: # name: \"cel\" # params: # - name: \"filter\" # value: \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_SPRING}' && body.repository.full_name == '${GIT_ORG}/mq-spring-app'\" # bindings: # - ref: cnkt-trigger-binding # template: # ref: mq-spring-app-dev If you are using OpenShift 4.7, replace the above template with the following: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - bindings : - kind : TriggerBinding ref : cnkt-trigger-binding interceptors : - cel : filter : header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_QM1}' && body.repository.full_name == '${GIT_ORG}/mq-infra' name : mq-infra-dev template : ref : mq-infra-dev # - bindings: # - kind: TriggerBinding # ref: cnkt-trigger-binding # interceptors: # - cel: # filter: header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_SPRING}' && body.repository.full_name == '${GIT_ORG}/mq-spring-app' # name: mq-spring-app-dev # template: # ref: mq-spring-app-dev Customize the event listeners Create the YAML for the event listener as follows. Issue the following command: cd mq/environments/ci/eventlisteners/ sh ./cntk-event-listener.sh cd ../../../../ Once, this script is run successfully, you should see a new file named cntk-event-listener.yaml . Exploring the event listener customization Let's examine a customized event listener in your local clone. We'll see how it maps to our diagram above, and has been customized for our source repository. Issue the following command: cat mq/environments/ci/eventlisteners/cntk-event-listener.yaml to show the customized event listener: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - name : mq-infra-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/qm1-prod-ref-guide-test' && body.repository.full_name == 'prod-ref-guide-test/mq-infra'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-infra-dev # - name: mq-spring-app-dev # interceptors: # - ref: # name: \"cel\" # params: # - name: \"filter\" # value: \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/master' && body.repository.full_name == 'prod-ref-guide-test/mq-spring-app'\" # bindings: # - ref: cnkt-trigger-binding # template: # ref: mq-spring-app-dev If using OpenShift 4.7, it will be as follows: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - bindings : - kind : TriggerBinding ref : cnkt-trigger-binding interceptors : - cel : filter : header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_QM1}' && body.repository.full_name == 'prod-ref-guide-test/mq-infra' name : mq-infra-dev template : ref : mq-infra-dev # - bindings: # - kind: TriggerBinding # ref: cnkt-trigger-binding # interceptors: # - cel: # filter: header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/qm1-prod-ref-guide-test' && body.repository.full_name == '${GIT_ORG}/mq-spring-app' # name: mq-spring-app-dev # template: # ref: mq-spring-app-dev An event listener can produce one or more triggers as specified by the list spec.triggers: . Our event listener produces a trigger called mq-infra-dev . This trigger comprises the template mq-infra-dev and template binding cntk-trigger-binding . It also has an interceptor; the trigger will only be called if filter: evaluates to true . Even though we haven't explored interceptors yet, filter value of the interceptor: is clearly filtering GitHub push events for the prod-ref-guide/mq-infra repository, i.e. the source repository for QM1 . It was the customization script that made this customization. You can read more about event listeners, triggers and interceptors here . Push GitOps changes to GitHub Let\u2019s make these GitOps changes and push them. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Customizing the mq-infra event trigger\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 12, done. Counting objects: 100% (12/12), done. Delta compression using up to 8 threads Compressing objects: 100% (7/7), done. Writing objects: 100% (7/7), 863 bytes | 863.00 KiB/s, done. Total 7 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git edf584c..9d1e713 master -> master Activate the webhook components Now that we've customized the event listener for our source repository, let's activate all the components necessary to process our webhook: the route, event listener, trigger template and trigger binding. Open the kustomization.yaml that is under mq/environments/ci/ . This contains all the resources that belong to the ci namespace. cat mq/environments/ci/kustomization.yaml resources : #- certificates/ci-mq-client-certificate.yaml #- certificates/ci-mq-server-certificate.yaml - configmaps/gitops-repo-configmap.yaml #- eventlisteners/cntk-event-listener.yaml #- triggerbindings/cntk-binding.yaml #- triggertemplates/mq-infra-dev.yaml #- triggertemplates/mq-spring-app-dev.yaml - pipelines/ibm-test-pipeline-for-dev.yaml - pipelines/ibm-test-pipeline-for-stage.yaml #- pipelines/java-maven-dev-pipeline.yaml - pipelines/mq-pipeline-dev.yaml - pipelines/mq-spring-app-dev-pipeline.yaml - roles/custom-pipeline-sa-clusterrole.yaml - roles/custom-pipeline-sa-role.yaml - roles/custom-ci-pipeline-sa-rolebinding.yaml - roles/custom-dev-pipeline-sa-rolebinding.yaml - roles/custom-staging-pipeline-sa-rolebinding.yaml - roles/custom-prod-pipeline-sa-rolebinding.yaml #- routes/cntk-route.yaml - secrets/artifactory-access-secret.yaml - secrets/git-credentials-secret.yaml - secrets/ibm-entitled-registry-credentials-secret.yaml #- secrets/mq-client-jks-password-secret.yaml - tasks/10-gitops.yaml - tasks/12-functional-tests.yaml - tasks/13-jmeter-performance-test.yaml - tasks/4-smoke-tests-mq.yaml - tasks/4-smoke-tests.yaml - tasks/ibm-build-tag-push-v2-6-13.yaml - tasks/ibm-helm-release-v2-6-13.yaml - tasks/ibm-img-release-v2-6-13.yaml - tasks/ibm-img-scan-v2-6-13.yaml - tasks/ibm-java-maven-test-v2-6-13.yaml - tasks/ibm-setup-v2-6-13.yaml - tasks/ibm-tag-release-v2-6-13.yaml Open this file in your editor and uncomment the below resources. - eventlisteners/cntk-event-listener.yaml - triggerbindings/cntk-binding.yaml - triggertemplates/mq-infra-dev.yaml - routes/cntk-route.yaml You will have the following resources un-commented: resources : #- certificates/ci-mq-client-certificate.yaml #- certificates/ci-mq-server-certificate.yaml - configmaps/gitops-repo-configmap.yaml - eventlisteners/cntk-event-listener.yaml - triggerbindings/cntk-binding.yaml - triggertemplates/mq-infra-dev.yaml #- triggertemplates/mq-spring-app-dev.yaml - pipelines/ibm-test-pipeline-for-dev.yaml - pipelines/ibm-test-pipeline-for-stage.yaml #- pipelines/java-maven-dev-pipeline.yaml - pipelines/mq-pipeline-dev.yaml - pipelines/mq-spring-app-dev-pipeline.yaml - roles/custom-pipeline-sa-clusterrole.yaml - roles/custom-pipeline-sa-role.yaml - roles/custom-ci-pipeline-sa-rolebinding.yaml - roles/custom-dev-pipeline-sa-rolebinding.yaml - roles/custom-staging-pipeline-sa-rolebinding.yaml - roles/custom-prod-pipeline-sa-rolebinding.yaml - routes/cntk-route.yaml - secrets/artifactory-access-secret.yaml - secrets/git-credentials-secret.yaml - secrets/ibm-entitled-registry-credentials-secret.yaml #- secrets/mq-client-jks-password-secret.yaml - tasks/10-gitops.yaml - tasks/12-functional-tests.yaml - tasks/13-jmeter-performance-test.yaml - tasks/4-smoke-tests-mq.yaml - tasks/4-smoke-tests.yaml - tasks/ibm-build-tag-push-v2-6-13.yaml - tasks/ibm-helm-release-v2-6-13.yaml - tasks/ibm-img-release-v2-6-13.yaml - tasks/ibm-img-scan-v2-6-13.yaml - tasks/ibm-java-maven-test-v2-6-13.yaml - tasks/ibm-setup-v2-6-13.yaml - tasks/ibm-tag-release-v2-6-13.yaml Update the GitOps repository Let\u2019s commit these changes to make the event listener and trigger resources active in the cluster. Issue the following command: git add mq/environments/ci/kustomization.yaml git commit -s -m \"Add qmgr pipeline webhook\" git push origin $GIT_BRANCH which shows that the changes are pushed to GitHub: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 522 bytes | 522.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps.git e28693e..25f7430 master -> master This change to the GitOps repository can now be used by ArgoCD. The apps-mq-rest-ci-1 argocd application Let's examine the ArgoCD application that manage the applications in ci namespace . In the ArgoCD UI Applications view, click on the icon for the apps-mq-rest-ci-1 application: You will see all the necessary configurations. Now, you will see trigger template, trigger binding and event listener we just created under this application. Exploring the cntk-event-listener Kubernetes resource Let's spend a few moments exploring the event listener resources. If you observe the above picture carefully, below are few things to note: See how the eventlistener custom resource deployed by the ArgoCD application has created a service named el-cntk-event-listener . The event listener route points to this service. The event listener runs in a pod, managed by a replicaset, defined by a deployment. It's this pod that will handle the webhook request, and call the trigger if the interceptor filter evaluates to true . Exploring the resources using the oc command We can use also use the oc command to explore our webhook resources: To view the route that is called by the webhook, issue the following command: oc get route -n ci To view the event listener service that is accessed via the route, issue the following command: oc get eventlistener -n ci To view the trigger template that is used by the event service to call the mq-infra-dev pipeline, issue the following command: oc get triggertemplate -n ci To view the trigger binding that is used to marshal the input for the pipeline run, issue the following command: oc get triggerbinding -n ci You can explore these resources in more detail by adding the -o yaml option. For now, it's enough to understand understand that webhook -> route -> event listener -> trigger template -> pipeline . Let's now configure a GitHub webhook to use these components. The GitHub webhook UI Let's now use the GitHub UI, to configure a webhook that creates an event and sends it to the route whenever the queue manager source repository changes. We'll configure the webhook using the GitHub UI. Issue the following command to determine the URL for the UI: echo https://github.com/ ${ GIT_ORG } /mq-infra/settings/hooks/new for example: https://github.com/prod-ref-guide/mq-infra/settings/hooks/new Copy the URL into your browser to launch the GitHub webpage to configure a new webhook for your mq-infra repository: We need to complete these details to configure our webhook. Configure the webhook To configure a webhook, we need to identify the URL it will call when it generates an event, the format of the event, and for which GitHub events we'll generate an event. Issue the following command to determine the URL of the event listener route: echo http:// $( oc get route el-cnkt-event-listener -n ci -o jsonpath = '{.spec.host}' ) for example: http://el-cnkt-event-listener-ci.xxxxx.containers.appdomain.cloud Here's a sample webhook configuration: Configure the following arguments: Set the Payload URL using the event listener route address. Set Content type to application/json . Select Let me select individual event Select Pull requests and Pushes from the list of available events. Click on Add webhook to create the webhook. A new webhook has been added In the GitHub UI, you can see that a new webhook has been added: Notice the webhook's name and that it's generating and event whenever a pull-request or push is issued against this repository. Let's now make a change to the QM1 source repository and watch the webhook at work. Making a change to the queue manager QM1 \u00b6 Now that we've configured our webhook, together with the Kubernetes resources to process an event generated by it, let's put it to work. In this section, we'll add a new queue to QM1 using GitOps. To do this, we'll update the QM1 source repository -- adding a the new queue definition to the queue manager MQSC configuration -- and push this change to GitHub. This will trigger a new pipeline run that will build and test our change. If successful, new versions of the queue manager image version and Helm chart will be produced. The GitOps Helm chart will be automatically deployed to the cluster by our ArgoCD application, resulting in an updated deployment for QM1 that contains the new queue definition. Select the QM1 source repository terminal window We're going to make changes to the QM1 source repository, so let's make sure we're in the correct terminal window and folder. Return to the terminal window you're using for the mq-infra source repository. ( Rather than the terminal window you're using for the multi-tenancy-gitops-apps GitOps repository. ) Issue the following command to ensure you're in the correct folder. cd $HOME /git/mq-infra Issue the following command to ensure that the $GIT_ORG environment variable is set: echo $GIT_ORG which will show your configured value, for example: prod-ref-guide Set up environment variable for new Git branch The sample repository has a single master branch. We're going to create a new branch based on master for our changes. Set the $GIT_BRANCH_QM1 using your $GIT_ORG environment variable: export GIT_BRANCH_QM1 = qm1- $GIT_ORG then create a new branch using its value: git checkout -b $GIT_BRANCH_QM1 you'll see something like: Switched to a new branch 'qm1-prod-ref-guide' Notice how we've created a new branch $GIT_BRANCH_QM1 based on the master branch. All changes will be made in $GIT_BRANCH_QM1 ; whereas master will remain unchanged. Also note that we use $GIT_BRANCH_QM1 to name the mq-infra queue manager branch in contrast to $GIT_BRANCH for the multi-tenancy-gitops-apps GitOps branch. This helps stop us accidentally promoting a change if we use the wrong terminal window in the tutorial. Verify you're on the new branch We're doing our changes in the $GIT_BRANCH_QM1 branch. Confirm you're on the new branch: git branch which will list all the branches for your local clone of the mq-infra repository: master * qm1-prod-ref-guide Note how * confirms your branch. Recap where QM1 MQSC definitions are stored in the source repository The QM1 configuration is completely held within the mq-infra source repository. Let's recall this structure: Issue the following command: tree to view the folder structure: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 README.md \u2514\u2500\u2500 chart \u2514\u2500\u2500 base \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 config.mqsc \u251c\u2500\u2500 security \u2502 \u2514\u2500\u2500 config.mqsc \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 configmap.yaml \u2502 \u2514\u2500\u2500 qm-template.yaml \u2514\u2500\u2500 values.yaml You may recall that the that the values.yaml file has security: false set by default. This value is used by the configmap.yaml template to generate a configmap using chart/bases/config/config.msqc rather than chart/bases/security/config.msqc . It's therefore this file we need to update to add a new queue. Update config.mqsc MQSC file Let's quickly review the current MQSC file, before we add our new queue. Issue the following command: cat chart/base/config/config.mqsc to view the MQSC definitions: DEFINE QLOCAL(IBM.DEMO.Q) BOQNAME(IBM.DEMO.Q.BOQ) BOTHRESH(3) REPLACE DEFINE QLOCAL(IBM.DEMO.Q.BOQ) REPLACE * Use a different dead letter queue, for undeliverable messages DEFINE QLOCAL('DEV.DEAD.LETTER.QUEUE') REPLACE ALTER QMGR DEADQ('DEV.DEAD.LETTER.QUEUE') DEFINE CHANNEL('IBM.APP.SVRCONN') CHLTYPE(SVRCONN) ALTER QMGR CHLAUTH (DISABLED) REFRESH SECURITY TYPE(CONNAUTH) Using your favorite editor, add a new queue definition to chart/base/config/config.mqsc : DEFINE QLOCAL(IBM.DEMO.Q.NEW) BOQNAME(IBM.DEMO.Q.BOQ) BOTHRESH(3) REPLACE DEFINE QLOCAL(IBM.DEMO.Q) BOQNAME(IBM.DEMO.Q.BOQ) BOTHRESH(3) REPLACE DEFINE QLOCAL(IBM.DEMO.Q.BOQ) REPLACE * Use a different dead letter queue, for undeliverable messages DEFINE QLOCAL('DEV.DEAD.LETTER.QUEUE') REPLACE ALTER QMGR DEADQ('DEV.DEAD.LETTER.QUEUE') DEFINE CHANNEL('IBM.APP.SVRCONN') CHLTYPE(SVRCONN) ALTER QMGR CHLAUTH (DISABLED) REFRESH SECURITY TYPE(CONNAUTH) Notice how we've added the IBM.DEMO.Q.NEW queue to the list of queues for this queue manager. Commit change to GitHub Let's now commit these changes to GitHub. git add chart/base/config/config.mqsc git commit -s -m \"Updated MQSC definition with queue IBM.DEMO.Q.NEW\" git push origin $GIT_BRANCH_QM1 We see that the changes are pushed to GitHub: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (6/6), 508 bytes | 508.00 KiB/s, done. Total 6 (delta 3), reused 0 (delta 0) remote: Resolving deltas: 100% (3/3), completed with 3 local objects. To https://github.com/prod-ref-guide/mq-infra 5245247..cf4dd8d qm1-prod-ref-guide -> qm1-prod-ref-guide GitHub event generated This push to GitHub should generate a GitHub webhook event for the mq-infra repository. We can look at the most recent webhook events. Click on the newly created webhook http://el-cnkt-event-listener... and select the Recent Deliveries tab. This is one of our sample Deliveries: Notice: The date and time when the webhook event was generated in the format yyyy-mm-dd hh:mm:ss The Request URL header identifies the event. listener http:://el-cnkt-event-listener... \"ref\": \"refs/head/qm1-odowdaibm\" identifies the branch that was pushed. \"repository.full_name\": \"odowdaibm-mq-guide/mq-infra\" identifies the repository that was changed. The Recent Deliveries tab can be very helpful to diagnose any issues you might have webhook event generation. Let's now see how this webhook event is handled by the event listener in the cluster. Event handled in the cluster Let's have a look at how the GitHub event is processed by the event listener in the cluster. We can look at the event listener logs to see how the event is received and processed using the oc logs command. Issue the following command: oc logs deploy/el-cnkt-event-listener -n ci to show the log messages issued by the el-cnkt-event-listener : { \"level\" : \"info\" , \"ts\" : \"2021-08-05T14:31:05.484Z\" , \"logger\" : \"eventlistener\" , \"caller\" : \"sink/sink.go:236\" , \"msg\" : \"ResolvedParams : [{Name:gitrevision Value:840ec76946c891ace8ebf591592077f259f4e5c1} {Name:gitrepositoryurl Value:https://github.com/odowdaibm-mq-guide/mq-infra}]\" , \"knative.dev/controller\" : \"eventlistener\" , \"/triggers-eventid\" : \"2dd90e6b-b97e-4a2e-8aa1-0de0704af7ba\" , \"/trigger\" : \"mq-infra-dev\" } { \"level\" : \"info\" , \"ts\" : \"2021-08-05T14:31:05.490Z\" , \"logger\" : \"eventlistener\" , \"caller\" : \"resources/create.go:95\" , \"msg\" : \"Generating resource: kind: &APIResource{Name:pipelineruns,Namespaced:true,Kind:PipelineRun,Verbs:[delete deletecollection get list patch create update watch],ShortNames:[pr prs],SingularName:pipelinerun,Categories:[tekton tekton-pipelines],Group:tekton.dev,Version:v1beta1,StorageVersionHash:RcAKAgPYYoo=,}, name: mq-infra-dev-\" , \"knative.dev/controller\" : \"eventlistener\" } { \"level\" : \"info\" , \"ts\" : \"2021-08-05T14:31:05.490Z\" , \"logger\" : \"eventlistener\" , \"caller\" : \"resources/create.go:103\" , \"msg\" : \"For event ID \\\"2dd90ez6b-b97e-4a2e-8aa1-0de0704af7babeta1, Resource=pipelineruns\" , \"knative.dev/controller\" : \"eventlistener\" } (You can scroll right in the above output.) Notice: Multiple log entries have been issued for the same input event. Notice how the timestamps \"ts\": are the same. The first entry identifies the data generated by the trigger template that will be passed to the pipeline: See how gitrevision value matches that in the webhook \"after\" value. See how gitrepositoryurl value matches your fork of the mq-infra repository. The second entry informs us that a new PipelineRun has been created for the pipeline mq-infra-dev . The third entry shows us the event ID that the event listener is using for the processing of this webhook event. The order of these events isn't really important; what's clear is that the pipeline run has been launched based on the webhook notification processes by the event listener. The oc logs command can be very helpful to diagnose any issues you might have with the event listener. The new pipeline run starts We can now see that our pipeline run has started. In the OpenShift web console, navigate to Pipelines->Pipelines . Select the mq-infra-dev pipeline and Project: ci : We can see a new pipeline run for mq-infra-dev is in progress. Notice the Labels for this pipeline run: See how the tekton event listener cnkt-event-listener is identified. See how the trigger-eventid allows us to link this pipeline run back to a Tekton event for traceability. Exploring the new pipeline run \u00b6 Our pipeline run has started and will take a little time to complete. We already know what our pipeline run will do; in this section we explore how it does it by looking at the build task in more detail. We'll also review the new and updated resources produced by the pipeline run. These include new versions of the image and Artifactory Helm chart, as well as an updated GitOps Helm chart. We'll see how these combine to reflect the change we made to QM1 . Look at pipeline run Let\u2019s find the queue manager pipeline using the OpenShift web console. Navigate to Pipelines->Pipelines in the left hand pane, and select Project: ci , to show all pipelines in the ci namespace: You can see that there is an mq-infra-dev pipeline run in progress. (You may need to scroll through the list of pipelines.) Pipeline tasks run and complete As before, because we can think of a pipeline run as an instance of a pipeline, we can see the pipeline structure from its run. In the Pipelines view, click on the mq-infra-dev-xxxxx pipeline run: Again, as before, we can see the tasks comprising the run, such as setup , build and GitOps . Notice how tasks are in one of three states: completed , in progress or waiting to run . See how the build task is implemented We're going to examine the build task in some detail. Once you see how the build task works, you'll be able to work out how any task works and how to modify it, as well as how to write your own tasks. We'll look at the build task YAML to see how it's implemented. We'll see how it gets its input, how it is coded, how it references secrets and how it interacts with external components, for example storing a new image in the image registry. In the Pipeline Run details view, click on the YAML tab: This is the pipeline run YAML. You may recall that a pipeline YAML has no code defined within it. In contrast, a pipelinerun YAML includes all the code that implements each task. It means we can scroll through a single file to see the pipeline run inputs, the task inputs and the code that implements each of the steps within each task. It makes it easy to understand what's happening as the pipeline executes because everything is in one place. We can of course see the same output on the command line. Issue the following command, replacing xxxxx with the current pipeline run identifier: oc get pipelinerun mq-infra-dev-xxxxx -n ci -o yaml which will show you all the same information on the command line: apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : app.openshift.io/runtime : mq ... creationTimestamp : \"2021-08-05T14:31:05Z\" generateName : mq-infra-dev- ... We've only included the beginning of the YAML file, as it's quite large, but you can see that it's the same as the web console YAML file. Exploring the build task YAML We can locate the source of the task code in our GitOps source repository. In the Pipeline Run details YAML tab view, search for ibm-build-tag-push-v2-6-13 to find where the build task refers to its implementation code. Use Cmd+F to search for ibm-build-tag-push-v2-6-13 : ... - name : build params : - name : git-url value : $(tasks.setup.results.git-url) - name : git-revision value : $(tasks.setup.results.git-revision) - name : source-dir value : $(tasks.setup.results.source-dir) - name : image-server value : $(tasks.setup.results.image-server) - name : image-namespace value : $(tasks.setup.results.image-namespace) - name : image-repository value : $(tasks.setup.results.image-repository) - name : image-tag value : $(tasks.setup.results.image-tag) runAfter : - setup taskRef : kind : Task name : ibm-build-tag-push-v2-6-13 - name : smoke-tests-mq ... We've already reviewed this build task definition. Recall that the taskRef refers to the task ibm-build-tag-push-v2-6-13 which implements the task code for build . The build task YAML source This build task is just one of many tasks defined in the mq-pipeline-dev.yaml file we explored earlier. Lets have a look at the definition of this task. Issue the following command: cat mq/environments/ci/tasks/ibm-build-tag-push-v2-6-13.yaml to show ibm-build-tag-push-v2-6-13 among the many YAMLs in this file: apiVersion : tekton.dev/v1beta1 kind : Task metadata : name : ibm-build-tag-push-v2-6-13 annotations : description : Executes logic to build, tag and push a container image using the intermediate sha tag to the image-url app.openshift.io/description : Executes logic to build, tag and push a container image using the intermediate sha tag to the image-url app.openshift.io/vcs-uri : https://github.com/IBM/ibm-garage-tekton-tasks app.openshift.io/vcs-ref : master labels : version : 2.6.13 spec : params : - name : git-url - name : git-revision default : master - name : source-dir default : /source - name : image-server default : \"\" - name : image-namespace default : \"\" - name : image-repository default : \"\" - name : image-tag default : \"\" - name : BUILDER_IMAGE default : quay.io/buildah/stable:v1.15.0 - name : DOCKERFILE default : ./Dockerfile - name : CONTEXT default : . - name : TLSVERIFY default : \"false\" - name : FORMAT default : \"docker\" - name : STORAGE_DRIVER description : Set buildah storage driver default : overlay volumes : - name : varlibcontainers emptyDir : {} - name : source emptyDir : {} stepTemplate : volumeMounts : - name : source mountPath : $(params.source-dir) steps : - name : git-clone image : quay.io/ibmgaragecloud/alpine-git env : - name : GIT_PASSWORD valueFrom : secretKeyRef : name : git-credentials key : password optional : true - name : GIT_USERNAME valueFrom : secretKeyRef : name : git-credentials key : username optional : true script : | set +x if [[ -n \"${GIT_USERNAME}\" ]] && [[ -n \"${GIT_PASSWORD}\" ]]; then git clone \"$(echo $(params.git-url) | awk -F '://' '{print $1}')://${GIT_USERNAME}:${GIT_PASSWORD}@$(echo $(params.git-url) | awk -F '://' '{print $2}')\" $(params.source-dir) else set -x git clone $(params.git-url) $(params.source-dir) fi set -x cd $(params.source-dir) git checkout $(params.git-revision) - name : build image : $(params.BUILDER_IMAGE) workingDir : $(params.source-dir) env : - name : REGISTRY_USER valueFrom : secretKeyRef : name : registry-access key : REGISTRY_USER optional : true - name : REGISTRY_PASSWORD valueFrom : secretKeyRef : name : registry-access key : REGISTRY_PASSWORD optional : true - name : IBM_ENTITLED_REGISTRY_USER valueFrom : secretKeyRef : name : ibm-entitled-registry-credentials key : IBM_ENTITLED_REGISTRY_USER optional : true - name : IBM_ENTITLED_REGISTRY_PASSWORD valueFrom : secretKeyRef : name : ibm-entitled-registry-credentials key : IBM_ENTITLED_REGISTRY_PASSWORD optional : true volumeMounts : - mountPath : /var/lib/containers name : varlibcontainers securityContext : privileged : true script : | APP_IMAGE=\"$(params.image-server)/$(params.image-namespace)/$(params.image-repository):$(params.image-tag)\" if [[ -n \"${IBM_ENTITLED_REGISTRY_USER}\" ]] && [[ -n \"${IBM_ENTITLED_REGISTRY_PASSWORD}\" ]]; then buildah login -u ${IBM_ENTITLED_REGISTRY_USER} -p ${IBM_ENTITLED_REGISTRY_PASSWORD} cp.icr.io echo \"buildah login -u \"${IBM_ENTITLED_REGISTRY_USER}\" -p \"xxxxx\" cp.icr.io\" fi buildah --layers --storage-driver=$(params.STORAGE_DRIVER) bud --format=$(params.FORMAT) --tls-verify=$(params.TLSVERIFY) -f $(params.DOCKERFILE) -t ${APP_IMAGE} $(params.CONTEXT) set +x if [[ -n \"${REGISTRY_USER}\" ]] && [[ -n \"${REGISTRY_PASSWORD}\" ]] && [[ \"$(params.image-server)\" != \"image-registry.openshift-image-registry.svc:5000\" ]]; then buildah login -u \"${REGISTRY_USER}\" -p \"${REGISTRY_PASSWORD}\" \"$(params.image-server)\" echo \"buildah login -u \"${REGISTRY_USER}\" -p \"xxxxx\" \"$(params.image-server)\"\" fi set -x buildah --storage-driver=$(params.STORAGE_DRIVER) push --tls-verify=$(params.TLSVERIFY) --digestfile ./image-digest ${APP_IMAGE} docker://${APP_IMAGE} Notice how: spec: contains a list of params: as the set of task inputs. name: git-revision is an example of named input, and it has a default value specified as default: master . spec: contains volumes: and volumeMounts: that apply to every step. Exploring the build task steps -- configuration We can explore the build task structure using the output from the OpenShift web console, the oc get pipelinerun command. Let's spend a few moments understanding how build works; you can review the code using your local editor or the OpenShift web console. We've also included the relevant snippets: steps : - name : git-clone image : quay.io/ibmgaragecloud/alpine-git env : - name : GIT_PASSWORD valueFrom : secretKeyRef : name : git-credentials key : password optional : true - name : GIT_USERNAME valueFrom : secretKeyRef : name : git-credentials key : username optional : true script : | ... - name : build image : $(params.BUILDER_IMAGE) workingDir : $(params.source-dir) env : ... volumeMounts : - mountPath : /var/lib/containers name : varlibcontainers securityContext : privileged : true script : | ... Notice how: The build task steps are defined in the steps: YAML node. The build task has two steps: name: git-clone and name: build . The git-clone step runs in its own container using image: quay.io/ibmgaragecloud/alpine-git The build step runs in its own container using the image specified by the task input variable $(params.BUILDER_IMAGE) . Notice this has a default value quay.io/buildah/stable:v1.15.0 The git-clone step sets up an environment variable GIT_PASSWORD using the the password key in the git-credentials secret. This allows the task to use the git credentials we configured in this chapter earlier As required, a step can specify any node from the spec.containers[]: specification such as workingDir: , volumeMount: and securityContext: . The code executed in each step container is specified by its script: . Let's now see how a build step can be coded to use this configuration. Exploring the build task steps -- scripts When the build step container runs, it executes the code specified in its script: . Find the build step script in web console pipelinerun YAML output, using the oc command. We've shown the script: below: script : | APP_IMAGE=\"$(params.image-server)/$(params.image-namespace)/$(params.image-repository):$(params.image-tag)\" if [[ -n \"${IBM_ENTITLED_REGISTRY_USER}\" ]] && [[ -n \"${IBM_ENTITLED_REGISTRY_PASSWORD}\" ]]; then buildah login -u ${IBM_ENTITLED_REGISTRY_USER} -p ${IBM_ENTITLED_REGISTRY_PASSWORD} cp.icr.io echo \"buildah login -u \"${IBM_ENTITLED_REGISTRY_USER}\" -p \"xxxxx\" cp.icr.io\" fi buildah --layers --storage-driver=$(params.STORAGE_DRIVER) bud --format=$(params.FORMAT) --tls-verify=$(params.TLSVERIFY) -f $(params.DOCKERFILE) -t ${APP_IMAGE} $(params.CONTEXT) set +x if [[ -n \"${REGISTRY_USER}\" ]] && [[ -n \"${REGISTRY_PASSWORD}\" ]] && [[ \"$(params.image-server)\" != \"image-registry.openshift-image-registry.svc:5000\" ]]; then buildah login -u \"${REGISTRY_USER}\" -p \"${REGISTRY_PASSWORD}\" \"$(params.image-server)\" echo \"buildah login -u \"${REGISTRY_USER}\" -p \"xxxxx\" \"$(params.image-server)\"\" fi set -x buildah --storage-driver=$(params.STORAGE_DRIVER) push --tls-verify=$(params.TLSVERIFY) --digestfile ./image-digest ${APP_IMAGE} docker://${APP_IMAGE} This script will execute within the step-build container and will have access to the container environment set up for this step and task. The script makes extensive use of the buildah command; it is often used in combination with skopeo and podman as a compatible alternative to the docker command for container management. You can learn more about these commands in this short article . See how the script will: Use the buildah login command to login to the IBM Entitled Registry using the $IBM_ENTITLED_REGISTRY_USER and $IBM_ENTITLED_REGISTRY_PASSWORD credentials you set up earlier. It needs this access because the Dockerfile used to build the queue manager image uses an IBM entitled MQ image. Use the buildah bud command to build a queue manager image using the Dockerfile specified in $params.DOCKERFILE . Use the buildah push command to store the newly built queue manager image to the cluster image registry. The step does support external registries such as quay.io if $REGISTRY_USER and $REGISTRY_PASSWORD are set. Name the queue manager image using the $APP_IMAGE variable. See how the name is structured as $(params.image-server)/$(params.image-namespace)/$(params.image-repository):$(params.image-tag) to uniquely identify it. We'll see an example in a moment. Linking task output to the build log As well as reading the script, we can see the actual output generated by the script OpenShift web console. In the web console, select the Pipeline Run details Logs tab for mq-infra-dev-xxxxx and select the build task. Scroll down to the STEP-BUILD output: STEP-BUILD + APP_IMAGE=image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 + [[ -n cp ]] + [[ -n eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJJQk0gTWFya2V0cGxhY2UiLCJpYXQiOjE1OTE3MDA1MTgsImp0aSI6Ijk3ODY4NWY2OTBlYjQwYzhhYzE0YTZjYTMxODA0OTgzIn0.a_pcyExLFeCMRks0DjFwW-VQAM_U5imFwEOu_qm79hY ]] + buildah login -u cp -p eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJJQk0gTWFya2V0cGxhY2UiLCJpYXQiOjE1OTE3MDA1MTgsImp0aSI6Ijk3ODY4NWY2OTBlYjQwYzhhYzE0YTZjYTMxODA0OTgzIn0.a_pcyExLFeCMRks0DjFwW-VQAM_U5imFwEOu_qm79hY cp.icr.io Login Succeeded! + echo 'buildah login -u cp -p xxxxx cp.icr.io' buildah login -u cp -p xxxxx cp.icr.io + buildah --layers --storage-driver=overlay bud --format=docker --tls-verify=false -f ./Dockerfile -t image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 . STEP 1: FROM cp.icr.io/cp/ibm-mqadvanced-server-integration@sha256:cfe3a4cec7a353e7496d367f9789dbe21fbf60dac46f127d288dda329560d13a Getting image source signatures Copying blob sha256:9bd64c9a8b8aef587728a77441425dfb80f760372ffb25bf4df746b1119812c5 Copying blob sha256:33d29d2675612fb16d6440409730a67f6bfcd92e79fb34fc868084d99de717c0 ... Copying config sha256:f573afcfc94cdd00bed6f497048f8adb9968475cdecd86a66aa576f0b3267d5a Writing manifest to image destination Storing signatures STEP 2: COMMIT image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 --> f573afcfc94 f573afcfc94cdd00bed6f497048f8adb9968475cdecd86a66aa576f0b3267d5a + set +x + buildah --storage-driver=overlay push --tls-verify=false --digestfile ./image-digest image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 docker://image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 Getting image source signatures Copying blob sha256:90c2e42f948b524cf98005073e0b0aa2065160abf9e8b314976c064e270d92ac ... Copying config sha256:f573afcfc94cdd00bed6f497048f8adb9968475cdecd86a66aa576f0b3267d5a Writing manifest to image destination Storing signatures This is the output from the container that runs the build step. You can see the results of the script: execution. Scripts often use the set +x , set +e option and echo commands to to generate log output. For example, we can see that the queue manager image name is: + APP_IMAGE=image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 We can also see this image being pushed to the OpenShift image registry: + buildah --storage-driver=overlay push --tls-verify=false --digestfile ./image-digest image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 docker://image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 together with its successful completion: ... Copying config sha256:f573afcfc94cdd00bed6f497048f8adb9968475cdecd86a66aa576f0b3267d5a Writing manifest to image destination Storing signatures Accessing the build task logs using oc logs Each task in the mq-infra-dev-xxxxx pipeline run executes in its own pod. Within these pods, each step runs in its own container. This allows us to view the pipelinerun output using the oc logs command. Issue the following command, replacing xxxxx with your pipelinerun identifier: oc get pods -n ci | grep mq-infra-dev-xxxxx You see all the pods used to execute the different tasks in the pipeline run: mq-infra-dev-mhpw9-build-mqtlq-pod-glrgn 0 /2 Completed 0 2d18h mq-infra-dev-mhpw9-gitops-8tkbp-pod-cz2rc 0 /1 Completed 0 2d18h mq-infra-dev-mhpw9-helm-release-v2crx-pod-fqjwq 0 /2 Completed 0 2d18h mq-infra-dev-mhpw9-img-release-w9lpx-pod-xnk26 0 /1 Completed 0 2d18h mq-infra-dev-mhpw9-img-scan-h7cnd-pod-9xfj7 0 /3 Completed 0 2d18h mq-infra-dev-mhpw9-setup-fz8vs-pod-h4pdm 0 /1 Completed 0 2d18h mq-infra-dev-mhpw9-smoke-tests-mq-dc5kc-pod-ngrjb 0 /4 Completed 0 2d18h mq-infra-dev-mhpw9-tag-release-n7nx2-pod-jc5rf 0 /2 Completed 0 2d18h Notice that: the number of pods matches the number of tasks in the mq-infra-dev pipeline. (In the above example, the pipeline run has finished.) the name of the pod includes the name of the task in the pipeline. each step runs in its own container, for example, the build pod has 0/2 containers running. We can view the logs the of any step in a tasks by viewing its container log. Issue the following command: oc logs mq-infra-dev-mhpw9-build-mqtlq-pod-glrgn step-build -n ci to see the step-build log in the build task: + APP_IMAGE=image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 + [[ -n cp ]] + [[ -n eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJJQk0gTWFya2V0cGxhY2UiLCJpYXQiOjE1OTE3MDA1MTgsImp0aSI6Ijk3ODY4NWY2OTBlYjQwYzhhYzE0YTZjYTMxODA0OTgzIn0.a_pcyExLFeCMRks0DjFwW-VQAM_U5imFwEOu_qm79hY ]] + buildah login -u cp -p eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJJQk0gTWFya2V0cGxhY2UiLCJpYXQiOjE1OTE3MDA1MTgsImp0aSI6Ijk3ODY4NWY2OTBlYjQwYzhhYzE0YTZjYTMxODA0OTgzIn0.a_pcyExLFeCMRks0DjFwW-VQAM_U5imFwEOu_qm79hY cp.icr.io Login Succeeded! + echo 'buildah login -u cp -p xxxxx cp.icr.io' buildah login -u cp -p xxxxx cp.icr.io + buildah --layers --storage-driver=overlay bud --format=docker --tls-verify=false -f ./Dockerfile -t image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 . STEP 1: FROM cp.icr.io/cp/ibm-mqadvanced-server-integration@sha256:cfe3a4cec7a353e7496d367f9789dbe21fbf60dac46f127d288dda329560d13a Getting image source signatures Copying blob sha256:9bd64c9a8b8aef587728a77441425dfb80f760372ffb25bf4df746b1119812c5 ... See how this is the same output as the web console Pipeline Run details view. Wait for pipeline run to finish Return to Pipeline Run details view. If the pipeline run hasn't finished then wait until it does. If it hasn't finished, explore some of the logs of the tasks still running, and see if you can find the step code that they are executing. When the pipeline run completes, you'll see: The pipeline run is now complete. Let's explore the artifacts that mq-infra-dev-xxxxx has updated and created. New Helm chart in Artifactory The first new artifact we look at is the new Helm chart in Artifactory. Navigate to the Artifactory UI On the left hand pane select Artifactory->Artifacts . Within this view, expand the following zipped Helm charts: generic-local->ci->mq-infra-0.0.1.tgz->mq-infra generic-local->ci->mq-infra-0.0.2.tgz->mq-infra Select the Chart.yaml in mq-infra-0.0.2.tgz->mq-infra and View Source tab: Notice that: The pipeline run created a new zipped version of the Helm chart mq-infra-0.0.2.tgz The tgz file name includes a version change from 0.0.1 to 0.0.2 . The Chart.yaml shows also contains a new version 0.0.2 . The config.mqsc file in version 0.0.2 is significantly larger than its equivalent in 0.0.1 corresponding to the new MQSC definition for IBM.DEMO.Q.NEW In summary, the helm-release task in the mq-infra-dev pipeline run has created a new version of the Helm chart containing the new MQSC definition and stored it in Artifactory. The importance of Git tags Notice how the new Helm chart in Artifactory has a version 0.0.2 , incremented from version 0.0.1 . Every time we make a change to a source queue manager repository, a pipeline run generates a new version of the Helm chart to capture this change. As Helm charts (and container images) evolve over time, its version tag reflects the change; the highest version tag always points to the latest version of a Helm chart or image. This version matches the git tag of the mq-infra repository for the most recent change to update the config.mqsc file to add the IBM.DEMO.Q.NEW queue to QM1 . We can see the tags for mq-infra in the GitHub for via the git tags command. Issue the following command: echo https://github.com/ $GIT_ORG /mq-infra/tags to determine the URL for the mq-infra tags UI, for example: https://github.com/odowdaibm-mq-guide/mq-infra/tags Copy your generated URL into your browser to launch the Tags web page for your fork of the mq-infra GitHub repository. ( You can safely ignore any browser certificate warnings. ) You'll see the following page: Notice: Two tags: 0.0.1 and 0.0.2 . 0.0.1 was generated by the first (manual) pipeline run. 0.0.2 was generated by the second (webhook triggered) pipeline run. These tags are used to version the Helm charts in Artifactory. These tags are used by other tasks in the mq-infra-dev pipeline such as image-release and gitops . Generating the tag version When the mq-infra repository is forked, it has no tags. Tags are created by the tag-release task in the mq-infra-dev pipeline. Let's spend a few moments exploring this task: In the Pipeline Run details view, select the tag-release task and Logs tab: Scroll to the bottom of the log output, which culminates in: Pushing to https://odowdaibm:xxxxx@github.com/odowdaibm-mq-guide/mq-infra POST git-receive-pack (340 bytes) To https://github.com/odowdaibm-mq-guide/mq-infra = [up to date] 0.0.1 -> 0.0.1 * [new tag] 0.0.2 -> 0.0.2 ++ git describe --abbrev=0 --tags + NEW_TAG=0.0.2 + [[ -z 0.0.2 ]] + echo -n 0.0.2 + tee /tekton/results/tag 0.0.2 We can see that the end result of this task is to push the new 0.0.2 tag to your fork of the mq-infra repository. This tag can now be used throughout the pipeline whenever we need to version a Helm chart, container image or any other generated artifact. Spend a few moments examining the full log output of the step-git-tag step in the tag-release task. You might also like to explore the ibm-tag-release-v2-6-13 task code: apiVersion : tekton.dev/v1beta1 kind : Task metadata : name : ibm-tag-release-v2-6-13 ... spec : ... steps : ... - name : git-tag image : $(params.js-image) workingDir : $(params.source-dir) script : | #!/usr/bin/env bash set -ex echo \"Current branch: $(git rev-parse --abbrev-ref HEAD)\" git fetch --tags git config --global user.email \"cloud-native-toolkit@example.com\" git config --global user.name \"Cloud Native Toolkit Pipeline\" if [[ $(git describe --tag `git rev-parse HEAD`) =~ (^[0-9]+.[0-9]+.[0-9]+$) ]]; then echo \"Latest commit is already tagged\" NEW_TAG=\"$(git describe --abbrev=0 --tags)\" echo -n \"${NEW_TAG}\" | tee $(results.tag.path) exit 0 fi mkdir -p ~/.npm npm config set prefix ~/.npm export PATH=$PATH:~/.npm/bin npm i -g release-it release-it patch \\ --ci \\ --no-npm \\ --no-git.push \\ --no-git.requireCleanWorkingDir \\ --no-git.requireUpstream \\ -VV if [[ -z \"$(params.skip-push)\" ]]; then set +x git push --tags -v set -x fi NEW_TAG=\"$(git describe --abbrev=0 --tags)\" if [[ -z \"${NEW_TAG}\" ]]; then echo \"Error: NEW_TAG not defined\" exit 1 fi echo -n \"${NEW_TAG}\" | tee $(results.tag.path) See if you can work out how the code in the above script: generates git-tag log output. Updated Helm chart in GitOps The Helm chart stored in Artifactory is often referred to as a base chart because it contains the default configuration for QM1 . It can be overridden by the Helm chart stored in the GitOps repository, the GitOps Helm chart uses a requirements.yaml file to identify the relationship between these two Helm charts. Let's explore the GitOps Helm chart that was updated by the pipeline run. Issue the following command: echo https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps/blob/ $GIT_BRANCH /mq/environments/dev/mq-infra/requirements.yaml to calculate the URL of the GitOps version of the requirements.yaml file, for example: https://github.com/odowdaibm-mq-guide/multi-tenancy-gitops/blob/master/mq/environments/dev/mq-infra/requirements.yaml Copy your version of this URL into a browser to see the GitOps version of the requirements.yaml file that is being accessed by ArgoCD: dependencies: - name: mq-infra version: 0.0.2 repository: http://artifactory-artifactory.tools:8082/artifactory/generic-local/ci Notice: the version: has been updated to 0.0.2 . this chart will now override the newly generated mq-infra-0.0.2.tgz base chart in Artifactory. the 0.0.1 version of the base chart is remains in Artifactory as we saw earlier, but is no longer referenced. It's the gitops task in the mq-infra-dev pipeline run that updated this Helm chart stored in the GitOps repository. You might like to explore the gitops step log in Pipeline Run detail Logs tab. When the queue manager ArgoCD application sees that this GitOps Helm chart has changed, it will redeploy this Helm chart. It will be the combination of the GitOps and Artifactory Helm charts that will define the new queue manager deployment in the the cluster. Viewing the new deployment in ArgoCD The completion of the gitops task results in an update to the GitOps Helm chart requirements.yaml file to identify the base Helm chart version 0.0.2 in Artifactory. This change to the requirements.yaml file will result in the ibm-mq-dev-instance ArgoCD application redeploying the Helm chart. Let's explore this change using ArgoCD. In the ArgoCD UI, select the ibm-mq-dev-instance ArgoCD application: We can see the two components of the queue manager Kubernetes deployment, namely the queuemanager custom resource defining the properties of QM1 and the configmap containing its MQSC configuration. Click on the qm-dev queuemanager custom resource and select the DESIRED MANIFEST tab: Notice: This queue manager uses the newly built container image image-registry.openshift-image-registry.svc:5000/ci/mq-infra:0.0.2 stored in the OpenShift image registry. This image was generated by the build task and tagged by the tag-release task as 0.0.2 . The image version 0.0.2 matches the git repository tag 0.0.2 in the same way as the Helm charts. Now click on the mqsc-configmap configmap and select the DESIRED MANIFEST tab: Notice: New queue IBM.DEMO.Q.NEW has been added to the MQSC file. This configmap matches the configmap we updated in QM1 source repository. Let's now try out the new queue manager. Verify the updates to QueueManager \u00b6 Let's now verify and explore the changes we have made to QM1 , using both the MQ web console and the oc command. Exploring QM1 with the web console Switch to the MQ web console Select Manage QM1 from the home page to see: Notice that: The new queue IBM.DEMO.Q.NEW has been added to the list of queues for QM1 . This is the queue we added to the config.mqsc file in the source repository. The Maximum depth for IBM.DEMO.Q is 0/5000 even though we put a message to this queue in the previous topic. We'll find out a little later what's happened to this message. Exploring the queue manager details To understand a little more about the changes we've made to QM1 , let's examine its statistics page on the MQ console. Click on the Configuration icon in the top right of the Manage view. In the QM configuration view, select Statistics in the left hand pane: Notice the details of when QM1 was created, as well as its QMID. The QMID is a unique name generated when a queue manager is first created. Let's compare these values to the previous values which we generated using the oc command. We've copied the most relevant ones below: 5724-H72 (C) Copyright IBM Corp. 1994, 2020. Starting MQSC for queue manager QM1. 1 : dis qmgr AMQ8408I: Display Queue Manager details. QMNAME(QM1) ... ALTDATE(2021-07-30) ALTTIME(17.17.48) ... CRDATE(2021-07-30) CRTIME(17.17.44) ... QMID(QM1_2021-07-30_17.17.44) One MQSC command read. No commands have a syntax error. All valid MQSC commands were processed. Notice that these two sets of values are completely different ; it's as if we've got a new queue manager. Indeed, that's exactly what happened when we made our configuration change, as we'll now discover. Ephemeral queue manager As we've seen, pods are the fundamental building blocks of Kubernetes. The QM1 queue manager runs in a pod container and when its configuration changes, this pod is replaced by a new one that whose container has an updated spec: that reflects the updated configuration for QM1 . In our scenario this includes both a new image and a new MQSC file containing our new queue definition. Let's look a little more closely at the queuemanager custom resource for QM1 to understand what's happening. Issue the following command: oc get queuemanager qm-dev -n dev -o yaml to show the details of the queue manager: apiVersion : mq.ibm.com/v1beta1 kind : QueueManager ... spec : license : accept : true license : L-RJON-BN7PN3 metric : VirtualProcessorCore use : NonProduction queueManager : availability : type : SingleInstance debug : false image : image-registry.openshift-image-registry.svc:5000/ci/mq-infra:0.0.2 imagePullPolicy : Always livenessProbe : failureThreshold : 1 initialDelaySeconds : 90 periodSeconds : 10 successThreshold : 1 timeoutSeconds : 5 logFormat : Basic metrics : enabled : true mqsc : - configMap : items : - config.mqsc name : mqsc-configmap name : QM1 readinessProbe : failureThreshold : 1 initialDelaySeconds : 10 periodSeconds : 5 successThreshold : 1 timeoutSeconds : 3 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : \"1\" memory : 1Gi route : enabled : true storage : persistedData : enabled : false queueManager : type : ephemeral recoveryLogs : enabled : false securityContext : initVolumeAsRoot : false template : pod : containers : - env : - name : MQSNOAUT value : \"yes\" name : qmgr resources : {} terminationGracePeriodSeconds : 30 tracing : agent : {} collector : {} enabled : false namespace : \"\" version : 9.2.2.0-r1 web : enabled : true status : adminUiUrl : https://qm-dev-ibm-mq-web-dev.mqcloud1-odowda-d02cf90349a0fe46c9804e3ab1fe2643-0000.eu-gb.containers.appdomain.cloud/ibmmq/console conditions : [] endpoints : - name : ui type : UI uri : https://qm-dev-ibm-mq-web-dev.mqcloud1-odowda-d02cf90349a0fe46c9804e3ab1fe2643-0000.eu-gb.containers.appdomain.cloud/ibmmq/console name : QM1 phase : Running versions : available : channels : [] versions : - name : 9.2.2.0-r1 reconciled : 9.2.2.0-r1 Notice the spec.queueManager.storage.queueManager.type: ephemeral . As you can see from the API reference for the QueueManager this defines an ephemeral queue manager. An ephemeral queue manager is one whose logs and queue files are defined within the container file system rather than external to it. It means that when the pod is replaced, all the container queue files and log files are lost. That's why our QMID is changed and our message was lost -- they were stored in the file system of the old container. When the pod restarted for the updated queue manager deployment, it was like a new queue manager was being created because the container file system was new. We rarely use ephemeral queue managers for production work; usually a queue manager stores its queue and log files in a persistent volume using a persistent volume claim that is mounted into the container when the pod starts. Using a persistent volume means that restarting a pod doesn't affect the long term state of the queue manager -- properties like QMID, together with messages in queues are stored in queue and log files that are preserved in the persistent volume claim that is reattached to a restarted container pod. We'll use persistent volumes in the High Availability chapter. For now, our use of an ephemeral queue manager has shown nicely how a configuration change results in an updated queue manager deployment resulting in a pod restart for the queue manager. Congratulations! You've now got a fully automated CICD process for your queue manager. You started the chapter by configuring a webhook to generate an event notification whenever the QM1 source repository was changed. You then created an event listener and trigger in your cluster to process these events and start a pipeline run using the data in the event. After updating the QM1 source repository to add a new queue to its MQSC definitions, a pipeline run was triggered automatically. As the pipeline run progressed, you explored its task logs and how key tasks worked. You explored the detailed structure of the mq-infra-dev pipeline in terms of its tasks and their steps. You explored theses task steps produced new and updated artifacts such as a container image and Helm charts. You saw how git tags were used to version control these artifacts. Once the pipeline run completed, you examined the updated queue manager deployment using ArgoCD. You tried out the updated queue manager to confirm that the new queue had been added to QM1 . Finally, you explored the concept of an ephemeral queue manager its persistent volume, together with how these combine with Kubernetes pods and containers. In the next chapter, we're going to create an MQ application that exploits this queue manager.","title":"Continuous updates"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic4/#continuous-updates","text":"","title":"Continuous updates"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic4/#overview","text":"In the previous topic of this chapter, we ran the queue manager pipeline to build a queue manager ready for deployment. We then deployed it using ArgoCD. Finally, we tested it using the MQ web console and MQ command line. In this topic, we're going to make this integration and deployment processes fully automatic. A change to the queue manager source repository will automatically result in a pipeline run for it. If the pipeline is successful, a new Helm chart will be built and automatically deployed by ArgoCD into the dev namespace. This deployment will update the previously active instance of QM1 . To recap, the whole process will be automatic from end-to-end. Look at the following diagram: We've already seen most of the highlighted components. Notice that we've added a webhook for the source repository. This web hook will be triggered every time the QM1 repository changes and the notification will cause the queue manager pipeline to run automatically. As before, a successful pipeline run will result in a new image being stored in the Image registry, and new Helm charts in Artifactory and the GitOps repository. The already active ArgoCD application will apply the GiOps Helm chart to the cluster resulting in an updated instance of QM1 . In this topic, we're going to: Set up a webhook for queue manager source repository. Make changes to the queue manager source code and push the updates to the source repository. Confirm the new pipeline run and ArgoCD deployment. Verify the change made to QM1 . By the end of this topic we'll have a fully functioning CICD set up in place to deploy the latest changes made to QueueManager to the cluster in an automated fashion.","title":"Overview"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic4/#pre-requisites","text":"Before attempting this topic, you should have successfully completed the previous topic .","title":"Pre-requisites"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic4/#set-up-the-webhook-and-its-processing","text":"A webhook is useful because it allows a system like GitHub to inform another component -- such as our queue manager pipeline -- that a particular repository owned by a user or organization has changed. Importantly, a webhook provides a loosely coupled interaction; GitHub and our pipeline are unaware of each other -- their interaction is mediated by the webhook and some other components that we'll now set up. In this topic section we're going to set up the components we need for our webhook: A GitHub event containing information about the repository change is generated by our webhook and sent to an event listener . In Kubernetes, this event listener comprises a normal route and service front-ending a Tekton eventlistener custom resource. The event listener is driven by the webhook, and runs the pipeline using a Tekton trigger . A trigger has two components, a triggertemplate that specifies the pipeline to run, and a triggerbinding that allows the trigger event data to be presented to the pipeline run in the order it expects. A webhook design looks a little more complex than directly linking GitHub to our pipeline, but as we'll see, this design is much more flexible. This short article provides a nice overview on how to use webhooks with OpenShift. Your GitOps apps repository contains a sample event listener. Let's customize this sample with the relevant values for our cluster. We'll then deploy the event listener resources and trigger resources to the cluster using GitOps. Select the GitOps terminal window Return to the terminal window you're using for the multi-tenancy-gitops-apps GitOps repository. We're going to make use of a script in the sample GitOps apps repository that makes use of the $GIT_ORG and $GIT_BRANCH_QM1 environment variables you've seen earlier. Previously we used them in the mq-infra terminal window; now we're going to use this in the multi-tenancy-gitops-apps terminal window. Again, make sure in you're in the multi-tenancy-gitops-apps terminal window. Issue the following command to ensure you're in the correct starting folder: cd $HOME /git/multi-tenancy-gitops-apps Set up the $GIT_ORG and $GIT_BRANCH_QM1 environment variables Let's set up the environment variables that are used by the customization script. Replace <git-org> in the following command with your GitHub user name: export GIT_ORG = <git-org> Then create the $GIT_BRANCH_QM1 environment variable: export GIT_BRANCH_QM1 = qm1- $GIT_ORG Verify that your $GIT_USER and $GIT_BRANCH environment variables are set: echo $GIT_ORG echo $GIT_BRANCH_QM1 For example: (base) anthonyodowd/git/multi-tenancy-gitops echo GIT_ORG prod-ref-guide (base) anthonyodowd/git/multi-tenancy-gitops echo GIT_BRANCH_QM1 qm1-prod-ref-guide The sample event listener The GitOps repository contains a template for the event listener. Issue the following command to view the template of the event listener YAML: cat mq/environments/ci/eventlisteners/cntk-event-listener.yaml_template apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - name : mq-infra-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_QM1}' && body.repository.full_name == '${GIT_ORG}/mq-infra'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-infra-dev # - name: mq-spring-app-dev # interceptors: # - ref: # name: \"cel\" # params: # - name: \"filter\" # value: \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_SPRING}' && body.repository.full_name == '${GIT_ORG}/mq-spring-app'\" # bindings: # - ref: cnkt-trigger-binding # template: # ref: mq-spring-app-dev If you are using OpenShift 4.7, replace the above template with the following: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - bindings : - kind : TriggerBinding ref : cnkt-trigger-binding interceptors : - cel : filter : header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_QM1}' && body.repository.full_name == '${GIT_ORG}/mq-infra' name : mq-infra-dev template : ref : mq-infra-dev # - bindings: # - kind: TriggerBinding # ref: cnkt-trigger-binding # interceptors: # - cel: # filter: header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_SPRING}' && body.repository.full_name == '${GIT_ORG}/mq-spring-app' # name: mq-spring-app-dev # template: # ref: mq-spring-app-dev Customize the event listeners Create the YAML for the event listener as follows. Issue the following command: cd mq/environments/ci/eventlisteners/ sh ./cntk-event-listener.sh cd ../../../../ Once, this script is run successfully, you should see a new file named cntk-event-listener.yaml . Exploring the event listener customization Let's examine a customized event listener in your local clone. We'll see how it maps to our diagram above, and has been customized for our source repository. Issue the following command: cat mq/environments/ci/eventlisteners/cntk-event-listener.yaml to show the customized event listener: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - name : mq-infra-dev interceptors : - ref : name : \"cel\" params : - name : \"filter\" value : \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/qm1-prod-ref-guide-test' && body.repository.full_name == 'prod-ref-guide-test/mq-infra'\" bindings : - ref : cnkt-trigger-binding template : ref : mq-infra-dev # - name: mq-spring-app-dev # interceptors: # - ref: # name: \"cel\" # params: # - name: \"filter\" # value: \"header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/master' && body.repository.full_name == 'prod-ref-guide-test/mq-spring-app'\" # bindings: # - ref: cnkt-trigger-binding # template: # ref: mq-spring-app-dev If using OpenShift 4.7, it will be as follows: apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : cnkt-event-listener name : cnkt-event-listener spec : serviceAccountName : pipeline triggers : - bindings : - kind : TriggerBinding ref : cnkt-trigger-binding interceptors : - cel : filter : header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/${GIT_BRANCH_QM1}' && body.repository.full_name == 'prod-ref-guide-test/mq-infra' name : mq-infra-dev template : ref : mq-infra-dev # - bindings: # - kind: TriggerBinding # ref: cnkt-trigger-binding # interceptors: # - cel: # filter: header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/qm1-prod-ref-guide-test' && body.repository.full_name == '${GIT_ORG}/mq-spring-app' # name: mq-spring-app-dev # template: # ref: mq-spring-app-dev An event listener can produce one or more triggers as specified by the list spec.triggers: . Our event listener produces a trigger called mq-infra-dev . This trigger comprises the template mq-infra-dev and template binding cntk-trigger-binding . It also has an interceptor; the trigger will only be called if filter: evaluates to true . Even though we haven't explored interceptors yet, filter value of the interceptor: is clearly filtering GitHub push events for the prod-ref-guide/mq-infra repository, i.e. the source repository for QM1 . It was the customization script that made this customization. You can read more about event listeners, triggers and interceptors here . Push GitOps changes to GitHub Let\u2019s make these GitOps changes and push them. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Customizing the mq-infra event trigger\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 12, done. Counting objects: 100% (12/12), done. Delta compression using up to 8 threads Compressing objects: 100% (7/7), done. Writing objects: 100% (7/7), 863 bytes | 863.00 KiB/s, done. Total 7 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git edf584c..9d1e713 master -> master Activate the webhook components Now that we've customized the event listener for our source repository, let's activate all the components necessary to process our webhook: the route, event listener, trigger template and trigger binding. Open the kustomization.yaml that is under mq/environments/ci/ . This contains all the resources that belong to the ci namespace. cat mq/environments/ci/kustomization.yaml resources : #- certificates/ci-mq-client-certificate.yaml #- certificates/ci-mq-server-certificate.yaml - configmaps/gitops-repo-configmap.yaml #- eventlisteners/cntk-event-listener.yaml #- triggerbindings/cntk-binding.yaml #- triggertemplates/mq-infra-dev.yaml #- triggertemplates/mq-spring-app-dev.yaml - pipelines/ibm-test-pipeline-for-dev.yaml - pipelines/ibm-test-pipeline-for-stage.yaml #- pipelines/java-maven-dev-pipeline.yaml - pipelines/mq-pipeline-dev.yaml - pipelines/mq-spring-app-dev-pipeline.yaml - roles/custom-pipeline-sa-clusterrole.yaml - roles/custom-pipeline-sa-role.yaml - roles/custom-ci-pipeline-sa-rolebinding.yaml - roles/custom-dev-pipeline-sa-rolebinding.yaml - roles/custom-staging-pipeline-sa-rolebinding.yaml - roles/custom-prod-pipeline-sa-rolebinding.yaml #- routes/cntk-route.yaml - secrets/artifactory-access-secret.yaml - secrets/git-credentials-secret.yaml - secrets/ibm-entitled-registry-credentials-secret.yaml #- secrets/mq-client-jks-password-secret.yaml - tasks/10-gitops.yaml - tasks/12-functional-tests.yaml - tasks/13-jmeter-performance-test.yaml - tasks/4-smoke-tests-mq.yaml - tasks/4-smoke-tests.yaml - tasks/ibm-build-tag-push-v2-6-13.yaml - tasks/ibm-helm-release-v2-6-13.yaml - tasks/ibm-img-release-v2-6-13.yaml - tasks/ibm-img-scan-v2-6-13.yaml - tasks/ibm-java-maven-test-v2-6-13.yaml - tasks/ibm-setup-v2-6-13.yaml - tasks/ibm-tag-release-v2-6-13.yaml Open this file in your editor and uncomment the below resources. - eventlisteners/cntk-event-listener.yaml - triggerbindings/cntk-binding.yaml - triggertemplates/mq-infra-dev.yaml - routes/cntk-route.yaml You will have the following resources un-commented: resources : #- certificates/ci-mq-client-certificate.yaml #- certificates/ci-mq-server-certificate.yaml - configmaps/gitops-repo-configmap.yaml - eventlisteners/cntk-event-listener.yaml - triggerbindings/cntk-binding.yaml - triggertemplates/mq-infra-dev.yaml #- triggertemplates/mq-spring-app-dev.yaml - pipelines/ibm-test-pipeline-for-dev.yaml - pipelines/ibm-test-pipeline-for-stage.yaml #- pipelines/java-maven-dev-pipeline.yaml - pipelines/mq-pipeline-dev.yaml - pipelines/mq-spring-app-dev-pipeline.yaml - roles/custom-pipeline-sa-clusterrole.yaml - roles/custom-pipeline-sa-role.yaml - roles/custom-ci-pipeline-sa-rolebinding.yaml - roles/custom-dev-pipeline-sa-rolebinding.yaml - roles/custom-staging-pipeline-sa-rolebinding.yaml - roles/custom-prod-pipeline-sa-rolebinding.yaml - routes/cntk-route.yaml - secrets/artifactory-access-secret.yaml - secrets/git-credentials-secret.yaml - secrets/ibm-entitled-registry-credentials-secret.yaml #- secrets/mq-client-jks-password-secret.yaml - tasks/10-gitops.yaml - tasks/12-functional-tests.yaml - tasks/13-jmeter-performance-test.yaml - tasks/4-smoke-tests-mq.yaml - tasks/4-smoke-tests.yaml - tasks/ibm-build-tag-push-v2-6-13.yaml - tasks/ibm-helm-release-v2-6-13.yaml - tasks/ibm-img-release-v2-6-13.yaml - tasks/ibm-img-scan-v2-6-13.yaml - tasks/ibm-java-maven-test-v2-6-13.yaml - tasks/ibm-setup-v2-6-13.yaml - tasks/ibm-tag-release-v2-6-13.yaml Update the GitOps repository Let\u2019s commit these changes to make the event listener and trigger resources active in the cluster. Issue the following command: git add mq/environments/ci/kustomization.yaml git commit -s -m \"Add qmgr pipeline webhook\" git push origin $GIT_BRANCH which shows that the changes are pushed to GitHub: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 522 bytes | 522.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/hp-gitops-mq/multi-tenancy-gitops-apps.git e28693e..25f7430 master -> master This change to the GitOps repository can now be used by ArgoCD. The apps-mq-rest-ci-1 argocd application Let's examine the ArgoCD application that manage the applications in ci namespace . In the ArgoCD UI Applications view, click on the icon for the apps-mq-rest-ci-1 application: You will see all the necessary configurations. Now, you will see trigger template, trigger binding and event listener we just created under this application. Exploring the cntk-event-listener Kubernetes resource Let's spend a few moments exploring the event listener resources. If you observe the above picture carefully, below are few things to note: See how the eventlistener custom resource deployed by the ArgoCD application has created a service named el-cntk-event-listener . The event listener route points to this service. The event listener runs in a pod, managed by a replicaset, defined by a deployment. It's this pod that will handle the webhook request, and call the trigger if the interceptor filter evaluates to true . Exploring the resources using the oc command We can use also use the oc command to explore our webhook resources: To view the route that is called by the webhook, issue the following command: oc get route -n ci To view the event listener service that is accessed via the route, issue the following command: oc get eventlistener -n ci To view the trigger template that is used by the event service to call the mq-infra-dev pipeline, issue the following command: oc get triggertemplate -n ci To view the trigger binding that is used to marshal the input for the pipeline run, issue the following command: oc get triggerbinding -n ci You can explore these resources in more detail by adding the -o yaml option. For now, it's enough to understand understand that webhook -> route -> event listener -> trigger template -> pipeline . Let's now configure a GitHub webhook to use these components. The GitHub webhook UI Let's now use the GitHub UI, to configure a webhook that creates an event and sends it to the route whenever the queue manager source repository changes. We'll configure the webhook using the GitHub UI. Issue the following command to determine the URL for the UI: echo https://github.com/ ${ GIT_ORG } /mq-infra/settings/hooks/new for example: https://github.com/prod-ref-guide/mq-infra/settings/hooks/new Copy the URL into your browser to launch the GitHub webpage to configure a new webhook for your mq-infra repository: We need to complete these details to configure our webhook. Configure the webhook To configure a webhook, we need to identify the URL it will call when it generates an event, the format of the event, and for which GitHub events we'll generate an event. Issue the following command to determine the URL of the event listener route: echo http:// $( oc get route el-cnkt-event-listener -n ci -o jsonpath = '{.spec.host}' ) for example: http://el-cnkt-event-listener-ci.xxxxx.containers.appdomain.cloud Here's a sample webhook configuration: Configure the following arguments: Set the Payload URL using the event listener route address. Set Content type to application/json . Select Let me select individual event Select Pull requests and Pushes from the list of available events. Click on Add webhook to create the webhook. A new webhook has been added In the GitHub UI, you can see that a new webhook has been added: Notice the webhook's name and that it's generating and event whenever a pull-request or push is issued against this repository. Let's now make a change to the QM1 source repository and watch the webhook at work.","title":"Set up the webhook and its processing"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic4/#making-a-change-to-the-queue-manager-qm1","text":"Now that we've configured our webhook, together with the Kubernetes resources to process an event generated by it, let's put it to work. In this section, we'll add a new queue to QM1 using GitOps. To do this, we'll update the QM1 source repository -- adding a the new queue definition to the queue manager MQSC configuration -- and push this change to GitHub. This will trigger a new pipeline run that will build and test our change. If successful, new versions of the queue manager image version and Helm chart will be produced. The GitOps Helm chart will be automatically deployed to the cluster by our ArgoCD application, resulting in an updated deployment for QM1 that contains the new queue definition. Select the QM1 source repository terminal window We're going to make changes to the QM1 source repository, so let's make sure we're in the correct terminal window and folder. Return to the terminal window you're using for the mq-infra source repository. ( Rather than the terminal window you're using for the multi-tenancy-gitops-apps GitOps repository. ) Issue the following command to ensure you're in the correct folder. cd $HOME /git/mq-infra Issue the following command to ensure that the $GIT_ORG environment variable is set: echo $GIT_ORG which will show your configured value, for example: prod-ref-guide Set up environment variable for new Git branch The sample repository has a single master branch. We're going to create a new branch based on master for our changes. Set the $GIT_BRANCH_QM1 using your $GIT_ORG environment variable: export GIT_BRANCH_QM1 = qm1- $GIT_ORG then create a new branch using its value: git checkout -b $GIT_BRANCH_QM1 you'll see something like: Switched to a new branch 'qm1-prod-ref-guide' Notice how we've created a new branch $GIT_BRANCH_QM1 based on the master branch. All changes will be made in $GIT_BRANCH_QM1 ; whereas master will remain unchanged. Also note that we use $GIT_BRANCH_QM1 to name the mq-infra queue manager branch in contrast to $GIT_BRANCH for the multi-tenancy-gitops-apps GitOps branch. This helps stop us accidentally promoting a change if we use the wrong terminal window in the tutorial. Verify you're on the new branch We're doing our changes in the $GIT_BRANCH_QM1 branch. Confirm you're on the new branch: git branch which will list all the branches for your local clone of the mq-infra repository: master * qm1-prod-ref-guide Note how * confirms your branch. Recap where QM1 MQSC definitions are stored in the source repository The QM1 configuration is completely held within the mq-infra source repository. Let's recall this structure: Issue the following command: tree to view the folder structure: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 README.md \u2514\u2500\u2500 chart \u2514\u2500\u2500 base \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 config.mqsc \u251c\u2500\u2500 security \u2502 \u2514\u2500\u2500 config.mqsc \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 configmap.yaml \u2502 \u2514\u2500\u2500 qm-template.yaml \u2514\u2500\u2500 values.yaml You may recall that the that the values.yaml file has security: false set by default. This value is used by the configmap.yaml template to generate a configmap using chart/bases/config/config.msqc rather than chart/bases/security/config.msqc . It's therefore this file we need to update to add a new queue. Update config.mqsc MQSC file Let's quickly review the current MQSC file, before we add our new queue. Issue the following command: cat chart/base/config/config.mqsc to view the MQSC definitions: DEFINE QLOCAL(IBM.DEMO.Q) BOQNAME(IBM.DEMO.Q.BOQ) BOTHRESH(3) REPLACE DEFINE QLOCAL(IBM.DEMO.Q.BOQ) REPLACE * Use a different dead letter queue, for undeliverable messages DEFINE QLOCAL('DEV.DEAD.LETTER.QUEUE') REPLACE ALTER QMGR DEADQ('DEV.DEAD.LETTER.QUEUE') DEFINE CHANNEL('IBM.APP.SVRCONN') CHLTYPE(SVRCONN) ALTER QMGR CHLAUTH (DISABLED) REFRESH SECURITY TYPE(CONNAUTH) Using your favorite editor, add a new queue definition to chart/base/config/config.mqsc : DEFINE QLOCAL(IBM.DEMO.Q.NEW) BOQNAME(IBM.DEMO.Q.BOQ) BOTHRESH(3) REPLACE DEFINE QLOCAL(IBM.DEMO.Q) BOQNAME(IBM.DEMO.Q.BOQ) BOTHRESH(3) REPLACE DEFINE QLOCAL(IBM.DEMO.Q.BOQ) REPLACE * Use a different dead letter queue, for undeliverable messages DEFINE QLOCAL('DEV.DEAD.LETTER.QUEUE') REPLACE ALTER QMGR DEADQ('DEV.DEAD.LETTER.QUEUE') DEFINE CHANNEL('IBM.APP.SVRCONN') CHLTYPE(SVRCONN) ALTER QMGR CHLAUTH (DISABLED) REFRESH SECURITY TYPE(CONNAUTH) Notice how we've added the IBM.DEMO.Q.NEW queue to the list of queues for this queue manager. Commit change to GitHub Let's now commit these changes to GitHub. git add chart/base/config/config.mqsc git commit -s -m \"Updated MQSC definition with queue IBM.DEMO.Q.NEW\" git push origin $GIT_BRANCH_QM1 We see that the changes are pushed to GitHub: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (6/6), 508 bytes | 508.00 KiB/s, done. Total 6 (delta 3), reused 0 (delta 0) remote: Resolving deltas: 100% (3/3), completed with 3 local objects. To https://github.com/prod-ref-guide/mq-infra 5245247..cf4dd8d qm1-prod-ref-guide -> qm1-prod-ref-guide GitHub event generated This push to GitHub should generate a GitHub webhook event for the mq-infra repository. We can look at the most recent webhook events. Click on the newly created webhook http://el-cnkt-event-listener... and select the Recent Deliveries tab. This is one of our sample Deliveries: Notice: The date and time when the webhook event was generated in the format yyyy-mm-dd hh:mm:ss The Request URL header identifies the event. listener http:://el-cnkt-event-listener... \"ref\": \"refs/head/qm1-odowdaibm\" identifies the branch that was pushed. \"repository.full_name\": \"odowdaibm-mq-guide/mq-infra\" identifies the repository that was changed. The Recent Deliveries tab can be very helpful to diagnose any issues you might have webhook event generation. Let's now see how this webhook event is handled by the event listener in the cluster. Event handled in the cluster Let's have a look at how the GitHub event is processed by the event listener in the cluster. We can look at the event listener logs to see how the event is received and processed using the oc logs command. Issue the following command: oc logs deploy/el-cnkt-event-listener -n ci to show the log messages issued by the el-cnkt-event-listener : { \"level\" : \"info\" , \"ts\" : \"2021-08-05T14:31:05.484Z\" , \"logger\" : \"eventlistener\" , \"caller\" : \"sink/sink.go:236\" , \"msg\" : \"ResolvedParams : [{Name:gitrevision Value:840ec76946c891ace8ebf591592077f259f4e5c1} {Name:gitrepositoryurl Value:https://github.com/odowdaibm-mq-guide/mq-infra}]\" , \"knative.dev/controller\" : \"eventlistener\" , \"/triggers-eventid\" : \"2dd90e6b-b97e-4a2e-8aa1-0de0704af7ba\" , \"/trigger\" : \"mq-infra-dev\" } { \"level\" : \"info\" , \"ts\" : \"2021-08-05T14:31:05.490Z\" , \"logger\" : \"eventlistener\" , \"caller\" : \"resources/create.go:95\" , \"msg\" : \"Generating resource: kind: &APIResource{Name:pipelineruns,Namespaced:true,Kind:PipelineRun,Verbs:[delete deletecollection get list patch create update watch],ShortNames:[pr prs],SingularName:pipelinerun,Categories:[tekton tekton-pipelines],Group:tekton.dev,Version:v1beta1,StorageVersionHash:RcAKAgPYYoo=,}, name: mq-infra-dev-\" , \"knative.dev/controller\" : \"eventlistener\" } { \"level\" : \"info\" , \"ts\" : \"2021-08-05T14:31:05.490Z\" , \"logger\" : \"eventlistener\" , \"caller\" : \"resources/create.go:103\" , \"msg\" : \"For event ID \\\"2dd90ez6b-b97e-4a2e-8aa1-0de0704af7babeta1, Resource=pipelineruns\" , \"knative.dev/controller\" : \"eventlistener\" } (You can scroll right in the above output.) Notice: Multiple log entries have been issued for the same input event. Notice how the timestamps \"ts\": are the same. The first entry identifies the data generated by the trigger template that will be passed to the pipeline: See how gitrevision value matches that in the webhook \"after\" value. See how gitrepositoryurl value matches your fork of the mq-infra repository. The second entry informs us that a new PipelineRun has been created for the pipeline mq-infra-dev . The third entry shows us the event ID that the event listener is using for the processing of this webhook event. The order of these events isn't really important; what's clear is that the pipeline run has been launched based on the webhook notification processes by the event listener. The oc logs command can be very helpful to diagnose any issues you might have with the event listener. The new pipeline run starts We can now see that our pipeline run has started. In the OpenShift web console, navigate to Pipelines->Pipelines . Select the mq-infra-dev pipeline and Project: ci : We can see a new pipeline run for mq-infra-dev is in progress. Notice the Labels for this pipeline run: See how the tekton event listener cnkt-event-listener is identified. See how the trigger-eventid allows us to link this pipeline run back to a Tekton event for traceability.","title":"Making a change to the queue manager QM1"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic4/#exploring-the-new-pipeline-run","text":"Our pipeline run has started and will take a little time to complete. We already know what our pipeline run will do; in this section we explore how it does it by looking at the build task in more detail. We'll also review the new and updated resources produced by the pipeline run. These include new versions of the image and Artifactory Helm chart, as well as an updated GitOps Helm chart. We'll see how these combine to reflect the change we made to QM1 . Look at pipeline run Let\u2019s find the queue manager pipeline using the OpenShift web console. Navigate to Pipelines->Pipelines in the left hand pane, and select Project: ci , to show all pipelines in the ci namespace: You can see that there is an mq-infra-dev pipeline run in progress. (You may need to scroll through the list of pipelines.) Pipeline tasks run and complete As before, because we can think of a pipeline run as an instance of a pipeline, we can see the pipeline structure from its run. In the Pipelines view, click on the mq-infra-dev-xxxxx pipeline run: Again, as before, we can see the tasks comprising the run, such as setup , build and GitOps . Notice how tasks are in one of three states: completed , in progress or waiting to run . See how the build task is implemented We're going to examine the build task in some detail. Once you see how the build task works, you'll be able to work out how any task works and how to modify it, as well as how to write your own tasks. We'll look at the build task YAML to see how it's implemented. We'll see how it gets its input, how it is coded, how it references secrets and how it interacts with external components, for example storing a new image in the image registry. In the Pipeline Run details view, click on the YAML tab: This is the pipeline run YAML. You may recall that a pipeline YAML has no code defined within it. In contrast, a pipelinerun YAML includes all the code that implements each task. It means we can scroll through a single file to see the pipeline run inputs, the task inputs and the code that implements each of the steps within each task. It makes it easy to understand what's happening as the pipeline executes because everything is in one place. We can of course see the same output on the command line. Issue the following command, replacing xxxxx with the current pipeline run identifier: oc get pipelinerun mq-infra-dev-xxxxx -n ci -o yaml which will show you all the same information on the command line: apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : annotations : app.openshift.io/runtime : mq ... creationTimestamp : \"2021-08-05T14:31:05Z\" generateName : mq-infra-dev- ... We've only included the beginning of the YAML file, as it's quite large, but you can see that it's the same as the web console YAML file. Exploring the build task YAML We can locate the source of the task code in our GitOps source repository. In the Pipeline Run details YAML tab view, search for ibm-build-tag-push-v2-6-13 to find where the build task refers to its implementation code. Use Cmd+F to search for ibm-build-tag-push-v2-6-13 : ... - name : build params : - name : git-url value : $(tasks.setup.results.git-url) - name : git-revision value : $(tasks.setup.results.git-revision) - name : source-dir value : $(tasks.setup.results.source-dir) - name : image-server value : $(tasks.setup.results.image-server) - name : image-namespace value : $(tasks.setup.results.image-namespace) - name : image-repository value : $(tasks.setup.results.image-repository) - name : image-tag value : $(tasks.setup.results.image-tag) runAfter : - setup taskRef : kind : Task name : ibm-build-tag-push-v2-6-13 - name : smoke-tests-mq ... We've already reviewed this build task definition. Recall that the taskRef refers to the task ibm-build-tag-push-v2-6-13 which implements the task code for build . The build task YAML source This build task is just one of many tasks defined in the mq-pipeline-dev.yaml file we explored earlier. Lets have a look at the definition of this task. Issue the following command: cat mq/environments/ci/tasks/ibm-build-tag-push-v2-6-13.yaml to show ibm-build-tag-push-v2-6-13 among the many YAMLs in this file: apiVersion : tekton.dev/v1beta1 kind : Task metadata : name : ibm-build-tag-push-v2-6-13 annotations : description : Executes logic to build, tag and push a container image using the intermediate sha tag to the image-url app.openshift.io/description : Executes logic to build, tag and push a container image using the intermediate sha tag to the image-url app.openshift.io/vcs-uri : https://github.com/IBM/ibm-garage-tekton-tasks app.openshift.io/vcs-ref : master labels : version : 2.6.13 spec : params : - name : git-url - name : git-revision default : master - name : source-dir default : /source - name : image-server default : \"\" - name : image-namespace default : \"\" - name : image-repository default : \"\" - name : image-tag default : \"\" - name : BUILDER_IMAGE default : quay.io/buildah/stable:v1.15.0 - name : DOCKERFILE default : ./Dockerfile - name : CONTEXT default : . - name : TLSVERIFY default : \"false\" - name : FORMAT default : \"docker\" - name : STORAGE_DRIVER description : Set buildah storage driver default : overlay volumes : - name : varlibcontainers emptyDir : {} - name : source emptyDir : {} stepTemplate : volumeMounts : - name : source mountPath : $(params.source-dir) steps : - name : git-clone image : quay.io/ibmgaragecloud/alpine-git env : - name : GIT_PASSWORD valueFrom : secretKeyRef : name : git-credentials key : password optional : true - name : GIT_USERNAME valueFrom : secretKeyRef : name : git-credentials key : username optional : true script : | set +x if [[ -n \"${GIT_USERNAME}\" ]] && [[ -n \"${GIT_PASSWORD}\" ]]; then git clone \"$(echo $(params.git-url) | awk -F '://' '{print $1}')://${GIT_USERNAME}:${GIT_PASSWORD}@$(echo $(params.git-url) | awk -F '://' '{print $2}')\" $(params.source-dir) else set -x git clone $(params.git-url) $(params.source-dir) fi set -x cd $(params.source-dir) git checkout $(params.git-revision) - name : build image : $(params.BUILDER_IMAGE) workingDir : $(params.source-dir) env : - name : REGISTRY_USER valueFrom : secretKeyRef : name : registry-access key : REGISTRY_USER optional : true - name : REGISTRY_PASSWORD valueFrom : secretKeyRef : name : registry-access key : REGISTRY_PASSWORD optional : true - name : IBM_ENTITLED_REGISTRY_USER valueFrom : secretKeyRef : name : ibm-entitled-registry-credentials key : IBM_ENTITLED_REGISTRY_USER optional : true - name : IBM_ENTITLED_REGISTRY_PASSWORD valueFrom : secretKeyRef : name : ibm-entitled-registry-credentials key : IBM_ENTITLED_REGISTRY_PASSWORD optional : true volumeMounts : - mountPath : /var/lib/containers name : varlibcontainers securityContext : privileged : true script : | APP_IMAGE=\"$(params.image-server)/$(params.image-namespace)/$(params.image-repository):$(params.image-tag)\" if [[ -n \"${IBM_ENTITLED_REGISTRY_USER}\" ]] && [[ -n \"${IBM_ENTITLED_REGISTRY_PASSWORD}\" ]]; then buildah login -u ${IBM_ENTITLED_REGISTRY_USER} -p ${IBM_ENTITLED_REGISTRY_PASSWORD} cp.icr.io echo \"buildah login -u \"${IBM_ENTITLED_REGISTRY_USER}\" -p \"xxxxx\" cp.icr.io\" fi buildah --layers --storage-driver=$(params.STORAGE_DRIVER) bud --format=$(params.FORMAT) --tls-verify=$(params.TLSVERIFY) -f $(params.DOCKERFILE) -t ${APP_IMAGE} $(params.CONTEXT) set +x if [[ -n \"${REGISTRY_USER}\" ]] && [[ -n \"${REGISTRY_PASSWORD}\" ]] && [[ \"$(params.image-server)\" != \"image-registry.openshift-image-registry.svc:5000\" ]]; then buildah login -u \"${REGISTRY_USER}\" -p \"${REGISTRY_PASSWORD}\" \"$(params.image-server)\" echo \"buildah login -u \"${REGISTRY_USER}\" -p \"xxxxx\" \"$(params.image-server)\"\" fi set -x buildah --storage-driver=$(params.STORAGE_DRIVER) push --tls-verify=$(params.TLSVERIFY) --digestfile ./image-digest ${APP_IMAGE} docker://${APP_IMAGE} Notice how: spec: contains a list of params: as the set of task inputs. name: git-revision is an example of named input, and it has a default value specified as default: master . spec: contains volumes: and volumeMounts: that apply to every step. Exploring the build task steps -- configuration We can explore the build task structure using the output from the OpenShift web console, the oc get pipelinerun command. Let's spend a few moments understanding how build works; you can review the code using your local editor or the OpenShift web console. We've also included the relevant snippets: steps : - name : git-clone image : quay.io/ibmgaragecloud/alpine-git env : - name : GIT_PASSWORD valueFrom : secretKeyRef : name : git-credentials key : password optional : true - name : GIT_USERNAME valueFrom : secretKeyRef : name : git-credentials key : username optional : true script : | ... - name : build image : $(params.BUILDER_IMAGE) workingDir : $(params.source-dir) env : ... volumeMounts : - mountPath : /var/lib/containers name : varlibcontainers securityContext : privileged : true script : | ... Notice how: The build task steps are defined in the steps: YAML node. The build task has two steps: name: git-clone and name: build . The git-clone step runs in its own container using image: quay.io/ibmgaragecloud/alpine-git The build step runs in its own container using the image specified by the task input variable $(params.BUILDER_IMAGE) . Notice this has a default value quay.io/buildah/stable:v1.15.0 The git-clone step sets up an environment variable GIT_PASSWORD using the the password key in the git-credentials secret. This allows the task to use the git credentials we configured in this chapter earlier As required, a step can specify any node from the spec.containers[]: specification such as workingDir: , volumeMount: and securityContext: . The code executed in each step container is specified by its script: . Let's now see how a build step can be coded to use this configuration. Exploring the build task steps -- scripts When the build step container runs, it executes the code specified in its script: . Find the build step script in web console pipelinerun YAML output, using the oc command. We've shown the script: below: script : | APP_IMAGE=\"$(params.image-server)/$(params.image-namespace)/$(params.image-repository):$(params.image-tag)\" if [[ -n \"${IBM_ENTITLED_REGISTRY_USER}\" ]] && [[ -n \"${IBM_ENTITLED_REGISTRY_PASSWORD}\" ]]; then buildah login -u ${IBM_ENTITLED_REGISTRY_USER} -p ${IBM_ENTITLED_REGISTRY_PASSWORD} cp.icr.io echo \"buildah login -u \"${IBM_ENTITLED_REGISTRY_USER}\" -p \"xxxxx\" cp.icr.io\" fi buildah --layers --storage-driver=$(params.STORAGE_DRIVER) bud --format=$(params.FORMAT) --tls-verify=$(params.TLSVERIFY) -f $(params.DOCKERFILE) -t ${APP_IMAGE} $(params.CONTEXT) set +x if [[ -n \"${REGISTRY_USER}\" ]] && [[ -n \"${REGISTRY_PASSWORD}\" ]] && [[ \"$(params.image-server)\" != \"image-registry.openshift-image-registry.svc:5000\" ]]; then buildah login -u \"${REGISTRY_USER}\" -p \"${REGISTRY_PASSWORD}\" \"$(params.image-server)\" echo \"buildah login -u \"${REGISTRY_USER}\" -p \"xxxxx\" \"$(params.image-server)\"\" fi set -x buildah --storage-driver=$(params.STORAGE_DRIVER) push --tls-verify=$(params.TLSVERIFY) --digestfile ./image-digest ${APP_IMAGE} docker://${APP_IMAGE} This script will execute within the step-build container and will have access to the container environment set up for this step and task. The script makes extensive use of the buildah command; it is often used in combination with skopeo and podman as a compatible alternative to the docker command for container management. You can learn more about these commands in this short article . See how the script will: Use the buildah login command to login to the IBM Entitled Registry using the $IBM_ENTITLED_REGISTRY_USER and $IBM_ENTITLED_REGISTRY_PASSWORD credentials you set up earlier. It needs this access because the Dockerfile used to build the queue manager image uses an IBM entitled MQ image. Use the buildah bud command to build a queue manager image using the Dockerfile specified in $params.DOCKERFILE . Use the buildah push command to store the newly built queue manager image to the cluster image registry. The step does support external registries such as quay.io if $REGISTRY_USER and $REGISTRY_PASSWORD are set. Name the queue manager image using the $APP_IMAGE variable. See how the name is structured as $(params.image-server)/$(params.image-namespace)/$(params.image-repository):$(params.image-tag) to uniquely identify it. We'll see an example in a moment. Linking task output to the build log As well as reading the script, we can see the actual output generated by the script OpenShift web console. In the web console, select the Pipeline Run details Logs tab for mq-infra-dev-xxxxx and select the build task. Scroll down to the STEP-BUILD output: STEP-BUILD + APP_IMAGE=image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 + [[ -n cp ]] + [[ -n eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJJQk0gTWFya2V0cGxhY2UiLCJpYXQiOjE1OTE3MDA1MTgsImp0aSI6Ijk3ODY4NWY2OTBlYjQwYzhhYzE0YTZjYTMxODA0OTgzIn0.a_pcyExLFeCMRks0DjFwW-VQAM_U5imFwEOu_qm79hY ]] + buildah login -u cp -p eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJJQk0gTWFya2V0cGxhY2UiLCJpYXQiOjE1OTE3MDA1MTgsImp0aSI6Ijk3ODY4NWY2OTBlYjQwYzhhYzE0YTZjYTMxODA0OTgzIn0.a_pcyExLFeCMRks0DjFwW-VQAM_U5imFwEOu_qm79hY cp.icr.io Login Succeeded! + echo 'buildah login -u cp -p xxxxx cp.icr.io' buildah login -u cp -p xxxxx cp.icr.io + buildah --layers --storage-driver=overlay bud --format=docker --tls-verify=false -f ./Dockerfile -t image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 . STEP 1: FROM cp.icr.io/cp/ibm-mqadvanced-server-integration@sha256:cfe3a4cec7a353e7496d367f9789dbe21fbf60dac46f127d288dda329560d13a Getting image source signatures Copying blob sha256:9bd64c9a8b8aef587728a77441425dfb80f760372ffb25bf4df746b1119812c5 Copying blob sha256:33d29d2675612fb16d6440409730a67f6bfcd92e79fb34fc868084d99de717c0 ... Copying config sha256:f573afcfc94cdd00bed6f497048f8adb9968475cdecd86a66aa576f0b3267d5a Writing manifest to image destination Storing signatures STEP 2: COMMIT image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 --> f573afcfc94 f573afcfc94cdd00bed6f497048f8adb9968475cdecd86a66aa576f0b3267d5a + set +x + buildah --storage-driver=overlay push --tls-verify=false --digestfile ./image-digest image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 docker://image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 Getting image source signatures Copying blob sha256:90c2e42f948b524cf98005073e0b0aa2065160abf9e8b314976c064e270d92ac ... Copying config sha256:f573afcfc94cdd00bed6f497048f8adb9968475cdecd86a66aa576f0b3267d5a Writing manifest to image destination Storing signatures This is the output from the container that runs the build step. You can see the results of the script: execution. Scripts often use the set +x , set +e option and echo commands to to generate log output. For example, we can see that the queue manager image name is: + APP_IMAGE=image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 We can also see this image being pushed to the OpenShift image registry: + buildah --storage-driver=overlay push --tls-verify=false --digestfile ./image-digest image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 docker://image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 together with its successful completion: ... Copying config sha256:f573afcfc94cdd00bed6f497048f8adb9968475cdecd86a66aa576f0b3267d5a Writing manifest to image destination Storing signatures Accessing the build task logs using oc logs Each task in the mq-infra-dev-xxxxx pipeline run executes in its own pod. Within these pods, each step runs in its own container. This allows us to view the pipelinerun output using the oc logs command. Issue the following command, replacing xxxxx with your pipelinerun identifier: oc get pods -n ci | grep mq-infra-dev-xxxxx You see all the pods used to execute the different tasks in the pipeline run: mq-infra-dev-mhpw9-build-mqtlq-pod-glrgn 0 /2 Completed 0 2d18h mq-infra-dev-mhpw9-gitops-8tkbp-pod-cz2rc 0 /1 Completed 0 2d18h mq-infra-dev-mhpw9-helm-release-v2crx-pod-fqjwq 0 /2 Completed 0 2d18h mq-infra-dev-mhpw9-img-release-w9lpx-pod-xnk26 0 /1 Completed 0 2d18h mq-infra-dev-mhpw9-img-scan-h7cnd-pod-9xfj7 0 /3 Completed 0 2d18h mq-infra-dev-mhpw9-setup-fz8vs-pod-h4pdm 0 /1 Completed 0 2d18h mq-infra-dev-mhpw9-smoke-tests-mq-dc5kc-pod-ngrjb 0 /4 Completed 0 2d18h mq-infra-dev-mhpw9-tag-release-n7nx2-pod-jc5rf 0 /2 Completed 0 2d18h Notice that: the number of pods matches the number of tasks in the mq-infra-dev pipeline. (In the above example, the pipeline run has finished.) the name of the pod includes the name of the task in the pipeline. each step runs in its own container, for example, the build pod has 0/2 containers running. We can view the logs the of any step in a tasks by viewing its container log. Issue the following command: oc logs mq-infra-dev-mhpw9-build-mqtlq-pod-glrgn step-build -n ci to see the step-build log in the build task: + APP_IMAGE=image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 + [[ -n cp ]] + [[ -n eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJJQk0gTWFya2V0cGxhY2UiLCJpYXQiOjE1OTE3MDA1MTgsImp0aSI6Ijk3ODY4NWY2OTBlYjQwYzhhYzE0YTZjYTMxODA0OTgzIn0.a_pcyExLFeCMRks0DjFwW-VQAM_U5imFwEOu_qm79hY ]] + buildah login -u cp -p eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJJQk0gTWFya2V0cGxhY2UiLCJpYXQiOjE1OTE3MDA1MTgsImp0aSI6Ijk3ODY4NWY2OTBlYjQwYzhhYzE0YTZjYTMxODA0OTgzIn0.a_pcyExLFeCMRks0DjFwW-VQAM_U5imFwEOu_qm79hY cp.icr.io Login Succeeded! + echo 'buildah login -u cp -p xxxxx cp.icr.io' buildah login -u cp -p xxxxx cp.icr.io + buildah --layers --storage-driver=overlay bud --format=docker --tls-verify=false -f ./Dockerfile -t image-registry.openshift-image-registry.svc:5000/ci/mq-infra:840ec76 . STEP 1: FROM cp.icr.io/cp/ibm-mqadvanced-server-integration@sha256:cfe3a4cec7a353e7496d367f9789dbe21fbf60dac46f127d288dda329560d13a Getting image source signatures Copying blob sha256:9bd64c9a8b8aef587728a77441425dfb80f760372ffb25bf4df746b1119812c5 ... See how this is the same output as the web console Pipeline Run details view. Wait for pipeline run to finish Return to Pipeline Run details view. If the pipeline run hasn't finished then wait until it does. If it hasn't finished, explore some of the logs of the tasks still running, and see if you can find the step code that they are executing. When the pipeline run completes, you'll see: The pipeline run is now complete. Let's explore the artifacts that mq-infra-dev-xxxxx has updated and created. New Helm chart in Artifactory The first new artifact we look at is the new Helm chart in Artifactory. Navigate to the Artifactory UI On the left hand pane select Artifactory->Artifacts . Within this view, expand the following zipped Helm charts: generic-local->ci->mq-infra-0.0.1.tgz->mq-infra generic-local->ci->mq-infra-0.0.2.tgz->mq-infra Select the Chart.yaml in mq-infra-0.0.2.tgz->mq-infra and View Source tab: Notice that: The pipeline run created a new zipped version of the Helm chart mq-infra-0.0.2.tgz The tgz file name includes a version change from 0.0.1 to 0.0.2 . The Chart.yaml shows also contains a new version 0.0.2 . The config.mqsc file in version 0.0.2 is significantly larger than its equivalent in 0.0.1 corresponding to the new MQSC definition for IBM.DEMO.Q.NEW In summary, the helm-release task in the mq-infra-dev pipeline run has created a new version of the Helm chart containing the new MQSC definition and stored it in Artifactory. The importance of Git tags Notice how the new Helm chart in Artifactory has a version 0.0.2 , incremented from version 0.0.1 . Every time we make a change to a source queue manager repository, a pipeline run generates a new version of the Helm chart to capture this change. As Helm charts (and container images) evolve over time, its version tag reflects the change; the highest version tag always points to the latest version of a Helm chart or image. This version matches the git tag of the mq-infra repository for the most recent change to update the config.mqsc file to add the IBM.DEMO.Q.NEW queue to QM1 . We can see the tags for mq-infra in the GitHub for via the git tags command. Issue the following command: echo https://github.com/ $GIT_ORG /mq-infra/tags to determine the URL for the mq-infra tags UI, for example: https://github.com/odowdaibm-mq-guide/mq-infra/tags Copy your generated URL into your browser to launch the Tags web page for your fork of the mq-infra GitHub repository. ( You can safely ignore any browser certificate warnings. ) You'll see the following page: Notice: Two tags: 0.0.1 and 0.0.2 . 0.0.1 was generated by the first (manual) pipeline run. 0.0.2 was generated by the second (webhook triggered) pipeline run. These tags are used to version the Helm charts in Artifactory. These tags are used by other tasks in the mq-infra-dev pipeline such as image-release and gitops . Generating the tag version When the mq-infra repository is forked, it has no tags. Tags are created by the tag-release task in the mq-infra-dev pipeline. Let's spend a few moments exploring this task: In the Pipeline Run details view, select the tag-release task and Logs tab: Scroll to the bottom of the log output, which culminates in: Pushing to https://odowdaibm:xxxxx@github.com/odowdaibm-mq-guide/mq-infra POST git-receive-pack (340 bytes) To https://github.com/odowdaibm-mq-guide/mq-infra = [up to date] 0.0.1 -> 0.0.1 * [new tag] 0.0.2 -> 0.0.2 ++ git describe --abbrev=0 --tags + NEW_TAG=0.0.2 + [[ -z 0.0.2 ]] + echo -n 0.0.2 + tee /tekton/results/tag 0.0.2 We can see that the end result of this task is to push the new 0.0.2 tag to your fork of the mq-infra repository. This tag can now be used throughout the pipeline whenever we need to version a Helm chart, container image or any other generated artifact. Spend a few moments examining the full log output of the step-git-tag step in the tag-release task. You might also like to explore the ibm-tag-release-v2-6-13 task code: apiVersion : tekton.dev/v1beta1 kind : Task metadata : name : ibm-tag-release-v2-6-13 ... spec : ... steps : ... - name : git-tag image : $(params.js-image) workingDir : $(params.source-dir) script : | #!/usr/bin/env bash set -ex echo \"Current branch: $(git rev-parse --abbrev-ref HEAD)\" git fetch --tags git config --global user.email \"cloud-native-toolkit@example.com\" git config --global user.name \"Cloud Native Toolkit Pipeline\" if [[ $(git describe --tag `git rev-parse HEAD`) =~ (^[0-9]+.[0-9]+.[0-9]+$) ]]; then echo \"Latest commit is already tagged\" NEW_TAG=\"$(git describe --abbrev=0 --tags)\" echo -n \"${NEW_TAG}\" | tee $(results.tag.path) exit 0 fi mkdir -p ~/.npm npm config set prefix ~/.npm export PATH=$PATH:~/.npm/bin npm i -g release-it release-it patch \\ --ci \\ --no-npm \\ --no-git.push \\ --no-git.requireCleanWorkingDir \\ --no-git.requireUpstream \\ -VV if [[ -z \"$(params.skip-push)\" ]]; then set +x git push --tags -v set -x fi NEW_TAG=\"$(git describe --abbrev=0 --tags)\" if [[ -z \"${NEW_TAG}\" ]]; then echo \"Error: NEW_TAG not defined\" exit 1 fi echo -n \"${NEW_TAG}\" | tee $(results.tag.path) See if you can work out how the code in the above script: generates git-tag log output. Updated Helm chart in GitOps The Helm chart stored in Artifactory is often referred to as a base chart because it contains the default configuration for QM1 . It can be overridden by the Helm chart stored in the GitOps repository, the GitOps Helm chart uses a requirements.yaml file to identify the relationship between these two Helm charts. Let's explore the GitOps Helm chart that was updated by the pipeline run. Issue the following command: echo https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps/blob/ $GIT_BRANCH /mq/environments/dev/mq-infra/requirements.yaml to calculate the URL of the GitOps version of the requirements.yaml file, for example: https://github.com/odowdaibm-mq-guide/multi-tenancy-gitops/blob/master/mq/environments/dev/mq-infra/requirements.yaml Copy your version of this URL into a browser to see the GitOps version of the requirements.yaml file that is being accessed by ArgoCD: dependencies: - name: mq-infra version: 0.0.2 repository: http://artifactory-artifactory.tools:8082/artifactory/generic-local/ci Notice: the version: has been updated to 0.0.2 . this chart will now override the newly generated mq-infra-0.0.2.tgz base chart in Artifactory. the 0.0.1 version of the base chart is remains in Artifactory as we saw earlier, but is no longer referenced. It's the gitops task in the mq-infra-dev pipeline run that updated this Helm chart stored in the GitOps repository. You might like to explore the gitops step log in Pipeline Run detail Logs tab. When the queue manager ArgoCD application sees that this GitOps Helm chart has changed, it will redeploy this Helm chart. It will be the combination of the GitOps and Artifactory Helm charts that will define the new queue manager deployment in the the cluster. Viewing the new deployment in ArgoCD The completion of the gitops task results in an update to the GitOps Helm chart requirements.yaml file to identify the base Helm chart version 0.0.2 in Artifactory. This change to the requirements.yaml file will result in the ibm-mq-dev-instance ArgoCD application redeploying the Helm chart. Let's explore this change using ArgoCD. In the ArgoCD UI, select the ibm-mq-dev-instance ArgoCD application: We can see the two components of the queue manager Kubernetes deployment, namely the queuemanager custom resource defining the properties of QM1 and the configmap containing its MQSC configuration. Click on the qm-dev queuemanager custom resource and select the DESIRED MANIFEST tab: Notice: This queue manager uses the newly built container image image-registry.openshift-image-registry.svc:5000/ci/mq-infra:0.0.2 stored in the OpenShift image registry. This image was generated by the build task and tagged by the tag-release task as 0.0.2 . The image version 0.0.2 matches the git repository tag 0.0.2 in the same way as the Helm charts. Now click on the mqsc-configmap configmap and select the DESIRED MANIFEST tab: Notice: New queue IBM.DEMO.Q.NEW has been added to the MQSC file. This configmap matches the configmap we updated in QM1 source repository. Let's now try out the new queue manager.","title":"Exploring the new pipeline run"},{"location":"guides/cp4i/mq/qmgr-pipeline/topic4/#verify-the-updates-to-queuemanager","text":"Let's now verify and explore the changes we have made to QM1 , using both the MQ web console and the oc command. Exploring QM1 with the web console Switch to the MQ web console Select Manage QM1 from the home page to see: Notice that: The new queue IBM.DEMO.Q.NEW has been added to the list of queues for QM1 . This is the queue we added to the config.mqsc file in the source repository. The Maximum depth for IBM.DEMO.Q is 0/5000 even though we put a message to this queue in the previous topic. We'll find out a little later what's happened to this message. Exploring the queue manager details To understand a little more about the changes we've made to QM1 , let's examine its statistics page on the MQ console. Click on the Configuration icon in the top right of the Manage view. In the QM configuration view, select Statistics in the left hand pane: Notice the details of when QM1 was created, as well as its QMID. The QMID is a unique name generated when a queue manager is first created. Let's compare these values to the previous values which we generated using the oc command. We've copied the most relevant ones below: 5724-H72 (C) Copyright IBM Corp. 1994, 2020. Starting MQSC for queue manager QM1. 1 : dis qmgr AMQ8408I: Display Queue Manager details. QMNAME(QM1) ... ALTDATE(2021-07-30) ALTTIME(17.17.48) ... CRDATE(2021-07-30) CRTIME(17.17.44) ... QMID(QM1_2021-07-30_17.17.44) One MQSC command read. No commands have a syntax error. All valid MQSC commands were processed. Notice that these two sets of values are completely different ; it's as if we've got a new queue manager. Indeed, that's exactly what happened when we made our configuration change, as we'll now discover. Ephemeral queue manager As we've seen, pods are the fundamental building blocks of Kubernetes. The QM1 queue manager runs in a pod container and when its configuration changes, this pod is replaced by a new one that whose container has an updated spec: that reflects the updated configuration for QM1 . In our scenario this includes both a new image and a new MQSC file containing our new queue definition. Let's look a little more closely at the queuemanager custom resource for QM1 to understand what's happening. Issue the following command: oc get queuemanager qm-dev -n dev -o yaml to show the details of the queue manager: apiVersion : mq.ibm.com/v1beta1 kind : QueueManager ... spec : license : accept : true license : L-RJON-BN7PN3 metric : VirtualProcessorCore use : NonProduction queueManager : availability : type : SingleInstance debug : false image : image-registry.openshift-image-registry.svc:5000/ci/mq-infra:0.0.2 imagePullPolicy : Always livenessProbe : failureThreshold : 1 initialDelaySeconds : 90 periodSeconds : 10 successThreshold : 1 timeoutSeconds : 5 logFormat : Basic metrics : enabled : true mqsc : - configMap : items : - config.mqsc name : mqsc-configmap name : QM1 readinessProbe : failureThreshold : 1 initialDelaySeconds : 10 periodSeconds : 5 successThreshold : 1 timeoutSeconds : 3 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : \"1\" memory : 1Gi route : enabled : true storage : persistedData : enabled : false queueManager : type : ephemeral recoveryLogs : enabled : false securityContext : initVolumeAsRoot : false template : pod : containers : - env : - name : MQSNOAUT value : \"yes\" name : qmgr resources : {} terminationGracePeriodSeconds : 30 tracing : agent : {} collector : {} enabled : false namespace : \"\" version : 9.2.2.0-r1 web : enabled : true status : adminUiUrl : https://qm-dev-ibm-mq-web-dev.mqcloud1-odowda-d02cf90349a0fe46c9804e3ab1fe2643-0000.eu-gb.containers.appdomain.cloud/ibmmq/console conditions : [] endpoints : - name : ui type : UI uri : https://qm-dev-ibm-mq-web-dev.mqcloud1-odowda-d02cf90349a0fe46c9804e3ab1fe2643-0000.eu-gb.containers.appdomain.cloud/ibmmq/console name : QM1 phase : Running versions : available : channels : [] versions : - name : 9.2.2.0-r1 reconciled : 9.2.2.0-r1 Notice the spec.queueManager.storage.queueManager.type: ephemeral . As you can see from the API reference for the QueueManager this defines an ephemeral queue manager. An ephemeral queue manager is one whose logs and queue files are defined within the container file system rather than external to it. It means that when the pod is replaced, all the container queue files and log files are lost. That's why our QMID is changed and our message was lost -- they were stored in the file system of the old container. When the pod restarted for the updated queue manager deployment, it was like a new queue manager was being created because the container file system was new. We rarely use ephemeral queue managers for production work; usually a queue manager stores its queue and log files in a persistent volume using a persistent volume claim that is mounted into the container when the pod starts. Using a persistent volume means that restarting a pod doesn't affect the long term state of the queue manager -- properties like QMID, together with messages in queues are stored in queue and log files that are preserved in the persistent volume claim that is reattached to a restarted container pod. We'll use persistent volumes in the High Availability chapter. For now, our use of an ephemeral queue manager has shown nicely how a configuration change results in an updated queue manager deployment resulting in a pod restart for the queue manager. Congratulations! You've now got a fully automated CICD process for your queue manager. You started the chapter by configuring a webhook to generate an event notification whenever the QM1 source repository was changed. You then created an event listener and trigger in your cluster to process these events and start a pipeline run using the data in the event. After updating the QM1 source repository to add a new queue to its MQSC definitions, a pipeline run was triggered automatically. As the pipeline run progressed, you explored its task logs and how key tasks worked. You explored the detailed structure of the mq-infra-dev pipeline in terms of its tasks and their steps. You explored theses task steps produced new and updated artifacts such as a container image and Helm charts. You saw how git tags were used to version control these artifacts. Once the pipeline run completed, you examined the updated queue manager deployment using ArgoCD. You tried out the updated queue manager to confirm that the new queue had been added to QM1 . Finally, you explored the concept of an ephemeral queue manager its persistent volume, together with how these combine with Kubernetes pods and containers. In the next chapter, we're going to create an MQ application that exploits this queue manager.","title":"Verify the updates to QueueManager"},{"location":"guides/cp4i/mq/scalability/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/mq/scalability/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/mq/security/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/mq/security/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4i/mq/security/handson/mtls/","text":"Enabling mTLS security and delegated authentication using LDAP \u00b6 Overview \u00b6 In this guide, we will setup mTLS to secure the data in motion between the mq-spring-app and the QM1 queue manager. We will also setup an LDAP server to configure the system to delegate authentication of mq users (system and human) to an OpenLDAP server. The details of the solution are described here mq security . Prerequisites \u00b6 You have a cluster provisioned with the mq-spring-app deployed and interacting with the QM1 queue manager without any security configured (i.e. no TLS and no LDAP for delegated authentication). You have cloned the git repositories for: * multi-tenancy-gitops (https://github.com/cloud-native-toolkit/multi-tenancy-gitops) * mq-infra (https://github.com/cloud-native-toolkit/mq-infra) * mq-spring-app (https://github.com/cloud-native-toolkit/mq-spring-app) Configure mTLS and Delegated Authentication \u00b6 Create openldap project Go to the multi-tenancy-gitops folder where the local copy of the repo is: cd multi-tenancy-gitops Edit file 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the line: - argocd/namespace-openldap.yaml Commit the change and push it back to the repo. git add . git commit -m \"create openldap project\" git push origin master This will create the openldap project in the cluster. Install the OpenLDAP Server Edit file 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the line: - argocd/instances/openldap.yaml Commit the change and push it back to the repo. git add . git commit -m \"enable OpenLDAP to be installed\" git push origin master This will create the openldap Argo application and install OpenLDAP into the cluster in namespace openldap . In ArgoCD, when you search for the openldap application, you should see a screen that look like this: Once, ArgoCD Synchs with the OpenShift cluster, you will see two deployments in the openldap project as follows: OpenLDAP has a admin UI to manage the users. Get the route and open the admin UI in a browser: oc get route openldap-admin -n openldap -o jsonpath='{ .spec.host }' The OpenLDAP admin UI will appear as follows: Login using Login DN credentials: cn=admin,dc=ibm,dc=com and password : admin Once, you login, you will see several users in the directory including the mqapp user which we will use in this guide. Activate cert-manager Operator Edit file 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the line: - argocd/operators/cert-manager.yaml Commit the change and push it back to the repo git add . git commit -m \"enable cert-manager operator\" git push origin master The cert-manager ArgoCD application will be created: The cert-manager operator will show up in the OpenShift cluster. Activate cert-manager instance Edit file 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the line: - argocd/instances/cert-manager-instance.yaml Commit the change and push it back to the repo git add . git commit -m \"enable cert-manager instance\" git push origin master The cert-manager instance ArgoCD application will be created: The cert-manager instance will show up in the OpenShift cluster. Activate Certificate Issuer and Certificates NOTE: ??? CHECK WITH HOLLIS. IS THIS GITOPS STRUCTURE STILL CORRECT???? Edit file 0-bootstrap/single-cluster/3-apps/kustomization.yaml and uncomment the line: - argocd/mq/tools.yaml Commit the change and push it back to the repo git add . git commit -m \"enable certificate issuer\" git push origin master Creates the following resources in the cluster: Resource type Resource name ClusterIssuer selfsigned-cluster-issuer Certificate mq-self-signed-ca-cert ClusterIssuer self-signed-mq-cluster-issuer Create JKS password secret NOTE: master branch now has the script and the sealed secret yaml to generate the mq-client-jks-password secret. https://github.com/mqpdg2/multi-tenancy-gitops-apps/tree/master/mq/environments/ci/secrets So I believe this step does not need to be in this guide. I believe that the secret will exist at this point at least in the ci namespace. Not sure of the other namespaces. Activate Certificates in ci namespace Edit file `mq/environments/ci/kustomization.yaml and uncomment the line: - certificates/ci-mq-client-certificate.yaml - certificates/ci-mq-server-certificate.yaml Commit the change and push it back to the repo git add . git commit -m \"create the mq client and server certificate in the ci namespace\" git push origin master Verify that the certificate was created successfully and the secret was generated by cert-manager. oc get certificate -n ci You should see the certificates with a Ready status of True as follows: NAME READY SECRET AGE ci-mq-client-cert True mq-client-jks 5d16h ci-mq-server-cert True mq-server-cert 5d16h The cert-manager operator generates the secrets from these certificates. Lets verify the secrets were generated correctly. Run the command: oc get secret mq-client-jks -n ci You should see the secret with a Data value of 5 representing 5 key value pairs as follows: NAME TYPE DATA AGE mq-client-jks kubernetes.io/tls 5 5d16h Run the command: oc get secret mq-server-cert -n ci You should see the secret with a Data value of 3 representing 3 key value pairs as follows: NAME TYPE DATA AGE mq-server-cert kubernetes.io/tls 3 5d17h Activate Certificates in dev namespace Edit file `mq/environments/dev/kustomization.yaml and uncomment the line: - certificates/dev-mq-client-certificate.yaml - certificates/dev-mq-server-certificate.yaml Commit the change and push it back to the repo git add . git commit -m \"create the mq client and server certificate in the dev namespace\" git push origin master Verify that the certificate was created successfully and the secret was generated by cert-manager. oc get certificate -n dev You should see the certificates with a Ready status of True as follows: NAME READY SECRET AGE dev-mq-client-cert True mq-client-jks 5d16h dev-mq-server-cert True mq-server-cert 5d16h The cert-manager operator generates the secrets from these certificates. Lets verify the secrets were generated correctly. Run the command: oc get secret mq-client-jks -n dev You should see the secret with a Data value of 5 representing 5 key value pairs as follows: NAME TYPE DATA AGE mq-client-jks kubernetes.io/tls 5 5d16h Run the command: oc get secret mq-server-cert -n ci You should see the secret with a Data value of 3 representing 3 key value pairs as follows: NAME TYPE DATA AGE mq-server-cert kubernetes.io/tls 3 5d17h Deploy/Create an mq manager instance with security enabled Edit the values.yaml file in the source mq-infra repo and change the value of the security to true as follows: security: true Commit the change and push it back to the repo git add . git commit -m \"Build and deploy the mq manager with security enabled\" git push origin master Run the mq-infra-dev pipeline. From the OpenShift console, go to Pipelines and set the Project to ci . You should see the mq-infra-dev pipeline as follows: Click on the mq-infra-dev pipeline link. Click on the Actions drop-down and select Start as follows: The Start Pipeline screen will pop up. Fill the git-url field and use the defaults for the other fields. Then click the Start button. Wait till the pipeline completes successfully. The final stage of the pipeline will update the gitops repo: multi-tenancy-gitops-apps/mq/environments/dev/mq-infra/ which will force ArgoCD to deploy the secure MQ manager to the dev project. Note: It may take a few minutes for ArgoCD to synch with the OpenShift cluster. Deploy mq-spring-app with security enabled Edit the values.yaml file in the source mq-spring-app repo and change the value of the security to true as follows: security: true Commit the change and push it back to the repo git add . git commit -m \"Build and deploy the mq-spring-app with security enabled\" git push origin master Run the mq-spring-app-dev pipeline. From the OpenShift console, go to Pipelines and set the Project to ci . You should see the mq-spring-app-dev pipeline as follows: Click on the mq-spring-dev pipeline link. Click on the Actions drop-down and select Start as follows: The Start Pipeline screen will pop up. Fill the git-url field and use the defaults for the other fields. Then click the Start button. Wait till the pipeline completes successfully. The final stage of the pipeline will update the gitops repo: multi-tenancy-gitops-apps/mq/environments/dev/mq-spring-app/ which will force ArgoCD to deploy the secure mq-spring-app client app to the dev project. Note: It may take a few minutes for ArgoCD to synch with the OpenShift cluster. Test the mq-spring-app Check the health of the app: export APP_URL=$(oc get route -n dev mq-spring-app -o jsonpath=\"{.spec.host}\") curl -X GET https://$APP_URL/actuator/health Invoke the api to put a message onto the queue: curl -X GET https://$APP_URL/api/send-hello-world Invoke the api to get a message from the queue: curl -X GET https://$APP_URL/api/recv To view the swagger docs of the mq-spring-app app, you can open a browser and enter the $APP_URL value.","title":"Secure data in motion"},{"location":"guides/cp4i/mq/security/handson/mtls/#enabling-mtls-security-and-delegated-authentication-using-ldap","text":"","title":"Enabling mTLS security and delegated authentication using LDAP"},{"location":"guides/cp4i/mq/security/handson/mtls/#overview","text":"In this guide, we will setup mTLS to secure the data in motion between the mq-spring-app and the QM1 queue manager. We will also setup an LDAP server to configure the system to delegate authentication of mq users (system and human) to an OpenLDAP server. The details of the solution are described here mq security .","title":"Overview"},{"location":"guides/cp4i/mq/security/handson/mtls/#prerequisites","text":"You have a cluster provisioned with the mq-spring-app deployed and interacting with the QM1 queue manager without any security configured (i.e. no TLS and no LDAP for delegated authentication). You have cloned the git repositories for: * multi-tenancy-gitops (https://github.com/cloud-native-toolkit/multi-tenancy-gitops) * mq-infra (https://github.com/cloud-native-toolkit/mq-infra) * mq-spring-app (https://github.com/cloud-native-toolkit/mq-spring-app)","title":"Prerequisites"},{"location":"guides/cp4i/mq/security/handson/mtls/#configure-mtls-and-delegated-authentication","text":"Create openldap project Go to the multi-tenancy-gitops folder where the local copy of the repo is: cd multi-tenancy-gitops Edit file 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the line: - argocd/namespace-openldap.yaml Commit the change and push it back to the repo. git add . git commit -m \"create openldap project\" git push origin master This will create the openldap project in the cluster. Install the OpenLDAP Server Edit file 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the line: - argocd/instances/openldap.yaml Commit the change and push it back to the repo. git add . git commit -m \"enable OpenLDAP to be installed\" git push origin master This will create the openldap Argo application and install OpenLDAP into the cluster in namespace openldap . In ArgoCD, when you search for the openldap application, you should see a screen that look like this: Once, ArgoCD Synchs with the OpenShift cluster, you will see two deployments in the openldap project as follows: OpenLDAP has a admin UI to manage the users. Get the route and open the admin UI in a browser: oc get route openldap-admin -n openldap -o jsonpath='{ .spec.host }' The OpenLDAP admin UI will appear as follows: Login using Login DN credentials: cn=admin,dc=ibm,dc=com and password : admin Once, you login, you will see several users in the directory including the mqapp user which we will use in this guide. Activate cert-manager Operator Edit file 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the line: - argocd/operators/cert-manager.yaml Commit the change and push it back to the repo git add . git commit -m \"enable cert-manager operator\" git push origin master The cert-manager ArgoCD application will be created: The cert-manager operator will show up in the OpenShift cluster. Activate cert-manager instance Edit file 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the line: - argocd/instances/cert-manager-instance.yaml Commit the change and push it back to the repo git add . git commit -m \"enable cert-manager instance\" git push origin master The cert-manager instance ArgoCD application will be created: The cert-manager instance will show up in the OpenShift cluster. Activate Certificate Issuer and Certificates NOTE: ??? CHECK WITH HOLLIS. IS THIS GITOPS STRUCTURE STILL CORRECT???? Edit file 0-bootstrap/single-cluster/3-apps/kustomization.yaml and uncomment the line: - argocd/mq/tools.yaml Commit the change and push it back to the repo git add . git commit -m \"enable certificate issuer\" git push origin master Creates the following resources in the cluster: Resource type Resource name ClusterIssuer selfsigned-cluster-issuer Certificate mq-self-signed-ca-cert ClusterIssuer self-signed-mq-cluster-issuer Create JKS password secret NOTE: master branch now has the script and the sealed secret yaml to generate the mq-client-jks-password secret. https://github.com/mqpdg2/multi-tenancy-gitops-apps/tree/master/mq/environments/ci/secrets So I believe this step does not need to be in this guide. I believe that the secret will exist at this point at least in the ci namespace. Not sure of the other namespaces. Activate Certificates in ci namespace Edit file `mq/environments/ci/kustomization.yaml and uncomment the line: - certificates/ci-mq-client-certificate.yaml - certificates/ci-mq-server-certificate.yaml Commit the change and push it back to the repo git add . git commit -m \"create the mq client and server certificate in the ci namespace\" git push origin master Verify that the certificate was created successfully and the secret was generated by cert-manager. oc get certificate -n ci You should see the certificates with a Ready status of True as follows: NAME READY SECRET AGE ci-mq-client-cert True mq-client-jks 5d16h ci-mq-server-cert True mq-server-cert 5d16h The cert-manager operator generates the secrets from these certificates. Lets verify the secrets were generated correctly. Run the command: oc get secret mq-client-jks -n ci You should see the secret with a Data value of 5 representing 5 key value pairs as follows: NAME TYPE DATA AGE mq-client-jks kubernetes.io/tls 5 5d16h Run the command: oc get secret mq-server-cert -n ci You should see the secret with a Data value of 3 representing 3 key value pairs as follows: NAME TYPE DATA AGE mq-server-cert kubernetes.io/tls 3 5d17h Activate Certificates in dev namespace Edit file `mq/environments/dev/kustomization.yaml and uncomment the line: - certificates/dev-mq-client-certificate.yaml - certificates/dev-mq-server-certificate.yaml Commit the change and push it back to the repo git add . git commit -m \"create the mq client and server certificate in the dev namespace\" git push origin master Verify that the certificate was created successfully and the secret was generated by cert-manager. oc get certificate -n dev You should see the certificates with a Ready status of True as follows: NAME READY SECRET AGE dev-mq-client-cert True mq-client-jks 5d16h dev-mq-server-cert True mq-server-cert 5d16h The cert-manager operator generates the secrets from these certificates. Lets verify the secrets were generated correctly. Run the command: oc get secret mq-client-jks -n dev You should see the secret with a Data value of 5 representing 5 key value pairs as follows: NAME TYPE DATA AGE mq-client-jks kubernetes.io/tls 5 5d16h Run the command: oc get secret mq-server-cert -n ci You should see the secret with a Data value of 3 representing 3 key value pairs as follows: NAME TYPE DATA AGE mq-server-cert kubernetes.io/tls 3 5d17h Deploy/Create an mq manager instance with security enabled Edit the values.yaml file in the source mq-infra repo and change the value of the security to true as follows: security: true Commit the change and push it back to the repo git add . git commit -m \"Build and deploy the mq manager with security enabled\" git push origin master Run the mq-infra-dev pipeline. From the OpenShift console, go to Pipelines and set the Project to ci . You should see the mq-infra-dev pipeline as follows: Click on the mq-infra-dev pipeline link. Click on the Actions drop-down and select Start as follows: The Start Pipeline screen will pop up. Fill the git-url field and use the defaults for the other fields. Then click the Start button. Wait till the pipeline completes successfully. The final stage of the pipeline will update the gitops repo: multi-tenancy-gitops-apps/mq/environments/dev/mq-infra/ which will force ArgoCD to deploy the secure MQ manager to the dev project. Note: It may take a few minutes for ArgoCD to synch with the OpenShift cluster. Deploy mq-spring-app with security enabled Edit the values.yaml file in the source mq-spring-app repo and change the value of the security to true as follows: security: true Commit the change and push it back to the repo git add . git commit -m \"Build and deploy the mq-spring-app with security enabled\" git push origin master Run the mq-spring-app-dev pipeline. From the OpenShift console, go to Pipelines and set the Project to ci . You should see the mq-spring-app-dev pipeline as follows: Click on the mq-spring-dev pipeline link. Click on the Actions drop-down and select Start as follows: The Start Pipeline screen will pop up. Fill the git-url field and use the defaults for the other fields. Then click the Start button. Wait till the pipeline completes successfully. The final stage of the pipeline will update the gitops repo: multi-tenancy-gitops-apps/mq/environments/dev/mq-spring-app/ which will force ArgoCD to deploy the secure mq-spring-app client app to the dev project. Note: It may take a few minutes for ArgoCD to synch with the OpenShift cluster. Test the mq-spring-app Check the health of the app: export APP_URL=$(oc get route -n dev mq-spring-app -o jsonpath=\"{.spec.host}\") curl -X GET https://$APP_URL/actuator/health Invoke the api to put a message onto the queue: curl -X GET https://$APP_URL/api/send-hello-world Invoke the api to get a message from the queue: curl -X GET https://$APP_URL/api/recv To view the swagger docs of the mq-spring-app app, you can open a browser and enter the $APP_URL value.","title":"Configure mTLS and Delegated Authentication"},{"location":"guides/cp4i/mq/security/information/container/","text":"Docker container Security \u00b6 As a best practice, each workload should run with its own unique service account. A service account is a Kubernetes resource that is used by Kubernetes entities such as pods to authenticate itself. By having a unique service account per workload, RBAC policies can be applied for each service account to limit the resources that a service account can access. The mq-spring-app workload is designed to run with a service account with the same name of mq-spring-app . When the helm chart gets deployed, the service account gets created. See: https://github.com/cloud-native-toolkit/mq-spring-app/blob/master/chart/base/templates/serviceaccount.yaml The service account manifest created is defined as follows: apiVersion: v1 kind: ServiceAccount metadata: name: mq-spring-app automountServiceAccountToken: false The automountServiceAccountToken: false will ensure that the service account secret containing the authentication token does not get mounted. This ensures, that a malicious user who gets access to the pod, cannot get the service account token and use it for accessing the cluster. The deployment resource is configured to run pods using the mq-spring-app service account. See: https://github.com/cloud-native-toolkit/mq-spring-app/blob/457fcd26564187397ed5de07cc1c37a1ead5faf8/chart/base/templates/deployment.yaml#L36 THe spec section of the deployment.yaml file has the following entry: spec: serviceAccountName: mq-spring-app","title":"Docker container Security"},{"location":"guides/cp4i/mq/security/information/container/#docker-container-security","text":"As a best practice, each workload should run with its own unique service account. A service account is a Kubernetes resource that is used by Kubernetes entities such as pods to authenticate itself. By having a unique service account per workload, RBAC policies can be applied for each service account to limit the resources that a service account can access. The mq-spring-app workload is designed to run with a service account with the same name of mq-spring-app . When the helm chart gets deployed, the service account gets created. See: https://github.com/cloud-native-toolkit/mq-spring-app/blob/master/chart/base/templates/serviceaccount.yaml The service account manifest created is defined as follows: apiVersion: v1 kind: ServiceAccount metadata: name: mq-spring-app automountServiceAccountToken: false The automountServiceAccountToken: false will ensure that the service account secret containing the authentication token does not get mounted. This ensures, that a malicious user who gets access to the pod, cannot get the service account token and use it for accessing the cluster. The deployment resource is configured to run pods using the mq-spring-app service account. See: https://github.com/cloud-native-toolkit/mq-spring-app/blob/457fcd26564187397ed5de07cc1c37a1ead5faf8/chart/base/templates/deployment.yaml#L36 THe spec section of the deployment.yaml file has the following entry: spec: serviceAccountName: mq-spring-app","title":"Docker container Security"},{"location":"guides/cp4i/mq/security/information/image/","text":"Docker image Security \u00b6 The docker image that is created when building the mq-spring-app addresses several security considerations. The git repo for the app contains a Dockerfile that is used to build the image. See the Dockerfile . This Dockerfile is a multistage file where the first stage builds the java code and the second stage builds the runtime image. The second stage of the Dockerfile looks as follows: FROM registry.access.redhat.com/ubi8/ubi:8.4 RUN dnf install -y java-11-openjdk.x86_64 COPY --from=builder /workspace/app/target/*.jar ./app.jar EXPOSE 8080/tcp USER 1001 CMD [\"java\", \"-jar\", \"./app.jar\"] Use a secure and trusted base images \u00b6 The base image used is the ubi8:8.4 (Red Hat Universal Base Image version 8.4) Red Hat UBI images are tested, maintained and contain the latest security patches. See the UBI eBook to understand the advantages of using UBI images. A blog by a leading security company also recommends using Red Hat UBI base images as they typically have much fewer security vulnerabilities. Do not run as a root user \u00b6 By default, if no USER is specified in a Dockerfile, the container will run as root on a Kubernetes platform. A security best practice is to specify a non root user in the Dockerfile. In our Dockerfile, we specify to run as USER 1001 which is a non root user. Note: OpenShift Container Platform, by default, does not allow containers to run as root but it is still a good practice to specify in the Dockerfile a non-root user. In fact OpenShift ignores the USER directive of the Dockerfile and launches the container with a random UUID for enhanced security.","title":"Docker image Security"},{"location":"guides/cp4i/mq/security/information/image/#docker-image-security","text":"The docker image that is created when building the mq-spring-app addresses several security considerations. The git repo for the app contains a Dockerfile that is used to build the image. See the Dockerfile . This Dockerfile is a multistage file where the first stage builds the java code and the second stage builds the runtime image. The second stage of the Dockerfile looks as follows: FROM registry.access.redhat.com/ubi8/ubi:8.4 RUN dnf install -y java-11-openjdk.x86_64 COPY --from=builder /workspace/app/target/*.jar ./app.jar EXPOSE 8080/tcp USER 1001 CMD [\"java\", \"-jar\", \"./app.jar\"]","title":"Docker image Security"},{"location":"guides/cp4i/mq/security/information/image/#use-a-secure-and-trusted-base-images","text":"The base image used is the ubi8:8.4 (Red Hat Universal Base Image version 8.4) Red Hat UBI images are tested, maintained and contain the latest security patches. See the UBI eBook to understand the advantages of using UBI images. A blog by a leading security company also recommends using Red Hat UBI base images as they typically have much fewer security vulnerabilities.","title":"Use a secure and trusted base images"},{"location":"guides/cp4i/mq/security/information/image/#do-not-run-as-a-root-user","text":"By default, if no USER is specified in a Dockerfile, the container will run as root on a Kubernetes platform. A security best practice is to specify a non root user in the Dockerfile. In our Dockerfile, we specify to run as USER 1001 which is a non root user. Note: OpenShift Container Platform, by default, does not allow containers to run as root but it is still a good practice to specify in the Dockerfile a non-root user. In fact OpenShift ignores the USER directive of the Dockerfile and launches the container with a random UUID for enhanced security.","title":"Do not run as a root user"},{"location":"guides/cp4i/mq/security/information/mtls/","text":"mTLS Security \u00b6 This section will explain mTLS (mutual TLS) security between the mq-spring-app and the mq queue manager. Mutual TLS encrypts the data in motion between the app and the queue manager by exchanging and validating each others certificates that are issued by a certificate authority. The mq manager also authenticates the mq-spring-app leveraging the OpenLDAP directory server. The diagram below shows the key components in the solution. The key parts of the solution are as follows: OpenLDAP - Directory server for authenticating users and applications. See: https://www.openldap.org/ cert-manager - a Kubernetes add-on operator to automate the issuance and management of TLS certificates from various issuing sources. See: https://github.com/jetstack/cert-manager sealed-secrets - a Kubernetes add-on operator to securely manage kubernetes secrets. See: https://github.com/bitnami-labs/sealed-secrets mq-self-signed-issuer ClusterIssuer resource - Issues certificates to the mq manager and the mq-spring-app components. It is configured to use the mq-self-signed-ca-cert to sign the issued certificates. mq-self-signed-ca-cert Certificate resource - generates the self signed root CA certificate. This certificate is used to sign all certificates issued by mq-self-signed-issuer . mq-server-certificate Certificate resource - cert-manager custom resource that generates the mq-server-cert secret that contains certificate, private key and CA cert. This is used by the mq queue manager to establish the mTLS connection with the mq-spring-app . mq-client-certificate Certificate resource - cert-manager custom resource that generates the mq-client-cert secret that contains keystore and truststore jks files. The keystore.jks file contains the certificate used by the mq-spring-app to establish the mTLS connection with the mq queue manager. The truststore.jks file contains the certificate used validate that a certificate is signed by the root CA. mq-client-jks-password sealed secret - generates mq-client-jks-password secret that contains the password to access the jks files. This is used by the mq-spring-app microservice. mq-spring-app sealed secret resource - sealed secret that generates the user and password credentials in the mq-spring-app secret for the client app to authenticate with the mq queue manager. mq-spring-app - Spring Boot java application that has REST endpoints to interact with the mq queue manager. mq - IBM MQ queue manager mqsc-configMap - configMap resource that contains mqsc commands that configures the queue manager. From a security perspective, the MQ manager is configured to delegate authentication to the OpenLDAP server as well as configuring the the mq queue manager to allow mTLS connections. During the establishment of the mTLS connection between the mq-spring-app and the mq manager, the mq-spring-app validates the tls.crt certificate that it receives from the mq manager. The mq-spring-app is able to validate the certificate using the truststore.jks file it has access to via the mq-client-jks secret. The truststore.jks file has the ca certificate of the issuer. Similarly, the mq-spring-app sends the mq manager its certificate which is stored within the keystore.jks file in the mq-client-jks Secret. The mq manager is able to validate it using the ca certificate of the issuer (ca.crt value in mq-server-cert secret). In addition, the mq manager is configured to allow connections for user mqapp . The mq-spring-app has a secret configured that contains values for the mqapp user and the corresponding password. The mq queue manager delegates the user authentication to the OpenLDAP server. The spring app gets these values injected via the application.yml file as follows: ibm: mq: user: ${USER} password: ${PASSWORD}","title":"mTLS Security"},{"location":"guides/cp4i/mq/security/information/mtls/#mtls-security","text":"This section will explain mTLS (mutual TLS) security between the mq-spring-app and the mq queue manager. Mutual TLS encrypts the data in motion between the app and the queue manager by exchanging and validating each others certificates that are issued by a certificate authority. The mq manager also authenticates the mq-spring-app leveraging the OpenLDAP directory server. The diagram below shows the key components in the solution. The key parts of the solution are as follows: OpenLDAP - Directory server for authenticating users and applications. See: https://www.openldap.org/ cert-manager - a Kubernetes add-on operator to automate the issuance and management of TLS certificates from various issuing sources. See: https://github.com/jetstack/cert-manager sealed-secrets - a Kubernetes add-on operator to securely manage kubernetes secrets. See: https://github.com/bitnami-labs/sealed-secrets mq-self-signed-issuer ClusterIssuer resource - Issues certificates to the mq manager and the mq-spring-app components. It is configured to use the mq-self-signed-ca-cert to sign the issued certificates. mq-self-signed-ca-cert Certificate resource - generates the self signed root CA certificate. This certificate is used to sign all certificates issued by mq-self-signed-issuer . mq-server-certificate Certificate resource - cert-manager custom resource that generates the mq-server-cert secret that contains certificate, private key and CA cert. This is used by the mq queue manager to establish the mTLS connection with the mq-spring-app . mq-client-certificate Certificate resource - cert-manager custom resource that generates the mq-client-cert secret that contains keystore and truststore jks files. The keystore.jks file contains the certificate used by the mq-spring-app to establish the mTLS connection with the mq queue manager. The truststore.jks file contains the certificate used validate that a certificate is signed by the root CA. mq-client-jks-password sealed secret - generates mq-client-jks-password secret that contains the password to access the jks files. This is used by the mq-spring-app microservice. mq-spring-app sealed secret resource - sealed secret that generates the user and password credentials in the mq-spring-app secret for the client app to authenticate with the mq queue manager. mq-spring-app - Spring Boot java application that has REST endpoints to interact with the mq queue manager. mq - IBM MQ queue manager mqsc-configMap - configMap resource that contains mqsc commands that configures the queue manager. From a security perspective, the MQ manager is configured to delegate authentication to the OpenLDAP server as well as configuring the the mq queue manager to allow mTLS connections. During the establishment of the mTLS connection between the mq-spring-app and the mq manager, the mq-spring-app validates the tls.crt certificate that it receives from the mq manager. The mq-spring-app is able to validate the certificate using the truststore.jks file it has access to via the mq-client-jks secret. The truststore.jks file has the ca certificate of the issuer. Similarly, the mq-spring-app sends the mq manager its certificate which is stored within the keystore.jks file in the mq-client-jks Secret. The mq manager is able to validate it using the ca certificate of the issuer (ca.crt value in mq-server-cert secret). In addition, the mq manager is configured to allow connections for user mqapp . The mq-spring-app has a secret configured that contains values for the mqapp user and the corresponding password. The mq queue manager delegates the user authentication to the OpenLDAP server. The spring app gets these values injected via the application.yml file as follows: ibm: mq: user: ${USER} password: ${PASSWORD}","title":"mTLS Security"},{"location":"guides/cp4i/mq/security/information/overview/","text":"App Connect Enterprise \u00b6 Table of Contents mTLS security Container security Image security mTLS Security \u00b6 Describes the mTLS security concepts for establishing a secure connection between a mq queue manager and a sample client application. Please see the mTLS security page for further details. Container Security \u00b6 Describes best practices for running containers securely on OpenShift. Please see the container security page for further details. Image Security \u00b6 Describes best practices for creating secure container images. Please see the image security page for further details.","title":"Security"},{"location":"guides/cp4i/mq/security/information/overview/#app-connect-enterprise","text":"Table of Contents mTLS security Container security Image security","title":"App Connect Enterprise"},{"location":"guides/cp4i/mq/security/information/overview/#mtls-security","text":"Describes the mTLS security concepts for establishing a secure connection between a mq queue manager and a sample client application. Please see the mTLS security page for further details.","title":"mTLS Security"},{"location":"guides/cp4i/mq/security/information/overview/#container-security","text":"Describes best practices for running containers securely on OpenShift. Please see the container security page for further details.","title":"Container Security"},{"location":"guides/cp4i/mq/security/information/overview/#image-security","text":"Describes best practices for creating secure container images. Please see the image security page for further details.","title":"Image Security"},{"location":"guides/cp4i/mq/using/this-guide/","text":"The MQ tutorials \u00b6 Audience : Architects, Application developers, Administrators Overview \u00b6 In this topic we're going to: Introduce you to the MQ tutorials Outline the structure of the tutorials By the end of this topic you'll understand the MQ tutorials, and how they can help you design and build cloud native MQ systems. Introduction \u00b6 The purpose of the MQ tutorials is to teach architects, developers and operations staff how to implement a production-ready MQ deployment on the OpenShift Container Platform . It makes extensive use of the IBM Cloud Pak for Integration (CP4I) and other cloud native technologies such as Tekton, Kustomize, ArgoCD, Prometheus, Grafana and Kibana. Cloud native systems are loosely coupled, making them resilient, manageable, and observable. A cloud native systems brings robust DevOps automation which allows MQ developers, administrators and architects to make high quality changes with predictability and minimal toil as frequently as their organization requires. Using worked examples, you will build and run a modern MQ deployment according to cloud native principles, gaining hands-on experience of these technologies and their benefits. The tutorial is structured as a set of tutorials and it is recommended that you follow them in order. Earlier tutorials are foundational, whereas later ones such as high availability and disaster recovery cover advanced features. If you already have a good working knowledge of MQ and OpenShift, these more advanced topics can be attempted stand-alone. There is an intuitive table of contents. Next and Previous links at the bottom of each page to help you navigate. The tutorial is intended as a complement to MQ product documentation; where relevant the tutorial will refer to it and other documents. Tutorial structure \u00b6 This tutorial is divided into chapters that help you build, step-by-step, a set of working MQ deployments, based on a cloud native approach. Each chapter comprises topics which you should complete in order to gain a full understanding. The tutorial starts with a simple MQ application and queue manager. It progresses to MQ uniform cluster, MQ NativeHA and multiple availability zone and region configurations. The tutorial is structured to match the development and operations lifecycle: Install and upgrade Build and test Deployment Promotion Security Monitoring Scalability Performance High Availability Disaster recovery CPU, memory, network utilization Server consolidation See how the table of contents on the left of this tutorial is arranged to match this lifecycle. Architecture decisions \u00b6 Throughout the tutorial you'll see alternative implementation choices: Info Baking : Extending the official MQ image for the purpose of embedding configuration or adding third party binaries. Baking happens at build time. Frying : Using a ConfigMap to add configuration information to an existing MQ image. Frying happens at deployment time. More The inline text gives a summary of the architecture decision used to make the technical choice. The link provides a full explanation of the architecture decision. All architecture decisions are described in the Architecture decisions registry .","title":"The MQ tutorial"},{"location":"guides/cp4i/mq/using/this-guide/#the-mq-tutorials","text":"Audience : Architects, Application developers, Administrators","title":"The MQ tutorials"},{"location":"guides/cp4i/mq/using/this-guide/#overview","text":"In this topic we're going to: Introduce you to the MQ tutorials Outline the structure of the tutorials By the end of this topic you'll understand the MQ tutorials, and how they can help you design and build cloud native MQ systems.","title":"Overview"},{"location":"guides/cp4i/mq/using/this-guide/#introduction","text":"The purpose of the MQ tutorials is to teach architects, developers and operations staff how to implement a production-ready MQ deployment on the OpenShift Container Platform . It makes extensive use of the IBM Cloud Pak for Integration (CP4I) and other cloud native technologies such as Tekton, Kustomize, ArgoCD, Prometheus, Grafana and Kibana. Cloud native systems are loosely coupled, making them resilient, manageable, and observable. A cloud native systems brings robust DevOps automation which allows MQ developers, administrators and architects to make high quality changes with predictability and minimal toil as frequently as their organization requires. Using worked examples, you will build and run a modern MQ deployment according to cloud native principles, gaining hands-on experience of these technologies and their benefits. The tutorial is structured as a set of tutorials and it is recommended that you follow them in order. Earlier tutorials are foundational, whereas later ones such as high availability and disaster recovery cover advanced features. If you already have a good working knowledge of MQ and OpenShift, these more advanced topics can be attempted stand-alone. There is an intuitive table of contents. Next and Previous links at the bottom of each page to help you navigate. The tutorial is intended as a complement to MQ product documentation; where relevant the tutorial will refer to it and other documents.","title":"Introduction"},{"location":"guides/cp4i/mq/using/this-guide/#tutorial-structure","text":"This tutorial is divided into chapters that help you build, step-by-step, a set of working MQ deployments, based on a cloud native approach. Each chapter comprises topics which you should complete in order to gain a full understanding. The tutorial starts with a simple MQ application and queue manager. It progresses to MQ uniform cluster, MQ NativeHA and multiple availability zone and region configurations. The tutorial is structured to match the development and operations lifecycle: Install and upgrade Build and test Deployment Promotion Security Monitoring Scalability Performance High Availability Disaster recovery CPU, memory, network utilization Server consolidation See how the table of contents on the left of this tutorial is arranged to match this lifecycle.","title":"Tutorial structure"},{"location":"guides/cp4i/mq/using/this-guide/#architecture-decisions","text":"Throughout the tutorial you'll see alternative implementation choices: Info Baking : Extending the official MQ image for the purpose of embedding configuration or adding third party binaries. Baking happens at build time. Frying : Using a ConfigMap to add configuration information to an existing MQ image. Frying happens at deployment time. More The inline text gives a summary of the architecture decision used to make the technical choice. The link provides a full explanation of the architecture decision. All architecture decisions are described in the Architecture decisions registry .","title":"Architecture decisions"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/","text":"Upgrading IBM Cloud Pak\u00ae for Security \u00b6 The steps to upgrade are the same as the steps to install. The process automatically detects an older version and initiates an upgrade. Ensure that when filling in the parameters, the following parameters matches what you have in the version of IBM Cloud Pak\u00ae for Security currently installed in your cluster: If you are upgrading from IBM Cloud Pak\u00ae for Security 1.8.X, use the following procedure to retrieve the values for the parameters you need to pass in during the upgrade. Login to your Red Hat OpenShift web console. Go to Operators > Installed Operators and ensure that the Project dropdown is set to the namespace where IBM Cloud Pak\u00ae for Security 1.8.X was installed. In the list of installed operators, click IBM Cloud Pak for Security. Navigate to the Threat Management tab and click on the threatmgmt instance. From the Threat Management Overview page, make note of the value currently set for the following parameters: Admin User Domain Storage class Enable ROKS Authentication If you are upgrading from IBM Cloud Pak\u00ae for Security 1.7.2.0, use the following commands to retrieve the values for the parameters you need to pass in during the upgrade. namespace - Provide the namespace where IBM Cloud Pak\u00ae for Security 1.7.2.0 was installed. adminUser - The admin user ID set during the IBM Cloud Pak\u00ae for Security 1.7.2.0 installation. You can verify the value by running the following command: oc get deploy isc-entitlements -o yaml -n <CP4S_NAMESPACE> | grep \"name: ADMIN_USER_ID\" -A1 domain - The current domain being used by {{site.data.keyword.isc}} can be retrieved by running the following command: oc get route isc-route-default -o jsonpath = '{.spec.host}' -n <CP4S_NAMESPACE> storageClass - Set the storage class to the same storage class being used in IBM Cloud Pak\u00ae for Security 1.7.2.0 which should be default storage class. You can run the following command to verify the default storage class in the cluster: oc get sc roksAuthentication - Set this to the same value you used in the cp4sOpenshiftAuthentication parameter when installing IBM Cloud Pak\u00ae for Security 1.7.2.0. Verify the value that you used by running the following command: oc get cm platform-auth-idp -n ibm-common-services -o jsonpath = '{.data.ROKS_ENABLED}' Backup and restore \u00b6 To recover from any data loss that might occur, regularly back up the data in your IBM Cloud Pak\u00ae for Security and integrated databases. You can use the backup and restore process to support a disaster recovery that requires a redeployment of your environment. Prerequisites \u00b6 Cluster administrator level privileges are required to complete the backup and restore process. To install Cloud Pak for Security, you configure a suitable storage class in the cluster. You support the configuration with one or more persistent volumes of suitable size. For more information about storage, see Persistent storage requirements You provide secure storage for the backups that is mounted as a Persistent Volume Claim (PVC) in a pod. The backup and restore process uses a Backup and Restore pod, which contains all of the necessary utilities that are required for the backup and restore process. The Backup and Restore pod is deployed automatically as part of the installation or upgrade of Cloud Pak for Security. For the backup data, you can opt to provision your own storage instead of using the default specified for installation. For more information, see Creating the backup and restore PVC Backing up Cloud Pak for Security \u00b6 To back up your databases for IBM Cloud Pak\u00ae for Security, you must run the backup scripts from within the Backup and Restore pod. Before you begin The OpenShift\u00ae command-line interface tool must be connected to the cluster. The Backup and Restore pod must be running on the cluster. To ensure that the Backup and Restore pod is running, type the following command in the Cloud Pak for Security namespace. oc get pods | grep cp4s-backup-restore Verify that the command completes and take note in the output of the full pod name, in the following example the name of the pod is cp4s-backup-restore-8ffb54b4f-lfblg. cp4s-backup-restore-8ffb54b4f-lfblg 1/1 Running 0 8m53s In the following procedure, replace with the value that you noted from the output of the previous command. Procedure \u00b6 Back up all of the Cloud Pak for Security data stores by typing one of the following commands in the Cloud Pak for Security namespace. oc exec <backup_restore_podname> -- /opt/bin/backup-cp4s.sh -p <encryption_password> The backup script automatically prunes older backups to preserve disk space on the PVC. The number of backups to keep can be set by using the keepfiles parameter. The default is 7. When the command is run without scoping arguments, it backs up all of the data stores. Command-line scoping arguments can be used to back up individual components, or combinations of components, as required. For example, the following command backs up CouchDB and Cases data only with an encryption password: oc exec <backup_restore_podname> -- /opt/bin/backup-cp4s.sh -couch -cases -p <encryption_password> Backup files are stored in the Persistent Volume Claim that is mounted on the pod under /opt/data/backup/ , where indicates the data store. The file naming conventions are outlined in the following tables. It is good practice to maintain a copy of the backup files in a secure alternative location outside of the Cloud Pak for Security cluster. After you complete the backup procedure, the backup files can be copied to your local system by typing the following command. oc cp <backup_restore_podname>:/opt/data/backup ./backup The result is a copy of the complete backup directory structure on your local system, in your current directory, in a new subfolder called backup. This folder can then be transferred to an appropriate secure storage location. Scheduling Cloud Pak for Security backup \u00b6 IBM Cloud Pak\u00ae for Security provides a support action that enables you to schedule Cloud Pak for Security backup. Before you begin \u00b6 To access the schedule_cp4s_full_backup action, you must install the command-line interface (CLI) utility cpctl from the cp-serviceability pod. For more information, see Installing the cpctl utility Setting up the backup schedule \u00b6 Procedure \u00b6 To set up the schedule, type the following command. In an online environment cpctl tools schedule_cp4s_full_backup --token \" $( oc whoami -t ) \" --password <add-any-password-of-your-choice> --schedule <your schedule> In an air-gapped environment cpctl tools schedule_cp4s_full_backup --token \" $( oc whoami -t ) \" --password <add-any-password-of-your-choice> --schedule \"schedule\" --airgap <local-docker-registry:5000> Disabling scheduled backup \u00b6 Procedure \u00b6 To disable the scheduled backup, use the disable flag. cpctl tools schedule_cp4s_full_backup --token \" $( oc whoami -t ) \" --password <add-any-password-of-your-choice> --disable Configuring the number of backup files maintained \u00b6 Procedure \u00b6 When running a backup, the oldest backups are deleted to save space on the PVC. The default can be changed by using --keepfiles. cpctl tools schedule_cp4s_full_backup --token \" $( oc whoami -t ) \" --password <add-any-password-of-your-choice> --keepfiles 3 Restoring Cloud Pak for Security \u00b6 When the restore process is completed, data is restored and the system returns to the state at the time of the backup. The corresponding databases are restored in the appropriate persistent volume of the IBM Cloud Pak\u00ae for Security instance. Before you begin \u00b6 The OpenShift\u00ae command-line interface tool must be connected to the cluster. The Backup and Restore pod must be running on the cluster. To ensure that the Backup and Restore pod is running, type the following command in the Cloud Pak for Security namespace. oc get pods | grep cp4s-backup-restore Verify that the command completes and take note in the output of the full pod name. In the following example, the name of the pod is cp4s-backup-restore-8ffb54b4f-lfblg. cp4s-backup-restore-8ffb54b4f-lfblg 1 /1 Running 0 8m53s In the following commands, replace with the value that you noted from the previous command. For more information about the process and what is restored, see Backup and restore Procedure \u00b6 Copy backup data, which is stored outside of the cluster, into the toolbox PVC from your local system by typing the following command in the Cloud Pak for Security namespace. oc cp <localpath>/backup <backup_restore_podname>:/opt/data /backup is the backup folder name and full path location on your local system. Restore all of the Cloud Pak for Security data stores by typing one of the following command in the Cloud Pak for Security namespace. oc exec <backup_restore_podname> -- /opt/bin/restore-cp4s.sh -p <encryption_password> The encryption password that is used in the restore command must be the same as the one used to back up the data. When the command is run without any scoping arguments, by default it completes a full restore of all of the data stores. The following command-line arguments can be inserted in the command to restore individual components, or combinations of components. After the restore is complete, allow up to 15 minutes for the pods to complete the restart operation. If the first attempt at restoring the system is not successful for any reason, the full restore procedure can be run again without any impact. Run the validate action to ensure that Cloud Pak for Security is restored successfully. cloudctl case launch --case ibm-cp-security --namespace <cp4s_namespace> --inventory ibmSecurityOperatorSetup --action validate -t 1 --namespace The namespace where Cloud Pak for Security is installed. The following sample command restores CouchDB by using the default file and an encryption password. oc exec <backup_restore_podname> -- /opt/bin/restore-cp4s.sh -couch -p <encryption_password> The following sample command restores CouchDB and Cases by using a specific backup file for Cases and the default backup file for CouchDB, and an encryption password. oc exec <backup_restore_podname> -- /opt/bin/restore-cp4s.sh -p <encryption_password> -couch -cases -cases-file /opt/data/backup/cases/cases_backup_2020_10_22__10_38_17.gz","title":"Managing Lifecycle for CP4S"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#upgrading-ibm-cloud-pak-for-security","text":"The steps to upgrade are the same as the steps to install. The process automatically detects an older version and initiates an upgrade. Ensure that when filling in the parameters, the following parameters matches what you have in the version of IBM Cloud Pak\u00ae for Security currently installed in your cluster: If you are upgrading from IBM Cloud Pak\u00ae for Security 1.8.X, use the following procedure to retrieve the values for the parameters you need to pass in during the upgrade. Login to your Red Hat OpenShift web console. Go to Operators > Installed Operators and ensure that the Project dropdown is set to the namespace where IBM Cloud Pak\u00ae for Security 1.8.X was installed. In the list of installed operators, click IBM Cloud Pak for Security. Navigate to the Threat Management tab and click on the threatmgmt instance. From the Threat Management Overview page, make note of the value currently set for the following parameters: Admin User Domain Storage class Enable ROKS Authentication If you are upgrading from IBM Cloud Pak\u00ae for Security 1.7.2.0, use the following commands to retrieve the values for the parameters you need to pass in during the upgrade. namespace - Provide the namespace where IBM Cloud Pak\u00ae for Security 1.7.2.0 was installed. adminUser - The admin user ID set during the IBM Cloud Pak\u00ae for Security 1.7.2.0 installation. You can verify the value by running the following command: oc get deploy isc-entitlements -o yaml -n <CP4S_NAMESPACE> | grep \"name: ADMIN_USER_ID\" -A1 domain - The current domain being used by {{site.data.keyword.isc}} can be retrieved by running the following command: oc get route isc-route-default -o jsonpath = '{.spec.host}' -n <CP4S_NAMESPACE> storageClass - Set the storage class to the same storage class being used in IBM Cloud Pak\u00ae for Security 1.7.2.0 which should be default storage class. You can run the following command to verify the default storage class in the cluster: oc get sc roksAuthentication - Set this to the same value you used in the cp4sOpenshiftAuthentication parameter when installing IBM Cloud Pak\u00ae for Security 1.7.2.0. Verify the value that you used by running the following command: oc get cm platform-auth-idp -n ibm-common-services -o jsonpath = '{.data.ROKS_ENABLED}'","title":"Upgrading IBM Cloud Pak\u00ae for Security"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#backup-and-restore","text":"To recover from any data loss that might occur, regularly back up the data in your IBM Cloud Pak\u00ae for Security and integrated databases. You can use the backup and restore process to support a disaster recovery that requires a redeployment of your environment.","title":"Backup and restore"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#prerequisites","text":"Cluster administrator level privileges are required to complete the backup and restore process. To install Cloud Pak for Security, you configure a suitable storage class in the cluster. You support the configuration with one or more persistent volumes of suitable size. For more information about storage, see Persistent storage requirements You provide secure storage for the backups that is mounted as a Persistent Volume Claim (PVC) in a pod. The backup and restore process uses a Backup and Restore pod, which contains all of the necessary utilities that are required for the backup and restore process. The Backup and Restore pod is deployed automatically as part of the installation or upgrade of Cloud Pak for Security. For the backup data, you can opt to provision your own storage instead of using the default specified for installation. For more information, see Creating the backup and restore PVC","title":"Prerequisites"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#backing-up-cloud-pak-for-security","text":"To back up your databases for IBM Cloud Pak\u00ae for Security, you must run the backup scripts from within the Backup and Restore pod. Before you begin The OpenShift\u00ae command-line interface tool must be connected to the cluster. The Backup and Restore pod must be running on the cluster. To ensure that the Backup and Restore pod is running, type the following command in the Cloud Pak for Security namespace. oc get pods | grep cp4s-backup-restore Verify that the command completes and take note in the output of the full pod name, in the following example the name of the pod is cp4s-backup-restore-8ffb54b4f-lfblg. cp4s-backup-restore-8ffb54b4f-lfblg 1/1 Running 0 8m53s In the following procedure, replace with the value that you noted from the output of the previous command.","title":"Backing up Cloud Pak for Security"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#procedure","text":"Back up all of the Cloud Pak for Security data stores by typing one of the following commands in the Cloud Pak for Security namespace. oc exec <backup_restore_podname> -- /opt/bin/backup-cp4s.sh -p <encryption_password> The backup script automatically prunes older backups to preserve disk space on the PVC. The number of backups to keep can be set by using the keepfiles parameter. The default is 7. When the command is run without scoping arguments, it backs up all of the data stores. Command-line scoping arguments can be used to back up individual components, or combinations of components, as required. For example, the following command backs up CouchDB and Cases data only with an encryption password: oc exec <backup_restore_podname> -- /opt/bin/backup-cp4s.sh -couch -cases -p <encryption_password> Backup files are stored in the Persistent Volume Claim that is mounted on the pod under /opt/data/backup/ , where indicates the data store. The file naming conventions are outlined in the following tables. It is good practice to maintain a copy of the backup files in a secure alternative location outside of the Cloud Pak for Security cluster. After you complete the backup procedure, the backup files can be copied to your local system by typing the following command. oc cp <backup_restore_podname>:/opt/data/backup ./backup The result is a copy of the complete backup directory structure on your local system, in your current directory, in a new subfolder called backup. This folder can then be transferred to an appropriate secure storage location.","title":"Procedure"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#scheduling-cloud-pak-for-security-backup","text":"IBM Cloud Pak\u00ae for Security provides a support action that enables you to schedule Cloud Pak for Security backup.","title":"Scheduling Cloud Pak for Security backup"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#before-you-begin","text":"To access the schedule_cp4s_full_backup action, you must install the command-line interface (CLI) utility cpctl from the cp-serviceability pod. For more information, see Installing the cpctl utility","title":"Before you begin"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#setting-up-the-backup-schedule","text":"","title":"Setting up the backup schedule"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#procedure_1","text":"To set up the schedule, type the following command. In an online environment cpctl tools schedule_cp4s_full_backup --token \" $( oc whoami -t ) \" --password <add-any-password-of-your-choice> --schedule <your schedule> In an air-gapped environment cpctl tools schedule_cp4s_full_backup --token \" $( oc whoami -t ) \" --password <add-any-password-of-your-choice> --schedule \"schedule\" --airgap <local-docker-registry:5000>","title":"Procedure"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#disabling-scheduled-backup","text":"","title":"Disabling scheduled backup"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#procedure_2","text":"To disable the scheduled backup, use the disable flag. cpctl tools schedule_cp4s_full_backup --token \" $( oc whoami -t ) \" --password <add-any-password-of-your-choice> --disable","title":"Procedure"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#configuring-the-number-of-backup-files-maintained","text":"","title":"Configuring the number of backup files maintained"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#procedure_3","text":"When running a backup, the oldest backups are deleted to save space on the PVC. The default can be changed by using --keepfiles. cpctl tools schedule_cp4s_full_backup --token \" $( oc whoami -t ) \" --password <add-any-password-of-your-choice> --keepfiles 3","title":"Procedure"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#restoring-cloud-pak-for-security","text":"When the restore process is completed, data is restored and the system returns to the state at the time of the backup. The corresponding databases are restored in the appropriate persistent volume of the IBM Cloud Pak\u00ae for Security instance.","title":"Restoring Cloud Pak for Security"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#before-you-begin_1","text":"The OpenShift\u00ae command-line interface tool must be connected to the cluster. The Backup and Restore pod must be running on the cluster. To ensure that the Backup and Restore pod is running, type the following command in the Cloud Pak for Security namespace. oc get pods | grep cp4s-backup-restore Verify that the command completes and take note in the output of the full pod name. In the following example, the name of the pod is cp4s-backup-restore-8ffb54b4f-lfblg. cp4s-backup-restore-8ffb54b4f-lfblg 1 /1 Running 0 8m53s In the following commands, replace with the value that you noted from the previous command. For more information about the process and what is restored, see Backup and restore","title":"Before you begin"},{"location":"guides/cp4s/cp4s-tree/cluster-config/backup-recovery-upgrade/#procedure_4","text":"Copy backup data, which is stored outside of the cluster, into the toolbox PVC from your local system by typing the following command in the Cloud Pak for Security namespace. oc cp <localpath>/backup <backup_restore_podname>:/opt/data /backup is the backup folder name and full path location on your local system. Restore all of the Cloud Pak for Security data stores by typing one of the following command in the Cloud Pak for Security namespace. oc exec <backup_restore_podname> -- /opt/bin/restore-cp4s.sh -p <encryption_password> The encryption password that is used in the restore command must be the same as the one used to back up the data. When the command is run without any scoping arguments, by default it completes a full restore of all of the data stores. The following command-line arguments can be inserted in the command to restore individual components, or combinations of components. After the restore is complete, allow up to 15 minutes for the pods to complete the restart operation. If the first attempt at restoring the system is not successful for any reason, the full restore procedure can be run again without any impact. Run the validate action to ensure that Cloud Pak for Security is restored successfully. cloudctl case launch --case ibm-cp-security --namespace <cp4s_namespace> --inventory ibmSecurityOperatorSetup --action validate -t 1 --namespace The namespace where Cloud Pak for Security is installed. The following sample command restores CouchDB by using the default file and an encryption password. oc exec <backup_restore_podname> -- /opt/bin/restore-cp4s.sh -couch -p <encryption_password> The following sample command restores CouchDB and Cases by using a specific backup file for Cases and the default backup file for CouchDB, and an encryption password. oc exec <backup_restore_podname> -- /opt/bin/restore-cp4s.sh -p <encryption_password> -couch -cases -cases-file /opt/data/backup/cases/cases_backup_2020_10_22__10_38_17.gz","title":"Procedure"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-config/","text":"Cloud Native MQ GitOps Configuration \u00b6 In the cluster configuration of this chapter, we installed the fundamental components for continuous integration and continuous deployment into our cluster. These included a sample GitOps repository, and ArgoCD. In this section we're going to customize and enable the GitOps repository, so that we can install the components highlighted in our CP4S CICD process: We'll examine these components in more detail throughout this section of the tutorial; here's an initial overview of their function: ci-namespace provide an execution namespace for our pipelines tools-namespace provide an execution namespace for tools such as Artifactory cp4s-namespace provide an execution namespace for our deployed CP4S when running in the development environment. Later in the tutorial, we'll add staging and prod namespaces. ArgoCD applications manage the ci , tools and dev namespaces. You may have already noticed that the GitOps repository contains YAMLs that refer to the ArgoCD and namespaces in the diagram above. We're now going to use these YAMLs to configure the cluster resources using GitOps. Becoming comfortable with the concepts by practicing GitOps will help us in later chapters when we CP4S Cloud Pak In this topic, we're going to: Explore the sample GitOps repository in a little more detail Customize the GitOps repository for our cluster Connect ArgoCD to the customized GitOps repository Bootstrap the cluster Explore how the ci namespace is created Try out some dynamic changes to the cluster with ArgoCD Explore how ArgoCD manages configuration drift By the end of this topic we'll have a cluster up and running, having used GitOps to do it. We'll fully understand how ArgoCD manages cluster change and configuration drift. Pre-requisites \u00b6 Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed on your local machine the oc command that matches the version of your cluster. You have also installed npm , git and tree commands. You have customized the GitOps repository and ArgoCD. Please see the previous sections of this guide for information on how to do these tasks. Video Walkthrough \u00b6 This video demonstrates how to use ArgoCD and the GitOps repository to set up different infra related components. This is a video walkthrough and it takes you step by step through the below sections. The sample GitOps repository \u00b6 Let's understand how the GitOps repository works by using it to install the components we've highlighted in the above diagram. In the first part of this topic, let's look at the high level structure of the GitOps repository. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Explore the high level folder structure Use the following command to display the folder structure: tree . -d -L 2 We can see the different folders in the GitOps repository: . \u251c\u2500\u2500 0-bootstrap \u2502 \u251c\u2500\u2500 others \u2502 \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 doc \u2502 \u251c\u2500\u2500 diagrams \u2502 \u251c\u2500\u2500 experimental \u2502 \u251c\u2500\u2500 images \u2502 \u2514\u2500\u2500 scenarios \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 bom \u2514\u2500\u2500 setup \u2514\u2500\u2500 ocp47 The 0-bootstrap folder is the key folder that contains all the available profiles. Within this folders are Cluster Profiles that control which resources are deployed to the cluster. There are other folders, but these contain utility scripts; and some document related information. Let us examine the 0-bootstrap folder structure: tree ./0-bootstrap/ -d -L 2 We can see the different folders: ./0-bootstrap/ \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u2514\u2500\u2500 3-apps If we consider single-cluster profile, you can see the three layers under that covering infrastructure, services, and applications. Use single-cluster profile to configure the cluster Use the following command to display the folder structure: tree ./0-bootstrap/single-cluster/ -L 3 We can see the different folders: ./0-bootstrap/single-cluster/ \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 1-infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 2-services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3-apps \u2502 \u251c\u2500\u2500 3-apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 cp4a \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Notice that each high level folder has an argocd folder. Within these argocd folders are ArgoCD applications that control which resources are deployed to the cluster. Sample ArgoCD application We'll see later how these ArgoCD applications work in detail. For now, notice how each layer of the architecture ( infrastructure , service , application ) has a set of ArgoCD applications. Type the following command: tree 0 -bootstrap/single-cluster/1-infra/ It shows a list of ArgoCD applications that are used to manage Kubernetes infrastructure resources: 0-bootstrap/single-cluster/1-infra/ \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Notice the namespace- YAMLs; we'll see in a moment how these each define an ArgoCD application dedicated to managing a Kubernetes namespace in our reference architecture. A word on terminology As we get started with ArgoCD, it can be easy to confuse the term ArgoCD application with your application. That's because ArgoCD uses the term application to refer to a Kubernetes custom resource that was initially designed to automate application management. However, an ArgoCD application can automate the deployment of any Kubernetes resource within a cluster, such as namespaces or queue managers, as we'll see a little later. Let's now customize this repository to deploy the highlighted Kubernetes resources to our cluster. Customizing the GitOps repositories \u00b6 The cloned GitOps configuration repositories (specifically, multi-tenancy-gitops and multi-tenancy-gitops-apps ) needs to be customized for your cluster. The ArgoCD Application and AppProject resources and kustomization.yaml resources contains environment variables which will need to be updated to your specific GitHub organization and branch. Update multi-tenancy-gitops repository \u00b6 Have a look at the following kustomization.yaml as an example of what will need to be updated. cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS_INFRA} - op: add path: /spec/source/targetRevision value: ${GIT_GITOPS_INFRA_BRANCH} Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Run the customization script Let's customize the cloned multi-tenancy-gitops repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. This script has also made some customizations that we'll use much later in the tutorial; we'll refer to those at the relevant time. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 199 files changed, 290 insertions ( + ) , 290 deletions ( - ) Set your GitOps repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (48/48), 4.70 KiB | 1.57 MiB/s, done. Total 48 (delta 32), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 23 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git d95eca5..a900c39 master -> master Update multi-tenancy-gitops-apps repository \u00b6 Similar to the previous section, there are corresponding ArgoCD Applications resources that will need to be updated in the multi-tenancy-gitops-apps repository. Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Locate the multi-tenancy-gitops-apps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops-apps Run the customization script Let's customize the cloned multi-tenancy-gitops-apps repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops-apps repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 21 files changed, 42 insertions ( + ) , 42 deletions ( - ) Set your repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops-apps repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (42/42), 4.70 KiB | 1.57 MiB/s, done. Total 42 (delta 42), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 21 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git d95eca5..a900c39 master -> master Connect ArgoCD to the GitOps repository \u00b6 Let's now connect your customized GitOps repository to the instance of ArgoCD running in the cluster. Once connected, ArgoCD will use the contents of the repository to create necessary resources. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Review ArgoCD infrastructure folder Let's examine the 0-bootstrap/single-cluster/1-infra/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml We can see the contents of the kustomization.yaml : resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Notice that the resources that needs to be applied to the cluster are all inactivate and commented out. Let us enable the resources that are needed by un-commenting them. Deploy Kubernetes resources with kustomization.yaml Open 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the below resources: argocd/namespace-ci.yaml argocd/consolenotification.yaml argocd/namespace-ibm-common-services.yaml argocd/namespace-tools.yaml You will have the following resources un-commented for infrastructure: resources : #- argocd/consolelink.yaml - argocd/consolenotification.yaml - argocd/namespace-ibm-common-services.yaml - argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml - argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for infrastructure\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a900c39..e3f696d master -> master Examine bootstrap.yaml residing in 0-bootstrap/single-cluster/ . The bootstrap.yaml file is used to create our first ArgoCD application called bootstrap-single-cluster . This initial ArgoCD application will create all the other ArgoCD applications that control the application, service, and infrastructure resources (such as the ci and dev namespaces) deployed to the cluster. Examine the YAML that defines the ArgoCD bootstrap application: cat 0 -bootstrap/single-cluster/bootstrap.yaml Notice also how this ArgoCD application has been customized to use the GitOps repository repoURL: https://github.com/prod-ref-guide/multi-tenancy-gitops.git . apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-single-cluster namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/single-cluster repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Most importantly, see how path: 0-bootstrap/single-cluster refers to the 0-bootstrap/single-cluster folder within this repository. This will result in the creation of individual ArgoCD applications to manage our cluster resources. Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us for now only deploy infra resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and comment out the 2-services/2-services.yaml and 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml # - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Commit and push changes to your git repository: git add . git commit -s -m \"Using only infra\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 456 bytes | 456.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git e3f696d..ea3b43f master -> master Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. The repository contains a bootstrap-single-cluster ArgoCD application that is watching this repository and using its contents to manage the cluster. When the bootstrap-single-cluster ArgoCD application is applied to the cluster, it will continuously ensure that all the activated resources are applied to the cluster. Apply the bootstrap YAML to the cluster: oc apply -f 0 -bootstrap/single-cluster/bootstrap.yaml Kubernetes will confirm that the bootstrap ArgoCD application has been created: application.argoproj.io/bootstrap-single-cluster created The bootstrap ArgoCD application will watch the 0-bootstrap/single-cluster folder in our GitOps repository on GitHub. In this way, as resources are added to the infrastructure , service and application folders, they will be deployed to the cluster automatically. This is therefore the only direct cluster operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Verify the bootstrap deployment Verify that the bootstrap ArgoCD application is running with the following command: oc get app/bootstrap-single-cluster -n openshift-gitops You should see that the bootstrap application was recently updated: NAME SYNC STATUS HEALTH STATUS bootstrap-single-cluster Synced Healthy HEALTH_STATUS may temporarily show Missing ; simply re-issue the command to confirm it moves to Healthy . Using the UI to view the newly deployed ArgoCD applications In the previous section of this chapter you logged on to the ArgoCD web console. Switch back to that console, refresh the page and you should see the bootstrap-single-cluster ArgoCD application together with many other ArgoCD applications: (You may need to select List view rather than the Tiles view.) We can see that eleven ArgoCD applications have been deployed to the cluster as a result of applying bootstrap.yaml . In the next section of the tutorial, we'll examine these applications to see how and why they were created, but for now let's focus on one of them -- the namespace-ci ArgoCD application. Examining the namespace-ci ArgoCD application resources Let's examine the Kubernetes resources applied to the cluster by the namespace-ci ArgoCD application. In the ArgoCD application list, click on namespace-ci : (You may need to clear filters to see this screenshot.) The directed graph shows that the namespace-ci ArgoCD app has created 4 Kubernetes resources; our ci namespace and three role bindings. Verify the namespace using the oc CLI We've seen the new namespace definition in the GitOps repository and visually in the ArgoCD UI. Let's also verify it via the command line: Type the following command: oc get namespace ci -o yaml This will list the full details of the ci namespace: apiVersion : v1 kind : Namespace metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"namespace-ci\"},\"name\":\"ci\"},\"spec\":{}} openshift.io/sa.scc.mcs : s0:c27,c9 openshift.io/sa.scc.supplemental-groups : 1000720000/10000 openshift.io/sa.scc.uid-range : 1000720000/10000 creationTimestamp : \"2021-08-31T15:27:32Z\" labels : app.kubernetes.io/instance : namespace-ci managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:labels : . : {} f:app.kubernetes.io/instance : {} f:status : f:phase : {} manager : argocd-application-controller operation : Update time : \"2021-08-31T15:27:32Z\" - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : f:openshift.io/sa.scc.mcs : {} f:openshift.io/sa.scc.supplemental-groups : {} f:openshift.io/sa.scc.uid-range : {} manager : cluster-policy-controller operation : Update time : \"2021-08-31T15:27:32Z\" name : ci resourceVersion : \"2255607\" selfLink : /api/v1/namespaces/ci uid : fff6b82b-6318-4828-83bb-ade4e8e3c0cf spec : finalizers : - kubernetes status : phase : Active Notice how manager: argocd-application-controller identifies that this namespace was created by ArgoCD. It's important to understand the sequence of actions. We simply deployed the bootstrap-single-cluster ArgoCD application, and it ultimately resulted in the creation of the namespace-ci ArgoCD application which created the ci namespace. We don't directly apply resources to the cluster once the bootstrap-single-cluster ArgoCD application has been applied; the cluster state is determined by an ArgoCD application YAML in the corresponding application , service or infrastructure related folders. It's these ArgoCD applications that create and manage the underlying Kubernetes resources using the GitOps repository as the source of truth. Understanding the namespace-ci ArgoCD application Let's examine the ArgoCD application namespace-ci to see how it created the ci namespace and three role bindings in the cluster. Issue the following command to examine its YAML: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-ci.yaml Notice that apiVersion and kind identify this as an ArgoCD application: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ci labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" spec : destination : namespace : ci server : https://kubernetes.default.svc project : infra source : path : namespaces/ci syncPolicy : automated : prune : true selfHeal : true Most importantly, see how this namespace-ci ArgoCD application monitors the folder path: namespaces/ci in https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git ; it then applies the contents of this folder to the cluster whenever its content changes. Notice the SyncPolicy of automated ; any changes to this folder will automatically be applied to the cluster; we do not need to perform a manual Sync operation from the ArgoCD UI. Examine the ci namespace YAML To examine this, navigate to multi-tenancy-gitops-infra repository you cloned previously. cd $HOME /git cd multi-tenancy-gitops-infra Let's examine the ci namespace YAML in the namespaces/ci folder: cat namespaces/ci/namespace.yaml It's a very simple YAML: apiVersion : v1 kind : Namespace metadata : name : ci spec : {} This is a YAML that we would normally apply to the cluster manually or via a script. However, when we use GitOps, we push the ArgoCD application that uses this YAML to GitHub, and it applies the ci namespace YAML to the cluster. This is the essence of GitOps; we declare what we want to appear in the cluster using Git and ArgoCD synchronizes the cluster with this declaration. Examine the ci rolebinding YAML Now that we've seen how the namespace was created, let's see how the three other rolebindings were created by the namespace-ci ArgoCD application. In the same namespace/ci folder as the ci namespace YAML, there is a rolebinding.yaml file. This file will be also applied to the cluster by the namespace-ci ArgoCD application which is continuously watching this folder. Examine this file with the following command: cat namespaces/ci/rolebinding.yaml This YAML is slightly more complex than the namespace YAML: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-dev namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:dev --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-staging namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:staging --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-prod namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:prod --- However its structure is quite straightforward; look carefully and you'll see that there are indeed three rolebindings defined in this YAML. Each of these rolebindings binds different roles to different service accounts associated with components that will run within the ci namespace, such as Tekton pipelines for example. We'll see later how these rolebindings are important; they limit the operations that can be performed by the ci namespace, creating a well governed cluster. This confirms why we saw four resources created by the namespace-ci ArgoCD application in the ArgoCD UI: one namespace and three rolebindings. Again, notice the pattern: a single ArgoCD application manages one or more Kubernetes resources in the cluster -- using one or more YAML files in which those resources are defined. The bootstrap-single-cluster ArgoCD application in more detail In the ArgoCD UI Applications view, click on the bootstrap-single-cluster application: You can see the bootstrap application creates two Kubernetes resources, the infra ArgoCD application and the infra ArgoCD project. An ArgoCD project is a mechanism by which we can group related resources; we keep all our ArgoCD applications that manage infrastructure in the infra project. Later, we'll create a services project for the ArgoCD applications that manage the services we want deployed to the cluster such as the CP4S application. The infra ArgoCD application Let's examine the infra ArgoCD application in more detail to see how it works. In the ArgoCD UI Applications view, click on the open application icon for the infra application: We can see that the infra ArgoCD application creates 9 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It's the infra ArgoCD application that watches the 0-bootstrap/single-cluster/1-infra/argocd folder for ArgoCD applications that apply infrastructure resources to our cluster. It was the infra application that created the namespace-ci ArgoCD application which manages the ci namespace that we've been exploring in this section of the tutorial. We'll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it's not necessary, as you'll get lots of practice as we proceed. ArgoCD change management and governance \u00b6 In this final section about managing infrastructure resources, let's explore how ArgoCD provides some advanced resource management features: Dynamic Monitored Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Customize the web console banner Examine the banner in the OpenShift web console: We're going to use GitOps to modify this banner dynamically. The banner YAML This banner properties are defined by the YAML in /0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml . This YAML is currently being used by the cntk-consolenotification ArgoCD application that was deployed earlier. We can examine the YAML with the following command: cat 0 -bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml which shows the banner properties are part of the ConsoleNotification custom resource: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : cntk-consolenotification labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" finalizers : - resources-finalizer.argocd.argoproj.io spec : syncPolicy : automated : prune : true selfHeal : true destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : path : consolenotification helm : values : | ocp-console-notification: ## The name of the ConsoleNotification resource in the cluster name: \"banner-env\" ## The background color that should be used for the banner backgroundColor: teal ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text: \"Cluster Description\" See how the banner at the top of the screen: * contains the text Cluster Description * is located at top of the screen * has the color Teal Modify the YAML for this banner Let's now change this YAML In your editor, modify this YAML and change the below fields as follows: ocp-console-notification : ## The name of the ConsoleNotification resource in the cluster name : \"banner-env\" ## The background color that should be used for the banner backgroundColor : red ## The color of the text that will appear in the banner color : \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location : BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text : \"Production reference guide\" It's clear that our intention is to modify the banner's backgroundColor:red and text: Production reference guide to the newly specified values. If you look at the diff : git diff you should see the following diff --git a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml index 30adf1a..596e821 100644 --- a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml +++ b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml @@ -26,10 +26,10 @@ spec: name: \"banner-env\" ## The background color that should be used for the banner - backgroundColor: teal + backgroundColor: red ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created - text: \"Cluster Description\" + text: \"Production reference guide\" Make the web console YAML change active Let's make these changes visible to the cntk-consolenotification ArgoCD application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Modify console banner\" git push origin $GIT_BRANCH You'll see the changes being pushed to GitHub: Enumerating objects: 13, done. Counting objects: 100% (13/13), done. Delta compression using up to 8 threads Compressing objects: 100% (7/7), done. Writing objects: 100% (7/7), 670 bytes | 670.00 KiB/s, done. Total 7 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a1e8292..b49dff5 master -> master Let's see what effect they have on the web console. A dynamic change to the web console You can either wait for ArgoCD to automatically sync the cntk-consolenotification application or manually Sync it yourself: Returning to the OpenShift web console, you'll notice changes. Notice the dynamic nature of these changes; we updated the console YAML, pushed our changes to our GitOps repository and everything else happened automatically. As a result, our OpenShift console has a new banner color and text. Governing changes to the dev namespace Let's now look at how ArgoCD monitors Kubernetes resources for configuration drift, and what happens if it detects an unexpected change to a monitored resource. Don't worry about the following command; it might seem drastic and even reckless, but as you'll see, everything will be OK. Let's delete the dev namespace from the cluster: oc get namespace dev oc delete namespace dev See how the active namespace: NAME STATUS AGE dev Active 2d18h is deleted: namespace \"dev\" deleted We can see that the dev namespace has been manually deleted from the cluster. GitOps repository as a source of truth If you switch back to the ArgoCD UI, you may see that ArgoCD has detected a configuration drift: a resource is Missing (the dev namespace) namespace-dev therefore is OutOfSync namespace-dev is therefore Syncing with the GitOps repository After a while we'll see that namespace-dev is Healthy and Synced : ArgoCD has detected a configuration drift, and resynched with GitOps repository, re-applying the dev namespace to the cluster. Note You may miss seeing the first screenshot if ArgoCD detects and corrects the missing dev namespace before you get a chance to switch to the ArgoCD UI. Don't worry, you can try this operator again! The restored dev namespace Issue the following command to determine the status of the dev namespace: oc get namespace dev which confirms that the dev namespace has been re-instated: NAME STATUS AGE dev Active 115s Note that it is a different instance of the dev namespace; as indicated by its AGE value. Notice the governed nature of these changes; GitOps is our source of truth about the resources deployed to the cluster. ArgoCD restores any resources that suffer from configuration drift to their GitOps-defined configuration. Congratulations! You've used ArgoCD and the GitOps repository to set up ci , tools and dev namespaces. You've seen how to create ArgoCD applications that watch their respective GitOps namespace folders for details of the namespace resources they should apply to the cluster. You've seen how you can dynamically change deployed resources by updating the resource definition in the GitOps repository. Finally, you've experience how ArgoCD keeps the cluster synchronized with the GitOps repository as a source of truth; any unexpected configuration drift will be corrected without intervention. In the next section of this tutorial, we're going to deploy some services into the infrastructure namespaces we've created in this topic. These services will include Artifactory and MQ operator.","title":"Add infrastructure"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-config/#cloud-native-mq-gitops-configuration","text":"In the cluster configuration of this chapter, we installed the fundamental components for continuous integration and continuous deployment into our cluster. These included a sample GitOps repository, and ArgoCD. In this section we're going to customize and enable the GitOps repository, so that we can install the components highlighted in our CP4S CICD process: We'll examine these components in more detail throughout this section of the tutorial; here's an initial overview of their function: ci-namespace provide an execution namespace for our pipelines tools-namespace provide an execution namespace for tools such as Artifactory cp4s-namespace provide an execution namespace for our deployed CP4S when running in the development environment. Later in the tutorial, we'll add staging and prod namespaces. ArgoCD applications manage the ci , tools and dev namespaces. You may have already noticed that the GitOps repository contains YAMLs that refer to the ArgoCD and namespaces in the diagram above. We're now going to use these YAMLs to configure the cluster resources using GitOps. Becoming comfortable with the concepts by practicing GitOps will help us in later chapters when we CP4S Cloud Pak In this topic, we're going to: Explore the sample GitOps repository in a little more detail Customize the GitOps repository for our cluster Connect ArgoCD to the customized GitOps repository Bootstrap the cluster Explore how the ci namespace is created Try out some dynamic changes to the cluster with ArgoCD Explore how ArgoCD manages configuration drift By the end of this topic we'll have a cluster up and running, having used GitOps to do it. We'll fully understand how ArgoCD manages cluster change and configuration drift.","title":"Cloud Native MQ GitOps Configuration"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-config/#pre-requisites","text":"Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed on your local machine the oc command that matches the version of your cluster. You have also installed npm , git and tree commands. You have customized the GitOps repository and ArgoCD. Please see the previous sections of this guide for information on how to do these tasks.","title":"Pre-requisites"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-config/#video-walkthrough","text":"This video demonstrates how to use ArgoCD and the GitOps repository to set up different infra related components. This is a video walkthrough and it takes you step by step through the below sections.","title":"Video Walkthrough"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-config/#the-sample-gitops-repository","text":"Let's understand how the GitOps repository works by using it to install the components we've highlighted in the above diagram. In the first part of this topic, let's look at the high level structure of the GitOps repository. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Explore the high level folder structure Use the following command to display the folder structure: tree . -d -L 2 We can see the different folders in the GitOps repository: . \u251c\u2500\u2500 0-bootstrap \u2502 \u251c\u2500\u2500 others \u2502 \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 doc \u2502 \u251c\u2500\u2500 diagrams \u2502 \u251c\u2500\u2500 experimental \u2502 \u251c\u2500\u2500 images \u2502 \u2514\u2500\u2500 scenarios \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 bom \u2514\u2500\u2500 setup \u2514\u2500\u2500 ocp47 The 0-bootstrap folder is the key folder that contains all the available profiles. Within this folders are Cluster Profiles that control which resources are deployed to the cluster. There are other folders, but these contain utility scripts; and some document related information. Let us examine the 0-bootstrap folder structure: tree ./0-bootstrap/ -d -L 2 We can see the different folders: ./0-bootstrap/ \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u2514\u2500\u2500 3-apps If we consider single-cluster profile, you can see the three layers under that covering infrastructure, services, and applications. Use single-cluster profile to configure the cluster Use the following command to display the folder structure: tree ./0-bootstrap/single-cluster/ -L 3 We can see the different folders: ./0-bootstrap/single-cluster/ \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 1-infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 2-services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3-apps \u2502 \u251c\u2500\u2500 3-apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 cp4a \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Notice that each high level folder has an argocd folder. Within these argocd folders are ArgoCD applications that control which resources are deployed to the cluster. Sample ArgoCD application We'll see later how these ArgoCD applications work in detail. For now, notice how each layer of the architecture ( infrastructure , service , application ) has a set of ArgoCD applications. Type the following command: tree 0 -bootstrap/single-cluster/1-infra/ It shows a list of ArgoCD applications that are used to manage Kubernetes infrastructure resources: 0-bootstrap/single-cluster/1-infra/ \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Notice the namespace- YAMLs; we'll see in a moment how these each define an ArgoCD application dedicated to managing a Kubernetes namespace in our reference architecture. A word on terminology As we get started with ArgoCD, it can be easy to confuse the term ArgoCD application with your application. That's because ArgoCD uses the term application to refer to a Kubernetes custom resource that was initially designed to automate application management. However, an ArgoCD application can automate the deployment of any Kubernetes resource within a cluster, such as namespaces or queue managers, as we'll see a little later. Let's now customize this repository to deploy the highlighted Kubernetes resources to our cluster.","title":"The sample GitOps repository"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-config/#customizing-the-gitops-repositories","text":"The cloned GitOps configuration repositories (specifically, multi-tenancy-gitops and multi-tenancy-gitops-apps ) needs to be customized for your cluster. The ArgoCD Application and AppProject resources and kustomization.yaml resources contains environment variables which will need to be updated to your specific GitHub organization and branch.","title":"Customizing the GitOps repositories"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-config/#update-multi-tenancy-gitops-repository","text":"Have a look at the following kustomization.yaml as an example of what will need to be updated. cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS_INFRA} - op: add path: /spec/source/targetRevision value: ${GIT_GITOPS_INFRA_BRANCH} Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Run the customization script Let's customize the cloned multi-tenancy-gitops repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. This script has also made some customizations that we'll use much later in the tutorial; we'll refer to those at the relevant time. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 199 files changed, 290 insertions ( + ) , 290 deletions ( - ) Set your GitOps repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (48/48), 4.70 KiB | 1.57 MiB/s, done. Total 48 (delta 32), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 23 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git d95eca5..a900c39 master -> master","title":"Update multi-tenancy-gitops repository"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-config/#update-multi-tenancy-gitops-apps-repository","text":"Similar to the previous section, there are corresponding ArgoCD Applications resources that will need to be updated in the multi-tenancy-gitops-apps repository. Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Locate the multi-tenancy-gitops-apps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops-apps Run the customization script Let's customize the cloned multi-tenancy-gitops-apps repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops-apps repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 21 files changed, 42 insertions ( + ) , 42 deletions ( - ) Set your repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops-apps repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (42/42), 4.70 KiB | 1.57 MiB/s, done. Total 42 (delta 42), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 21 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git d95eca5..a900c39 master -> master","title":"Update multi-tenancy-gitops-apps repository"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-config/#connect-argocd-to-the-gitops-repository","text":"Let's now connect your customized GitOps repository to the instance of ArgoCD running in the cluster. Once connected, ArgoCD will use the contents of the repository to create necessary resources. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Review ArgoCD infrastructure folder Let's examine the 0-bootstrap/single-cluster/1-infra/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml We can see the contents of the kustomization.yaml : resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Notice that the resources that needs to be applied to the cluster are all inactivate and commented out. Let us enable the resources that are needed by un-commenting them. Deploy Kubernetes resources with kustomization.yaml Open 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the below resources: argocd/namespace-ci.yaml argocd/consolenotification.yaml argocd/namespace-ibm-common-services.yaml argocd/namespace-tools.yaml You will have the following resources un-commented for infrastructure: resources : #- argocd/consolelink.yaml - argocd/consolenotification.yaml - argocd/namespace-ibm-common-services.yaml - argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml - argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for infrastructure\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a900c39..e3f696d master -> master Examine bootstrap.yaml residing in 0-bootstrap/single-cluster/ . The bootstrap.yaml file is used to create our first ArgoCD application called bootstrap-single-cluster . This initial ArgoCD application will create all the other ArgoCD applications that control the application, service, and infrastructure resources (such as the ci and dev namespaces) deployed to the cluster. Examine the YAML that defines the ArgoCD bootstrap application: cat 0 -bootstrap/single-cluster/bootstrap.yaml Notice also how this ArgoCD application has been customized to use the GitOps repository repoURL: https://github.com/prod-ref-guide/multi-tenancy-gitops.git . apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-single-cluster namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/single-cluster repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Most importantly, see how path: 0-bootstrap/single-cluster refers to the 0-bootstrap/single-cluster folder within this repository. This will result in the creation of individual ArgoCD applications to manage our cluster resources. Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us for now only deploy infra resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and comment out the 2-services/2-services.yaml and 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml # - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Commit and push changes to your git repository: git add . git commit -s -m \"Using only infra\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 456 bytes | 456.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git e3f696d..ea3b43f master -> master Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. The repository contains a bootstrap-single-cluster ArgoCD application that is watching this repository and using its contents to manage the cluster. When the bootstrap-single-cluster ArgoCD application is applied to the cluster, it will continuously ensure that all the activated resources are applied to the cluster. Apply the bootstrap YAML to the cluster: oc apply -f 0 -bootstrap/single-cluster/bootstrap.yaml Kubernetes will confirm that the bootstrap ArgoCD application has been created: application.argoproj.io/bootstrap-single-cluster created The bootstrap ArgoCD application will watch the 0-bootstrap/single-cluster folder in our GitOps repository on GitHub. In this way, as resources are added to the infrastructure , service and application folders, they will be deployed to the cluster automatically. This is therefore the only direct cluster operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Verify the bootstrap deployment Verify that the bootstrap ArgoCD application is running with the following command: oc get app/bootstrap-single-cluster -n openshift-gitops You should see that the bootstrap application was recently updated: NAME SYNC STATUS HEALTH STATUS bootstrap-single-cluster Synced Healthy HEALTH_STATUS may temporarily show Missing ; simply re-issue the command to confirm it moves to Healthy . Using the UI to view the newly deployed ArgoCD applications In the previous section of this chapter you logged on to the ArgoCD web console. Switch back to that console, refresh the page and you should see the bootstrap-single-cluster ArgoCD application together with many other ArgoCD applications: (You may need to select List view rather than the Tiles view.) We can see that eleven ArgoCD applications have been deployed to the cluster as a result of applying bootstrap.yaml . In the next section of the tutorial, we'll examine these applications to see how and why they were created, but for now let's focus on one of them -- the namespace-ci ArgoCD application. Examining the namespace-ci ArgoCD application resources Let's examine the Kubernetes resources applied to the cluster by the namespace-ci ArgoCD application. In the ArgoCD application list, click on namespace-ci : (You may need to clear filters to see this screenshot.) The directed graph shows that the namespace-ci ArgoCD app has created 4 Kubernetes resources; our ci namespace and three role bindings. Verify the namespace using the oc CLI We've seen the new namespace definition in the GitOps repository and visually in the ArgoCD UI. Let's also verify it via the command line: Type the following command: oc get namespace ci -o yaml This will list the full details of the ci namespace: apiVersion : v1 kind : Namespace metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"namespace-ci\"},\"name\":\"ci\"},\"spec\":{}} openshift.io/sa.scc.mcs : s0:c27,c9 openshift.io/sa.scc.supplemental-groups : 1000720000/10000 openshift.io/sa.scc.uid-range : 1000720000/10000 creationTimestamp : \"2021-08-31T15:27:32Z\" labels : app.kubernetes.io/instance : namespace-ci managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:labels : . : {} f:app.kubernetes.io/instance : {} f:status : f:phase : {} manager : argocd-application-controller operation : Update time : \"2021-08-31T15:27:32Z\" - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : f:openshift.io/sa.scc.mcs : {} f:openshift.io/sa.scc.supplemental-groups : {} f:openshift.io/sa.scc.uid-range : {} manager : cluster-policy-controller operation : Update time : \"2021-08-31T15:27:32Z\" name : ci resourceVersion : \"2255607\" selfLink : /api/v1/namespaces/ci uid : fff6b82b-6318-4828-83bb-ade4e8e3c0cf spec : finalizers : - kubernetes status : phase : Active Notice how manager: argocd-application-controller identifies that this namespace was created by ArgoCD. It's important to understand the sequence of actions. We simply deployed the bootstrap-single-cluster ArgoCD application, and it ultimately resulted in the creation of the namespace-ci ArgoCD application which created the ci namespace. We don't directly apply resources to the cluster once the bootstrap-single-cluster ArgoCD application has been applied; the cluster state is determined by an ArgoCD application YAML in the corresponding application , service or infrastructure related folders. It's these ArgoCD applications that create and manage the underlying Kubernetes resources using the GitOps repository as the source of truth. Understanding the namespace-ci ArgoCD application Let's examine the ArgoCD application namespace-ci to see how it created the ci namespace and three role bindings in the cluster. Issue the following command to examine its YAML: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-ci.yaml Notice that apiVersion and kind identify this as an ArgoCD application: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ci labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" spec : destination : namespace : ci server : https://kubernetes.default.svc project : infra source : path : namespaces/ci syncPolicy : automated : prune : true selfHeal : true Most importantly, see how this namespace-ci ArgoCD application monitors the folder path: namespaces/ci in https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git ; it then applies the contents of this folder to the cluster whenever its content changes. Notice the SyncPolicy of automated ; any changes to this folder will automatically be applied to the cluster; we do not need to perform a manual Sync operation from the ArgoCD UI. Examine the ci namespace YAML To examine this, navigate to multi-tenancy-gitops-infra repository you cloned previously. cd $HOME /git cd multi-tenancy-gitops-infra Let's examine the ci namespace YAML in the namespaces/ci folder: cat namespaces/ci/namespace.yaml It's a very simple YAML: apiVersion : v1 kind : Namespace metadata : name : ci spec : {} This is a YAML that we would normally apply to the cluster manually or via a script. However, when we use GitOps, we push the ArgoCD application that uses this YAML to GitHub, and it applies the ci namespace YAML to the cluster. This is the essence of GitOps; we declare what we want to appear in the cluster using Git and ArgoCD synchronizes the cluster with this declaration. Examine the ci rolebinding YAML Now that we've seen how the namespace was created, let's see how the three other rolebindings were created by the namespace-ci ArgoCD application. In the same namespace/ci folder as the ci namespace YAML, there is a rolebinding.yaml file. This file will be also applied to the cluster by the namespace-ci ArgoCD application which is continuously watching this folder. Examine this file with the following command: cat namespaces/ci/rolebinding.yaml This YAML is slightly more complex than the namespace YAML: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-dev namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:dev --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-staging namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:staging --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-prod namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:prod --- However its structure is quite straightforward; look carefully and you'll see that there are indeed three rolebindings defined in this YAML. Each of these rolebindings binds different roles to different service accounts associated with components that will run within the ci namespace, such as Tekton pipelines for example. We'll see later how these rolebindings are important; they limit the operations that can be performed by the ci namespace, creating a well governed cluster. This confirms why we saw four resources created by the namespace-ci ArgoCD application in the ArgoCD UI: one namespace and three rolebindings. Again, notice the pattern: a single ArgoCD application manages one or more Kubernetes resources in the cluster -- using one or more YAML files in which those resources are defined. The bootstrap-single-cluster ArgoCD application in more detail In the ArgoCD UI Applications view, click on the bootstrap-single-cluster application: You can see the bootstrap application creates two Kubernetes resources, the infra ArgoCD application and the infra ArgoCD project. An ArgoCD project is a mechanism by which we can group related resources; we keep all our ArgoCD applications that manage infrastructure in the infra project. Later, we'll create a services project for the ArgoCD applications that manage the services we want deployed to the cluster such as the CP4S application. The infra ArgoCD application Let's examine the infra ArgoCD application in more detail to see how it works. In the ArgoCD UI Applications view, click on the open application icon for the infra application: We can see that the infra ArgoCD application creates 9 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It's the infra ArgoCD application that watches the 0-bootstrap/single-cluster/1-infra/argocd folder for ArgoCD applications that apply infrastructure resources to our cluster. It was the infra application that created the namespace-ci ArgoCD application which manages the ci namespace that we've been exploring in this section of the tutorial. We'll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it's not necessary, as you'll get lots of practice as we proceed.","title":"Connect ArgoCD to the GitOps repository"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-config/#argocd-change-management-and-governance","text":"In this final section about managing infrastructure resources, let's explore how ArgoCD provides some advanced resource management features: Dynamic Monitored Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Customize the web console banner Examine the banner in the OpenShift web console: We're going to use GitOps to modify this banner dynamically. The banner YAML This banner properties are defined by the YAML in /0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml . This YAML is currently being used by the cntk-consolenotification ArgoCD application that was deployed earlier. We can examine the YAML with the following command: cat 0 -bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml which shows the banner properties are part of the ConsoleNotification custom resource: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : cntk-consolenotification labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" finalizers : - resources-finalizer.argocd.argoproj.io spec : syncPolicy : automated : prune : true selfHeal : true destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : path : consolenotification helm : values : | ocp-console-notification: ## The name of the ConsoleNotification resource in the cluster name: \"banner-env\" ## The background color that should be used for the banner backgroundColor: teal ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text: \"Cluster Description\" See how the banner at the top of the screen: * contains the text Cluster Description * is located at top of the screen * has the color Teal Modify the YAML for this banner Let's now change this YAML In your editor, modify this YAML and change the below fields as follows: ocp-console-notification : ## The name of the ConsoleNotification resource in the cluster name : \"banner-env\" ## The background color that should be used for the banner backgroundColor : red ## The color of the text that will appear in the banner color : \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location : BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text : \"Production reference guide\" It's clear that our intention is to modify the banner's backgroundColor:red and text: Production reference guide to the newly specified values. If you look at the diff : git diff you should see the following diff --git a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml index 30adf1a..596e821 100644 --- a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml +++ b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml @@ -26,10 +26,10 @@ spec: name: \"banner-env\" ## The background color that should be used for the banner - backgroundColor: teal + backgroundColor: red ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created - text: \"Cluster Description\" + text: \"Production reference guide\" Make the web console YAML change active Let's make these changes visible to the cntk-consolenotification ArgoCD application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Modify console banner\" git push origin $GIT_BRANCH You'll see the changes being pushed to GitHub: Enumerating objects: 13, done. Counting objects: 100% (13/13), done. Delta compression using up to 8 threads Compressing objects: 100% (7/7), done. Writing objects: 100% (7/7), 670 bytes | 670.00 KiB/s, done. Total 7 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a1e8292..b49dff5 master -> master Let's see what effect they have on the web console. A dynamic change to the web console You can either wait for ArgoCD to automatically sync the cntk-consolenotification application or manually Sync it yourself: Returning to the OpenShift web console, you'll notice changes. Notice the dynamic nature of these changes; we updated the console YAML, pushed our changes to our GitOps repository and everything else happened automatically. As a result, our OpenShift console has a new banner color and text. Governing changes to the dev namespace Let's now look at how ArgoCD monitors Kubernetes resources for configuration drift, and what happens if it detects an unexpected change to a monitored resource. Don't worry about the following command; it might seem drastic and even reckless, but as you'll see, everything will be OK. Let's delete the dev namespace from the cluster: oc get namespace dev oc delete namespace dev See how the active namespace: NAME STATUS AGE dev Active 2d18h is deleted: namespace \"dev\" deleted We can see that the dev namespace has been manually deleted from the cluster. GitOps repository as a source of truth If you switch back to the ArgoCD UI, you may see that ArgoCD has detected a configuration drift: a resource is Missing (the dev namespace) namespace-dev therefore is OutOfSync namespace-dev is therefore Syncing with the GitOps repository After a while we'll see that namespace-dev is Healthy and Synced : ArgoCD has detected a configuration drift, and resynched with GitOps repository, re-applying the dev namespace to the cluster. Note You may miss seeing the first screenshot if ArgoCD detects and corrects the missing dev namespace before you get a chance to switch to the ArgoCD UI. Don't worry, you can try this operator again! The restored dev namespace Issue the following command to determine the status of the dev namespace: oc get namespace dev which confirms that the dev namespace has been re-instated: NAME STATUS AGE dev Active 115s Note that it is a different instance of the dev namespace; as indicated by its AGE value. Notice the governed nature of these changes; GitOps is our source of truth about the resources deployed to the cluster. ArgoCD restores any resources that suffer from configuration drift to their GitOps-defined configuration. Congratulations! You've used ArgoCD and the GitOps repository to set up ci , tools and dev namespaces. You've seen how to create ArgoCD applications that watch their respective GitOps namespace folders for details of the namespace resources they should apply to the cluster. You've seen how you can dynamically change deployed resources by updating the resource definition in the GitOps repository. Finally, you've experience how ArgoCD keeps the cluster synchronized with the GitOps repository as a source of truth; any unexpected configuration drift will be corrected without intervention. In the next section of this tutorial, we're going to deploy some services into the infrastructure namespaces we've created in this topic. These services will include Artifactory and MQ operator.","title":"ArgoCD change management and governance"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-tekton-argocd/","text":"Configuring the cluster for GitOps \u00b6 Audience : Architects, Application developers, Administrators Overview \u00b6 In this topic, we're going to: Create a new GitHub organization for our tutorial Copy the sample repositories for this tutorial to our new organization Briefly review these repositories Install ArgoCD to help us set up the cluster Customize ArgoCD Login in to the ArgoCD UI By the end of this topic we'll have a all the basic components in place to perform GitOps in our cluster. Introduction \u00b6 Continuous integration and continuous deployment (CICD) are at the core of our CP4S deployment . CICD ensures that any changes to source applications and queue managers are automatically built and tested before they are deployed, helping to ensure their correctness and the integrity of the cluster. CP4S Cloud Pak are defined, configured and changed using a GitOps model. GitOps puts git repositories and git commands such as git push (to request a change) and git merge (to approve a change) at the heart of configuration management. A GitOps approach helps an organization implement best practices in version control and release governance based on a widely used open standard -- git . The following diagram outlines the major components in a GitOps CP4S CICD process(Missing Image): Notice the clear separation of concerns: Tekton pipelines (OpenShift Pipelines) use MQ application and queue manager source repositories to build and store successfully tested Kubernetes artifacts in a Git config repository, image registry and Artifactory. While Kubernetes resources (e.g. pods, routes...) can be created as part of the pipeline run to test the source change, these resources only last for the duration of the pipeline run. It is the resultant Git, container image registry and Artifactory resources that are used to affect changes to the cluster in a subsequent, asynchronous processing step controlled by ArgoCD. ArgoCD applications (OpenShift GitOps) watch a Git config repository for changes built as a result of successful pipeline runs. This repository identifies the latest version of the application or queue manager using YAMLs stored in Git which reference the image repository and Artifactory. ArgoCD applies recently updated Kubernetes resources to the cluster, resulting in new or updated Kubernetes resources that represent the changed MQ applications and queue managers, such as pods, routes etc. In contrast to pipeline runs, ArgoCD changes are durable; they remain as defined unless and until they are explicitly changed or deleted in the GitOps repository. Moreover, if the cluster resources drift from their repository-defined values, ArgoCD will restore them to these values; only changes that are applied to the Git config repository affect the long term state of the cluster. The Git configuration repository , often referred to as the GitOps repository , is used to store the MQ application and queue manager YAML artifacts built by Tekton and watched by ArgoCD. We think of this repository as a transfer point between Continuous Integration and Continuous Deployment; the successful built and tested YAMLs are stored in the Git configuration repository by a Tekton pipeline, which is continuously being watched by an ArgoCD application which deploys the latest YAML which references information in Artifactory and the image registry. Often, a Tekton pipeline will perform its changes to a GitOps repository under a pull-request (PR) to provide an explicit approval mechanism for cluster changes. ArgoCD will only see the changes once a PR is merged, providing the formal sign-off which is often so important in higher environments such as production. In contrast, lower environments such as development, are often committed into directly and therefore immediately seen by ArgoCD and applied to the cluster. It's worth noting that although we show a single GitOps repository, there are in fact multiple repositories -- each corresponding to a different architectural layer in our cluster such as infrastructure , services and applications . As we'll see, we don't just use the GitOps repository to deploy MQ applications and queue managers, but every component in the cluster. When we look at our target architecture , every single component (Cert manager, SonarQube, JMeter...) will be deployed to the cluster using ArgoCD and a set of YAMLs from the appropriate specific GitOps repository. In this section of the tutorial, we're going to set up the GitOps repository, and install ArgoCD. Later, we'll customize our GitOps config repositories, and them in conjunction with ArgoCD it to install the rest of the components in the cluster, including MQ applications and queue managers. Pre-requisites \u00b6 Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed the oc command on your local machine. Its version must be compatible with the version of your cluster. Use these instructions to get the latest version of oc . Use oc version to confirm that you have Client Version: 4.6 or higher Server Version: 4.7 or higher You have also installed npm , git , tree and jq commands. See these instructions about how install these prerequisites. Video Walkthrough \u00b6 This video demonstrates how to create a Github Organization and set up all the necessary repositories for this tutorial. It also goes through how to install ArgoCD. This is a video walkthrough and it takes you step by step through the below sections. Downloading the sample GitOps repository \u00b6 In this topic section, we're going to create the GitOps repository for our cluster. We'll use a sample GitOps repository as a starting point; we'll make our own copy and review its contents. Later in the tutorial, we'll customize it for our organization and cluster. ( If you'd like to re-familiarize yourself with git, GitHub or managing repositories, you'll find the following short video and short read helpful. ) Create a new GitHub organization Let's start by creating our own GitHub organization. In your browser, navigate to https://github.com/ . In the upper-right corner, click on the profile photo, and then click Settings . In user settings sidebar, click Organizations . In the \"Organizations\" section, click New organization. Create a new organization. Create the GitOps repositories in the new GitHub organization Let's start by making our own copy of the GitOps repository. In your browser, navigate to https://github.com/cloud-native-toolkit/multi-tenancy-gitops : Click on Use this template at the top RHS of the screen and you'll be asked where you'd like to use this repository: Select your organization, name the repository and click Create repository from template . After a few seconds the repository will be present in your chosen account: You now have a copy of the sample GitOps repository in your GitHub account. Create the infrastructure, services and applications GitOps repositories from the templates into the organization your GitHub organization As shown above, follow similar steps and copy the the repository templates into your organization. GitOps Infrastructure Repository GitOps Services Repository GitOps Application Repository Your organization now will have the below repositories in it. Set up environment variable for GitHub Organization This tutorial uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Replace <git-organization> in the following command: export GIT_ORG = <git-organization> You can verify your $GIT_ORG as follows: echo $GIT_ORG Change to your working git folder Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . We've used this default in the following command, but you may have a different location on your local machine. Change to your working git folder: cd $HOME /git You may need to create this folder if it's not already created. Clone the GitOps repository We're going to work on a local clone of the four repositories. Clone the GitOps repositories to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops'... remote: Enumerating objects: 132, done. remote: Counting objects: 100% (132/132), done. remote: Compressing objects: 100% (54/54), done. remote: Total 132 (delta 72), reused 118 (delta 67), pack-reused 0 Receiving objects: 100% (132/132), 310.92 KiB | 3.24 MiB/s, done. Resolving deltas: 100% (72/72), done. Clone the GitOps Infra repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-infra.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-infra'... remote: Enumerating objects: 62, done. remote: Counting objects: 100% (62/62), done. remote: Compressing objects: 100% (44/44), done. remote: Total 62 (delta 7), reused 53 (delta 3), pack-reused 0 Unpacking objects: 100% (62/62), done. Clone the GitOps Services repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-services.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-services'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Clone the GitOps Applications repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-apps'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Info Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the folder containing the local copy of your new repository: cd multi-tenancy-gitops Review the GitOps folder structure Show the folder structure with the following command: tree . -L 1 This will list all the top level folders and files: . \u251c\u2500\u2500 0-bootstrap \u251c\u2500\u2500 README.md \u251c\u2500\u2500 doc \u251c\u2500\u2500 scripts \u2514\u2500\u2500 setup The main folders perform the following roles: 0-bootstrap contains different profiles that can be used to initialize the cluster ready. We can choose one of the available profiles. scripts and setup contain useful scripts that we'll use during this tutorial. Feel free to explore this structure. We'll examine it in much more detail throughout this tutorial, and you'll become very familiar with it by trying it out. Installing ArgoCD for GitOps \u00b6 Now that we've had an initial view of the GitOps repository, let's install ArgoCD to make use of it. We will install ArgoCD using the Red Hat OpenShift GitOps operator. We will also configure it with the appropriate access rights to limit the resources it can create in the cluster. In so doing, we ensure that ArgoCD acts like a well-governed administrator of the cluster, only doing what it needs to do. Also, we will customize our ArgoCD instance by adding necessary custom checks. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". On successful login, you'll see: Logged into \"https://c100-e.jp-tok.containers.cloud.ibm.com:30749\" as \"IAM#hperabat@in.ibm.com\" using the token provided. You have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\" . This shows some basic information about your user within this cluster. (This user is different to your GitHub user.) Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. The following command creates the subscription: oc apply -f setup/ocp47/ The response confirms that the below resources has been created: clusterrole.rbac.authorization.k8s.io/custom-argocd-cluster-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-cntk-argocd-application-controller created subscription.operators.coreos.com/openshift-gitops-operator created Wait for the ArgoCD installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/gitops-operator-controller-manager -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/gitops-operator condition met ArgoCD is now installed and ready to use. If you see something like: Error from server (NotFound): deployments.apps \"gitops-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Alternatively, you can also validate as follows: while ! oc wait crd applications.argoproj.io --timeout = -1s --for = condition = Established 2 >/dev/null ; do sleep 30 ; done while ! oc wait pod --timeout = -1s --for = condition = Ready -l '!job-name' -n openshift-gitops > /dev/null ; do sleep 30 ; done ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are also setup along with the subscription. oc get clusterrole custom-argocd-cluster-argocd-application-controller oc get clusterrolebinding openshift-gitops-argocd-application-controller oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller You'll see the resources are created in the cluster: $ oc get clusterrole custom-argocd-cluster-argocd-application-controller NAME CREATED AT custom-argocd-cluster-argocd-application-controller 2021-08-27T13:35:13Z $ oc get clusterrolebinding openshift-gitops-argocd-application-controller NAME ROLE AGE openshift-gitops-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m43s $ oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller NAME ROLE AGE openshift-gitops-cntk-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m45s The clusterrole custom-argocd-cluster-argocd-application-controller defines a specific set of specific resources that are required by the ArgoCD service account and access rights over them. The cluster role bindings openshift-gitops-argocd-application-controller and openshift-gitops-cntk-argocd-application-controller binds the ArgoCD service account to the cluster role above. In combination, these definitions limit ArgoCD to perform the minimum set of operations required to manage the cluster. This is important; it means that ArgoCD behaves as a well governed administrator of the cluster. We'll also see later in the tutorial how ArgoCD creates resource in the cluster. Delete default ArgoCD instance Let us now delete the default ArgoCD instance that is created earlier. oc delete gitopsservice cluster -n openshift-gitops || true You will see the resources being deleted as follows: warning: deleting cluster-scoped resources, not scoped to the provided namespace gitopsservice.pipelines.openshift.io \"cluster\" deleted We are deleting the existing default instance and planning to make use of a customized ArgoCD instance with additional capabilities. Create a custom ArgoCD instance This customized ArgoCD instance implements custom checks. By default ArgoCD provides some built-in health checks for validating standard kubernetes resources. However, these checks will not be sufficient to validate the custom resources that belong to IBM Cloud Pak for Integration. Argo CD supports custom health checks that are defined in Lua . If you want explore more about the ArgoCD Resource Health, check this link out. Let us create a custom ArgoCD instance as follows: oc apply -f setup/ocp47/argocd-instance/ -n openshift-gitops The response confirms that the below resources has been created: argocd.argoproj.io/openshift-gitops-cntk created Wait till the ArgoCD instance pod is up and running. Issue the below command to verify the same: while ! oc wait pod --timeout = -1s --for = condition = ContainersReady -l app.kubernetes.io/name = openshift-gitops-cntk-server -n openshift-gitops > /dev/null ; do sleep 30 ; done Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops | grep openshift-gitops-cntk-server | awk '{print \"https://\"$2}' This will list the route to the ArgoCD we've just installed, for example: https://openshift-gitops-cntk-server-openshift-gitops.ibmcloud-roks-xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/openshift-gitops-cntk-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Once the UI launches, you'll see: See how there are no ArgoCD applications active at the moment. In the next section of the tutorial, we'll configure ArgoCD to create the ArgoCD applications that will in turn spin up infrastructure , service , and application resources to apply to the cluster. Congratulations! You've created the GitOps repository for your cluster based on the sample MQ repository and examined its high level structure. You also installed ArgoCD. You created a specific clusterrole and clusterrolebinding for the ArgoCD service account to ensure that it manages the cluster in a well governed manner. Finally, you launched the UI for ArgoCD; you'll make extensive use of it during this tutorial. In the next topic of this chapter, we're going to customize the GitOps repository for your cluster and use Tekton and ArgoCD to create and manage the Kubernetes resources for our MQ applications and queue managers.","title":"Set up GitOps"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-tekton-argocd/#configuring-the-cluster-for-gitops","text":"Audience : Architects, Application developers, Administrators","title":"Configuring the cluster for GitOps"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-tekton-argocd/#overview","text":"In this topic, we're going to: Create a new GitHub organization for our tutorial Copy the sample repositories for this tutorial to our new organization Briefly review these repositories Install ArgoCD to help us set up the cluster Customize ArgoCD Login in to the ArgoCD UI By the end of this topic we'll have a all the basic components in place to perform GitOps in our cluster.","title":"Overview"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-tekton-argocd/#introduction","text":"Continuous integration and continuous deployment (CICD) are at the core of our CP4S deployment . CICD ensures that any changes to source applications and queue managers are automatically built and tested before they are deployed, helping to ensure their correctness and the integrity of the cluster. CP4S Cloud Pak are defined, configured and changed using a GitOps model. GitOps puts git repositories and git commands such as git push (to request a change) and git merge (to approve a change) at the heart of configuration management. A GitOps approach helps an organization implement best practices in version control and release governance based on a widely used open standard -- git . The following diagram outlines the major components in a GitOps CP4S CICD process(Missing Image): Notice the clear separation of concerns: Tekton pipelines (OpenShift Pipelines) use MQ application and queue manager source repositories to build and store successfully tested Kubernetes artifacts in a Git config repository, image registry and Artifactory. While Kubernetes resources (e.g. pods, routes...) can be created as part of the pipeline run to test the source change, these resources only last for the duration of the pipeline run. It is the resultant Git, container image registry and Artifactory resources that are used to affect changes to the cluster in a subsequent, asynchronous processing step controlled by ArgoCD. ArgoCD applications (OpenShift GitOps) watch a Git config repository for changes built as a result of successful pipeline runs. This repository identifies the latest version of the application or queue manager using YAMLs stored in Git which reference the image repository and Artifactory. ArgoCD applies recently updated Kubernetes resources to the cluster, resulting in new or updated Kubernetes resources that represent the changed MQ applications and queue managers, such as pods, routes etc. In contrast to pipeline runs, ArgoCD changes are durable; they remain as defined unless and until they are explicitly changed or deleted in the GitOps repository. Moreover, if the cluster resources drift from their repository-defined values, ArgoCD will restore them to these values; only changes that are applied to the Git config repository affect the long term state of the cluster. The Git configuration repository , often referred to as the GitOps repository , is used to store the MQ application and queue manager YAML artifacts built by Tekton and watched by ArgoCD. We think of this repository as a transfer point between Continuous Integration and Continuous Deployment; the successful built and tested YAMLs are stored in the Git configuration repository by a Tekton pipeline, which is continuously being watched by an ArgoCD application which deploys the latest YAML which references information in Artifactory and the image registry. Often, a Tekton pipeline will perform its changes to a GitOps repository under a pull-request (PR) to provide an explicit approval mechanism for cluster changes. ArgoCD will only see the changes once a PR is merged, providing the formal sign-off which is often so important in higher environments such as production. In contrast, lower environments such as development, are often committed into directly and therefore immediately seen by ArgoCD and applied to the cluster. It's worth noting that although we show a single GitOps repository, there are in fact multiple repositories -- each corresponding to a different architectural layer in our cluster such as infrastructure , services and applications . As we'll see, we don't just use the GitOps repository to deploy MQ applications and queue managers, but every component in the cluster. When we look at our target architecture , every single component (Cert manager, SonarQube, JMeter...) will be deployed to the cluster using ArgoCD and a set of YAMLs from the appropriate specific GitOps repository. In this section of the tutorial, we're going to set up the GitOps repository, and install ArgoCD. Later, we'll customize our GitOps config repositories, and them in conjunction with ArgoCD it to install the rest of the components in the cluster, including MQ applications and queue managers.","title":"Introduction"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-tekton-argocd/#pre-requisites","text":"Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed the oc command on your local machine. Its version must be compatible with the version of your cluster. Use these instructions to get the latest version of oc . Use oc version to confirm that you have Client Version: 4.6 or higher Server Version: 4.7 or higher You have also installed npm , git , tree and jq commands. See these instructions about how install these prerequisites.","title":"Pre-requisites"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-tekton-argocd/#video-walkthrough","text":"This video demonstrates how to create a Github Organization and set up all the necessary repositories for this tutorial. It also goes through how to install ArgoCD. This is a video walkthrough and it takes you step by step through the below sections.","title":"Video Walkthrough"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-tekton-argocd/#downloading-the-sample-gitops-repository","text":"In this topic section, we're going to create the GitOps repository for our cluster. We'll use a sample GitOps repository as a starting point; we'll make our own copy and review its contents. Later in the tutorial, we'll customize it for our organization and cluster. ( If you'd like to re-familiarize yourself with git, GitHub or managing repositories, you'll find the following short video and short read helpful. ) Create a new GitHub organization Let's start by creating our own GitHub organization. In your browser, navigate to https://github.com/ . In the upper-right corner, click on the profile photo, and then click Settings . In user settings sidebar, click Organizations . In the \"Organizations\" section, click New organization. Create a new organization. Create the GitOps repositories in the new GitHub organization Let's start by making our own copy of the GitOps repository. In your browser, navigate to https://github.com/cloud-native-toolkit/multi-tenancy-gitops : Click on Use this template at the top RHS of the screen and you'll be asked where you'd like to use this repository: Select your organization, name the repository and click Create repository from template . After a few seconds the repository will be present in your chosen account: You now have a copy of the sample GitOps repository in your GitHub account. Create the infrastructure, services and applications GitOps repositories from the templates into the organization your GitHub organization As shown above, follow similar steps and copy the the repository templates into your organization. GitOps Infrastructure Repository GitOps Services Repository GitOps Application Repository Your organization now will have the below repositories in it. Set up environment variable for GitHub Organization This tutorial uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Replace <git-organization> in the following command: export GIT_ORG = <git-organization> You can verify your $GIT_ORG as follows: echo $GIT_ORG Change to your working git folder Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . We've used this default in the following command, but you may have a different location on your local machine. Change to your working git folder: cd $HOME /git You may need to create this folder if it's not already created. Clone the GitOps repository We're going to work on a local clone of the four repositories. Clone the GitOps repositories to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops'... remote: Enumerating objects: 132, done. remote: Counting objects: 100% (132/132), done. remote: Compressing objects: 100% (54/54), done. remote: Total 132 (delta 72), reused 118 (delta 67), pack-reused 0 Receiving objects: 100% (132/132), 310.92 KiB | 3.24 MiB/s, done. Resolving deltas: 100% (72/72), done. Clone the GitOps Infra repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-infra.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-infra'... remote: Enumerating objects: 62, done. remote: Counting objects: 100% (62/62), done. remote: Compressing objects: 100% (44/44), done. remote: Total 62 (delta 7), reused 53 (delta 3), pack-reused 0 Unpacking objects: 100% (62/62), done. Clone the GitOps Services repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-services.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-services'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Clone the GitOps Applications repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-apps'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Info Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the folder containing the local copy of your new repository: cd multi-tenancy-gitops Review the GitOps folder structure Show the folder structure with the following command: tree . -L 1 This will list all the top level folders and files: . \u251c\u2500\u2500 0-bootstrap \u251c\u2500\u2500 README.md \u251c\u2500\u2500 doc \u251c\u2500\u2500 scripts \u2514\u2500\u2500 setup The main folders perform the following roles: 0-bootstrap contains different profiles that can be used to initialize the cluster ready. We can choose one of the available profiles. scripts and setup contain useful scripts that we'll use during this tutorial. Feel free to explore this structure. We'll examine it in much more detail throughout this tutorial, and you'll become very familiar with it by trying it out.","title":"Downloading the sample GitOps repository"},{"location":"guides/cp4s/cp4s-tree/cluster-config/gitops-tekton-argocd/#installing-argocd-for-gitops","text":"Now that we've had an initial view of the GitOps repository, let's install ArgoCD to make use of it. We will install ArgoCD using the Red Hat OpenShift GitOps operator. We will also configure it with the appropriate access rights to limit the resources it can create in the cluster. In so doing, we ensure that ArgoCD acts like a well-governed administrator of the cluster, only doing what it needs to do. Also, we will customize our ArgoCD instance by adding necessary custom checks. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". On successful login, you'll see: Logged into \"https://c100-e.jp-tok.containers.cloud.ibm.com:30749\" as \"IAM#hperabat@in.ibm.com\" using the token provided. You have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\" . This shows some basic information about your user within this cluster. (This user is different to your GitHub user.) Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. The following command creates the subscription: oc apply -f setup/ocp47/ The response confirms that the below resources has been created: clusterrole.rbac.authorization.k8s.io/custom-argocd-cluster-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-cntk-argocd-application-controller created subscription.operators.coreos.com/openshift-gitops-operator created Wait for the ArgoCD installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/gitops-operator-controller-manager -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/gitops-operator condition met ArgoCD is now installed and ready to use. If you see something like: Error from server (NotFound): deployments.apps \"gitops-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Alternatively, you can also validate as follows: while ! oc wait crd applications.argoproj.io --timeout = -1s --for = condition = Established 2 >/dev/null ; do sleep 30 ; done while ! oc wait pod --timeout = -1s --for = condition = Ready -l '!job-name' -n openshift-gitops > /dev/null ; do sleep 30 ; done ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are also setup along with the subscription. oc get clusterrole custom-argocd-cluster-argocd-application-controller oc get clusterrolebinding openshift-gitops-argocd-application-controller oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller You'll see the resources are created in the cluster: $ oc get clusterrole custom-argocd-cluster-argocd-application-controller NAME CREATED AT custom-argocd-cluster-argocd-application-controller 2021-08-27T13:35:13Z $ oc get clusterrolebinding openshift-gitops-argocd-application-controller NAME ROLE AGE openshift-gitops-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m43s $ oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller NAME ROLE AGE openshift-gitops-cntk-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m45s The clusterrole custom-argocd-cluster-argocd-application-controller defines a specific set of specific resources that are required by the ArgoCD service account and access rights over them. The cluster role bindings openshift-gitops-argocd-application-controller and openshift-gitops-cntk-argocd-application-controller binds the ArgoCD service account to the cluster role above. In combination, these definitions limit ArgoCD to perform the minimum set of operations required to manage the cluster. This is important; it means that ArgoCD behaves as a well governed administrator of the cluster. We'll also see later in the tutorial how ArgoCD creates resource in the cluster. Delete default ArgoCD instance Let us now delete the default ArgoCD instance that is created earlier. oc delete gitopsservice cluster -n openshift-gitops || true You will see the resources being deleted as follows: warning: deleting cluster-scoped resources, not scoped to the provided namespace gitopsservice.pipelines.openshift.io \"cluster\" deleted We are deleting the existing default instance and planning to make use of a customized ArgoCD instance with additional capabilities. Create a custom ArgoCD instance This customized ArgoCD instance implements custom checks. By default ArgoCD provides some built-in health checks for validating standard kubernetes resources. However, these checks will not be sufficient to validate the custom resources that belong to IBM Cloud Pak for Integration. Argo CD supports custom health checks that are defined in Lua . If you want explore more about the ArgoCD Resource Health, check this link out. Let us create a custom ArgoCD instance as follows: oc apply -f setup/ocp47/argocd-instance/ -n openshift-gitops The response confirms that the below resources has been created: argocd.argoproj.io/openshift-gitops-cntk created Wait till the ArgoCD instance pod is up and running. Issue the below command to verify the same: while ! oc wait pod --timeout = -1s --for = condition = ContainersReady -l app.kubernetes.io/name = openshift-gitops-cntk-server -n openshift-gitops > /dev/null ; do sleep 30 ; done Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops | grep openshift-gitops-cntk-server | awk '{print \"https://\"$2}' This will list the route to the ArgoCD we've just installed, for example: https://openshift-gitops-cntk-server-openshift-gitops.ibmcloud-roks-xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/openshift-gitops-cntk-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Once the UI launches, you'll see: See how there are no ArgoCD applications active at the moment. In the next section of the tutorial, we'll configure ArgoCD to create the ArgoCD applications that will in turn spin up infrastructure , service , and application resources to apply to the cluster. Congratulations! You've created the GitOps repository for your cluster based on the sample MQ repository and examined its high level structure. You also installed ArgoCD. You created a specific clusterrole and clusterrolebinding for the ArgoCD service account to ensure that it manages the cluster in a well governed manner. Finally, you launched the UI for ArgoCD; you'll make extensive use of it during this tutorial. In the next topic of this chapter, we're going to customize the GitOps repository for your cluster and use Tekton and ArgoCD to create and manage the Kubernetes resources for our MQ applications and queue managers.","title":"Installing ArgoCD for GitOps"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/","text":"Installing services with ArgoCD \u00b6 Overview \u00b6 In the previous section of this chapter, we used GitOps to create the ArgoCD applications that installed and managed the ci , tools and dev namespaces in the cluster. In this section we're going complete the installation of all the necessary services required by our CP4S CICD process: We'll examine these highlighted components in more detail throughout this section of the tutorial; here's an overview of their function. Tekton is used for Continuous Integration. Often, the Tekton pipeline will perform its changes under a pull-request (PR) to provide an explicit approval mechanism or through a push based on the requirements for cluster changes. Artifactory will provide a store for CP4S application and queue manager build artifacts such as Helm charts. It is used in conjunction with the GitOps repository and image registry. Sonarqube is used by the CP4S application pipeline for code quality and security scanning. It helps ensure the quality of a deployed application. ArgoCD applications will be created for each of these resources. Specifically, ArgoCD applications will keep the cluster synchronized with the application and queue manager YAML definitions stored in Artifactory and Git. Note how these application services are installed in the ci and tools namespaces we created in the previous topic. This section will reinforce our understanding of GitOps. We will then be ready to create CP4S applications that use the infrastructure we previously created and these services we are going to install further. In this topic, we're going to: Deploy services to the cluster using GitOps Explore the ArgoCD applications that manage these services Explore how more complex services work using Artifactory as an example Review how ArgoCD projects work See how infra and services ArgoCD applications manage the infrastructure and services layers in our architecture. By the end of this topic we'll have a fully configured cluster which is ready for us to deploy CP4S applications and queue managers. Pre-requisites \u00b6 Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed on your local machine the oc command that matches the version of your cluster. You have installed npm , git and tree commands. You have completed the tutorial section to customize the GitOps repository, and install ArgoCD. You have completed the tutorial section to create the ci , tools and dev namespaces using GitOps. Please see the previous sections of this guide for information on how to do these tasks. Video Walkthrough \u00b6 This video demonstrates how to install Tekton. It also shows how to use the GitOps repository to set up different service related components. This is a video walkthrough and it takes you step by step through the below sections. Post cluster provisioning tasks \u00b6 Red Hat OpenShift cluster \u00b6 An OpenShift v4.7+ cluster is required. CLI tools \u00b6 Install the OpenShift CLI oc (version 4.7+) . The binary can be downloaded from the Help menu from the OpenShift Console. Download oc cli Log in from a terminal window. oc login --token = <token> --server = <server> IBM Entitlement Key \u00b6 The IBM Entitlement Key is required to pull IBM Cloud Pak specific container images from the IBM Entitled Registry. To get an entitlement key, Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. A Secret containing the entitlement key is created in the tools namespace. oc new-project tools || true oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io Installing Tekton for GitOps \u00b6 Tekton is made available to your Red Hat OpenShift cluster through the Red Hat OpenShift Pipelines operator. Hence, let's see how to get that operator installed on your cluster. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Install Tekton into the cluster We use the Red Hat Pipelines operator to install Tekton into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: - argocd/operators/openshift-pipelines.yaml Your kustomization.yaml for services should match the following: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Install tekton using Red Hat OpenShift Pipelines Operator\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 524 bytes | 524.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 85a4c46..61e15b0 master -> master Activate the services in the GitOps repo Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us only deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see openshift-pipelines ArgoCD application have been created as follows: Wait for the Tekton installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/openshift-pipelines-operator -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/openshift-pipelines-operator condition met Tekton is now installed and ready to use. We\u2019ll explore Tekton in later topics. If you see something like: Error from server (NotFound): deployments.apps \"openshift-pipelines-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Deploy services to the cluster \u00b6 We've just had our first successful GitOps experience, using an ArgoCD application to create the ci , tools and dev namespaces in our cluster at the infrastructure layer level as well as using ArgoCD to deploy the Red Hat OpenShift Pipelines operator at the services layer level. There are few more components you need to create for this IBM CP4S tutorial. Edit the CP4SThreatManagements custom resource instance and specify a block or file storage class ${GITOPS_PROFILE}/2-services/argocd/instances/ibm-cp4sthreatmanagements-instance.yaml. The default is set to managed-nfs-storage. - name: spec.basicDeploymentConfiguration.storageClass value: managed-nfs-storage - name: spec.extendedDeploymentConfiguration.backupStorageClass value: managed-nfs-storage These components are part of the services layer in our architecture, and that requires us to access /0-bootstrap/single-cluster/2-services within our GitOps repository. Check services layer components As we saw earlier, the bootstrap-single-cluster application uses the contents of the /0-bootstrap/single-cluster/2-services folder to determine which Kubernetes resources are available to be deployed in the cluster. Issue the following command to see what could be deployed in the cluster: tree 0 -bootstrap/single-cluster/2-services/ -L 3 You should see a set of operators and instances of these available in the GitOps framework at the services layer: 0 -bootstrap/single-cluster/2-services/ \u251c\u2500\u2500 2 -services.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 artifactory.yaml \u2502 \u2502 \u251c\u2500\u2500 baas-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 cert-manager-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 chartmuseum.yaml \u2502 \u2502 \u251c\u2500\u2500 developer-dashboard.yaml \u2502 \u2502 \u251c\u2500\u2500 grafana-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-gateway-analytics-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-management-portal-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4sthreatmanagements-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cpd-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-foundational-services-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-platform-navigator-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-process-mining-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 oadp-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 openshift-service-mesh-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 pact-broker.yaml \u2502 \u2502 \u251c\u2500\u2500 sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 sonarqube.yaml \u2502 \u2502 \u251c\u2500\u2500 spp-instance.yaml \u2502 \u2502 \u2514\u2500\u2500 swaggereditor.yaml \u2502 \u2514\u2500\u2500 operators \u2502 \u251c\u2500\u2500 cert-manager.yaml \u2502 \u251c\u2500\u2500 elasticsearch.yaml \u2502 \u251c\u2500\u2500 grafana-operator.yaml \u2502 \u251c\u2500\u2500 ibm-ace-operator.yaml \u2502 \u251c\u2500\u2500 ibm-apic-operator.yaml \u2502 \u251c\u2500\u2500 ibm-aspera-operator.yaml \u2502 \u251c\u2500\u2500 ibm-assetrepository-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-core-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-operator.yaml \u2502 \u251c\u2500\u2500 ibm-catalogs.yaml \u2502 \u251c\u2500\u2500 ibm-cp4a-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4i-operators.yaml \u2502 \u251c\u2500\u2500 ibm-cp4s-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-platform-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-scheduling-operator.yaml \u2502 \u251c\u2500\u2500 ibm-datapower-operator.yaml \u2502 \u251c\u2500\u2500 ibm-db2u-operator.yaml \u2502 \u251c\u2500\u2500 ibm-eventstreams-operator.yaml \u2502 \u251c\u2500\u2500 ibm-foundations.yaml \u2502 \u251c\u2500\u2500 ibm-mq-operator.yaml \u2502 \u251c\u2500\u2500 ibm-opsdashboard-operator.yaml \u2502 \u251c\u2500\u2500 ibm-platform-navigator.yaml \u2502 \u251c\u2500\u2500 ibm-process-mining-operator.yaml \u2502 \u251c\u2500\u2500 jaeger.yaml \u2502 \u251c\u2500\u2500 kiali.yaml \u2502 \u251c\u2500\u2500 oadp-operator.yaml \u2502 \u251c\u2500\u2500 openshift-gitops.yaml \u2502 \u251c\u2500\u2500 openshift-pipelines.yaml \u2502 \u251c\u2500\u2500 openshift-service-mesh.yaml \u2502 \u251c\u2500\u2500 spp-catalog.yaml \u2502 \u2514\u2500\u2500 spp-operator.yaml \u2514\u2500\u2500 kustomization.yaml Review ArgoCD services folder Let\u2019s examine the 0-bootstrap/single-cluster/2-services/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/kustomization.yaml We can see the contents of the kustomization.yaml : resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Add the services to the cluster Note The IBM Platform Navigator instance requires a RWX storageclass and it is set to managed-nfs-storage by default in the ArgoCD Application 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-platform-navigator-instance.yaml . This storageclass is available for Red Hat OpenShift on IBM Cloud cluster provisioned from IBM Technology Zone with NFS storage selected. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: ## Cloud Pak for Security - argocd/operators/ibm-cp4s-operator.yaml - argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml - argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD - argocd/instances/artifactory.yaml - argocd/instances/sonarqube.yaml You will have the following resources deployed for services: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security - argocd/operators/ibm-cp4s-operator.yaml - argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml - argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml - argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 564 bytes | 564.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git b49dff5..533602c master -> master The intention of this operation is to indicate that we'd like the resources declared in 0-bootstrap/single-cluster/2-services/kustomization.yaml to be deployed in the cluster. Like the infra ArgoCD application, the resources created by the services ArgoCD application will manage the Kubernetes relevant services resources applied to the cluster. Optional: Activate the services in the GitOps repo Run this step if you skip installing tekton in the previous section. If you did not skip that step go to Step 7 . Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see lots of new ArgoCD applications have been created to manage the services we have deployed by modifying kustomization.yaml under 0-bootstrap/single-cluster/2-services folder: See how most ArgoCD applications are Synced almost immediately, but some spend time in Progressing . That's because the Artifactory and Sonarqube ArgoCD applications are more complex -- they create more Kubernetes resources than other ArgoCD applications and therefore take longer to sync . After a few minutes you'll see that all ArgoCD applications become Healthy and Synced : Notice how many more Synched ArgoCD applications are now in the cluster; these are as a result of the newly added services layer in our architecture. The services application Let's examine the ArgoCD application that manage the services in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the services application: We can see that the services ArgoCD application creates 10 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It\u2019s the services ArgoCD application that watches the 0-bootstrap/single-cluster/2-services/argocd folder for ArgoCD applications that apply service resources to our cluster. It was the services application that created the artifactory ArgoCD application which manages the Artifactory instance that is used for application configuration management that we will be exploring later in this section of the tutorial. We\u2019ll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it\u2019s not necessary, as you\u2019ll get lots of practice as we proceed. Examine Artifactory resources in detail You can see the Kubernetes resources that the Artifactory ArgoCD application has created for Artifactory. Click on the Artifactory : See how many different Kubernetes resources were created when the Artifactory ArgoCD application YAML was applied to the cluster. That's why it took the Artifactory ArgoCD application a little time to sync. If you'd like, have a look at some of the other ArgoCD applications, such as Sonarqube and the Kubernetes resources they created. The services ArgoCD project As we've seen in the ArgoCD UI, the services ArgoCD application is responsible for creating the ArgoCD applications that manage the services within the cluster. Let's examine their definitions to see how they do this. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/2-services.yaml The following YAML may initially look a little intimidating; we'll discuss the major elements below: --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : services labels : gitops.tier.layer : services spec : sourceRepos : [] # Populated by kustomize patches in 2-services/kustomization.yaml destinations : - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : redhat-operators server : https://kubernetes.default.svc - namespace : openshift-operators server : https://kubernetes.default.svc - namespace : openshift-marketplace server : https://kubernetes.default.svc - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : # TODO: SCC needs to be moved to 1-infra, here for now for artifactory - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleLink - group : \"apps\" kind : statefulsets - group : \"apps\" kind : deployments - group : \"\" kind : services - group : \"\" kind : configmaps - group : \"\" kind : secrets - group : \"\" kind : serviceaccounts - group : \"batch\" kind : jobs - group : \"\" kind : roles - group : \"route.openshift.io\" kind : routes - group : \"\" kind : RoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRole - group : apiextensions.k8s.io kind : CustomResourceDefinition roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : services annotations : argocd.argoproj.io/sync-wave : \"200\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : services source : # repoURL and targetRevision populated by kustomize patches in 2-services/kustomization.yaml path : 0-bootstrap/single-cluster/2-services syncPolicy : automated : prune : true selfHeal : true Notice how this YAML defines three ArgoCD resources: a services project which manages all the necessary services that are needed by the applications. Notice how the destinations for the services project are limited to the ci , tools and dev namespaces -- as well as a few others that we'll use in the tutorial. These destinations restrict the namespaces where ArgoCD applications in the services project can manage resources. The same is true for clusterResourceWhiteList . It limits the Kubernetes resources that can be managed to configmaps , deployments and rolebindings amongst others. In summary, we see that the service project is used to group all the ArgoCD applications that will manage the services in our cluster. These ArgoCD applications can only perform specific actions on specific resource types in specific namespaces. See how ArgoCD is acting as a well-governed administrator. The similar structure of services and infra ArgoCD applications Even though we didn't closely examine the infra ArgoCD application YAML in the previous topic, it has has a very similar structure to the ArgoCD services applications we've just examined. Type the following command to list the ArgoCD infra app YAML. cat 0 -bootstrap/single-cluster/1-infra/1-infra.yaml Again, although this YAML might look a little intimidating, the overall structure is the same as for services : --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : infra labels : gitops.tier.layer : infra spec : sourceRepos : [] # Populated by kustomize patches in 1-infra/kustomization.yaml destinations : - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : - group : \"\" kind : Namespace - group : \"\" kind : RoleBinding - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleNotification - group : \"console.openshift.io\" kind : ConsoleLink roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : infra annotations : argocd.argoproj.io/sync-wave : \"100\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : # repoURL and targetRevision populated by kustomize patches in 1-infra/kustomization.yaml path : 0-bootstrap/single-cluster/1-infra syncPolicy : automated : prune : true selfHeal : true As with the 2-services.yaml , we can see An ArgoCD project called infra . ArgoCD applications defined in this project will be limited by the destinations: and clusterResourceWhitelist: specified in the YAML. An ArgoCD app called infra . This is the ArgoCD application that we used in the previous section of the tutorial. It watches the path: 0-bootstrap/single-cluster/1-infra folder for ArgoCD applications that it applied to the cluster. It was these applications that managed the ci , tools and dev namespaces for example. The installation of the operators will take approximately 30 - 45 minutes. Monitor your installation in the workspace \u00b6 From this stage, the installation will take approximately 1.5 hours to complete. After you start the installation, you are brought to the Schematics workspace for your Cloud Pak. You can track progress by viewing the logs. Go to the Activity tab, and click View logs. To further track the installation, you can monitor the status of IBM Cloud Pak\u00ae for Security Threat Management: Log in to the OpenShift web console and ensure you are in the Administrator view. Go to Operators > Installed Operators and ensure that the Project is set to the namespace where IBM Cloud Pak\u00ae for Security was installed. In the list of installed operators, click IBM Cloud Pak for Security. On the Threat Management tab, select the threatmgmt instance. On the Details page, the following message is displayed in the Conditions section when installation is complete. Cloudpak for Security Deployment is successful. The installation is complete when you see the message Cloudpak for Security Deployment is successful. Retrieve foundational services login details \u00b6 Use the following commands to retrieve IBM Cloud Pak foundational services hostname, default username, and password: Hostname: oc -n ibm-common-services get route | grep cp-console | awk '{print $2}' Username: oc -n ibm-common-services get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_username}' | base64 --decode Password: oc -n ibm-common-services get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_password}' | base64 --decode These login details are required to access foundational services and configure your LDAP directory. Next steps \u00b6 If the adminUser you provided is a user ID that you added and authenticated by using the IBM Cloud account that is associated with the cluster and roksAuthentication was enabled, go to step 2. Otherwise, Configure LDAP authentication and ensure that the adminUser that you provided exists in the LDAP directory. Log in to Cloud Pak\u00ae for Security using the domain and the adminUser that you provided during installation. The domain, also known as application URL, can be retrieved by running the following command: oc get route isc-route-default --no-headers -n <CP4S_NAMESPACE> | awk '{print $2}' Select Enterprise LDAP in the login screen if you are logging in using an LDAP you connected to Foundational Services, otherwise use OpenShift Authentication if it is enabled. Add users to Cloud Pak\u00ae for Security Install the IBM\u00ae Security Orchestration & Automation license . If you choose Orchestration & Automation as part of your Cloud Pak\u00ae for Security bundle, you must install your Orchestration & Automation license to access the complete orchestration and automation capabilities that are provided by Orchestration & Automation. Configure data sources Validation \u00b6 Check the status of the CommonService and PlatformNavigator custom resource. oc get CP4SThreatManagement threatmgmt -n tools -o jsonpath = '{.status.conditions}' # Expected output = Cloudpak for Security Deployment is successful Before users can log in to the console for Cloud Pak for Security, an identity provider must be configured. The documentation provides further instructions. For DEMO purposes, OpenLDAP can be deployed and instructions are provided below. Download the cpctl utility Log in to the OpenShift cluster oc login --token = <token> --server = <openshift_url> -n <namespace> Retrieve the pod that contains the utility POD = $( oc get pod --no-headers -lrun = cp-serviceability | cut -d ' ' -f1 ) Copy the utility locally oc cp $POD :/opt/bin/<operatingsystem>/cpctl ./cpctl && chmod +x ./cpctl Install OpenLDAP Start a session ./cpctl load Install OpenLDAP cpctl tools deploy_openldap --token $( oc whoami -t ) --ldap_usernames 'adminUser,user1,user2,user3' --ldap_password cloudpak Initial user log in Retrieve Cloud Pak for Security Console URL oc get route isc-route-default --no-headers -n <CP4S_NAMESPACE> | awk '{print $2}' Log in with the user ID and password specified (ie. adminUser / cloudpak ). Other important ArgoCD features \u00b6 In this final section of this chapter, let's explore ArgoCD features you may have noticed as you explored different YAML files in this chapter: SyncWave \u00b6 Using SyncWave to control deployment sequencing When ArgoCD uses a GitOps repository to apply ArgoCD applications to the cluster, it applies all ArgoCD applications concurrently. Even though Kubernetes use of eventual consistency means that resources which depend on each other can be deployed in any order, it often makes sense to help deploy certain resources in a certain order. This can avoid spurious error messages or wasted compute resources for example. Let's compare two ArgoCD YAML files. Firstly, let's examine the namespace-tools in the infrastructure layer: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-tools.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: tools labels: gitops.tier.layer: infra annotations: argocd.argoproj.io/sync-wave: \"100\" ... Then examine the artifactory in the services layer: cat 0 -bootstrap/single-cluster/2-services/argocd/instances/artifactory.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: artifactory annotations: argocd.argoproj.io/sync-wave: \"250\" labels: gitops.tier.group: cntk gitops.tier.layer: services finalizers: - resources-finalizer.argocd.argoproj.io ... Notice the use of sync-wave: \"100\" for tools and how it contrasts with sync-wave: \"250\" for artifactory . The lower 100 will be deployed before the higher sync-wave 250 because it only makes sense to deploy the pipelines into the tools namespace, after the tools namespace has been created. You are free to choose any number for sync-wave . In our deployment, we have chosen 100-199 for infrastructure, 200-299 for services and 300-399 for applications; it provides alignment with high level folder numbering such as 1-infra and so on. In our tutorial, we're going to incrementally add infrastructure, services and applications so that you can understand how everything fits together incrementally. This makes sync-wave less important than in a real-world system, where you might be making updates to each of these deployed layers simultaneously. In such cases the use of sync-wave provides a high degree of confidence in the effectiveness of a deployment change because all changes go through in the correct order. Congratulations! You've installed tekton and configured the key services in the cluster to support continuous integration and continuous delivery. We installed them into the tools namespaces we created previously. This chapter is now complete.","title":"Add services"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#installing-services-with-argocd","text":"","title":"Installing services with ArgoCD"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#overview","text":"In the previous section of this chapter, we used GitOps to create the ArgoCD applications that installed and managed the ci , tools and dev namespaces in the cluster. In this section we're going complete the installation of all the necessary services required by our CP4S CICD process: We'll examine these highlighted components in more detail throughout this section of the tutorial; here's an overview of their function. Tekton is used for Continuous Integration. Often, the Tekton pipeline will perform its changes under a pull-request (PR) to provide an explicit approval mechanism or through a push based on the requirements for cluster changes. Artifactory will provide a store for CP4S application and queue manager build artifacts such as Helm charts. It is used in conjunction with the GitOps repository and image registry. Sonarqube is used by the CP4S application pipeline for code quality and security scanning. It helps ensure the quality of a deployed application. ArgoCD applications will be created for each of these resources. Specifically, ArgoCD applications will keep the cluster synchronized with the application and queue manager YAML definitions stored in Artifactory and Git. Note how these application services are installed in the ci and tools namespaces we created in the previous topic. This section will reinforce our understanding of GitOps. We will then be ready to create CP4S applications that use the infrastructure we previously created and these services we are going to install further. In this topic, we're going to: Deploy services to the cluster using GitOps Explore the ArgoCD applications that manage these services Explore how more complex services work using Artifactory as an example Review how ArgoCD projects work See how infra and services ArgoCD applications manage the infrastructure and services layers in our architecture. By the end of this topic we'll have a fully configured cluster which is ready for us to deploy CP4S applications and queue managers.","title":"Overview"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#pre-requisites","text":"Before attempting this section, you must have completed the following tasks: You have created an OCP cluster instance. You have installed on your local machine the oc command that matches the version of your cluster. You have installed npm , git and tree commands. You have completed the tutorial section to customize the GitOps repository, and install ArgoCD. You have completed the tutorial section to create the ci , tools and dev namespaces using GitOps. Please see the previous sections of this guide for information on how to do these tasks.","title":"Pre-requisites"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#video-walkthrough","text":"This video demonstrates how to install Tekton. It also shows how to use the GitOps repository to set up different service related components. This is a video walkthrough and it takes you step by step through the below sections.","title":"Video Walkthrough"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#post-cluster-provisioning-tasks","text":"","title":"Post cluster provisioning tasks"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#red-hat-openshift-cluster","text":"An OpenShift v4.7+ cluster is required.","title":"Red Hat OpenShift cluster"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#cli-tools","text":"Install the OpenShift CLI oc (version 4.7+) . The binary can be downloaded from the Help menu from the OpenShift Console. Download oc cli Log in from a terminal window. oc login --token = <token> --server = <server>","title":"CLI tools"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#ibm-entitlement-key","text":"The IBM Entitlement Key is required to pull IBM Cloud Pak specific container images from the IBM Entitled Registry. To get an entitlement key, Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. A Secret containing the entitlement key is created in the tools namespace. oc new-project tools || true oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io","title":"IBM Entitlement Key"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#installing-tekton-for-gitops","text":"Tekton is made available to your Red Hat OpenShift cluster through the Red Hat OpenShift Pipelines operator. Hence, let's see how to get that operator installed on your cluster. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Install Tekton into the cluster We use the Red Hat Pipelines operator to install Tekton into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: - argocd/operators/openshift-pipelines.yaml Your kustomization.yaml for services should match the following: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Install tekton using Red Hat OpenShift Pipelines Operator\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 524 bytes | 524.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 85a4c46..61e15b0 master -> master Activate the services in the GitOps repo Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us only deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see openshift-pipelines ArgoCD application have been created as follows: Wait for the Tekton installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/openshift-pipelines-operator -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/openshift-pipelines-operator condition met Tekton is now installed and ready to use. We\u2019ll explore Tekton in later topics. If you see something like: Error from server (NotFound): deployments.apps \"openshift-pipelines-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected.","title":"Installing Tekton for GitOps"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#deploy-services-to-the-cluster","text":"We've just had our first successful GitOps experience, using an ArgoCD application to create the ci , tools and dev namespaces in our cluster at the infrastructure layer level as well as using ArgoCD to deploy the Red Hat OpenShift Pipelines operator at the services layer level. There are few more components you need to create for this IBM CP4S tutorial. Edit the CP4SThreatManagements custom resource instance and specify a block or file storage class ${GITOPS_PROFILE}/2-services/argocd/instances/ibm-cp4sthreatmanagements-instance.yaml. The default is set to managed-nfs-storage. - name: spec.basicDeploymentConfiguration.storageClass value: managed-nfs-storage - name: spec.extendedDeploymentConfiguration.backupStorageClass value: managed-nfs-storage These components are part of the services layer in our architecture, and that requires us to access /0-bootstrap/single-cluster/2-services within our GitOps repository. Check services layer components As we saw earlier, the bootstrap-single-cluster application uses the contents of the /0-bootstrap/single-cluster/2-services folder to determine which Kubernetes resources are available to be deployed in the cluster. Issue the following command to see what could be deployed in the cluster: tree 0 -bootstrap/single-cluster/2-services/ -L 3 You should see a set of operators and instances of these available in the GitOps framework at the services layer: 0 -bootstrap/single-cluster/2-services/ \u251c\u2500\u2500 2 -services.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 artifactory.yaml \u2502 \u2502 \u251c\u2500\u2500 baas-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 cert-manager-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 chartmuseum.yaml \u2502 \u2502 \u251c\u2500\u2500 developer-dashboard.yaml \u2502 \u2502 \u251c\u2500\u2500 grafana-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-gateway-analytics-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-management-portal-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4sthreatmanagements-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cpd-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-foundational-services-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-platform-navigator-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-process-mining-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 oadp-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 openshift-service-mesh-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 pact-broker.yaml \u2502 \u2502 \u251c\u2500\u2500 sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 sonarqube.yaml \u2502 \u2502 \u251c\u2500\u2500 spp-instance.yaml \u2502 \u2502 \u2514\u2500\u2500 swaggereditor.yaml \u2502 \u2514\u2500\u2500 operators \u2502 \u251c\u2500\u2500 cert-manager.yaml \u2502 \u251c\u2500\u2500 elasticsearch.yaml \u2502 \u251c\u2500\u2500 grafana-operator.yaml \u2502 \u251c\u2500\u2500 ibm-ace-operator.yaml \u2502 \u251c\u2500\u2500 ibm-apic-operator.yaml \u2502 \u251c\u2500\u2500 ibm-aspera-operator.yaml \u2502 \u251c\u2500\u2500 ibm-assetrepository-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-core-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-operator.yaml \u2502 \u251c\u2500\u2500 ibm-catalogs.yaml \u2502 \u251c\u2500\u2500 ibm-cp4a-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4i-operators.yaml \u2502 \u251c\u2500\u2500 ibm-cp4s-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-platform-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-scheduling-operator.yaml \u2502 \u251c\u2500\u2500 ibm-datapower-operator.yaml \u2502 \u251c\u2500\u2500 ibm-db2u-operator.yaml \u2502 \u251c\u2500\u2500 ibm-eventstreams-operator.yaml \u2502 \u251c\u2500\u2500 ibm-foundations.yaml \u2502 \u251c\u2500\u2500 ibm-mq-operator.yaml \u2502 \u251c\u2500\u2500 ibm-opsdashboard-operator.yaml \u2502 \u251c\u2500\u2500 ibm-platform-navigator.yaml \u2502 \u251c\u2500\u2500 ibm-process-mining-operator.yaml \u2502 \u251c\u2500\u2500 jaeger.yaml \u2502 \u251c\u2500\u2500 kiali.yaml \u2502 \u251c\u2500\u2500 oadp-operator.yaml \u2502 \u251c\u2500\u2500 openshift-gitops.yaml \u2502 \u251c\u2500\u2500 openshift-pipelines.yaml \u2502 \u251c\u2500\u2500 openshift-service-mesh.yaml \u2502 \u251c\u2500\u2500 spp-catalog.yaml \u2502 \u2514\u2500\u2500 spp-operator.yaml \u2514\u2500\u2500 kustomization.yaml Review ArgoCD services folder Let\u2019s examine the 0-bootstrap/single-cluster/2-services/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/kustomization.yaml We can see the contents of the kustomization.yaml : resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Add the services to the cluster Note The IBM Platform Navigator instance requires a RWX storageclass and it is set to managed-nfs-storage by default in the ArgoCD Application 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-platform-navigator-instance.yaml . This storageclass is available for Red Hat OpenShift on IBM Cloud cluster provisioned from IBM Technology Zone with NFS storage selected. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: ## Cloud Pak for Security - argocd/operators/ibm-cp4s-operator.yaml - argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml - argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD - argocd/instances/artifactory.yaml - argocd/instances/sonarqube.yaml You will have the following resources deployed for services: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security - argocd/operators/ibm-cp4s-operator.yaml - argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml - argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml - argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 564 bytes | 564.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git b49dff5..533602c master -> master The intention of this operation is to indicate that we'd like the resources declared in 0-bootstrap/single-cluster/2-services/kustomization.yaml to be deployed in the cluster. Like the infra ArgoCD application, the resources created by the services ArgoCD application will manage the Kubernetes relevant services resources applied to the cluster. Optional: Activate the services in the GitOps repo Run this step if you skip installing tekton in the previous section. If you did not skip that step go to Step 7 . Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see lots of new ArgoCD applications have been created to manage the services we have deployed by modifying kustomization.yaml under 0-bootstrap/single-cluster/2-services folder: See how most ArgoCD applications are Synced almost immediately, but some spend time in Progressing . That's because the Artifactory and Sonarqube ArgoCD applications are more complex -- they create more Kubernetes resources than other ArgoCD applications and therefore take longer to sync . After a few minutes you'll see that all ArgoCD applications become Healthy and Synced : Notice how many more Synched ArgoCD applications are now in the cluster; these are as a result of the newly added services layer in our architecture. The services application Let's examine the ArgoCD application that manage the services in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the services application: We can see that the services ArgoCD application creates 10 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It\u2019s the services ArgoCD application that watches the 0-bootstrap/single-cluster/2-services/argocd folder for ArgoCD applications that apply service resources to our cluster. It was the services application that created the artifactory ArgoCD application which manages the Artifactory instance that is used for application configuration management that we will be exploring later in this section of the tutorial. We\u2019ll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it\u2019s not necessary, as you\u2019ll get lots of practice as we proceed. Examine Artifactory resources in detail You can see the Kubernetes resources that the Artifactory ArgoCD application has created for Artifactory. Click on the Artifactory : See how many different Kubernetes resources were created when the Artifactory ArgoCD application YAML was applied to the cluster. That's why it took the Artifactory ArgoCD application a little time to sync. If you'd like, have a look at some of the other ArgoCD applications, such as Sonarqube and the Kubernetes resources they created. The services ArgoCD project As we've seen in the ArgoCD UI, the services ArgoCD application is responsible for creating the ArgoCD applications that manage the services within the cluster. Let's examine their definitions to see how they do this. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/2-services.yaml The following YAML may initially look a little intimidating; we'll discuss the major elements below: --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : services labels : gitops.tier.layer : services spec : sourceRepos : [] # Populated by kustomize patches in 2-services/kustomization.yaml destinations : - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : redhat-operators server : https://kubernetes.default.svc - namespace : openshift-operators server : https://kubernetes.default.svc - namespace : openshift-marketplace server : https://kubernetes.default.svc - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : # TODO: SCC needs to be moved to 1-infra, here for now for artifactory - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleLink - group : \"apps\" kind : statefulsets - group : \"apps\" kind : deployments - group : \"\" kind : services - group : \"\" kind : configmaps - group : \"\" kind : secrets - group : \"\" kind : serviceaccounts - group : \"batch\" kind : jobs - group : \"\" kind : roles - group : \"route.openshift.io\" kind : routes - group : \"\" kind : RoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRole - group : apiextensions.k8s.io kind : CustomResourceDefinition roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : services annotations : argocd.argoproj.io/sync-wave : \"200\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : services source : # repoURL and targetRevision populated by kustomize patches in 2-services/kustomization.yaml path : 0-bootstrap/single-cluster/2-services syncPolicy : automated : prune : true selfHeal : true Notice how this YAML defines three ArgoCD resources: a services project which manages all the necessary services that are needed by the applications. Notice how the destinations for the services project are limited to the ci , tools and dev namespaces -- as well as a few others that we'll use in the tutorial. These destinations restrict the namespaces where ArgoCD applications in the services project can manage resources. The same is true for clusterResourceWhiteList . It limits the Kubernetes resources that can be managed to configmaps , deployments and rolebindings amongst others. In summary, we see that the service project is used to group all the ArgoCD applications that will manage the services in our cluster. These ArgoCD applications can only perform specific actions on specific resource types in specific namespaces. See how ArgoCD is acting as a well-governed administrator. The similar structure of services and infra ArgoCD applications Even though we didn't closely examine the infra ArgoCD application YAML in the previous topic, it has has a very similar structure to the ArgoCD services applications we've just examined. Type the following command to list the ArgoCD infra app YAML. cat 0 -bootstrap/single-cluster/1-infra/1-infra.yaml Again, although this YAML might look a little intimidating, the overall structure is the same as for services : --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : infra labels : gitops.tier.layer : infra spec : sourceRepos : [] # Populated by kustomize patches in 1-infra/kustomization.yaml destinations : - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : - group : \"\" kind : Namespace - group : \"\" kind : RoleBinding - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleNotification - group : \"console.openshift.io\" kind : ConsoleLink roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : infra annotations : argocd.argoproj.io/sync-wave : \"100\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : # repoURL and targetRevision populated by kustomize patches in 1-infra/kustomization.yaml path : 0-bootstrap/single-cluster/1-infra syncPolicy : automated : prune : true selfHeal : true As with the 2-services.yaml , we can see An ArgoCD project called infra . ArgoCD applications defined in this project will be limited by the destinations: and clusterResourceWhitelist: specified in the YAML. An ArgoCD app called infra . This is the ArgoCD application that we used in the previous section of the tutorial. It watches the path: 0-bootstrap/single-cluster/1-infra folder for ArgoCD applications that it applied to the cluster. It was these applications that managed the ci , tools and dev namespaces for example. The installation of the operators will take approximately 30 - 45 minutes.","title":"Deploy services to the cluster"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#monitor-your-installation-in-the-workspace","text":"From this stage, the installation will take approximately 1.5 hours to complete. After you start the installation, you are brought to the Schematics workspace for your Cloud Pak. You can track progress by viewing the logs. Go to the Activity tab, and click View logs. To further track the installation, you can monitor the status of IBM Cloud Pak\u00ae for Security Threat Management: Log in to the OpenShift web console and ensure you are in the Administrator view. Go to Operators > Installed Operators and ensure that the Project is set to the namespace where IBM Cloud Pak\u00ae for Security was installed. In the list of installed operators, click IBM Cloud Pak for Security. On the Threat Management tab, select the threatmgmt instance. On the Details page, the following message is displayed in the Conditions section when installation is complete. Cloudpak for Security Deployment is successful. The installation is complete when you see the message Cloudpak for Security Deployment is successful.","title":"Monitor your installation in the workspace"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#retrieve-foundational-services-login-details","text":"Use the following commands to retrieve IBM Cloud Pak foundational services hostname, default username, and password: Hostname: oc -n ibm-common-services get route | grep cp-console | awk '{print $2}' Username: oc -n ibm-common-services get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_username}' | base64 --decode Password: oc -n ibm-common-services get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_password}' | base64 --decode These login details are required to access foundational services and configure your LDAP directory.","title":"Retrieve foundational services login details"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#next-steps","text":"If the adminUser you provided is a user ID that you added and authenticated by using the IBM Cloud account that is associated with the cluster and roksAuthentication was enabled, go to step 2. Otherwise, Configure LDAP authentication and ensure that the adminUser that you provided exists in the LDAP directory. Log in to Cloud Pak\u00ae for Security using the domain and the adminUser that you provided during installation. The domain, also known as application URL, can be retrieved by running the following command: oc get route isc-route-default --no-headers -n <CP4S_NAMESPACE> | awk '{print $2}' Select Enterprise LDAP in the login screen if you are logging in using an LDAP you connected to Foundational Services, otherwise use OpenShift Authentication if it is enabled. Add users to Cloud Pak\u00ae for Security Install the IBM\u00ae Security Orchestration & Automation license . If you choose Orchestration & Automation as part of your Cloud Pak\u00ae for Security bundle, you must install your Orchestration & Automation license to access the complete orchestration and automation capabilities that are provided by Orchestration & Automation. Configure data sources","title":"Next steps"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#validation","text":"Check the status of the CommonService and PlatformNavigator custom resource. oc get CP4SThreatManagement threatmgmt -n tools -o jsonpath = '{.status.conditions}' # Expected output = Cloudpak for Security Deployment is successful Before users can log in to the console for Cloud Pak for Security, an identity provider must be configured. The documentation provides further instructions. For DEMO purposes, OpenLDAP can be deployed and instructions are provided below. Download the cpctl utility Log in to the OpenShift cluster oc login --token = <token> --server = <openshift_url> -n <namespace> Retrieve the pod that contains the utility POD = $( oc get pod --no-headers -lrun = cp-serviceability | cut -d ' ' -f1 ) Copy the utility locally oc cp $POD :/opt/bin/<operatingsystem>/cpctl ./cpctl && chmod +x ./cpctl Install OpenLDAP Start a session ./cpctl load Install OpenLDAP cpctl tools deploy_openldap --token $( oc whoami -t ) --ldap_usernames 'adminUser,user1,user2,user3' --ldap_password cloudpak Initial user log in Retrieve Cloud Pak for Security Console URL oc get route isc-route-default --no-headers -n <CP4S_NAMESPACE> | awk '{print $2}' Log in with the user ID and password specified (ie. adminUser / cloudpak ).","title":"Validation"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#other-important-argocd-features","text":"In this final section of this chapter, let's explore ArgoCD features you may have noticed as you explored different YAML files in this chapter:","title":"Other important ArgoCD features"},{"location":"guides/cp4s/cp4s-tree/cluster-config/services/#syncwave","text":"Using SyncWave to control deployment sequencing When ArgoCD uses a GitOps repository to apply ArgoCD applications to the cluster, it applies all ArgoCD applications concurrently. Even though Kubernetes use of eventual consistency means that resources which depend on each other can be deployed in any order, it often makes sense to help deploy certain resources in a certain order. This can avoid spurious error messages or wasted compute resources for example. Let's compare two ArgoCD YAML files. Firstly, let's examine the namespace-tools in the infrastructure layer: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-tools.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: tools labels: gitops.tier.layer: infra annotations: argocd.argoproj.io/sync-wave: \"100\" ... Then examine the artifactory in the services layer: cat 0 -bootstrap/single-cluster/2-services/argocd/instances/artifactory.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: artifactory annotations: argocd.argoproj.io/sync-wave: \"250\" labels: gitops.tier.group: cntk gitops.tier.layer: services finalizers: - resources-finalizer.argocd.argoproj.io ... Notice the use of sync-wave: \"100\" for tools and how it contrasts with sync-wave: \"250\" for artifactory . The lower 100 will be deployed before the higher sync-wave 250 because it only makes sense to deploy the pipelines into the tools namespace, after the tools namespace has been created. You are free to choose any number for sync-wave . In our deployment, we have chosen 100-199 for infrastructure, 200-299 for services and 300-399 for applications; it provides alignment with high level folder numbering such as 1-infra and so on. In our tutorial, we're going to incrementally add infrastructure, services and applications so that you can understand how everything fits together incrementally. This makes sync-wave less important than in a real-world system, where you might be making updates to each of these deployed layers simultaneously. In such cases the use of sync-wave provides a high degree of confidence in the effectiveness of a deployment change because all changes go through in the correct order. Congratulations! You've installed tekton and configured the key services in the cluster to support continuous integration and continuous delivery. We installed them into the tools namespaces we created previously. This chapter is now complete.","title":"SyncWave"},{"location":"guides/cp4s/cp4s-tree/cluster-create/aws-setup/","text":"Overview \u00b6 This sprint does not support AWS clusters Later sprints will support clusters provisioned on AWS","title":"Creating a cluster on AWS"},{"location":"guides/cp4s/cp4s-tree/cluster-create/aws-setup/#overview","text":"This sprint does not support AWS clusters Later sprints will support clusters provisioned on AWS","title":"Overview"},{"location":"guides/cp4s/cp4s-tree/cluster-create/azure-setup/","text":"Overview \u00b6 This sprint does not support Azure clusters Later sprints will support clusters on different platform","title":"Creating a cluster on Azure"},{"location":"guides/cp4s/cp4s-tree/cluster-create/azure-setup/#overview","text":"This sprint does not support Azure clusters Later sprints will support clusters on different platform","title":"Overview"},{"location":"guides/cp4s/cp4s-tree/cluster-create/ibm-setup/","text":"Creating a Red Hat OpenShift cluster \u00b6 Audience : Architects, Application developers, Administrators Overview \u00b6 In this topic we're going to: Introduce you to IBM Technology Zone Provision an OpenShift cluster By the end of this topic you'll have created an OpenShift cluster for the MQ tutorials. IBM Technology Zone \u00b6 IBM Technology Zone is a one-stop shop to get access to technical environments and software for demos, prototyping and deployment. We're going to use it to provision the OpenShift cluster used by our tutorial. Technology zone can be used to provision a Red Hat OpenShift cluster on different infrastructure including IBM Cloud, Azure, AWS and VMWare. There is also a degree of flexibility to configure the compute, storage and network for these infrastructures. The MQ tutorial will work on all of these platforms. It is also possible to provision a cluster via the command line using the instructions in the Hands-on section of this guide. Decide infrastructure \u00b6 Decide which infrastructure platform you are going to use for your Red Hat OpenShift cluster from the available options: Red Hat OpenShift on IBM Cloud Click on your chosen infrastructure link above or scroll to down to the appropriate section in this topic. OpenShift on IBM Cloud \u00b6 Navigate to the IBM Technology Zone In your browser, navigate to the IBM Technology Zone to create your cluster. You will need to sign-in with your IBM credentials if your browser has not authenticated. You will need to read and accept Terms and conditions to proceed to the IBM Technology Zone. Reserve a cluster You can reserve a cluster immediately on IBM Cloud. You'll see the following options: Click the Reserve now radio button. It takes between 30 and 60 minutes to provision a cluster. Moreover, once a cluster has been provisioned, by default, it will be deleted after 3 days -- unless you renew it. It can therefore be helpful to schedule the environment to be created later if you are not going to be using it for a while -- such as for a future demonstration or classroom session. Come back and explore the Schedule for later option another time. Complete reservation details - step 1 You will be asked to complete your reservations details in two steps. Firstly, when you make a cluster reservation, you can supply a set of details that describe: The cluster name The purpose of the reservation from a pre-defined set A description, which will appear for a Practice/Self-Education reservation Notes to help you identify this reservation In the following web page: Complete the details as follows: Type a helpful Name such as My cluster 1 Select Practice/Self-Education from the Purpose drop-down list Type a helpful Purpose Description such as Education Do not select Preferred Geography yet; we will do that in a moment Add some helpful Notes such as Learn how to build a cloud native production reference deployment using a CloudPak and OpenShift on IBM Cloud Complete reservation details - step 2 The second step in registration is to specify the physical details of the compute infrastructure for your cluster. When you make a cluster reservation, you can supply a set of details that describe: The cluster's geographical location The version of OpenShift Container Platform used by the cluster The number of CPUs in the cluster and their memory size The number of Kubernetes worker nodes in the cluster The NFS storage size for the cluster The End date and time for the cluster In the following web page: Complete the details as follows: Select your Preferred Geography such as London 5 , HongKong 2 or San Jose 04 Under End date and time , select a date and specify the time along with its time zone. Select Worker Node Count as 3 from the drop-down list Select Worker Node Flavor as 8CPU x 32GB from the drop-down list Select NFS Size as 500 GB from the drop-down list Select OpenShift Version as 4.8 from the drop-down-list Info By default, your cluster will be reserved for three days , after which time it will be deleted. This is more than enough time to complete this tutorial. If required, you can increase the cluster lifetime by configuring the End date and time . Don't do this unless you really need to do so; as you'll learn in the tutorial, the use of GitOps makes it quick and easy to provision a cluster when you need to do so. Create the cluster Check your reservation details and press Submit when you're ready to create the cluster: The cluster provisioning process will take between 30 and 60 minutes to complete on IBM Cloud. Provisioning email As the provisioning process starts, you will receive an email to confirm that the provisioning process has started: Checking the cluster provisioning status You can also view the status of your provisioning request on IBM Technology Zone. Click on My reservations to see: Note: The Status: field will be Ready when the cluster has been provisioned. The creation and expiry date-time for your cluster. You can return to the reservation at any time to get the details of your cluster. Cluster provisioned email You will receive another email once your cluster is ready: Your cluster is now provisioned and ready to use. Note the expiry date-time. Navigate to your cluster You can use the information in the notification email to access your cluster. Click on the Cluster URL link to login to IBM Cloud and locate your OpenShift cluster: Note: The Cluster ID that has been assigned by IBM Cloud. The Zone matches your requested geography. The OCP Version matches your requested version of OCP. The Node status identifies the 3 worker nodes in the Normal state. Feel free to explore this web page. Receiving cluster deletion email and extending your cluster's lifetime When your cluster is within 3 days of deletion, you will receive a daily email: Note: You can extend your cluster if required. This option may be limited to certain types of reservations. The My reservations page allows you to manage your cluster reservation Congratulations, you've now created your cluster. In the next topic, we're going to configure the cluster for GitOps, so that we can manage it quickly, easily and effectively.","title":"Create the cluster"},{"location":"guides/cp4s/cp4s-tree/cluster-create/ibm-setup/#creating-a-red-hat-openshift-cluster","text":"Audience : Architects, Application developers, Administrators","title":"Creating a Red Hat OpenShift cluster"},{"location":"guides/cp4s/cp4s-tree/cluster-create/ibm-setup/#overview","text":"In this topic we're going to: Introduce you to IBM Technology Zone Provision an OpenShift cluster By the end of this topic you'll have created an OpenShift cluster for the MQ tutorials.","title":"Overview"},{"location":"guides/cp4s/cp4s-tree/cluster-create/ibm-setup/#ibm-technology-zone","text":"IBM Technology Zone is a one-stop shop to get access to technical environments and software for demos, prototyping and deployment. We're going to use it to provision the OpenShift cluster used by our tutorial. Technology zone can be used to provision a Red Hat OpenShift cluster on different infrastructure including IBM Cloud, Azure, AWS and VMWare. There is also a degree of flexibility to configure the compute, storage and network for these infrastructures. The MQ tutorial will work on all of these platforms. It is also possible to provision a cluster via the command line using the instructions in the Hands-on section of this guide.","title":"IBM Technology Zone"},{"location":"guides/cp4s/cp4s-tree/cluster-create/ibm-setup/#decide-infrastructure","text":"Decide which infrastructure platform you are going to use for your Red Hat OpenShift cluster from the available options: Red Hat OpenShift on IBM Cloud Click on your chosen infrastructure link above or scroll to down to the appropriate section in this topic.","title":"Decide infrastructure"},{"location":"guides/cp4s/cp4s-tree/cluster-create/ibm-setup/#openshift-on-ibm-cloud","text":"Navigate to the IBM Technology Zone In your browser, navigate to the IBM Technology Zone to create your cluster. You will need to sign-in with your IBM credentials if your browser has not authenticated. You will need to read and accept Terms and conditions to proceed to the IBM Technology Zone. Reserve a cluster You can reserve a cluster immediately on IBM Cloud. You'll see the following options: Click the Reserve now radio button. It takes between 30 and 60 minutes to provision a cluster. Moreover, once a cluster has been provisioned, by default, it will be deleted after 3 days -- unless you renew it. It can therefore be helpful to schedule the environment to be created later if you are not going to be using it for a while -- such as for a future demonstration or classroom session. Come back and explore the Schedule for later option another time. Complete reservation details - step 1 You will be asked to complete your reservations details in two steps. Firstly, when you make a cluster reservation, you can supply a set of details that describe: The cluster name The purpose of the reservation from a pre-defined set A description, which will appear for a Practice/Self-Education reservation Notes to help you identify this reservation In the following web page: Complete the details as follows: Type a helpful Name such as My cluster 1 Select Practice/Self-Education from the Purpose drop-down list Type a helpful Purpose Description such as Education Do not select Preferred Geography yet; we will do that in a moment Add some helpful Notes such as Learn how to build a cloud native production reference deployment using a CloudPak and OpenShift on IBM Cloud Complete reservation details - step 2 The second step in registration is to specify the physical details of the compute infrastructure for your cluster. When you make a cluster reservation, you can supply a set of details that describe: The cluster's geographical location The version of OpenShift Container Platform used by the cluster The number of CPUs in the cluster and their memory size The number of Kubernetes worker nodes in the cluster The NFS storage size for the cluster The End date and time for the cluster In the following web page: Complete the details as follows: Select your Preferred Geography such as London 5 , HongKong 2 or San Jose 04 Under End date and time , select a date and specify the time along with its time zone. Select Worker Node Count as 3 from the drop-down list Select Worker Node Flavor as 8CPU x 32GB from the drop-down list Select NFS Size as 500 GB from the drop-down list Select OpenShift Version as 4.8 from the drop-down-list Info By default, your cluster will be reserved for three days , after which time it will be deleted. This is more than enough time to complete this tutorial. If required, you can increase the cluster lifetime by configuring the End date and time . Don't do this unless you really need to do so; as you'll learn in the tutorial, the use of GitOps makes it quick and easy to provision a cluster when you need to do so. Create the cluster Check your reservation details and press Submit when you're ready to create the cluster: The cluster provisioning process will take between 30 and 60 minutes to complete on IBM Cloud. Provisioning email As the provisioning process starts, you will receive an email to confirm that the provisioning process has started: Checking the cluster provisioning status You can also view the status of your provisioning request on IBM Technology Zone. Click on My reservations to see: Note: The Status: field will be Ready when the cluster has been provisioned. The creation and expiry date-time for your cluster. You can return to the reservation at any time to get the details of your cluster. Cluster provisioned email You will receive another email once your cluster is ready: Your cluster is now provisioned and ready to use. Note the expiry date-time. Navigate to your cluster You can use the information in the notification email to access your cluster. Click on the Cluster URL link to login to IBM Cloud and locate your OpenShift cluster: Note: The Cluster ID that has been assigned by IBM Cloud. The Zone matches your requested geography. The OCP Version matches your requested version of OCP. The Node status identifies the 3 worker nodes in the Normal state. Feel free to explore this web page. Receiving cluster deletion email and extending your cluster's lifetime When your cluster is within 3 days of deletion, you will receive a daily email: Note: You can extend your cluster if required. This option may be limited to certain types of reservations. The My reservations page allows you to manage your cluster reservation Congratulations, you've now created your cluster. In the next topic, we're going to configure the cluster for GitOps, so that we can manage it quickly, easily and effectively.","title":"OpenShift on IBM Cloud"},{"location":"guides/cp4s/cp4s-tree/cluster-create/vmware-setup/","text":"Overview \u00b6 This sprint does not support VMWare clusters Later sprints will allow clusters on different platform","title":"Creating a cluster on VMWare"},{"location":"guides/cp4s/cp4s-tree/cluster-create/vmware-setup/#overview","text":"This sprint does not support VMWare clusters Later sprints will allow clusters on different platform","title":"Overview"},{"location":"guides/cp4s/cp4s-tree/disaster-recovery/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4s/cp4s-tree/disaster-recovery/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4s/cp4s-tree/monitoring/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4s/cp4s-tree/monitoring/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/","text":"Target architecture \u00b6 Audience : Architects, Application developers, Administrators Overview \u00b6 In this topic we're going to: Examine a system context diagram for an CP4S deployment Identify the components used to build a cloud native deployment Describe the role of each component Explore a typical architecture overview diagram that includes these components Describe the GitOps model for CP4S By the end of this topic you'll understand the architectural design of a typical cloud native CP4S system, its primary components and their function. System context \u00b6 A system context diagram helps us understand how our system interacts with its different users and other systems. IBM Cloud Pak\u00ae for Security can securely access IBM and third-party tools to search for threat indicators across any cloud or on-premises location. Connect your workflows with a unified interface so you can respond faster to security incidents. Use IBM Cloud Pak\u00ae for Security to orchestrate and automate your security response so that you can better prioritize your team's time. Component diagram \u00b6 The following diagram shows the technical components used in a typical CP4S production deployment. IBM Cloud Pak\u00ae for Security includes the following offerings. IBM\u00ae Security Threat Intelligence Insights is an application that delivers unique, actionable, and timely threat intelligence. The application provides most of the functions of IBM X-Force\u00ae Exchange. IBM\u00ae Security Data Explorer is a platform application that enables customers to do federated search and investigation across their hybrid, multi-cloud environment in a single interface and workflow. IBM\u00ae Security Case Management for IBM Cloud Pak\u00ae for Security provides organizations with the ability to track, manage, and resolve cybersecurity incidents. IBM\u00ae Security Orchestration & Automation application is integrated on IBM Cloud Pak\u00ae for Security to provide most of the IBM Resilient Security Orchestration, Automation, and Response Platform feature set. If you have an Orchestration & Automation license, you can choose between the stand-alone version on a virtual appliance, or the application on Cloud Pak for Security. IBM\u00ae QRadar\u00ae is offered as a stand-alone on-premises solution and delivers intelligent security analytics, enabling visibility, detection, and investigation for a wide range of known and unknown threats. Event analytics ingest, parse, normalize, correlate, and analyze log and event data to detect indicators of threats. Flow analytics collect, extract, and normalize valuable network flow data and packet metadata to augment log-based security insights. IBM\u00ae QRadar\u00ae Proxy application provides communication between IBM Cloud Pak for Security and IBM QRadar or QRadar on Cloud. This communication uses APIs to pull powerful QRadar data into the QRadar Security Information and Event Management (SIEM) dashboards. IBM\u00ae QRadar\u00ae User Behavior Analytics is an application for detecting insider threats in your organization. User Behavior Analytics, used in conjunction with the existing data in your QRadar system, can help you generate new insights around users and user risk. IBM\u00ae Security Risk Manager application provides early visibility into potential security risks by correlating insights from multiple vectors so that you can prioritize risks to take appropriate remedial actions. IBM\u00ae Security Threat Investigator is an application that automatically analyzes and investigates cases to help determine the criticality of exposure, how many systems are at risk, and the level of remediation effort that is required. IBM\u00ae Security Guardium Insights is a stand-alone collaborative, robust data security platform that is designed help to unify and modernize the security operations center (SOC). Collected data can be shared with Cloud Pak for Security. IBM\u00ae Security Guardium Data Protection is a data activity monitoring and compliance reporting solution that is purpose-built to protect sensitive data stored across platforms. IBM\u00ae Security Guardium Vulnerability Assessment solution identifies threats and security holes that might be used by malicious actors to access sensitive data. The solution recommends concrete actions to strengthen security. IBM\u00ae Detection and Response Center (Beta) provides a unified overview of your organization\u2019s security posture through use cases from different security tools and platforms, saving you hours of gathering the same insights by using individual tools. IBM Security Data Explorer \u00b6 IBM Security Data Explorer is an application that enables customers to do federated search and investigation across their hybrid, multi-cloud environment in a single interface and workflow. Data Explorer enables users to complete investigations in a timely manner without compromising visibility. Core underlying services and capabilities include: Federated data search to unite silos of security data and provide complete visibility across security solutions (for example, Security Information and Event Management (SIEM), Endpoint Detection and Response, Data lake), and cloud infrastructures (for example, Azure, Amazon Web Services (AWS)) Single, unified interface and workflow to investigate threats and Indicators of Compromise into user-selected data sources In-context data enhancements from Connected Assets and Risk data sources and IBM Security Threat Intelligence Insights Workflows to track, append, create security cases from the native platform case management system. Detection and Response Center (Beta) \u00b6 IBM Detection and Response Center (Beta) provides a unified overview of your organization's security posture through use cases from different security tools and platforms, saving you hours of gathering the same insights by using individual tools. The Beta version supports rules and use cases from IBM QRadar and the Sigma Community. Sigma rules, enhanced by STIX patterns, are used by Threat Investigator in its investigations. You can also run the STIX patterns in Data Explorer. Core features of the Beta version include: - Exploring rules through visualization and reports - Running STIX patterns from Sigma rules in Data Explorer - Visualizing threat coverage across the MITRE ATT&CK framework IBM Security Case Management \u00b6 IBM Security Case Management for IBM Cloud Pak for Security provides organizations with the ability to track, manage, and resolve cybersecurity incidents. With IBM Security Case Management, Security and IT teams can collaborate across their organization to rapidly and successfully respond to incidents. Case Management is a subset of the IBM Security Orchestration & Automation application and is available without an extra license on IBM Cloud Pak for Security. IBM Security Global Case Management (Beta) \u00b6 IBM Security Global Case Management (Beta) lists active cases that are associated with all the Standard accounts you belong to under your Provider account. As a managed security service provider, you can view IBM Security Case Management active cases, sort and filter to identify the highest priority cases, and investigate case details. IBM Security Orchestration & Automation \u00b6 IBM Security Orchestration & Automation requires a license and is available as an application that is fully integrated in IBM Cloud Pak for Security. Orchestration & Automation provides the following benefits: Create response plans that are based on industry standards and best practices. Integrate more easily with security and IT tools, and orchestrate responses to events and incidents. Collaborate across the organization, equipping various stakeholders with the tools to fulfill their roles and tasks as part of an incident response effort. The application that is integrated on Cloud Pak for Security provides most, but not all, of the IBM Security Orchestration, Automation, and Response Platform feature set. For more information for more information about this application, see Orchestration and Automation. If you have an Orchestration & Automation license, you can choose between the application on Cloud Pak for Security or the stand-alone version on a virtual appliance. The stand-alone virtual appliance version provides the full feature set of IBM Security Orchestration, Automation, and Response Platform. IBM Security Risk Manager \u00b6 IBM Security Risk Manager for Cloud Pak for Security is an application that automatically collects, correlates, and contextualizes risk insights across the IT and security ecosystem of your organization. These risk insights are presented in a business-consumable dashboard to prioritize and remediate security risks. Risk Manager provides the following key features: Unified view of disparate security risk metrics from multiple vectors, such as data, identity, and infrastructure that helps risk prioritization. Standard risk scoring framework for consistent and common risk definition that is easily understood by all stakeholders. Risk remediation management by using workflow management systems. The basic features of Risk Manager are freely available to all Cloud Pak for Security. The Risk Manager Advanced application provides differentiated capabilities for licensed users, such as, recommendations to implement remedial actions, risk trend to view and track risk progression over time, and custom risk configuration to compute the risk score. IBM Security Threat Intelligence Insights \u00b6 IBM Security Threat Intelligence Insights is an application that delivers unique, actionable, and timely threat intelligence. The application provides almost all of the functions that IBM X-Force\u00ae Exchange provides: IBM-derived threat intelligence that crosses threat activity, threat groups, malware, and industries. Continuous and automated Am I Affected searches that cross connected data sources to proactively identify your most relevant threats. Analytical and adaptive threat-scoring to help prioritize threats for further investigation and response. IBM Security Threat Investigator \u00b6 IBM Security Threat Investigator automatically analyzes and investigates cases to help you make more informed decisions. By showing potential threats and the assets that are impacted, Threat Investigator can help determine the criticality of exposure, how many systems are at risk, and the level of remediation effort that is required. By viewing the historical timeline of threats within your organization, you can better understand dwell times and the stage of the threat. ArgoCD \u00b6 ArgoCD is used for the continuous deployment of software components to the Kubernetes cluster. ArgoCD watches a Git repository for new or changed Kubernetes resource definitions, and applies them to a cluster. In this way, ArgoCD ensures that the component configuration stored in GitHub always reflects the state of the cluster. For example, we will use ArgoCD to deploy and maintain queue managers, MQ applications and the other cloud native components in our architecture. ArgoCD also has the added benefit of being able to monitor resources that it has deployed to ensure that if they drift from their desired values, they will be automatically restored to those values by ArgoCD. Tekton \u00b6 Tekton is used to automate manual tasks using the concept of a pipeline. A pipeline comprises a set of tasks that are executed in a specified order in order to accomplish a specific objective. We use pipelines as part of the continuous integration process to build, test and deliver queue managers and MQ applications ready for deployment by ArgoCD. We also use pipelines to run performance tests, and to promote queue managers and applications from dev to stage and production environments. Open LDAP \u00b6 Most existing MQ on-premise deployments use LDAP for access control. Our architecture provides an OpenLDAP to help migration of existing MQ estates. While LDAP is an excellent technology, most cloud native systems have started to use certificates for authentication, identification and authorization. Indeed, MQ has added the ability to use the identify from certificate for access control. This removes the need for LDAP, and removes a single point of failure. Many customers will want to exploit the Cert manager components to help move to certificate based authorization. Kustomize \u00b6 Kubernetes resources such as queue managers and applications, have their operational properties defined using YAMLs. As these resources move through environments such as dev stage and prod, Kustomize provides a natural way to adapt ( customize !) these YAMLs to these environments. For example, we might want to change the CPU or memory available to a queue manager in a production environment compared to a development environment. Because Kustomize is built into the kubectl and oc commands via the -k option, it makes configuration management both easy and natural. Kubernetes Cluster \u00b6 This is the \"operating system\" used to orchestrate our CP4S Cloud Pak, Kubernetes is portable across on-premise and cloud systems and allows us to easily scale our workloads across these environments as required. You'll be learning a lot more about Kubernetes in this tutorial; it's the foundation upon which everything else is built. GitOps model \u00b6 In our AOD, we've emphasized the two main components that are essential to a production-ready MQ cloud native deployment: A Kubernetes cluster containing: Cloud native components such as Tekton, ArgoCD, Kibana and Grafana. GitHub as a source of truth for the cluster runtime containing: Configuration information for the cloud native components running in the cluster Notice the set of users who interact with these components: IT or system administrators Managed Security Service Providers (MSSP) Security business leaders Security analysts In this tutorial, we'll see how these users work within this environment. All users will follow the GitOps model . In this model, Git holds the entire specification for the system -- hardware, cluster, build, test and deploy components, queue managers and applications. Whenever the system needs to be changed, whether by a developer, administrator, SRE or architect, they use Git and git operations such as Pull requests to make a change. A change must pass a full set of tests (stored in git) to ensure the change is correct. If successful, it is merged into the current system either automatically or after approval, if required. In the GitOps model, Git is at the heart of every operational change performed to every component of the system; you'll learn more about this model throughout the tutorial by using it.","title":"CP4S architecture"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#target-architecture","text":"Audience : Architects, Application developers, Administrators","title":"Target architecture"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#overview","text":"In this topic we're going to: Examine a system context diagram for an CP4S deployment Identify the components used to build a cloud native deployment Describe the role of each component Explore a typical architecture overview diagram that includes these components Describe the GitOps model for CP4S By the end of this topic you'll understand the architectural design of a typical cloud native CP4S system, its primary components and their function.","title":"Overview"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#system-context","text":"A system context diagram helps us understand how our system interacts with its different users and other systems. IBM Cloud Pak\u00ae for Security can securely access IBM and third-party tools to search for threat indicators across any cloud or on-premises location. Connect your workflows with a unified interface so you can respond faster to security incidents. Use IBM Cloud Pak\u00ae for Security to orchestrate and automate your security response so that you can better prioritize your team's time.","title":"System context"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#component-diagram","text":"The following diagram shows the technical components used in a typical CP4S production deployment. IBM Cloud Pak\u00ae for Security includes the following offerings. IBM\u00ae Security Threat Intelligence Insights is an application that delivers unique, actionable, and timely threat intelligence. The application provides most of the functions of IBM X-Force\u00ae Exchange. IBM\u00ae Security Data Explorer is a platform application that enables customers to do federated search and investigation across their hybrid, multi-cloud environment in a single interface and workflow. IBM\u00ae Security Case Management for IBM Cloud Pak\u00ae for Security provides organizations with the ability to track, manage, and resolve cybersecurity incidents. IBM\u00ae Security Orchestration & Automation application is integrated on IBM Cloud Pak\u00ae for Security to provide most of the IBM Resilient Security Orchestration, Automation, and Response Platform feature set. If you have an Orchestration & Automation license, you can choose between the stand-alone version on a virtual appliance, or the application on Cloud Pak for Security. IBM\u00ae QRadar\u00ae is offered as a stand-alone on-premises solution and delivers intelligent security analytics, enabling visibility, detection, and investigation for a wide range of known and unknown threats. Event analytics ingest, parse, normalize, correlate, and analyze log and event data to detect indicators of threats. Flow analytics collect, extract, and normalize valuable network flow data and packet metadata to augment log-based security insights. IBM\u00ae QRadar\u00ae Proxy application provides communication between IBM Cloud Pak for Security and IBM QRadar or QRadar on Cloud. This communication uses APIs to pull powerful QRadar data into the QRadar Security Information and Event Management (SIEM) dashboards. IBM\u00ae QRadar\u00ae User Behavior Analytics is an application for detecting insider threats in your organization. User Behavior Analytics, used in conjunction with the existing data in your QRadar system, can help you generate new insights around users and user risk. IBM\u00ae Security Risk Manager application provides early visibility into potential security risks by correlating insights from multiple vectors so that you can prioritize risks to take appropriate remedial actions. IBM\u00ae Security Threat Investigator is an application that automatically analyzes and investigates cases to help determine the criticality of exposure, how many systems are at risk, and the level of remediation effort that is required. IBM\u00ae Security Guardium Insights is a stand-alone collaborative, robust data security platform that is designed help to unify and modernize the security operations center (SOC). Collected data can be shared with Cloud Pak for Security. IBM\u00ae Security Guardium Data Protection is a data activity monitoring and compliance reporting solution that is purpose-built to protect sensitive data stored across platforms. IBM\u00ae Security Guardium Vulnerability Assessment solution identifies threats and security holes that might be used by malicious actors to access sensitive data. The solution recommends concrete actions to strengthen security. IBM\u00ae Detection and Response Center (Beta) provides a unified overview of your organization\u2019s security posture through use cases from different security tools and platforms, saving you hours of gathering the same insights by using individual tools.","title":"Component diagram"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#ibm-security-data-explorer","text":"IBM Security Data Explorer is an application that enables customers to do federated search and investigation across their hybrid, multi-cloud environment in a single interface and workflow. Data Explorer enables users to complete investigations in a timely manner without compromising visibility. Core underlying services and capabilities include: Federated data search to unite silos of security data and provide complete visibility across security solutions (for example, Security Information and Event Management (SIEM), Endpoint Detection and Response, Data lake), and cloud infrastructures (for example, Azure, Amazon Web Services (AWS)) Single, unified interface and workflow to investigate threats and Indicators of Compromise into user-selected data sources In-context data enhancements from Connected Assets and Risk data sources and IBM Security Threat Intelligence Insights Workflows to track, append, create security cases from the native platform case management system.","title":"IBM Security Data Explorer"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#detection-and-response-center-beta","text":"IBM Detection and Response Center (Beta) provides a unified overview of your organization's security posture through use cases from different security tools and platforms, saving you hours of gathering the same insights by using individual tools. The Beta version supports rules and use cases from IBM QRadar and the Sigma Community. Sigma rules, enhanced by STIX patterns, are used by Threat Investigator in its investigations. You can also run the STIX patterns in Data Explorer. Core features of the Beta version include: - Exploring rules through visualization and reports - Running STIX patterns from Sigma rules in Data Explorer - Visualizing threat coverage across the MITRE ATT&CK framework","title":"Detection and Response Center (Beta)"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#ibm-security-case-management","text":"IBM Security Case Management for IBM Cloud Pak for Security provides organizations with the ability to track, manage, and resolve cybersecurity incidents. With IBM Security Case Management, Security and IT teams can collaborate across their organization to rapidly and successfully respond to incidents. Case Management is a subset of the IBM Security Orchestration & Automation application and is available without an extra license on IBM Cloud Pak for Security.","title":"IBM Security Case Management"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#ibm-security-global-case-management-beta","text":"IBM Security Global Case Management (Beta) lists active cases that are associated with all the Standard accounts you belong to under your Provider account. As a managed security service provider, you can view IBM Security Case Management active cases, sort and filter to identify the highest priority cases, and investigate case details.","title":"IBM Security Global Case Management (Beta)"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#ibm-security-orchestration-automation","text":"IBM Security Orchestration & Automation requires a license and is available as an application that is fully integrated in IBM Cloud Pak for Security. Orchestration & Automation provides the following benefits: Create response plans that are based on industry standards and best practices. Integrate more easily with security and IT tools, and orchestrate responses to events and incidents. Collaborate across the organization, equipping various stakeholders with the tools to fulfill their roles and tasks as part of an incident response effort. The application that is integrated on Cloud Pak for Security provides most, but not all, of the IBM Security Orchestration, Automation, and Response Platform feature set. For more information for more information about this application, see Orchestration and Automation. If you have an Orchestration & Automation license, you can choose between the application on Cloud Pak for Security or the stand-alone version on a virtual appliance. The stand-alone virtual appliance version provides the full feature set of IBM Security Orchestration, Automation, and Response Platform.","title":"IBM Security Orchestration &amp; Automation"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#ibm-security-risk-manager","text":"IBM Security Risk Manager for Cloud Pak for Security is an application that automatically collects, correlates, and contextualizes risk insights across the IT and security ecosystem of your organization. These risk insights are presented in a business-consumable dashboard to prioritize and remediate security risks. Risk Manager provides the following key features: Unified view of disparate security risk metrics from multiple vectors, such as data, identity, and infrastructure that helps risk prioritization. Standard risk scoring framework for consistent and common risk definition that is easily understood by all stakeholders. Risk remediation management by using workflow management systems. The basic features of Risk Manager are freely available to all Cloud Pak for Security. The Risk Manager Advanced application provides differentiated capabilities for licensed users, such as, recommendations to implement remedial actions, risk trend to view and track risk progression over time, and custom risk configuration to compute the risk score.","title":"IBM Security Risk Manager"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#ibm-security-threat-intelligence-insights","text":"IBM Security Threat Intelligence Insights is an application that delivers unique, actionable, and timely threat intelligence. The application provides almost all of the functions that IBM X-Force\u00ae Exchange provides: IBM-derived threat intelligence that crosses threat activity, threat groups, malware, and industries. Continuous and automated Am I Affected searches that cross connected data sources to proactively identify your most relevant threats. Analytical and adaptive threat-scoring to help prioritize threats for further investigation and response.","title":"IBM Security Threat Intelligence Insights"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#ibm-security-threat-investigator","text":"IBM Security Threat Investigator automatically analyzes and investigates cases to help you make more informed decisions. By showing potential threats and the assets that are impacted, Threat Investigator can help determine the criticality of exposure, how many systems are at risk, and the level of remediation effort that is required. By viewing the historical timeline of threats within your organization, you can better understand dwell times and the stage of the threat.","title":"IBM Security Threat Investigator"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#argocd","text":"ArgoCD is used for the continuous deployment of software components to the Kubernetes cluster. ArgoCD watches a Git repository for new or changed Kubernetes resource definitions, and applies them to a cluster. In this way, ArgoCD ensures that the component configuration stored in GitHub always reflects the state of the cluster. For example, we will use ArgoCD to deploy and maintain queue managers, MQ applications and the other cloud native components in our architecture. ArgoCD also has the added benefit of being able to monitor resources that it has deployed to ensure that if they drift from their desired values, they will be automatically restored to those values by ArgoCD.","title":"ArgoCD"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#tekton","text":"Tekton is used to automate manual tasks using the concept of a pipeline. A pipeline comprises a set of tasks that are executed in a specified order in order to accomplish a specific objective. We use pipelines as part of the continuous integration process to build, test and deliver queue managers and MQ applications ready for deployment by ArgoCD. We also use pipelines to run performance tests, and to promote queue managers and applications from dev to stage and production environments.","title":"Tekton"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#open-ldap","text":"Most existing MQ on-premise deployments use LDAP for access control. Our architecture provides an OpenLDAP to help migration of existing MQ estates. While LDAP is an excellent technology, most cloud native systems have started to use certificates for authentication, identification and authorization. Indeed, MQ has added the ability to use the identify from certificate for access control. This removes the need for LDAP, and removes a single point of failure. Many customers will want to exploit the Cert manager components to help move to certificate based authorization.","title":"Open LDAP"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#kustomize","text":"Kubernetes resources such as queue managers and applications, have their operational properties defined using YAMLs. As these resources move through environments such as dev stage and prod, Kustomize provides a natural way to adapt ( customize !) these YAMLs to these environments. For example, we might want to change the CPU or memory available to a queue manager in a production environment compared to a development environment. Because Kustomize is built into the kubectl and oc commands via the -k option, it makes configuration management both easy and natural.","title":"Kustomize"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#kubernetes-cluster","text":"This is the \"operating system\" used to orchestrate our CP4S Cloud Pak, Kubernetes is portable across on-premise and cloud systems and allows us to easily scale our workloads across these environments as required. You'll be learning a lot more about Kubernetes in this tutorial; it's the foundation upon which everything else is built.","title":"Kubernetes Cluster"},{"location":"guides/cp4s/cp4s-tree/overview/architecture/#gitops-model","text":"In our AOD, we've emphasized the two main components that are essential to a production-ready MQ cloud native deployment: A Kubernetes cluster containing: Cloud native components such as Tekton, ArgoCD, Kibana and Grafana. GitHub as a source of truth for the cluster runtime containing: Configuration information for the cloud native components running in the cluster Notice the set of users who interact with these components: IT or system administrators Managed Security Service Providers (MSSP) Security business leaders Security analysts In this tutorial, we'll see how these users work within this environment. All users will follow the GitOps model . In this model, Git holds the entire specification for the system -- hardware, cluster, build, test and deploy components, queue managers and applications. Whenever the system needs to be changed, whether by a developer, administrator, SRE or architect, they use Git and git operations such as Pull requests to make a change. A change must pass a full set of tests (stored in git) to ensure the change is correct. If successful, it is merged into the current system either automatically or after approval, if required. In the GitOps model, Git is at the heart of every operational change performed to every component of the system; you'll learn more about this model throughout the tutorial by using it.","title":"GitOps model"},{"location":"guides/cp4s/cp4s-tree/overview/overview/","text":"Cloud Native CP4S \u00b6 Overview \u00b6 Audience : Architects, Application developers, Administrators In this introduction to Cloud Native CP4S, we're going to: Examine a system context diagram for an CP4S deployment Identify the different categories of concern for production operations Understand how the IBM CloudPak for Integration is used to build production deployments Highlight the benefits of a cloud native approach to CP4S deployment System context \u00b6 Examine the following system context diagram: We can see the different entities that interact with a typical CP4S deployment. These include physical users as well as applications and systems. This guide is divided into chapters that help you build, step-by-step, a working CP4S deployment, based on a cloud native approach. The guide addresses all the major aspects and best practices for: Installation Deployment Configuration management Operational monitoring High Availability Security Disaster recovery Performance The guide makes extensive use of the IBM Cloud Pak for Integration (CP4I) and other cloud native technologies. We'll use the the containers, operators, microservices, immutable infrastructure and declarative APIs provided by the CloudPak to to create a best-practice, production-ready CP4S deployment. Alongside the CloudPak, we'll also learn how CNCF technologies such as Kubernetes, Operators, Tekton, ArgoCD, or Prometheus integrate with the CloudPak in a production environment. Cloud native systems like CP4S are loosely coupled, making them resilient, manageable, and observable. A cloud native approach also brings robust DevOps automation which allows CP4S developers, administrators and architects to make high quality changes with predictability and minimal toil as frequently as their organization requires. Now that we've identified the entities that interact with a cloud native CP4S deployment, let's examine the architecture of an CP4S deployment in more detail.","title":"Cloud Native CP4S"},{"location":"guides/cp4s/cp4s-tree/overview/overview/#cloud-native-cp4s","text":"","title":"Cloud Native CP4S"},{"location":"guides/cp4s/cp4s-tree/overview/overview/#overview","text":"Audience : Architects, Application developers, Administrators In this introduction to Cloud Native CP4S, we're going to: Examine a system context diagram for an CP4S deployment Identify the different categories of concern for production operations Understand how the IBM CloudPak for Integration is used to build production deployments Highlight the benefits of a cloud native approach to CP4S deployment","title":"Overview"},{"location":"guides/cp4s/cp4s-tree/overview/overview/#system-context","text":"Examine the following system context diagram: We can see the different entities that interact with a typical CP4S deployment. These include physical users as well as applications and systems. This guide is divided into chapters that help you build, step-by-step, a working CP4S deployment, based on a cloud native approach. The guide addresses all the major aspects and best practices for: Installation Deployment Configuration management Operational monitoring High Availability Security Disaster recovery Performance The guide makes extensive use of the IBM Cloud Pak for Integration (CP4I) and other cloud native technologies. We'll use the the containers, operators, microservices, immutable infrastructure and declarative APIs provided by the CloudPak to to create a best-practice, production-ready CP4S deployment. Alongside the CloudPak, we'll also learn how CNCF technologies such as Kubernetes, Operators, Tekton, ArgoCD, or Prometheus integrate with the CloudPak in a production environment. Cloud native systems like CP4S are loosely coupled, making them resilient, manageable, and observable. A cloud native approach also brings robust DevOps automation which allows CP4S developers, administrators and architects to make high quality changes with predictability and minimal toil as frequently as their organization requires. Now that we've identified the entities that interact with a cloud native CP4S deployment, let's examine the architecture of an CP4S deployment in more detail.","title":"System context"},{"location":"guides/cp4s/cp4s-tree/performance/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4s/cp4s-tree/performance/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4s/cp4s-tree/scalability/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4s/cp4s-tree/scalability/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4s/cp4s-tree/security/overview/","text":"App Connect Enterprise \u00b6 placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4s/cp4s-tree/security/overview/#app-connect-enterprise","text":"placeholder...","title":"App Connect Enterprise"},{"location":"guides/cp4s/cp4s-tree/security/handson/mtls/","text":"Enabling mTLS security and delegated authentication using LDAP \u00b6 Overview \u00b6 In this guide, we will setup mTLS to secure the data in motion between the mq-spring-app and the QM1 queue manager. We will also setup an LDAP server to configure the system to delegate authentication of mq users (system and human) to an OpenLDAP server. The details of the solution are described here mq security . Prerequisites \u00b6 You have a cluster provisioned with the mq-spring-app deployed and interacting with the QM1 queue manager without any security configured (i.e. no TLS and no LDAP for delegated authentication). You have cloned the git repositories for: * multi-tenancy-gitops (https://github.com/cloud-native-toolkit/multi-tenancy-gitops) * mq-infra (https://github.com/cloud-native-toolkit/mq-infra) * mq-spring-app (https://github.com/cloud-native-toolkit/mq-spring-app) Configure mTLS and Delegated Authentication \u00b6 Create openldap project Go to the multi-tenancy-gitops folder where the local copy of the repo is: cd multi-tenancy-gitops Edit file 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the line: - argocd/namespace-openldap.yaml Commit the change and push it back to the repo. git add . git commit -m \"create openldap project\" git push origin master This will create the openldap project in the cluster. Install the OpenLDAP Server Edit file 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the line: - argocd/instances/openldap.yaml Commit the change and push it back to the repo. git add . git commit -m \"enable OpenLDAP to be installed\" git push origin master This will create the openldap Argo application and install OpenLDAP into the cluster in namespace openldap . In ArgoCD, when you search for the openldap application, you should see a screen that look like this: Once, ArgoCD Synchs with the OpenShift cluster, you will see two deployments in the openldap project as follows: OpenLDAP has a admin UI to manage the users. Get the route and open the admin UI in a browser: oc get route openldap-admin -n openldap -o jsonpath='{ .spec.host }' The OpenLDAP admin UI will appear as follows: Login using Login DN credentials: cn=admin,dc=ibm,dc=com and password : admin Once, you login, you will see several users in the directory including the mqapp user which we will use in this guide. Activate cert-manager Operator Edit file 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the line: - argocd/operators/cert-manager.yaml Commit the change and push it back to the repo git add . git commit -m \"enable cert-manager operator\" git push origin master The cert-manager ArgoCD application will be created: The cert-manager operator will show up in the OpenShift cluster. Activate cert-manager instance Edit file 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the line: - argocd/instances/cert-manager-instance.yaml Commit the change and push it back to the repo git add . git commit -m \"enable cert-manager instance\" git push origin master The cert-manager instance ArgoCD application will be created: The cert-manager instance will show up in the OpenShift cluster. Activate Certificate Issuer and Certificates NOTE: ??? CHECK WITH HOLLIS. IS THIS GITOPS STRUCTURE STILL CORRECT???? Edit file 0-bootstrap/single-cluster/3-apps/kustomization.yaml and uncomment the line: - argocd/mq/tools.yaml Commit the change and push it back to the repo git add . git commit -m \"enable certificate issuer\" git push origin master Creates the following resources in the cluster: Resource type Resource name ClusterIssuer selfsigned-cluster-issuer Certificate mq-self-signed-ca-cert ClusterIssuer self-signed-mq-cluster-issuer Create JKS password secret NOTE: master branch now has the script and the sealed secret yaml to generate the mq-client-jks-password secret. https://github.com/mqpdg2/multi-tenancy-gitops-apps/tree/master/mq/environments/ci/secrets So I believe this step does not need to be in this guide. I believe that the secret will exist at this point at least in the ci namespace. Not sure of the other namespaces. Activate Certificates in ci namespace Edit file `mq/environments/ci/kustomization.yaml and uncomment the line: - certificates/ci-mq-client-certificate.yaml - certificates/ci-mq-server-certificate.yaml Commit the change and push it back to the repo git add . git commit -m \"create the mq client and server certificate in the ci namespace\" git push origin master Verify that the certificate was created successfully and the secret was generated by cert-manager. oc get certificate -n ci You should see the certificates with a Ready status of True as follows: NAME READY SECRET AGE ci-mq-client-cert True mq-client-jks 5d16h ci-mq-server-cert True mq-server-cert 5d16h The cert-manager operator generates the secrets from these certificates. Lets verify the secrets were generated correctly. Run the command: oc get secret mq-client-jks -n ci You should see the secret with a Data value of 5 representing 5 key value pairs as follows: NAME TYPE DATA AGE mq-client-jks kubernetes.io/tls 5 5d16h Run the command: oc get secret mq-server-cert -n ci You should see the secret with a Data value of 3 representing 3 key value pairs as follows: NAME TYPE DATA AGE mq-server-cert kubernetes.io/tls 3 5d17h Activate Certificates in dev namespace Edit file `mq/environments/dev/kustomization.yaml and uncomment the line: - certificates/dev-mq-client-certificate.yaml - certificates/dev-mq-server-certificate.yaml Commit the change and push it back to the repo git add . git commit -m \"create the mq client and server certificate in the dev namespace\" git push origin master Verify that the certificate was created successfully and the secret was generated by cert-manager. oc get certificate -n dev You should see the certificates with a Ready status of True as follows: NAME READY SECRET AGE dev-mq-client-cert True mq-client-jks 5d16h dev-mq-server-cert True mq-server-cert 5d16h The cert-manager operator generates the secrets from these certificates. Lets verify the secrets were generated correctly. Run the command: oc get secret mq-client-jks -n dev You should see the secret with a Data value of 5 representing 5 key value pairs as follows: NAME TYPE DATA AGE mq-client-jks kubernetes.io/tls 5 5d16h Run the command: oc get secret mq-server-cert -n ci You should see the secret with a Data value of 3 representing 3 key value pairs as follows: NAME TYPE DATA AGE mq-server-cert kubernetes.io/tls 3 5d17h Deploy/Create an mq manager instance with security enabled Edit the values.yaml file in the source mq-infra repo and change the value of the security to true as follows: security: true Commit the change and push it back to the repo git add . git commit -m \"Build and deploy the mq manager with security enabled\" git push origin master Run the mq-infra-dev pipeline. From the OpenShift console, go to Pipelines and set the Project to ci . You should see the mq-infra-dev pipeline as follows: Click on the mq-infra-dev pipeline link. Click on the Actions drop-down and select Start as follows: The Start Pipeline screen will pop up. Fill the git-url field and use the defaults for the other fields. Then click the Start button. Wait till the pipeline completes successfully. The final stage of the pipeline will update the gitops repo: multi-tenancy-gitops-apps/mq/environments/dev/mq-infra/ which will force ArgoCD to deploy the secure MQ manager to the dev project. Note: It may take a few minutes for ArgoCD to synch with the OpenShift cluster. Deploy mq-spring-app with security enabled Edit the values.yaml file in the source mq-spring-app repo and change the value of the security to true as follows: security: true Commit the change and push it back to the repo git add . git commit -m \"Build and deploy the mq-spring-app with security enabled\" git push origin master Run the mq-spring-app-dev pipeline. From the OpenShift console, go to Pipelines and set the Project to ci . You should see the mq-spring-app-dev pipeline as follows: Click on the mq-spring-dev pipeline link. Click on the Actions drop-down and select Start as follows: The Start Pipeline screen will pop up. Fill the git-url field and use the defaults for the other fields. Then click the Start button. Wait till the pipeline completes successfully. The final stage of the pipeline will update the gitops repo: multi-tenancy-gitops-apps/mq/environments/dev/mq-spring-app/ which will force ArgoCD to deploy the secure mq-spring-app client app to the dev project. Note: It may take a few minutes for ArgoCD to synch with the OpenShift cluster. Test the mq-spring-app Check the health of the app: export APP_URL=$(oc get route -n dev mq-spring-app -o jsonpath=\"{.spec.host}\") curl -X GET https://$APP_URL/actuator/health Invoke the api to put a message onto the queue: curl -X GET https://$APP_URL/api/send-hello-world Invoke the api to get a message from the queue: curl -X GET https://$APP_URL/api/recv To view the swagger docs of the mq-spring-app app, you can open a browser and enter the $APP_URL value.","title":"Enabling mTLS security and delegated authentication using LDAP"},{"location":"guides/cp4s/cp4s-tree/security/handson/mtls/#enabling-mtls-security-and-delegated-authentication-using-ldap","text":"","title":"Enabling mTLS security and delegated authentication using LDAP"},{"location":"guides/cp4s/cp4s-tree/security/handson/mtls/#overview","text":"In this guide, we will setup mTLS to secure the data in motion between the mq-spring-app and the QM1 queue manager. We will also setup an LDAP server to configure the system to delegate authentication of mq users (system and human) to an OpenLDAP server. The details of the solution are described here mq security .","title":"Overview"},{"location":"guides/cp4s/cp4s-tree/security/handson/mtls/#prerequisites","text":"You have a cluster provisioned with the mq-spring-app deployed and interacting with the QM1 queue manager without any security configured (i.e. no TLS and no LDAP for delegated authentication). You have cloned the git repositories for: * multi-tenancy-gitops (https://github.com/cloud-native-toolkit/multi-tenancy-gitops) * mq-infra (https://github.com/cloud-native-toolkit/mq-infra) * mq-spring-app (https://github.com/cloud-native-toolkit/mq-spring-app)","title":"Prerequisites"},{"location":"guides/cp4s/cp4s-tree/security/handson/mtls/#configure-mtls-and-delegated-authentication","text":"Create openldap project Go to the multi-tenancy-gitops folder where the local copy of the repo is: cd multi-tenancy-gitops Edit file 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the line: - argocd/namespace-openldap.yaml Commit the change and push it back to the repo. git add . git commit -m \"create openldap project\" git push origin master This will create the openldap project in the cluster. Install the OpenLDAP Server Edit file 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the line: - argocd/instances/openldap.yaml Commit the change and push it back to the repo. git add . git commit -m \"enable OpenLDAP to be installed\" git push origin master This will create the openldap Argo application and install OpenLDAP into the cluster in namespace openldap . In ArgoCD, when you search for the openldap application, you should see a screen that look like this: Once, ArgoCD Synchs with the OpenShift cluster, you will see two deployments in the openldap project as follows: OpenLDAP has a admin UI to manage the users. Get the route and open the admin UI in a browser: oc get route openldap-admin -n openldap -o jsonpath='{ .spec.host }' The OpenLDAP admin UI will appear as follows: Login using Login DN credentials: cn=admin,dc=ibm,dc=com and password : admin Once, you login, you will see several users in the directory including the mqapp user which we will use in this guide. Activate cert-manager Operator Edit file 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the line: - argocd/operators/cert-manager.yaml Commit the change and push it back to the repo git add . git commit -m \"enable cert-manager operator\" git push origin master The cert-manager ArgoCD application will be created: The cert-manager operator will show up in the OpenShift cluster. Activate cert-manager instance Edit file 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the line: - argocd/instances/cert-manager-instance.yaml Commit the change and push it back to the repo git add . git commit -m \"enable cert-manager instance\" git push origin master The cert-manager instance ArgoCD application will be created: The cert-manager instance will show up in the OpenShift cluster. Activate Certificate Issuer and Certificates NOTE: ??? CHECK WITH HOLLIS. IS THIS GITOPS STRUCTURE STILL CORRECT???? Edit file 0-bootstrap/single-cluster/3-apps/kustomization.yaml and uncomment the line: - argocd/mq/tools.yaml Commit the change and push it back to the repo git add . git commit -m \"enable certificate issuer\" git push origin master Creates the following resources in the cluster: Resource type Resource name ClusterIssuer selfsigned-cluster-issuer Certificate mq-self-signed-ca-cert ClusterIssuer self-signed-mq-cluster-issuer Create JKS password secret NOTE: master branch now has the script and the sealed secret yaml to generate the mq-client-jks-password secret. https://github.com/mqpdg2/multi-tenancy-gitops-apps/tree/master/mq/environments/ci/secrets So I believe this step does not need to be in this guide. I believe that the secret will exist at this point at least in the ci namespace. Not sure of the other namespaces. Activate Certificates in ci namespace Edit file `mq/environments/ci/kustomization.yaml and uncomment the line: - certificates/ci-mq-client-certificate.yaml - certificates/ci-mq-server-certificate.yaml Commit the change and push it back to the repo git add . git commit -m \"create the mq client and server certificate in the ci namespace\" git push origin master Verify that the certificate was created successfully and the secret was generated by cert-manager. oc get certificate -n ci You should see the certificates with a Ready status of True as follows: NAME READY SECRET AGE ci-mq-client-cert True mq-client-jks 5d16h ci-mq-server-cert True mq-server-cert 5d16h The cert-manager operator generates the secrets from these certificates. Lets verify the secrets were generated correctly. Run the command: oc get secret mq-client-jks -n ci You should see the secret with a Data value of 5 representing 5 key value pairs as follows: NAME TYPE DATA AGE mq-client-jks kubernetes.io/tls 5 5d16h Run the command: oc get secret mq-server-cert -n ci You should see the secret with a Data value of 3 representing 3 key value pairs as follows: NAME TYPE DATA AGE mq-server-cert kubernetes.io/tls 3 5d17h Activate Certificates in dev namespace Edit file `mq/environments/dev/kustomization.yaml and uncomment the line: - certificates/dev-mq-client-certificate.yaml - certificates/dev-mq-server-certificate.yaml Commit the change and push it back to the repo git add . git commit -m \"create the mq client and server certificate in the dev namespace\" git push origin master Verify that the certificate was created successfully and the secret was generated by cert-manager. oc get certificate -n dev You should see the certificates with a Ready status of True as follows: NAME READY SECRET AGE dev-mq-client-cert True mq-client-jks 5d16h dev-mq-server-cert True mq-server-cert 5d16h The cert-manager operator generates the secrets from these certificates. Lets verify the secrets were generated correctly. Run the command: oc get secret mq-client-jks -n dev You should see the secret with a Data value of 5 representing 5 key value pairs as follows: NAME TYPE DATA AGE mq-client-jks kubernetes.io/tls 5 5d16h Run the command: oc get secret mq-server-cert -n ci You should see the secret with a Data value of 3 representing 3 key value pairs as follows: NAME TYPE DATA AGE mq-server-cert kubernetes.io/tls 3 5d17h Deploy/Create an mq manager instance with security enabled Edit the values.yaml file in the source mq-infra repo and change the value of the security to true as follows: security: true Commit the change and push it back to the repo git add . git commit -m \"Build and deploy the mq manager with security enabled\" git push origin master Run the mq-infra-dev pipeline. From the OpenShift console, go to Pipelines and set the Project to ci . You should see the mq-infra-dev pipeline as follows: Click on the mq-infra-dev pipeline link. Click on the Actions drop-down and select Start as follows: The Start Pipeline screen will pop up. Fill the git-url field and use the defaults for the other fields. Then click the Start button. Wait till the pipeline completes successfully. The final stage of the pipeline will update the gitops repo: multi-tenancy-gitops-apps/mq/environments/dev/mq-infra/ which will force ArgoCD to deploy the secure MQ manager to the dev project. Note: It may take a few minutes for ArgoCD to synch with the OpenShift cluster. Deploy mq-spring-app with security enabled Edit the values.yaml file in the source mq-spring-app repo and change the value of the security to true as follows: security: true Commit the change and push it back to the repo git add . git commit -m \"Build and deploy the mq-spring-app with security enabled\" git push origin master Run the mq-spring-app-dev pipeline. From the OpenShift console, go to Pipelines and set the Project to ci . You should see the mq-spring-app-dev pipeline as follows: Click on the mq-spring-dev pipeline link. Click on the Actions drop-down and select Start as follows: The Start Pipeline screen will pop up. Fill the git-url field and use the defaults for the other fields. Then click the Start button. Wait till the pipeline completes successfully. The final stage of the pipeline will update the gitops repo: multi-tenancy-gitops-apps/mq/environments/dev/mq-spring-app/ which will force ArgoCD to deploy the secure mq-spring-app client app to the dev project. Note: It may take a few minutes for ArgoCD to synch with the OpenShift cluster. Test the mq-spring-app Check the health of the app: export APP_URL=$(oc get route -n dev mq-spring-app -o jsonpath=\"{.spec.host}\") curl -X GET https://$APP_URL/actuator/health Invoke the api to put a message onto the queue: curl -X GET https://$APP_URL/api/send-hello-world Invoke the api to get a message from the queue: curl -X GET https://$APP_URL/api/recv To view the swagger docs of the mq-spring-app app, you can open a browser and enter the $APP_URL value.","title":"Configure mTLS and Delegated Authentication"},{"location":"guides/cp4s/cp4s-tree/security/information/container/","text":"Docker container Security \u00b6 As a best practice, each workload should run with its own unique service account. A service account is a Kubernetes resource that is used by Kubernetes entities such as pods to authenticate itself. By having a unique service account per workload, RBAC policies can be applied for each service account to limit the resources that a service account can access. The mq-spring-app workload is designed to run with a service account with the same name of mq-spring-app . When the helm chart gets deployed, the service account gets created. See: https://github.com/cloud-native-toolkit/mq-spring-app/blob/master/chart/base/templates/serviceaccount.yaml The service account manifest created is defined as follows: apiVersion: v1 kind: ServiceAccount metadata: name: mq-spring-app automountServiceAccountToken: false The automountServiceAccountToken: false will ensure that the service account secret containing the authentication token does not get mounted. This ensures, that a malicious user who gets access to the pod, cannot get the service account token and use it for accessing the cluster. The deployment resource is configured to run pods using the mq-spring-app service account. See: https://github.com/cloud-native-toolkit/mq-spring-app/blob/457fcd26564187397ed5de07cc1c37a1ead5faf8/chart/base/templates/deployment.yaml#L36 THe spec section of the deployment.yaml file has the following entry: spec: serviceAccountName: mq-spring-app","title":"Docker container Security"},{"location":"guides/cp4s/cp4s-tree/security/information/container/#docker-container-security","text":"As a best practice, each workload should run with its own unique service account. A service account is a Kubernetes resource that is used by Kubernetes entities such as pods to authenticate itself. By having a unique service account per workload, RBAC policies can be applied for each service account to limit the resources that a service account can access. The mq-spring-app workload is designed to run with a service account with the same name of mq-spring-app . When the helm chart gets deployed, the service account gets created. See: https://github.com/cloud-native-toolkit/mq-spring-app/blob/master/chart/base/templates/serviceaccount.yaml The service account manifest created is defined as follows: apiVersion: v1 kind: ServiceAccount metadata: name: mq-spring-app automountServiceAccountToken: false The automountServiceAccountToken: false will ensure that the service account secret containing the authentication token does not get mounted. This ensures, that a malicious user who gets access to the pod, cannot get the service account token and use it for accessing the cluster. The deployment resource is configured to run pods using the mq-spring-app service account. See: https://github.com/cloud-native-toolkit/mq-spring-app/blob/457fcd26564187397ed5de07cc1c37a1ead5faf8/chart/base/templates/deployment.yaml#L36 THe spec section of the deployment.yaml file has the following entry: spec: serviceAccountName: mq-spring-app","title":"Docker container Security"},{"location":"guides/cp4s/cp4s-tree/security/information/image/","text":"Docker image Security \u00b6 The docker image that is created when building the mq-spring-app addresses several security considerations. The git repo for the app contains a Dockerfile that is used to build the image. See the Dockerfile . This Dockerfile is a multistage file where the first stage builds the java code and the second stage builds the runtime image. The second stage of the Dockerfile looks as follows: FROM registry.access.redhat.com/ubi8/ubi:8.4 RUN dnf install -y java-11-openjdk.x86_64 COPY --from=builder /workspace/app/target/*.jar ./app.jar EXPOSE 8080/tcp USER 1001 CMD [\"java\", \"-jar\", \"./app.jar\"] Use a secure and trusted base images \u00b6 The base image used is the ubi8:8.4 (Red Hat Universal Base Image version 8.4) Red Hat UBI images are tested, maintained and contain the latest security patches. See the UBI eBook to understand the advantages of using UBI images. A blog by a leading security company also recommends using Red Hat UBI base images as they typically have much fewer security vulnerabilities. Do not run as a root user \u00b6 By default, if no USER is specified in a Dockerfile, the container will run as root on a Kubernetes platform. A security best practice is to specify a non root user in the Dockerfile. In our Dockerfile, we specify to run as USER 1001 which is a non root user. Note: OpenShift Container Platform, by default, does not allow containers to run as root but it is still a good practice to specify in the Dockerfile a non-root user. In fact OpenShift ignores the USER directive of the Dockerfile and launches the container with a random UUID for enhanced security.","title":"Docker image Security"},{"location":"guides/cp4s/cp4s-tree/security/information/image/#docker-image-security","text":"The docker image that is created when building the mq-spring-app addresses several security considerations. The git repo for the app contains a Dockerfile that is used to build the image. See the Dockerfile . This Dockerfile is a multistage file where the first stage builds the java code and the second stage builds the runtime image. The second stage of the Dockerfile looks as follows: FROM registry.access.redhat.com/ubi8/ubi:8.4 RUN dnf install -y java-11-openjdk.x86_64 COPY --from=builder /workspace/app/target/*.jar ./app.jar EXPOSE 8080/tcp USER 1001 CMD [\"java\", \"-jar\", \"./app.jar\"]","title":"Docker image Security"},{"location":"guides/cp4s/cp4s-tree/security/information/image/#use-a-secure-and-trusted-base-images","text":"The base image used is the ubi8:8.4 (Red Hat Universal Base Image version 8.4) Red Hat UBI images are tested, maintained and contain the latest security patches. See the UBI eBook to understand the advantages of using UBI images. A blog by a leading security company also recommends using Red Hat UBI base images as they typically have much fewer security vulnerabilities.","title":"Use a secure and trusted base images"},{"location":"guides/cp4s/cp4s-tree/security/information/image/#do-not-run-as-a-root-user","text":"By default, if no USER is specified in a Dockerfile, the container will run as root on a Kubernetes platform. A security best practice is to specify a non root user in the Dockerfile. In our Dockerfile, we specify to run as USER 1001 which is a non root user. Note: OpenShift Container Platform, by default, does not allow containers to run as root but it is still a good practice to specify in the Dockerfile a non-root user. In fact OpenShift ignores the USER directive of the Dockerfile and launches the container with a random UUID for enhanced security.","title":"Do not run as a root user"},{"location":"guides/cp4s/cp4s-tree/security/information/mtls/","text":"mTLS Security \u00b6 This section will explain mTLS (mutual TLS) security between the mq-spring-app and the mq queue manager. Mutual TLS encrypts the data in motion between the app and the queue manager by exchanging and validating each others certificates that are issued by a certificate authority. The mq manager also authenticates the mq-spring-app leveraging the OpenLDAP directory server. The diagram below shows the key components in the solution. The key parts of the solution are as follows: OpenLDAP - Directory server for authenticating users and applications. See: https://www.openldap.org/ cert-manager - a Kubernetes add-on operator to automate the issuance and management of TLS certificates from various issuing sources. See: https://github.com/jetstack/cert-manager sealed-secrets - a Kubernetes add-on operator to securely manage kubernetes secrets. See: https://github.com/bitnami-labs/sealed-secrets mq-self-signed-issuer ClusterIssuer resource - Issues certificates to the mq manager and the mq-spring-app components. It is configured to use the mq-self-signed-ca-cert to sign the issued certificates. mq-self-signed-ca-cert Certificate resource - generates the self signed root CA certificate. This certificate is used to sign all certificates issued by mq-self-signed-issuer . mq-server-certificate Certificate resource - cert-manager custom resource that generates the mq-server-cert secret that contains certificate, private key and CA cert. This is used by the mq queue manager to establish the mTLS connection with the mq-spring-app . mq-client-certificate Certificate resource - cert-manager custom resource that generates the mq-client-cert secret that contains keystore and truststore jks files. The keystore.jks file contains the certificate used by the mq-spring-app to establish the mTLS connection with the mq queue manager. The truststore.jks file contains the certificate used validate that a certificate is signed by the root CA. mq-client-jks-password sealed secret - generates mq-client-jks-password secret that contains the password to access the jks files. This is used by the mq-spring-app microservice. mq-spring-app sealed secret resource - sealed secret that generates the user and password credentials in the mq-spring-app secret for the client app to authenticate with the mq queue manager. mq-spring-app - Spring Boot java application that has REST endpoints to interact with the mq queue manager. mq - IBM MQ queue manager mqsc-configMap - configMap resource that contains mqsc commands that configures the queue manager. From a security perspective, the MQ manager is configured to delegate authentication to the OpenLDAP server as well as configuring the the mq queue manager to allow mTLS connections. During the establishment of the mTLS connection between the mq-spring-app and the mq manager, the mq-spring-app validates the tls.crt certificate that it receives from the mq manager. The mq-spring-app is able to validate the certificate using the truststore.jks file it has access to via the mq-client-jks secret. The truststore.jks file has the ca certificate of the issuer. Similarly, the mq-spring-app sends the mq manager its certificate which is stored within the keystore.jks file in the mq-client-jks Secret. The mq manager is able to validate it using the ca certificate of the issuer (ca.crt value in mq-server-cert secret). In addition, the mq manager is configured to allow connections for user mqapp . The mq-spring-app has a secret configured that contains values for the mqapp user and the corresponding password. The mq queue manager delegates the user authentication to the OpenLDAP server. The spring app gets these values injected via the application.yml file as follows: ibm: mq: user: ${USER} password: ${PASSWORD}","title":"mTLS Security"},{"location":"guides/cp4s/cp4s-tree/security/information/mtls/#mtls-security","text":"This section will explain mTLS (mutual TLS) security between the mq-spring-app and the mq queue manager. Mutual TLS encrypts the data in motion between the app and the queue manager by exchanging and validating each others certificates that are issued by a certificate authority. The mq manager also authenticates the mq-spring-app leveraging the OpenLDAP directory server. The diagram below shows the key components in the solution. The key parts of the solution are as follows: OpenLDAP - Directory server for authenticating users and applications. See: https://www.openldap.org/ cert-manager - a Kubernetes add-on operator to automate the issuance and management of TLS certificates from various issuing sources. See: https://github.com/jetstack/cert-manager sealed-secrets - a Kubernetes add-on operator to securely manage kubernetes secrets. See: https://github.com/bitnami-labs/sealed-secrets mq-self-signed-issuer ClusterIssuer resource - Issues certificates to the mq manager and the mq-spring-app components. It is configured to use the mq-self-signed-ca-cert to sign the issued certificates. mq-self-signed-ca-cert Certificate resource - generates the self signed root CA certificate. This certificate is used to sign all certificates issued by mq-self-signed-issuer . mq-server-certificate Certificate resource - cert-manager custom resource that generates the mq-server-cert secret that contains certificate, private key and CA cert. This is used by the mq queue manager to establish the mTLS connection with the mq-spring-app . mq-client-certificate Certificate resource - cert-manager custom resource that generates the mq-client-cert secret that contains keystore and truststore jks files. The keystore.jks file contains the certificate used by the mq-spring-app to establish the mTLS connection with the mq queue manager. The truststore.jks file contains the certificate used validate that a certificate is signed by the root CA. mq-client-jks-password sealed secret - generates mq-client-jks-password secret that contains the password to access the jks files. This is used by the mq-spring-app microservice. mq-spring-app sealed secret resource - sealed secret that generates the user and password credentials in the mq-spring-app secret for the client app to authenticate with the mq queue manager. mq-spring-app - Spring Boot java application that has REST endpoints to interact with the mq queue manager. mq - IBM MQ queue manager mqsc-configMap - configMap resource that contains mqsc commands that configures the queue manager. From a security perspective, the MQ manager is configured to delegate authentication to the OpenLDAP server as well as configuring the the mq queue manager to allow mTLS connections. During the establishment of the mTLS connection between the mq-spring-app and the mq manager, the mq-spring-app validates the tls.crt certificate that it receives from the mq manager. The mq-spring-app is able to validate the certificate using the truststore.jks file it has access to via the mq-client-jks secret. The truststore.jks file has the ca certificate of the issuer. Similarly, the mq-spring-app sends the mq manager its certificate which is stored within the keystore.jks file in the mq-client-jks Secret. The mq manager is able to validate it using the ca certificate of the issuer (ca.crt value in mq-server-cert secret). In addition, the mq manager is configured to allow connections for user mqapp . The mq-spring-app has a secret configured that contains values for the mqapp user and the corresponding password. The mq queue manager delegates the user authentication to the OpenLDAP server. The spring app gets these values injected via the application.yml file as follows: ibm: mq: user: ${USER} password: ${PASSWORD}","title":"mTLS Security"},{"location":"guides/cp4s/cp4s-tree/security/information/overview/","text":"App Connect Enterprise \u00b6 Table of Contents mTLS security Container security Image security mTLS Security \u00b6 Describes the mTLS security concepts for establishing a secure connection between a mq queue manager and a sample client application. Please see the mTLS security page for further details. Container Security \u00b6 Describes best practices for running containers securely on OpenShift. Please see the container security page for further details. Image Security \u00b6 Describes best practices for creating secure container images. Please see the image security page for further details.","title":"App Connect Enterprise"},{"location":"guides/cp4s/cp4s-tree/security/information/overview/#app-connect-enterprise","text":"Table of Contents mTLS security Container security Image security","title":"App Connect Enterprise"},{"location":"guides/cp4s/cp4s-tree/security/information/overview/#mtls-security","text":"Describes the mTLS security concepts for establishing a secure connection between a mq queue manager and a sample client application. Please see the mTLS security page for further details.","title":"mTLS Security"},{"location":"guides/cp4s/cp4s-tree/security/information/overview/#container-security","text":"Describes best practices for running containers securely on OpenShift. Please see the container security page for further details.","title":"Container Security"},{"location":"guides/cp4s/cp4s-tree/security/information/overview/#image-security","text":"Describes best practices for creating secure container images. Please see the image security page for further details.","title":"Image Security"},{"location":"guides/cp4s/cp4s-tree/using/this-guide/","text":"The CP4S tutorials \u00b6 Audience : Architects, Application developers, Administrators Overview \u00b6 In this topic we're going to: Introduce you to the CP4S tutorials Outline the structure of the tutorials By the end of this topic you'll understand the CP4S tutorials, and how they can help you design and build cloud native CP4S systems. Introduction \u00b6 The purpose of the CP4S tutorials is to teach architects, developers and operations staff how to implement a production-ready CP4S deployment on the OpenShift Container Platform . It makes extensive use of the IBM Cloud Pak for Integration (CP4S) and other cloud native technologies such as Tekton, Kustomize, ArgoCD, Prometheus, Grafana and Kibana. Cloud native systems are loosely coupled, making them resilient, manageable, and observable. A cloud native systems brings robust DevOps automation which allows CP4S developers, administrators and architects to make high quality changes with predictability and minimal toil as frequently as their organization requires. Using worked examples, you will build and run a modern CP4S deployment according to cloud native principles, gaining hands-on experience of these technologies and their benefits. The tutorial is structured as a set of tutorials and it is recommended that you follow them in order. Earlier tutorials are foundational, whereas later ones such as high availability and disaster recovery cover advanced features. If you already have a good working knowledge of CP4S and OpenShift, these more advanced topics can be attempted stand-alone. There is an intuitive table of contents. Next and Previous links at the bottom of each page to help you navigate. The tutorial is intended as a complement to CP4S product documentation; where relevant the tutorial will refer to it and other documents. Tutorial structure \u00b6 This tutorial is divided into chapters that help you build, step-by-step, a set of working CP4S deployments, based on a cloud native approach. Each chapter comprises topics which you should complete in order to gain a full understanding. The tutorial starts with a simple CP4S application and queue manager. It progresses to CP4S uniform cluster, CP4S NativeHA and multiple availability zone and region configurations. The tutorial is structured to match the development and operations lifecycle: Install and upgrade Build and test Deployment Promotion Security Monitoring Scalability Performance High Availability Disaster recovery CPU, memory, network utilization Server consolidation See how the table of contents on the left of this tutorial is arranged to match this lifecycle. Architecture decisions \u00b6 Throughout the tutorial you'll see alternative implementation choices: Info Baking : Extending the official CP4S image for the purpose of embedding configuration or adding third party binaries. Baking happens at build time. Frying : Using a ConfigMap to add configuration information to an existing CP4S image. Frying happens at deployment time. More The inline text gives a summary of the architecture decision used to make the technical choice. The link provides a full explanation of the architecture decision. All architecture decisions are described in the Architecture decisions registry .","title":"The CP4S tutorial"},{"location":"guides/cp4s/cp4s-tree/using/this-guide/#the-cp4s-tutorials","text":"Audience : Architects, Application developers, Administrators","title":"The CP4S tutorials"},{"location":"guides/cp4s/cp4s-tree/using/this-guide/#overview","text":"In this topic we're going to: Introduce you to the CP4S tutorials Outline the structure of the tutorials By the end of this topic you'll understand the CP4S tutorials, and how they can help you design and build cloud native CP4S systems.","title":"Overview"},{"location":"guides/cp4s/cp4s-tree/using/this-guide/#introduction","text":"The purpose of the CP4S tutorials is to teach architects, developers and operations staff how to implement a production-ready CP4S deployment on the OpenShift Container Platform . It makes extensive use of the IBM Cloud Pak for Integration (CP4S) and other cloud native technologies such as Tekton, Kustomize, ArgoCD, Prometheus, Grafana and Kibana. Cloud native systems are loosely coupled, making them resilient, manageable, and observable. A cloud native systems brings robust DevOps automation which allows CP4S developers, administrators and architects to make high quality changes with predictability and minimal toil as frequently as their organization requires. Using worked examples, you will build and run a modern CP4S deployment according to cloud native principles, gaining hands-on experience of these technologies and their benefits. The tutorial is structured as a set of tutorials and it is recommended that you follow them in order. Earlier tutorials are foundational, whereas later ones such as high availability and disaster recovery cover advanced features. If you already have a good working knowledge of CP4S and OpenShift, these more advanced topics can be attempted stand-alone. There is an intuitive table of contents. Next and Previous links at the bottom of each page to help you navigate. The tutorial is intended as a complement to CP4S product documentation; where relevant the tutorial will refer to it and other documents.","title":"Introduction"},{"location":"guides/cp4s/cp4s-tree/using/this-guide/#tutorial-structure","text":"This tutorial is divided into chapters that help you build, step-by-step, a set of working CP4S deployments, based on a cloud native approach. Each chapter comprises topics which you should complete in order to gain a full understanding. The tutorial starts with a simple CP4S application and queue manager. It progresses to CP4S uniform cluster, CP4S NativeHA and multiple availability zone and region configurations. The tutorial is structured to match the development and operations lifecycle: Install and upgrade Build and test Deployment Promotion Security Monitoring Scalability Performance High Availability Disaster recovery CPU, memory, network utilization Server consolidation See how the table of contents on the left of this tutorial is arranged to match this lifecycle.","title":"Tutorial structure"},{"location":"guides/cp4s/cp4s-tree/using/this-guide/#architecture-decisions","text":"Throughout the tutorial you'll see alternative implementation choices: Info Baking : Extending the official CP4S image for the purpose of embedding configuration or adding third party binaries. Baking happens at build time. Frying : Using a ConfigMap to add configuration information to an existing CP4S image. Frying happens at deployment time. More The inline text gives a summary of the architecture decision used to make the technical choice. The link provides a full explanation of the architecture decision. All architecture decisions are described in the Architecture decisions registry .","title":"Architecture decisions"},{"location":"infrastructure/aro/","text":"Azure Red Hat OpenShift (ARO) \u00b6 Overview This repository document experiences on working with ARO cluster on OpenShift 4.6 environment 2Q 2021 deployed on a ARO For additional information, refer to ARO documentation . Prerequisites \u00b6 To work with ARO, you need the following: Download the azure command-line utility. You will need version 2.6.0 or later. # On macOS $ brew update $ brew install azure-cli # on Linux $ curl -L https://aka.ms/InstallAzureCli | bash Download the jq command-line utility. # On macOS $ brew update $ brew install jq # on Ubuntu Linux $ sudo apt-get install jq # On Fedora $ sudo dnf isntall jq # Other OS # reference https://stedolan.github.io/jq/download/ for more options Verify the account deploying ARO has the User Access Administrator and Contributor permissions. Log on to your Azure Subscription # login to your azure subscription $ az login # If you have more than one subscription, set the subscription you want to use $ az account set --subscription <SUBSCRIPTION ID> Register Resource Providers \u00b6 This is a one-time setup that needs to be performed on the subscription. az provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait Set up Environment Variables \u00b6 LOCATION = eastus # the location of your cluster CLUSTER = prodref # the name of your cluster RESOURCEGROUP = aro- ${ CLUSTER } -rg # the name of the resource group where you want to create your cluster Create VNET Infrastructure \u00b6 In order to create a VNET that will meet our Golden Topology requirements, follow the procedure outlined bellow Create Resource Group . This resource group will host a single vnet, which in turn will contain two subnets, one for master nodes and one for worker nodes az group create \\ --name $RESOURCEGROUP \\ --location $LOCATION Create a VNET az network vnet create --resource-group $RESOURCEGROUP \\ --name $CLUSTER -vnet \\ --address-prefixes 10 .0.0.0/22 Create a subnet for control plane nodes az network vnet subnet create --resource-group $RESOURCEGROUP \\ --vnet-name $CLUSTER -vnet \\ --name master-subnet \\ --address-prefixes 10 .0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create a subnet for worker nodes az network vnet subnet create --resource-group $RESOURCEGROUP \\ --vnet-name $CLUSTER -vnet \\ --name worker-subnet \\ --address-prefixes 10 .0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Warning Infrastructure nodes are not required on a Managed Platform like ARO Disable subnet private endpoint policies on master subnet . This is required for the service to be able to connect to and manage the cluster. az network vnet subnet update \\ --name master-subnet \\ --resource-group $RESOURCEGROUP \\ --vnet-name $CLUSTER -vnet \\ --disable-private-link-service-network-policies true Create your cluster \u00b6 Run the following command to create a cluster. If you choose to use either of the following options, modify the command accordingly: Optionally, you can pass your Red Hat pull secret which enables your cluster to access Red Hat container registries along with additional content. Add the --pull-secret @pull-secret.txt argument to your command. Optionally, you can use a custom domain. Add the --domain foo.example.com argument to your command, replacing foo.example.com with your own custom domain. Optionally, you can deploy private endpoints for Cluster API ( --apiserver-visibility Private ) and Ingress ( --ingress-visibility Private ). Defaults are set to Public for both. Optionally, you can use pre-existing VNET and subnets for your cluster in separate resource groups ( --vnet-resource-group my-vnet-rg ) Optionally, you can configure the OpenShift SDN with the following flags: Cluster Network: Use --pod-cidr x.x.x.x/n to set the desired clusterNetwork CIDR Service Network: Use --service-cidr y.y.y.y/n to set the desired serviceNetwork CIDR Optionally, you can configure your master nodes with the following flags: VM Size: Use --master-vm-size XXXX for different compute profile. The default is Standard_D8_v3 Optionally, you can configure your worker nodes with the following flags: VM Size: Use --worker-vm-size XXXX for different compute profile. The default is Standard_D8_v3 Worker Count: Use --worker-count X to set the number of desired worker nodes. The default is 3 worker nodes, one per Availability Zone. Worker Storage Size: Use --worker-vm-disk-size-gb XXXX to set your desired worker storage size. The default is 128 Gb. Optionally, you can use a pre-generated Service Principal for your deployment: Client ID: Use the --client-id xxxxx-xxxxx-xxxxx-xxxxx flag to specify the ClientID or AppId Client Secret: Use the --client-secret xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx flag to specify the password for client-id az aro create \\ --resource-group $RESOURCEGROUP \\ --name $CLUSTER \\ --vnet $CLUSTER -vnet \\ --master-subnet master-subnet \\ --worker-subnet worker-subnet Warning If you don't specify your --pull-secret your cluster will not include samples or operators from Red Hat or from certified partners. If you specify the --domain foo.example.com parameter you are responsible for creating the api and *.apps DNS A or CNAME records . Accessing your cluster \u00b6 You can obtain your cluster endpoints and kubeadmin password by querying with the az cli. Cluster API Endpoint \u200b bash az aro show --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"apiserverProfile.url\" -o tsv Kubeadmin Password az aro list-credentials --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"kubeadminPassword\" -o tsv ) OpenShift Web Console az aro show --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"consoleProfile.url\" -o tsv Using a custom domain \u00b6 If you use a custom domain you must create 2 DNS A records in your DNS server for the --domain specified: api - points to the API Server *.apps - points to the Ingress Once your cluster is up, run the following commands to obtain your api and *.apps endpoints and update your DNS zone. # obtain IP Address for API Server az aro show --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"apiserverProfile.ip\" -o tsv # obtain IP Address for Ingress az aro show --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"ingressProfiles[0].ip\" -o tsv Creating a ReadWriteMany storage class using azure-file (SMB) \u00b6 Warning The default azure-file backed storage is not POSIX compliant. An NFS backed POSIX compliant azure-file storage is currently in Tech Preview. Please see here for implementation details. By default, your ARO cluster will contain the storage class managed-premium that can dynamically provision ReadWriteOnce PersistentVolumes using azure-disk . If your CloudPak requires a RWX storage class, you can create one using azure-file with the following process: Obtain the managed resource group for your ARO cluster. This is not the same as the $RESOURCEGROUP that we've used to deploy other components into. MANAGED_RG = \"aro- $( az aro show \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"clusterProfile.domain\" -o tsv ) \" Warning If you deploy with a custom domain ( --domain foo.example.com ), the value of clusterProfile.domain will be your domain FQDN. Log on to your azure portal, find the aro-$RANDOM resource group for your cluster, and manually set the MANAGED_RG variable Obtain the Storage Account created in the Managed Resource Group MANAGED_SA = $( az storage account list \\ --resource-group $MANAGED_RG \\ --query \"[?starts_with(name, 'cluster')].name\" -o tsv ) Log on to your openshift cluster via command line APISERVER = $( az aro show --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"apiserverProfile.url\" -o tsv ) PASSWORD = $( az aro list-credentials --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"kubeadminPassword\" -o tsv ) oc login $APISERVER -u kubeadmin -p $PASSWORD --insecure-skip-tls-verify Create a ClusterRole to access secrets cat << EOF | oc create -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:azure-file-volume-binder rules: - apiGroups: [''] resources: ['secrets'] verbs: ['get','create'] EOF Assign ClusterRole to a ServiceAccount oc adm policy add-cluster-role-to-user \\ system:azure-file-volume-binder \\ system:serviceaccount:kube-system:persistent-volume-binder Create the azure-file storage class cat << EOF | oc create -f - kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: azure-file provisioner: kubernetes.io/azure-file parameters: storageAccount: $MANAGED_SA reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true EOF Integrating with Azure AD \u00b6 Set up Environment Variables SP_AAD_PASSWORD = 'Y0urS3cr3tPa55w@rd' AZURE_TENNANT_ID = $( az account show --query tenantId -o tsv ) CONSOLE_URL = $( az aro show --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"consoleProfile.url\" -o tsv ) OAUTH_ENDPOINT = $( echo $CONSOLE_URL | sed 's/console-openshift-console/oauth-openshift/' ) Log on to your openshift cluster via command line (if needed) APISERVER = $( az aro show --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"apiserverProfile.url\" -o tsv ) PASSWORD = $( az aro list-credentials \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"kubeadminPassword\" -o tsv ) oc login $APISERVER -u kubeadmin -p $PASSWORD --insecure-skip-tls-verify Create a secret for Azure AD integration oc create secret generic openid-client-secret-azuread \\ --from-literal = clientSecret = $SP_AAD_PASSWORD \\ -n openshift-config Create an App Registration with callbacks to your console URL AZUREAD_APPID = $( az ad app create --display-name $CLUSTER -azuread-auth \\ --reply-urls $OAUTH_ENDPOINT /oauth2callback/AAD \\ --homepage $CONSOLE_URL \\ --identifier-uris $CONSOLE_URL \\ --password $SP_AAD_PASSWORD \\ --query appId -o tsv ) Update app optionalClaims cat > /tmp/manifest.json << EOF [{ \"name\": \"upn\", \"source\": null, \"essential\": false, \"additionalProperties\": [] }, { \"name\": \"email\", \"source\": null, \"essential\": false, \"additionalProperties\": [] }] EOF az ad app update \\ --set optionalClaims.idToken = @/tmp/manifest.json \\ --id $AZUREAD_APPID Update AAD app scope permissions az ad app permission add \\ --api 00000002 -0000-0000-c000-000000000000 \\ --api-permissions 311a71cc-e848-46a1-bdf8-97ff7156d8e6 = Scope \\ --id $AZUREAD_APPID az ad app permission grant \\ --id c89c64ce-2061-4c51-846a-d318a07e40fa \\ --api 00000002 -0000-0000-c000-000000000000 Warning You may need to ask a subscription administrator with Active Directory Graph access to run az ad app permission grant --id c89c64ce-2061-4c51-846a-d318a07e40fa --api 00000002-0000-0000-c000-000000000000 for you Update OpenShift OAuth cat << EOF | oc replace -f - apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: AAD mappingMethod: claim type: OpenID openID: clientID: ${AZUREAD_APPID} clientSecret: name: openid-client-secret-azuread extraScopes: - email - profile extraAuthorizeParameters: include_granted_scopes: \"true\" claims: preferredUsername: - email - upn name: - name email: - email issuer: https://login.microsoftonline.com/${AZURE_TENNANT_ID} EOF Cleanup \u00b6 Remove your cluster and all associated resources from your Azure subscription az aro delete -g $RESOURCEGROUP -n $CLUSTER -y az ad app delete --id $AZUREAD_APPID az group delete -g $RESOURCEGROUP --no-wait -y","title":"Microsoft ARO"},{"location":"infrastructure/aro/#azure-red-hat-openshift-aro","text":"Overview This repository document experiences on working with ARO cluster on OpenShift 4.6 environment 2Q 2021 deployed on a ARO For additional information, refer to ARO documentation .","title":"Azure Red Hat OpenShift (ARO)"},{"location":"infrastructure/aro/#prerequisites","text":"To work with ARO, you need the following: Download the azure command-line utility. You will need version 2.6.0 or later. # On macOS $ brew update $ brew install azure-cli # on Linux $ curl -L https://aka.ms/InstallAzureCli | bash Download the jq command-line utility. # On macOS $ brew update $ brew install jq # on Ubuntu Linux $ sudo apt-get install jq # On Fedora $ sudo dnf isntall jq # Other OS # reference https://stedolan.github.io/jq/download/ for more options Verify the account deploying ARO has the User Access Administrator and Contributor permissions. Log on to your Azure Subscription # login to your azure subscription $ az login # If you have more than one subscription, set the subscription you want to use $ az account set --subscription <SUBSCRIPTION ID>","title":"Prerequisites"},{"location":"infrastructure/aro/#register-resource-providers","text":"This is a one-time setup that needs to be performed on the subscription. az provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait","title":"Register Resource Providers"},{"location":"infrastructure/aro/#set-up-environment-variables","text":"LOCATION = eastus # the location of your cluster CLUSTER = prodref # the name of your cluster RESOURCEGROUP = aro- ${ CLUSTER } -rg # the name of the resource group where you want to create your cluster","title":"Set up Environment Variables"},{"location":"infrastructure/aro/#create-vnet-infrastructure","text":"In order to create a VNET that will meet our Golden Topology requirements, follow the procedure outlined bellow Create Resource Group . This resource group will host a single vnet, which in turn will contain two subnets, one for master nodes and one for worker nodes az group create \\ --name $RESOURCEGROUP \\ --location $LOCATION Create a VNET az network vnet create --resource-group $RESOURCEGROUP \\ --name $CLUSTER -vnet \\ --address-prefixes 10 .0.0.0/22 Create a subnet for control plane nodes az network vnet subnet create --resource-group $RESOURCEGROUP \\ --vnet-name $CLUSTER -vnet \\ --name master-subnet \\ --address-prefixes 10 .0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create a subnet for worker nodes az network vnet subnet create --resource-group $RESOURCEGROUP \\ --vnet-name $CLUSTER -vnet \\ --name worker-subnet \\ --address-prefixes 10 .0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Warning Infrastructure nodes are not required on a Managed Platform like ARO Disable subnet private endpoint policies on master subnet . This is required for the service to be able to connect to and manage the cluster. az network vnet subnet update \\ --name master-subnet \\ --resource-group $RESOURCEGROUP \\ --vnet-name $CLUSTER -vnet \\ --disable-private-link-service-network-policies true","title":"Create VNET Infrastructure"},{"location":"infrastructure/aro/#create-your-cluster","text":"Run the following command to create a cluster. If you choose to use either of the following options, modify the command accordingly: Optionally, you can pass your Red Hat pull secret which enables your cluster to access Red Hat container registries along with additional content. Add the --pull-secret @pull-secret.txt argument to your command. Optionally, you can use a custom domain. Add the --domain foo.example.com argument to your command, replacing foo.example.com with your own custom domain. Optionally, you can deploy private endpoints for Cluster API ( --apiserver-visibility Private ) and Ingress ( --ingress-visibility Private ). Defaults are set to Public for both. Optionally, you can use pre-existing VNET and subnets for your cluster in separate resource groups ( --vnet-resource-group my-vnet-rg ) Optionally, you can configure the OpenShift SDN with the following flags: Cluster Network: Use --pod-cidr x.x.x.x/n to set the desired clusterNetwork CIDR Service Network: Use --service-cidr y.y.y.y/n to set the desired serviceNetwork CIDR Optionally, you can configure your master nodes with the following flags: VM Size: Use --master-vm-size XXXX for different compute profile. The default is Standard_D8_v3 Optionally, you can configure your worker nodes with the following flags: VM Size: Use --worker-vm-size XXXX for different compute profile. The default is Standard_D8_v3 Worker Count: Use --worker-count X to set the number of desired worker nodes. The default is 3 worker nodes, one per Availability Zone. Worker Storage Size: Use --worker-vm-disk-size-gb XXXX to set your desired worker storage size. The default is 128 Gb. Optionally, you can use a pre-generated Service Principal for your deployment: Client ID: Use the --client-id xxxxx-xxxxx-xxxxx-xxxxx flag to specify the ClientID or AppId Client Secret: Use the --client-secret xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx flag to specify the password for client-id az aro create \\ --resource-group $RESOURCEGROUP \\ --name $CLUSTER \\ --vnet $CLUSTER -vnet \\ --master-subnet master-subnet \\ --worker-subnet worker-subnet Warning If you don't specify your --pull-secret your cluster will not include samples or operators from Red Hat or from certified partners. If you specify the --domain foo.example.com parameter you are responsible for creating the api and *.apps DNS A or CNAME records .","title":"Create your cluster"},{"location":"infrastructure/aro/#accessing-your-cluster","text":"You can obtain your cluster endpoints and kubeadmin password by querying with the az cli. Cluster API Endpoint \u200b bash az aro show --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"apiserverProfile.url\" -o tsv Kubeadmin Password az aro list-credentials --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"kubeadminPassword\" -o tsv ) OpenShift Web Console az aro show --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"consoleProfile.url\" -o tsv","title":"Accessing your cluster"},{"location":"infrastructure/aro/#using-a-custom-domain","text":"If you use a custom domain you must create 2 DNS A records in your DNS server for the --domain specified: api - points to the API Server *.apps - points to the Ingress Once your cluster is up, run the following commands to obtain your api and *.apps endpoints and update your DNS zone. # obtain IP Address for API Server az aro show --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"apiserverProfile.ip\" -o tsv # obtain IP Address for Ingress az aro show --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"ingressProfiles[0].ip\" -o tsv","title":"Using a custom domain"},{"location":"infrastructure/aro/#creating-a-readwritemany-storage-class-using-azure-file-smb","text":"Warning The default azure-file backed storage is not POSIX compliant. An NFS backed POSIX compliant azure-file storage is currently in Tech Preview. Please see here for implementation details. By default, your ARO cluster will contain the storage class managed-premium that can dynamically provision ReadWriteOnce PersistentVolumes using azure-disk . If your CloudPak requires a RWX storage class, you can create one using azure-file with the following process: Obtain the managed resource group for your ARO cluster. This is not the same as the $RESOURCEGROUP that we've used to deploy other components into. MANAGED_RG = \"aro- $( az aro show \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"clusterProfile.domain\" -o tsv ) \" Warning If you deploy with a custom domain ( --domain foo.example.com ), the value of clusterProfile.domain will be your domain FQDN. Log on to your azure portal, find the aro-$RANDOM resource group for your cluster, and manually set the MANAGED_RG variable Obtain the Storage Account created in the Managed Resource Group MANAGED_SA = $( az storage account list \\ --resource-group $MANAGED_RG \\ --query \"[?starts_with(name, 'cluster')].name\" -o tsv ) Log on to your openshift cluster via command line APISERVER = $( az aro show --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"apiserverProfile.url\" -o tsv ) PASSWORD = $( az aro list-credentials --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"kubeadminPassword\" -o tsv ) oc login $APISERVER -u kubeadmin -p $PASSWORD --insecure-skip-tls-verify Create a ClusterRole to access secrets cat << EOF | oc create -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:azure-file-volume-binder rules: - apiGroups: [''] resources: ['secrets'] verbs: ['get','create'] EOF Assign ClusterRole to a ServiceAccount oc adm policy add-cluster-role-to-user \\ system:azure-file-volume-binder \\ system:serviceaccount:kube-system:persistent-volume-binder Create the azure-file storage class cat << EOF | oc create -f - kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: azure-file provisioner: kubernetes.io/azure-file parameters: storageAccount: $MANAGED_SA reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true EOF","title":"Creating a ReadWriteMany storage class using azure-file (SMB)"},{"location":"infrastructure/aro/#integrating-with-azure-ad","text":"Set up Environment Variables SP_AAD_PASSWORD = 'Y0urS3cr3tPa55w@rd' AZURE_TENNANT_ID = $( az account show --query tenantId -o tsv ) CONSOLE_URL = $( az aro show --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"consoleProfile.url\" -o tsv ) OAUTH_ENDPOINT = $( echo $CONSOLE_URL | sed 's/console-openshift-console/oauth-openshift/' ) Log on to your openshift cluster via command line (if needed) APISERVER = $( az aro show --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"apiserverProfile.url\" -o tsv ) PASSWORD = $( az aro list-credentials \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"kubeadminPassword\" -o tsv ) oc login $APISERVER -u kubeadmin -p $PASSWORD --insecure-skip-tls-verify Create a secret for Azure AD integration oc create secret generic openid-client-secret-azuread \\ --from-literal = clientSecret = $SP_AAD_PASSWORD \\ -n openshift-config Create an App Registration with callbacks to your console URL AZUREAD_APPID = $( az ad app create --display-name $CLUSTER -azuread-auth \\ --reply-urls $OAUTH_ENDPOINT /oauth2callback/AAD \\ --homepage $CONSOLE_URL \\ --identifier-uris $CONSOLE_URL \\ --password $SP_AAD_PASSWORD \\ --query appId -o tsv ) Update app optionalClaims cat > /tmp/manifest.json << EOF [{ \"name\": \"upn\", \"source\": null, \"essential\": false, \"additionalProperties\": [] }, { \"name\": \"email\", \"source\": null, \"essential\": false, \"additionalProperties\": [] }] EOF az ad app update \\ --set optionalClaims.idToken = @/tmp/manifest.json \\ --id $AZUREAD_APPID Update AAD app scope permissions az ad app permission add \\ --api 00000002 -0000-0000-c000-000000000000 \\ --api-permissions 311a71cc-e848-46a1-bdf8-97ff7156d8e6 = Scope \\ --id $AZUREAD_APPID az ad app permission grant \\ --id c89c64ce-2061-4c51-846a-d318a07e40fa \\ --api 00000002 -0000-0000-c000-000000000000 Warning You may need to ask a subscription administrator with Active Directory Graph access to run az ad app permission grant --id c89c64ce-2061-4c51-846a-d318a07e40fa --api 00000002-0000-0000-c000-000000000000 for you Update OpenShift OAuth cat << EOF | oc replace -f - apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: AAD mappingMethod: claim type: OpenID openID: clientID: ${AZUREAD_APPID} clientSecret: name: openid-client-secret-azuread extraScopes: - email - profile extraAuthorizeParameters: include_granted_scopes: \"true\" claims: preferredUsername: - email - upn name: - name email: - email issuer: https://login.microsoftonline.com/${AZURE_TENNANT_ID} EOF","title":"Integrating with Azure AD"},{"location":"infrastructure/aro/#cleanup","text":"Remove your cluster and all associated resources from your Azure subscription az aro delete -g $RESOURCEGROUP -n $CLUSTER -y az ad app delete --id $AZUREAD_APPID az group delete -g $RESOURCEGROUP --no-wait -y","title":"Cleanup"},{"location":"infrastructure/aws/","text":"Production Deployment Guide for OpenShift on Amazon Web Services (AWS) \u00b6 Golden Topology \u00b6 Before reading this document, familiarize yourself with the Golden Topology section of this Production Deployment Guide. Unless otherwise stated, all the items in that section apply to the Cluster Infrastructure we will provision on AWS. Read Red Hat's documentation on Installing a cluster quickly on AWS to deploy a cluster with no modifications. Deploying a cluster with openshift-install will provide the following topology. MachineSets \u00b6 The following templates can be used to generate your infrastructure, cloudpak and storage MachineSets MachineSets Note Click on each tab above for sample yamls to create your infrastructure Infrastructure Infrastructure MachineConfigPool apiVersion : machineconfiguration.openshift.io/v1 kind : MachineConfigPool metadata : name : infra spec : machineConfigSelector : matchExpressions : - key : machineconfiguration.openshift.io/role operator : In values : - worker - infra nodeSelector : matchLabels : node-role.kubernetes.io/infra : \"\" Infrastructure MachineSet Create 3 Infrastructure MachineSets, the $ami, $region, $zone and $clusterid variables in the sample below. zone should be a, b, c and so-on, respectively, on each machineset yaml file. The default node type is m5.xlarge (4CPU, 16GB memory), if your infrastructure needs are bigger, you can increase it to a bigger node type, or increase the replica count per Availability Zone. To obtain your Cluster ID, you can run the following command. $ oc get -o jsonpath = '{.status.infrastructureName}{\"\\n\"}' infrastructure cluster To get your AMI id for your region, you can run the following command (change the OpenShift version accordingly). curl -s https://raw.githubusercontent.com/openshift/installer/release-4.7/data/data/rhcos.json | jq -r '.amis.\"us-east-1\".hvm' If you Bring Your Own Network , change lines 56-65. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 apiVersion : machine.openshift.io/v1beta1 kind : MachineSet metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : infra machine.openshift.io/cluster-api-machine-type : infra name : $clusterid-infra-$region$zone namespace : openshift-machine-api spec : replicas : 1 selector : matchLabels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machineset : $clusterid-infra-$region$zone template : metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : infra machine.openshift.io/cluster-api-machine-type : infra machine.openshift.io/cluster-api-machineset : $clusterid-infra-$region$zone spec : taints : - effect : NoSchedule key : infra value : \"\" metadata : labels : node-role.kubernetes.io/infra : \"\" providerSpec : value : ami : id : $ami apiVersion : awsproviderconfig.openshift.io/v1beta1 blockDevices : - ebs : encrypted : true iops : 0 kmsKey : arn : \"\" volumeSize : 200 volumeType : gp2 credentialsSecret : name : aws-cloud-credentials deviceIndex : 0 iamInstanceProfile : id : $clusterid-worker-profile instanceType : m5.xlarge kind : AWSMachineProviderConfig metadata : creationTimestamp : null placement : availabilityZone : $region$zone region : $region securityGroups : - filters : - name : tag:Name values : - $clusterid-worker-sg subnet : filters : - name : tag:Name values : - $clusterid-private-us-east-2a tags : - name : kubernetes.io/cluster/$clusterid value : owned userDataSecret : name : worker-user-data Infrastructure Component Placement Follow the Golden Topology Guidelines to control placement of infrastructure components. Storage Storage MachineConfigPool apiVersion : machineconfiguration.openshift.io/v1 kind : MachineConfigPool metadata : name : storage spec : machineConfigSelector : matchExpressions : - key : machineconfiguration.openshift.io/role operator : In values : - worker - storage nodeSelector : matchLabels : cluster.ocs.openshift.io/openshift-storage : \"\" node-role.kubernetes.io/storage : \"\" Storage MachineSet Create 3 Storage MachineSets, the $ami, $region, $zone and $clusterid variables in the sample below. zone should be a, b, c and so-on, respectively, on each machineset yaml file. The default node type is m5.4xlarge (16CPU, 64GB memory), - To obtain your Cluster ID, you can run the following command. $ oc get -o jsonpath = '{.status.infrastructureName}{\"\\n\"}' infrastructure cluster - To get your AMI id for your region, you can run the following command (change the OpenShift version accordingly). curl -s https://raw.githubusercontent.com/openshift/installer/release-4.7/data/data/rhcos.json | jq -r '.amis.\"us-east-1\".hvm' If you Bring Your Own Network , change lines 56-65. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 apiVersion : machine.openshift.io/v1beta1 kind : MachineSet metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : storage machine.openshift.io/cluster-api-machine-type : storage name : $clusterid-storage-$region$zone namespace : openshift-machine-api spec : replicas : 1 selector : matchLabels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machineset : $clusterid-storage-$region$zone template : metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : storage machine.openshift.io/cluster-api-machine-type : storage machine.openshift.io/cluster-api-machineset : $clusterid-storage-$region$zone spec : taints : - effect : NoSchedule key : node.ocs.openshift.io/storage value : \"true\" metadata : labels : cluster.ocs.openshift.io/openshift-storage : \"\" node-role.kubernetes.io/storage : \"\" providerSpec : value : ami : id : $ami apiVersion : awsproviderconfig.openshift.io/v1beta1 blockDevices : - ebs : encrypted : true iops : 0 kmsKey : arn : \"\" volumeSize : 200 volumeType : gp2 credentialsSecret : name : aws-cloud-credentials deviceIndex : 0 iamInstanceProfile : id : $clusterid-worker-profile instanceType : m5.4xlarge kind : AWSMachineProviderConfig metadata : creationTimestamp : null placement : availabilityZone : $region$zone region : $region securityGroups : - filters : - name : tag:Name values : - $clusterid-worker-sg subnet : filters : - name : tag:Name values : - $clusterid-private-us-east-2a tags : - name : kubernetes.io/cluster/$clusterid value : owned userDataSecret : name : worker-user-data CloudPak CloudPak MachineConfigPool apiVersion : machineconfiguration.openshift.io/v1 kind : MachineConfigPool metadata : name : cp4x spec : machineConfigSelector : matchExpressions : - key : machineconfiguration.openshift.io/role operator : In values : - worker - cp4x nodeSelector : matchLabels : node-role.kubernetes.io/cp4x : \"\" CloudPak Tuned Configuration apiVersion : tuned.openshift.io/v1 kind : Tuned metadata : name : cp4x-tuned namespace : openshift-cluster-node-tuning-operator spec : profile : - name : cp4x-tuned data : | [main] summary=Tune Kernel parameters on OpenShift Worker Nodes running CloudPak workloads [sysctl] kernel.shmall = 33554432 kernel.shmmax = 68719476736 kernel.shmmni = 16384 kernel.sem = 250 1024000 100 16384 kernel.msgmax = 65536 kernel.msgmnb = 65536 kernel.msgmni = 32768 vm.max_map_count = 262144 recommend : - match : - label : node-role.kubernetes.io/cp4x priority : 10 profile : cp4x-tuned CloudPak ContainerRuntimeConfig apiVersion : machineconfiguration.openshift.io/v1 kind : ContainerRuntimeConfig metadata : name : cp4x-containerruntimeconfig spec : machineConfigPoolSelector : matchLabels : limits-crio : cp4x-containerruntimeconfig containerRuntimeConfig : pidsLimit : 12288 CloudPak MachineSet Create 3 Storage MachineSets, the $ami, $region, $zone and $clusterid variables in the sample below. zone should be a, b, c and so-on, respectively, on each machineset yaml file. The default node type is m5.4xlarge (16CPU, 64GB memory), - To obtain your Cluster ID, you can run the following command. $ oc get -o jsonpath = '{.status.infrastructureName}{\"\\n\"}' infrastructure cluster - To get your AMI id for your region, you can run the following command (change the OpenShift version accordingly). curl -s https://raw.githubusercontent.com/openshift/installer/release-4.7/data/data/rhcos.json | jq -r '.amis.\"us-east-1\".hvm' If you Bring Your Own Network , change lines 56-65. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 apiVersion : machine.openshift.io/v1beta1 kind : MachineSet metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : cp4x machine.openshift.io/cluster-api-machine-type : cp4x name : $clusterid-cp4x-$region$zone namespace : openshift-machine-api spec : replicas : 0 selector : matchLabels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machineset : $clusterid-cp4x-$region$zone template : metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : worker machine.openshift.io/cluster-api-machine-type : worker machine.openshift.io/cluster-api-machineset : $clusterid-cp4x-$region$zone spec : metadata : labels : node-role.kubernetes.io/cp4x : \"\" providerSpec : value : ami : id : $ami apiVersion : awsproviderconfig.openshift.io/v1beta1 blockDevices : - ebs : encrypted : true iops : 0 kmsKey : arn : \"\" volumeSize : 200 volumeType : gp2 credentialsSecret : name : aws-cloud-credentials deviceIndex : 0 iamInstanceProfile : id : $clusterid-worker-profile instanceType : m5.4xlarge kind : AWSMachineProviderConfig metadata : creationTimestamp : null placement : availabilityZone : $region$zone region : $region securityGroups : - filters : - name : tag:Name values : - $clusterid-worker-sg subnet : filters : - name : tag:Name values : - $clusterid-private-us-east-2a tags : - name : kubernetes.io/cluster/$clusterid value : owned userDataSecret : name : worker-user-data Storage \u00b6 In a default OpenShift deployment on AWS, you will have storage classes called gp2 and gp2-csi which only provides ReadWriteOnce (RWO) access modes. $ oc get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE gp2 ( default ) kubernetes.io/aws-ebs Delete WaitForFirstConsumer true 12m gp2-csi ebs.csi.aws.com Delete WaitForFirstConsumer true 12m Both storage classes will consume Elastic Block Storage (EBS) resources from Amazon account for the installation. If your CloudPak workloads require a ReadWriteMany (RWX) compatible storage class, the recommended solution is to use OpenShift Container Storage . Installation Scenarios \u00b6 The recommended method for installing OpenShift into AWS is using an Installer Provisioned Infrastructure Deployment . It is a solid foundation that encompass most of the best practices and recommendations outlined in the Golden Topology . The installation process is fully supported by IBM and Red Hat, and you won't need to manage and maintain additional code assets to deploy your cluster. Customers requirements may prevent you from using an unmodified OpenShift installation in AWS. Bellow are some of the most common customer requests, as well as ways of handling. All these methods begin with the same process [ prod-ref-001 ] $ openshift-install create install-config ? SSH Public Key /Users/vbudi/.ssh/id_rsa.pub ? Platform aws ? AWS Access Key ID AAAAAAAAAAAAAAAAAAAAAAAAA ? AWS Secret Access Key [ ? for help ] **************************************** INFO Writing AWS credentials to \"/Users/vbudi/.aws/credentials\" ? Region us-east-1 INFO Credentials loaded from the \"default\" profile in file \"/Users/vbudi/.aws/credentials\" ? Base Domain gtmaa.ga ? Cluster Name prod-ref-001 ? Pull Secret [ ? for help ] ************************************** INFO Install-Config created in : . Once you've modified your install-config.yaml to fit your needs, create your manifests files. [ prod-ref-001 ] $ openshift-install create manifests INFO Credentials loaded from the \"default\" profile in file \"/Users/vbudi/.aws/credentials\" INFO Consuming Install Config from target directory INFO Manifests created in : manifests and openshift * Once you've modified your manifests files, you can proceed to create your OpenShift Cluster. $ openshift-install create cluster Below is a sample install-config.yaml file for an AWS OpenShift Cluster. apiVersion : v1 baseDomain : gtmaa.ga compute : - architecture : amd64 hyperthreading : Enabled name : worker platform : {} replicas : 3 controlPlane : architecture : amd64 hyperthreading : Enabled name : master platform : {} replicas : 3 metadata : creationTimestamp : null name : prod-ref-001 networking : clusterNetwork : - cidr : 10.128.0.0/14 hostPrefix : 23 machineNetwork : - cidr : 10.0.0.0/16 networkType : OpenShiftSDN serviceNetwork : - 172.30.0.0/16 platform : aws : region : us-east-1 pullSecret : '{\"auths\":{...}}' sshKey : | ssh-rsa AAAAB3N... Node Sizing \u00b6 Control Plane Nodes \u00b6 The default control plane node size and count in an unmodified IPI installation is 3 m4.xlarge nodes with a 120GB disk. You can configure the node size by modifying the controlPlane section of your install-config.yaml . ... controlPlane : architecture : amd64 hyperthreading : Enabled name : master platform : aws : rootVolume : size : 200 type : io1 iops : 4000 type : m5.xlarge replicas : 3 ... Warning Only 3 replicas are supported for control plane high availability configuration For a complete list of available platform.aws.type options, check out supported AWS machine types . The recommended series is the General Purpose Compute m5 series, with a minimum spec of m5.xlarge for production. Compute Nodes \u00b6 The default worker node size and count in an unmodified IPI installation is 3 m4.large nodes with a 120Gb disk. You can configure the node size by modifying the compute section of your install-config.yaml ... compute : architecture : amd64 hyperthreading : Enabled name : worker platform : aws : rootVolume : size : 200 type : io1 iops : 4000 type : m5.4xlarge replicas : 3 ... For a complete list of available platform.aws.type options, check out supported AWS machine types . The recommended series is the General Purpose Compute m5 series, but your workload requirements may dictate another series. For memory intensive computing, choose the R5 series, while for CPU intensive computing, choose the C5 series. Private Endpoints \u00b6 An unmodified OpenShift installation will provision publicly facing LoadBalancers for api and *.apps ( api-int is ALWAYS private). If your customer requires private endpoints, modify your install-config.yaml by appending publish: Internal to it. The api and *.apps LoadBalancers will be created within your VPC CIDR range on the master and worker subnets ... sshKey : | ssh-rsa AAAAB3N... publish : Internal Warning When the publish type is Internal the API endpoint will not be available outside of the VPC, you must provide the necessary connectivity for API access to the cluster. Bring Your Own Network \u00b6 Customers may wish to use their pre-existing VPC infrastructure which OpenShift will use as its networking environment. This can be achieved by setting the platform.aws.subnets field as an array of subnets (typically 3 of them for high availability) ... platform : aws : region : us-east-1 subnets : - subnet-07819677ffc6ce3ba - subnet-02eb7c3f205fbb9db - subnet-08ad401ad884ff160 ... Warning While deploying into existing VPC and Subnets is a supported scenario, it is the customer's responsibility that there are no policies that prevent OpenShift from deploying, any security groups that block traffic, etc; OpenShift installer will not change or check the existing network restrictions. OpenShift also assumes that all the subnets are in the same VPC and has the correct routing and security groups. Bring Your Own DNS \u00b6 Customers may not wish to use AWS Route53 hosted zones (public or private), opting instead to use their existing Enterprise DNS servers. This can be achieved by modifying the manifests/cluster-dns-02-config.yml DNS configuration manifest. Remove the spec.privateZone and spec.publicZone sections in the file. When done editing, the file should look like this: apiVersion : config.openshift.io/v1 kind : DNS metadata : creationTimestamp : null name : cluster spec : baseDomain : mycluster.aws.gtmaa.ga status : {} Warning By disabling this feature of the DNS Operator, the cluster will no longer be able to update DNS for you as LoadBalancers are created during installation. As the installation progresses, you will need to pay VERY close attention to the loadbalancers created in your cluster resource group. The first loadbalancer that gets created is <cluster_id>-int . You need to update your DNS records so that api-int.<cluster_name>.<base_domain> points to this LoadBalancer. This happens very early in the deployment process, and the OpenShift master nodes will not boot up until this record is created. The next loadbalancer created is <cluster_id>-ext . It hosts the endpoint for api . During the first phase of OpenShift deployment (pre-bootstrapping), the LoadBalancer will consist of a single IP address, public or private. During the second phase of OpenShift deployment (post-bootstrapping) another load balancer will be created by the OpenShift Ingress Operator, and it will have a long random character name. In the example above, you would point the DNS record api.<cluster_name>.<base_domain> to the <cluster-id>-int and <cluster-id>-ext ; and point *.apps.<cluster_name>.<base_domain> to the random character load balancer address. In the extreme case that your customer does not allow wildcard DNS entries on their Enterprise DNS servers, the following DNS records MUST be created, pointing to the apps Loadbalancer, instead of *.apps.<cluster_name>.<base_domain> . The cluster will not finish deployment until these records are created. oauth-openshift.apps.<cluster_name>.<base_domain> console-openshift-console.apps.<cluster_name>.<base_domain> downloads-openshift-console.apps.<cluster_name>.<base_domain> canary-openshift-ingress-canary.apps.<cluster_name>.<base_domain> alertmanager-main-openshift-monitoring.apps.<cluster_name>.<base_domain> grafana-openshift-monitoring.apps.<cluster_name>.<base_domain> prometheus-k8s-openshift-monitoring.apps.<cluster_name>.<base_domain> thanos-querier-openshift-monitoring.apps.<cluster_name>.<base_domain> User Provided Infrastructure \u00b6 If your customer has any requirements that can't be met by the above solutions, we've provided an implementation in terraform . This implementation is meant to be modified to meet your customer needs, and requires working knowledge of terraform. Deploying this implementation with no modifications is the equivalent of running openshift-install create cluster , and should be avoided.","title":"Amazon AWS"},{"location":"infrastructure/aws/#production-deployment-guide-for-openshift-on-amazon-web-services-aws","text":"","title":"Production Deployment Guide for OpenShift on Amazon Web Services (AWS)"},{"location":"infrastructure/aws/#golden-topology","text":"Before reading this document, familiarize yourself with the Golden Topology section of this Production Deployment Guide. Unless otherwise stated, all the items in that section apply to the Cluster Infrastructure we will provision on AWS. Read Red Hat's documentation on Installing a cluster quickly on AWS to deploy a cluster with no modifications. Deploying a cluster with openshift-install will provide the following topology.","title":"Golden Topology"},{"location":"infrastructure/aws/#machinesets","text":"The following templates can be used to generate your infrastructure, cloudpak and storage MachineSets MachineSets Note Click on each tab above for sample yamls to create your infrastructure Infrastructure Infrastructure MachineConfigPool apiVersion : machineconfiguration.openshift.io/v1 kind : MachineConfigPool metadata : name : infra spec : machineConfigSelector : matchExpressions : - key : machineconfiguration.openshift.io/role operator : In values : - worker - infra nodeSelector : matchLabels : node-role.kubernetes.io/infra : \"\" Infrastructure MachineSet Create 3 Infrastructure MachineSets, the $ami, $region, $zone and $clusterid variables in the sample below. zone should be a, b, c and so-on, respectively, on each machineset yaml file. The default node type is m5.xlarge (4CPU, 16GB memory), if your infrastructure needs are bigger, you can increase it to a bigger node type, or increase the replica count per Availability Zone. To obtain your Cluster ID, you can run the following command. $ oc get -o jsonpath = '{.status.infrastructureName}{\"\\n\"}' infrastructure cluster To get your AMI id for your region, you can run the following command (change the OpenShift version accordingly). curl -s https://raw.githubusercontent.com/openshift/installer/release-4.7/data/data/rhcos.json | jq -r '.amis.\"us-east-1\".hvm' If you Bring Your Own Network , change lines 56-65. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 apiVersion : machine.openshift.io/v1beta1 kind : MachineSet metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : infra machine.openshift.io/cluster-api-machine-type : infra name : $clusterid-infra-$region$zone namespace : openshift-machine-api spec : replicas : 1 selector : matchLabels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machineset : $clusterid-infra-$region$zone template : metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : infra machine.openshift.io/cluster-api-machine-type : infra machine.openshift.io/cluster-api-machineset : $clusterid-infra-$region$zone spec : taints : - effect : NoSchedule key : infra value : \"\" metadata : labels : node-role.kubernetes.io/infra : \"\" providerSpec : value : ami : id : $ami apiVersion : awsproviderconfig.openshift.io/v1beta1 blockDevices : - ebs : encrypted : true iops : 0 kmsKey : arn : \"\" volumeSize : 200 volumeType : gp2 credentialsSecret : name : aws-cloud-credentials deviceIndex : 0 iamInstanceProfile : id : $clusterid-worker-profile instanceType : m5.xlarge kind : AWSMachineProviderConfig metadata : creationTimestamp : null placement : availabilityZone : $region$zone region : $region securityGroups : - filters : - name : tag:Name values : - $clusterid-worker-sg subnet : filters : - name : tag:Name values : - $clusterid-private-us-east-2a tags : - name : kubernetes.io/cluster/$clusterid value : owned userDataSecret : name : worker-user-data Infrastructure Component Placement Follow the Golden Topology Guidelines to control placement of infrastructure components. Storage Storage MachineConfigPool apiVersion : machineconfiguration.openshift.io/v1 kind : MachineConfigPool metadata : name : storage spec : machineConfigSelector : matchExpressions : - key : machineconfiguration.openshift.io/role operator : In values : - worker - storage nodeSelector : matchLabels : cluster.ocs.openshift.io/openshift-storage : \"\" node-role.kubernetes.io/storage : \"\" Storage MachineSet Create 3 Storage MachineSets, the $ami, $region, $zone and $clusterid variables in the sample below. zone should be a, b, c and so-on, respectively, on each machineset yaml file. The default node type is m5.4xlarge (16CPU, 64GB memory), - To obtain your Cluster ID, you can run the following command. $ oc get -o jsonpath = '{.status.infrastructureName}{\"\\n\"}' infrastructure cluster - To get your AMI id for your region, you can run the following command (change the OpenShift version accordingly). curl -s https://raw.githubusercontent.com/openshift/installer/release-4.7/data/data/rhcos.json | jq -r '.amis.\"us-east-1\".hvm' If you Bring Your Own Network , change lines 56-65. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 apiVersion : machine.openshift.io/v1beta1 kind : MachineSet metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : storage machine.openshift.io/cluster-api-machine-type : storage name : $clusterid-storage-$region$zone namespace : openshift-machine-api spec : replicas : 1 selector : matchLabels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machineset : $clusterid-storage-$region$zone template : metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : storage machine.openshift.io/cluster-api-machine-type : storage machine.openshift.io/cluster-api-machineset : $clusterid-storage-$region$zone spec : taints : - effect : NoSchedule key : node.ocs.openshift.io/storage value : \"true\" metadata : labels : cluster.ocs.openshift.io/openshift-storage : \"\" node-role.kubernetes.io/storage : \"\" providerSpec : value : ami : id : $ami apiVersion : awsproviderconfig.openshift.io/v1beta1 blockDevices : - ebs : encrypted : true iops : 0 kmsKey : arn : \"\" volumeSize : 200 volumeType : gp2 credentialsSecret : name : aws-cloud-credentials deviceIndex : 0 iamInstanceProfile : id : $clusterid-worker-profile instanceType : m5.4xlarge kind : AWSMachineProviderConfig metadata : creationTimestamp : null placement : availabilityZone : $region$zone region : $region securityGroups : - filters : - name : tag:Name values : - $clusterid-worker-sg subnet : filters : - name : tag:Name values : - $clusterid-private-us-east-2a tags : - name : kubernetes.io/cluster/$clusterid value : owned userDataSecret : name : worker-user-data CloudPak CloudPak MachineConfigPool apiVersion : machineconfiguration.openshift.io/v1 kind : MachineConfigPool metadata : name : cp4x spec : machineConfigSelector : matchExpressions : - key : machineconfiguration.openshift.io/role operator : In values : - worker - cp4x nodeSelector : matchLabels : node-role.kubernetes.io/cp4x : \"\" CloudPak Tuned Configuration apiVersion : tuned.openshift.io/v1 kind : Tuned metadata : name : cp4x-tuned namespace : openshift-cluster-node-tuning-operator spec : profile : - name : cp4x-tuned data : | [main] summary=Tune Kernel parameters on OpenShift Worker Nodes running CloudPak workloads [sysctl] kernel.shmall = 33554432 kernel.shmmax = 68719476736 kernel.shmmni = 16384 kernel.sem = 250 1024000 100 16384 kernel.msgmax = 65536 kernel.msgmnb = 65536 kernel.msgmni = 32768 vm.max_map_count = 262144 recommend : - match : - label : node-role.kubernetes.io/cp4x priority : 10 profile : cp4x-tuned CloudPak ContainerRuntimeConfig apiVersion : machineconfiguration.openshift.io/v1 kind : ContainerRuntimeConfig metadata : name : cp4x-containerruntimeconfig spec : machineConfigPoolSelector : matchLabels : limits-crio : cp4x-containerruntimeconfig containerRuntimeConfig : pidsLimit : 12288 CloudPak MachineSet Create 3 Storage MachineSets, the $ami, $region, $zone and $clusterid variables in the sample below. zone should be a, b, c and so-on, respectively, on each machineset yaml file. The default node type is m5.4xlarge (16CPU, 64GB memory), - To obtain your Cluster ID, you can run the following command. $ oc get -o jsonpath = '{.status.infrastructureName}{\"\\n\"}' infrastructure cluster - To get your AMI id for your region, you can run the following command (change the OpenShift version accordingly). curl -s https://raw.githubusercontent.com/openshift/installer/release-4.7/data/data/rhcos.json | jq -r '.amis.\"us-east-1\".hvm' If you Bring Your Own Network , change lines 56-65. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 apiVersion : machine.openshift.io/v1beta1 kind : MachineSet metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : cp4x machine.openshift.io/cluster-api-machine-type : cp4x name : $clusterid-cp4x-$region$zone namespace : openshift-machine-api spec : replicas : 0 selector : matchLabels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machineset : $clusterid-cp4x-$region$zone template : metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : worker machine.openshift.io/cluster-api-machine-type : worker machine.openshift.io/cluster-api-machineset : $clusterid-cp4x-$region$zone spec : metadata : labels : node-role.kubernetes.io/cp4x : \"\" providerSpec : value : ami : id : $ami apiVersion : awsproviderconfig.openshift.io/v1beta1 blockDevices : - ebs : encrypted : true iops : 0 kmsKey : arn : \"\" volumeSize : 200 volumeType : gp2 credentialsSecret : name : aws-cloud-credentials deviceIndex : 0 iamInstanceProfile : id : $clusterid-worker-profile instanceType : m5.4xlarge kind : AWSMachineProviderConfig metadata : creationTimestamp : null placement : availabilityZone : $region$zone region : $region securityGroups : - filters : - name : tag:Name values : - $clusterid-worker-sg subnet : filters : - name : tag:Name values : - $clusterid-private-us-east-2a tags : - name : kubernetes.io/cluster/$clusterid value : owned userDataSecret : name : worker-user-data","title":"MachineSets"},{"location":"infrastructure/aws/#storage","text":"In a default OpenShift deployment on AWS, you will have storage classes called gp2 and gp2-csi which only provides ReadWriteOnce (RWO) access modes. $ oc get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE gp2 ( default ) kubernetes.io/aws-ebs Delete WaitForFirstConsumer true 12m gp2-csi ebs.csi.aws.com Delete WaitForFirstConsumer true 12m Both storage classes will consume Elastic Block Storage (EBS) resources from Amazon account for the installation. If your CloudPak workloads require a ReadWriteMany (RWX) compatible storage class, the recommended solution is to use OpenShift Container Storage .","title":"Storage"},{"location":"infrastructure/aws/#installation-scenarios","text":"The recommended method for installing OpenShift into AWS is using an Installer Provisioned Infrastructure Deployment . It is a solid foundation that encompass most of the best practices and recommendations outlined in the Golden Topology . The installation process is fully supported by IBM and Red Hat, and you won't need to manage and maintain additional code assets to deploy your cluster. Customers requirements may prevent you from using an unmodified OpenShift installation in AWS. Bellow are some of the most common customer requests, as well as ways of handling. All these methods begin with the same process [ prod-ref-001 ] $ openshift-install create install-config ? SSH Public Key /Users/vbudi/.ssh/id_rsa.pub ? Platform aws ? AWS Access Key ID AAAAAAAAAAAAAAAAAAAAAAAAA ? AWS Secret Access Key [ ? for help ] **************************************** INFO Writing AWS credentials to \"/Users/vbudi/.aws/credentials\" ? Region us-east-1 INFO Credentials loaded from the \"default\" profile in file \"/Users/vbudi/.aws/credentials\" ? Base Domain gtmaa.ga ? Cluster Name prod-ref-001 ? Pull Secret [ ? for help ] ************************************** INFO Install-Config created in : . Once you've modified your install-config.yaml to fit your needs, create your manifests files. [ prod-ref-001 ] $ openshift-install create manifests INFO Credentials loaded from the \"default\" profile in file \"/Users/vbudi/.aws/credentials\" INFO Consuming Install Config from target directory INFO Manifests created in : manifests and openshift * Once you've modified your manifests files, you can proceed to create your OpenShift Cluster. $ openshift-install create cluster Below is a sample install-config.yaml file for an AWS OpenShift Cluster. apiVersion : v1 baseDomain : gtmaa.ga compute : - architecture : amd64 hyperthreading : Enabled name : worker platform : {} replicas : 3 controlPlane : architecture : amd64 hyperthreading : Enabled name : master platform : {} replicas : 3 metadata : creationTimestamp : null name : prod-ref-001 networking : clusterNetwork : - cidr : 10.128.0.0/14 hostPrefix : 23 machineNetwork : - cidr : 10.0.0.0/16 networkType : OpenShiftSDN serviceNetwork : - 172.30.0.0/16 platform : aws : region : us-east-1 pullSecret : '{\"auths\":{...}}' sshKey : | ssh-rsa AAAAB3N...","title":"Installation Scenarios"},{"location":"infrastructure/aws/#node-sizing","text":"","title":"Node Sizing"},{"location":"infrastructure/aws/#private-endpoints","text":"An unmodified OpenShift installation will provision publicly facing LoadBalancers for api and *.apps ( api-int is ALWAYS private). If your customer requires private endpoints, modify your install-config.yaml by appending publish: Internal to it. The api and *.apps LoadBalancers will be created within your VPC CIDR range on the master and worker subnets ... sshKey : | ssh-rsa AAAAB3N... publish : Internal Warning When the publish type is Internal the API endpoint will not be available outside of the VPC, you must provide the necessary connectivity for API access to the cluster.","title":"Private Endpoints"},{"location":"infrastructure/aws/#bring-your-own-network","text":"Customers may wish to use their pre-existing VPC infrastructure which OpenShift will use as its networking environment. This can be achieved by setting the platform.aws.subnets field as an array of subnets (typically 3 of them for high availability) ... platform : aws : region : us-east-1 subnets : - subnet-07819677ffc6ce3ba - subnet-02eb7c3f205fbb9db - subnet-08ad401ad884ff160 ... Warning While deploying into existing VPC and Subnets is a supported scenario, it is the customer's responsibility that there are no policies that prevent OpenShift from deploying, any security groups that block traffic, etc; OpenShift installer will not change or check the existing network restrictions. OpenShift also assumes that all the subnets are in the same VPC and has the correct routing and security groups.","title":"Bring Your Own Network"},{"location":"infrastructure/aws/#bring-your-own-dns","text":"Customers may not wish to use AWS Route53 hosted zones (public or private), opting instead to use their existing Enterprise DNS servers. This can be achieved by modifying the manifests/cluster-dns-02-config.yml DNS configuration manifest. Remove the spec.privateZone and spec.publicZone sections in the file. When done editing, the file should look like this: apiVersion : config.openshift.io/v1 kind : DNS metadata : creationTimestamp : null name : cluster spec : baseDomain : mycluster.aws.gtmaa.ga status : {} Warning By disabling this feature of the DNS Operator, the cluster will no longer be able to update DNS for you as LoadBalancers are created during installation. As the installation progresses, you will need to pay VERY close attention to the loadbalancers created in your cluster resource group. The first loadbalancer that gets created is <cluster_id>-int . You need to update your DNS records so that api-int.<cluster_name>.<base_domain> points to this LoadBalancer. This happens very early in the deployment process, and the OpenShift master nodes will not boot up until this record is created. The next loadbalancer created is <cluster_id>-ext . It hosts the endpoint for api . During the first phase of OpenShift deployment (pre-bootstrapping), the LoadBalancer will consist of a single IP address, public or private. During the second phase of OpenShift deployment (post-bootstrapping) another load balancer will be created by the OpenShift Ingress Operator, and it will have a long random character name. In the example above, you would point the DNS record api.<cluster_name>.<base_domain> to the <cluster-id>-int and <cluster-id>-ext ; and point *.apps.<cluster_name>.<base_domain> to the random character load balancer address. In the extreme case that your customer does not allow wildcard DNS entries on their Enterprise DNS servers, the following DNS records MUST be created, pointing to the apps Loadbalancer, instead of *.apps.<cluster_name>.<base_domain> . The cluster will not finish deployment until these records are created. oauth-openshift.apps.<cluster_name>.<base_domain> console-openshift-console.apps.<cluster_name>.<base_domain> downloads-openshift-console.apps.<cluster_name>.<base_domain> canary-openshift-ingress-canary.apps.<cluster_name>.<base_domain> alertmanager-main-openshift-monitoring.apps.<cluster_name>.<base_domain> grafana-openshift-monitoring.apps.<cluster_name>.<base_domain> prometheus-k8s-openshift-monitoring.apps.<cluster_name>.<base_domain> thanos-querier-openshift-monitoring.apps.<cluster_name>.<base_domain>","title":"Bring Your Own DNS"},{"location":"infrastructure/aws/#user-provided-infrastructure","text":"If your customer has any requirements that can't be met by the above solutions, we've provided an implementation in terraform . This implementation is meant to be modified to meet your customer needs, and requires working knowledge of terraform. Deploying this implementation with no modifications is the equivalent of running openshift-install create cluster , and should be avoided.","title":"User Provided Infrastructure"},{"location":"infrastructure/azure/","text":"Production Deployment Guide for OpenShift on Azure \u00b6 Golden Topology \u00b6 Before reading this document, familiarize yourself with the Golden Topology section of this Production Deployment Guide. Unless otherwise stated, all the items in that section apply to the Cluster Infrastructure we will provision on Azure. Read Red Hat's documentation on Installing a cluster quickly on Azure to deploy a cluster with no modifications. Deploying a cluster with openshift-install will provide the following topology. MachineSets \u00b6 The following templates can be used to generate your infrastructure, cloudpak and storage MachineSets MachineSets Note Click on each tab above for sample yamls to create your infrastructure Infrastructure Infrastructure MachineConfigPool apiVersion : machineconfiguration.openshift.io/v1 kind : MachineConfigPool metadata : name : infra spec : machineConfigSelector : matchExpressions : - key : machineconfiguration.openshift.io/role operator : In values : - worker - infra nodeSelector : matchLabels : node-role.kubernetes.io/infra : \"\" Infrastructure MachineSet Create 3 Infrastructure MachineSets, the $region, $zone and $clusterid variables in the sample below. zone should be 1 2 and 3, respectively, on each machineset yaml file. The default node type is Standard_D4s_v3 (4CPU, 16GB mem), if your infrastructure needs are bigger, you can increase it to a bigger node type, or increase the replica count per Availability Zone. To obtain your Cluster ID, you can run the following command. $ oc get -o jsonpath = '{.status.infrastructureName}{\"\\n\"}' infrastructure cluster If you Bring Your Own Network , update lines 48, 57 and 61 If you Bring Your Own Resource Group , update lines 41 and 56. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 apiVersion : machine.openshift.io/v1beta1 kind : MachineSet metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : infra machine.openshift.io/cluster-api-machine-type : infra name : $clusterid-infra-$region$zone namespace : openshift-machine-api spec : replicas : 1 selector : matchLabels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machineset : $clusterid-infra-$region$zone template : metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : infra machine.openshift.io/cluster-api-machine-type : infra machine.openshift.io/cluster-api-machineset : $clusterid-infra-$region$zone spec : taints : - effect : NoSchedule key : infra value : \"\" metadata : labels : node-role.kubernetes.io/infra : \"\" providerSpec : value : apiVersion : azureproviderconfig.openshift.io/v1beta1 kind : AzureMachineProviderSpec credentialsSecret : name : azure-cloud-credentials namespace : openshift-machine-api image : offer : \"\" publisher : \"\" resourceID : /resourceGroups/$clusterid-rg/providers/Microsoft.Compute/images/$clusterid sku : \"\" version : \"\" location : $region managedIdentity : $clusterid-identity metadata : creationTimestamp : null networkResourceGroup : $clusterid-rg osDisk : diskSizeGB : 128 managedDisk : storageAccountType : Premium_LRS osType : Linux publicIP : false publicLoadBalancer : $clusterid resourceGroup : $clusterid-rg subnet : $clusterid-worker-subnet userDataSecret : name : worker-user-data vmSize : Standard_D4s_v3 vnet : $clusterid-vnet zone : \"$zone\" Infrastructure Component Placement Follow the Golden Topology Guidelines to control placement of infrastructure components. Storage Storage MachineConfigPool apiVersion : machineconfiguration.openshift.io/v1 kind : MachineConfigPool metadata : name : storage spec : machineConfigSelector : matchExpressions : - key : machineconfiguration.openshift.io/role operator : In values : - worker - storage nodeSelector : matchLabels : cluster.ocs.openshift.io/openshift-storage : \"\" node-role.kubernetes.io/storage : \"\" Storage MachineSet Create 3 Storage MachineSets, replacing the region, $zone and $clusterid variables in the sample below. zone should be 1 2 and 3, respectively, on each machineset yaml file. The default node type is Standard_D16s_v3 (16CPU, 64GB mem), To obtain your Cluster ID, you can run the following command. $ oc get -o jsonpath = '{.status.infrastructureName}{\"\\n\"}' infrastructure cluster If you Bring Your Own Network , update lines 49, 58 and 62 If you Bring Your Own Resource Group , update lines 42 and 57. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 apiVersion : machine.openshift.io/v1beta1 kind : MachineSet metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : storage machine.openshift.io/cluster-api-machine-type : storage name : $clusterid-storage-$region$zone namespace : openshift-machine-api spec : replicas : 1 selector : matchLabels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machineset : $clusterid-storage-$region$zone template : metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : storage machine.openshift.io/cluster-api-machine-type : storage machine.openshift.io/cluster-api-machineset : $clusterid-storage-$region$zone spec : taints : - effect : NoSchedule key : node.ocs.openshift.io/storage value : \"true\" metadata : labels : cluster.ocs.openshift.io/openshift-storage : \"\" node-role.kubernetes.io/storage : \"\" providerSpec : value : apiVersion : azureproviderconfig.openshift.io/v1beta1 kind : AzureMachineProviderSpec credentialsSecret : name : azure-cloud-credentials namespace : openshift-machine-api image : offer : \"\" publisher : \"\" resourceID : /resourceGroups/$clusterid-rg/providers/Microsoft.Compute/images/$clusterid sku : \"\" version : \"\" location : $region managedIdentity : $clusterid-identity metadata : creationTimestamp : null networkResourceGroup : $clusterid-rg osDisk : diskSizeGB : 128 managedDisk : storageAccountType : Premium_LRS osType : Linux publicIP : false publicLoadBalancer : $clusterid resourceGroup : $clusterid-rg subnet : $clusterid-worker-subnet userDataSecret : name : worker-user-data vmSize : Standard_D16s_v3 vnet : $clusterid-vnet zone : \"$zone\" CloudPak CloudPak MachineConfigPool apiVersion : machineconfiguration.openshift.io/v1 kind : MachineConfigPool metadata : name : cp4x spec : machineConfigSelector : matchExpressions : - key : machineconfiguration.openshift.io/role operator : In values : - worker - cp4x nodeSelector : matchLabels : node-role.kubernetes.io/cp4x : \"\" CloudPak Tuned Configuration apiVersion : tuned.openshift.io/v1 kind : Tuned metadata : name : cp4x-tuned namespace : openshift-cluster-node-tuning-operator spec : profile : - name : cp4x-tuned data : | [main] summary=Tune Kernel parameters on OpenShift Worker Nodes running CloudPak workloads [sysctl] kernel.shmall = 33554432 kernel.shmmax = 68719476736 kernel.shmmni = 16384 kernel.sem = 250 1024000 100 16384 kernel.msgmax = 65536 kernel.msgmnb = 65536 kernel.msgmni = 32768 vm.max_map_count = 262144 recommend : - match : - label : node-role.kubernetes.io/cp4x priority : 10 profile : cp4x-tuned CloudPak ContainerRuntimeConfig apiVersion : machineconfiguration.openshift.io/v1 kind : ContainerRuntimeConfig metadata : name : cp4x-containerruntimeconfig spec : machineConfigPoolSelector : matchLabels : limits-crio : cp4x-containerruntimeconfig containerRuntimeConfig : pidsLimit : 12288 CloudPak MachineSet Create 3 Storage MachineSets, replacing the region, $zone and $clusterid variables in the sample below. zone should be 1 2 and 3, respectively, on each machineset yaml file. The default node type is Standard_D8s_v3 (8CPU, 32GB mem) and can be modified on line 56. To obtain your Cluster ID, you can run the following command. $ oc get -o jsonpath = '{.status.infrastructureName}{\"\\n\"}' infrastructure cluster If you Bring Your Own Network , change lines 44, 53 and 57. If you Bring Your Own Resource Group , change line 37 and 52. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 apiVersion : machine.openshift.io/v1beta1 kind : MachineSet metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : cp4x machine.openshift.io/cluster-api-machine-type : cp4x name : $clusterid-cp4x-$region$zone namespace : openshift-machine-api spec : replicas : 0 selector : matchLabels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machineset : $clusterid-cp4x-$region$zone template : metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : worker machine.openshift.io/cluster-api-machine-type : worker machine.openshift.io/cluster-api-machineset : $clusterid-cp4x-$region$zone spec : metadata : labels : node-role.kubernetes.io/cp4x : \"\" providerSpec : value : apiVersion : azureproviderconfig.openshift.io/v1beta1 kind : AzureMachineProviderSpec credentialsSecret : name : azure-cloud-credentials namespace : openshift-machine-api image : offer : \"\" publisher : \"\" resourceID : /resourceGroups/$clusterid-rg/providers/Microsoft.Compute/images/$clusterid sku : \"\" version : \"\" location : $region managedIdentity : $clusterid-identity metadata : creationTimestamp : null networkResourceGroup : $clusterid-rg osDisk : diskSizeGB : 128 managedDisk : storageAccountType : Premium_LRS osType : Linux publicIP : false publicLoadBalancer : $clusterid resourceGroup : $clusterid-rg subnet : $clusterid-worker-subnet userDataSecret : name : worker-user-data vmSize : Standard_D8s_v3 vnet : $clusterid-vnet zone : \"$zone\" Storage \u00b6 In a default OpenShift deployment on Azure, you will have a single storage class called managed-premium which only provides ReadWriteOnce (RWO) access modes. $ oc get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE managed-premium ( default ) kubernetes.io/azure-disk Delete WaitForFirstConsumer true 34h This storage class will consume azure-disk resources from a Storage Account in the cluster resource group. If your CloudPak workloads require a ReadWriteMany (RWX) compatible storage class, the recommended solution is to use OpenShift Container Storage . Installation Scenarios \u00b6 The recommended method for installing OpenShift into Azure is using an Installer Provisioned Infrastructure Deployment . It is a solid foundation that encompass most of the best practices and recommendations outlined in the Golden Topology . The installation process is fully supported by IBM and Red Hat, and you won't need to manage and maintain additional code assets to deploy your cluster. Customers requirements may prevent you from using an unmodified OpenShift installation in Azure. Below are some of the most common customer requests, as well as ways of handling. All these methods begin with the same process $ openshift-install create install-config ? SSH Public Key /Users/ncolon/.ssh/openshift_rsa.pub ? Platform azure ? azure subscription id ceb9d1a9-d0e8-46f9-a404-f2635fba6829 ? azure tenant id fcf67057-50c9-4ad4-98f3-ffca64add9e9 ? azure service principal client id a10be0dd-d46f-4592-9a2c-31d1c0bbecbe ? azure service principal client secret [ ? for help ] ********************************** INFO Saving user credentials to \"/Users/ncolon/.azure/osServicePrincipal.json\" INFO Credentials loaded from file \"/Users/ncolon/.azure/osServicePrincipal.json\" ? Region centralus ? Base Domain azure.ibm-gtmaa.dev ? Cluster Name mycluster ? Pull Secret [ ? for help ] *************************** INFO Install-Config created in : . Once you've modified your install-config.yaml to fit your needs, create your manifests files. $ openshift-install create manifests INFO Credentials loaded from file \"/Users/ncolon/.azure/osServicePrincipal.json\" INFO Consuming Install Config from target directory INFO Manifests created in : manifests and openshift Once you've modified your manifests files, you can proceed to create your OpenShift Cluster. $ openshift-install create cluster Below is a sample install-config.yaml file for an Azure OpenShift Cluster. apiVersion : v1 baseDomain : azure.ibm-gtmaa.dev compute : - architecture : amd64 hyperthreading : Enabled name : worker platform : {} replicas : 3 controlPlane : architecture : amd64 hyperthreading : Enabled name : master platform : {} replicas : 3 metadata : creationTimestamp : null name : mycluster networking : clusterNetwork : - cidr : 10.128.0.0/14 hostPrefix : 23 machineNetwork : - cidr : 10.0.0.0/16 networkType : OpenShiftSDN serviceNetwork : - 172.30.0.0/16 platform : azure : baseDomainResourceGroupName : ncolon-openshift4-rg cloudName : AzurePublicCloud outboundType : Loadbalancer region : centralus publish : External pullSecret : '{\"auths\":{...}}' sshKey : | ssh-rsa AAAAB3N... Node Sizing \u00b6 Control Plane Nodes \u00b6 The default control plane node size and count in an unmodified IPI installation is 3 Standard_D8s_v3 nodes with a 1024Gb disk. You can configure the node size by modifying the compute section of your install-config.yaml . ... controlPlane : architecture : amd64 hyperthreading : Enabled name : master platform : azure : type : Standard_D16s_v3 osDisk : diskSizeGB : 1024 diskType : Premium_LRS replicas : 3 ... Warning Only 3 replicas are supported for HA Warning IOPS Performance on Azure is tied to disk size as well as type. In order to meet the performance requirements of the ETCD database on the control plane nodes, a minimum of 1024Gb disk size is recommended, as well as using the Premium_LRS storage tier. For a complete list of available platform.azure.type options, check out Azure's documentation . The recommended series is the General Purpose Compute D-Series, with a minimum spec of Standard_D8s_v3 for production. Compute Nodes \u00b6 The default worker node size and count in an unmodified IPI installation is 3 Standard_D2s_v3 nodes with a 128Gb disk. You can configure the node size by modifying the compute section of your install-config.yaml ... compute : - architecture : amd64 hyperthreading : Enabled name : worker platform : azure : type : Standard_D8s_v3 osDisk : diskSizeGB : 512 diskType : Premium_LRS replicas : 5 ... For a complete list of available platform.azure.type options, check out Azure's documentation . The recommended series is the General Purpose Compute D-Series, but your workload requirements may dictate another series. If your workloads require GPU intensive tasks, take a look at the N-Series GPU Enabled Virtual Machines. Private Endpoints \u00b6 An unmodified OpenShift installation will provision publicly facing LoadBalancers for api and *.apps ( api-int is ALWAYS private). If your customer requires private endpoints, modify your install-config.yaml by appending publish: Internal to it. The api and *.apps LoadBalancers will be created within your VNET CIDR range on the master and worker subnets ... sshKey : | ssh-rsa AAAAB3N... publish : Internal Warning There are limitations on how Azure VMs can reach the internet. They either have to Have a Public IP address attached to them Your VNET provides alternate ways of reaching the internet The VMs are connected to a LoadBalancer with a public IP address(although no traffic is send their way) When you deploy a cluster with publish: Internal , OpenShift operators will create a LoadBalancer with a public IP address (scenario 3 above) listening on port 27627. The loadbalancer is created as a service called outbound-provider in the openshift-config-managed Namespace. Bring Your Own Resource Group \u00b6 Customers may wish to use a pre-existing resource group as the target for OpenShift components. This can be achieved by setting the platform.azure.resourceGroupName configuration parameter in install-config.yaml ... platform : azure : baseDomainResourceGroupName : os4-common cloudName : AzurePublicCloud outboundType : Loadbalancer region : centralus resourceGroupName : example-rg ... Warning The resource group used MUST be empty, and will be deleted when the cluster is destroyed Bring Your Own Network \u00b6 Customers may wish to use their pre-existing VNET infrastructure as the target for OpenShift networking environment. This can be achieved by setting the networkResourceGroupName , virtualNetwork , controlPlaneSubnet and computeSubnet configuration parameters in the platform.azure section of install-config.yaml ... platform : azure : baseDomainResourceGroupName : ncolon-openshift4-rg cloudName : AzurePublicCloud outboundType : Loadbalancer region : centralus networkResourceGroupName : my-cluster-vnet-rg virtualNetwork : my-vnet controlPlaneSubnet : my-master-subnet computeSubnet : my-worker-subnet ... Warning While deploying into existing VNET and Subnets is a supported scenario, it is the customer's responsibility that there are no policies that prevent OpenShift from deploying, any security groups that block traffic, etc Only one virtualNetwork is supported per cluster. Note You can set your controlPlaneSubnet and computeSubnet to the same if your customer requires a flat network in their VNET. User Defined Routing \u00b6 If absolutely no public endpoints can be present in the deployment of OpenShift on the customer's VNET, the scenario described on Private Endpoints is not enough. You also need to set the platform.azure.outboundType flag to UserDefinedRouting . This will instruct the OpenShift operators not to create the Public LoadBalancer listening on port 27627. ... platform : azure : baseDomainResourceGroupName : ncolon-openshift4-rg cloudName : AzurePublicCloud outboundType : UserDefinedRouting region : centralus ... Warning This configuration parameter assumes that connectivity to the internet is somehow provided by the VNET infrastructure, via proxies, routing through customer on-prem network connected via ExpressRoute, or any other mechanisms in place. It is the customer's responsibility to provide the necessary connectivity to the internet so OpenShift can download its required installation artifacts. Bring Your Own DNS \u00b6 Customers may not wish to use Azure DNS Zones (public or private), opting instead to use their existing Enterprise DNS servers. This can be achieved by modifying the manifests/cluster-dns-02-config.yml DNS configuration manifest. Remove the spec.privateZone and spec.publicZone sections in the file. When done editing, the file should look like this: apiVersion : config.openshift.io/v1 kind : DNS metadata : creationTimestamp : null name : cluster spec : baseDomain : mycluster.azure.ibm-gtmaa.dev status : {} Warning By disabling this feature of the DNS Operator, the cluster will no longer be able to update DNS for you as LoadBalancers are created during installation. As the installation progresses, you will need to pay VERY close attention to the loadbalancers created in your cluster resource group. The first loadbalancer that gets created is <cluster_id>-internal . You need to update your DNS records so that api-int.<cluster_name>.<base_domain> points to this LoadBalancer. This happens very early in the deployment process, and the OpenShift master nodes will not boot up until this record is created. The next loadbalancer created is <cluster_id> . It hosts the endpoints for api and *.apps . During the first phase of OpenShift deployment (pre-bootstrapping), the LoadBalancer will consist of a single IP address, public or private. During the second phase of OpenShift deployment (post-bootstrapping) a 2nd IP address will be created by the OpenShift Ingress Operator, and it will have a long random character name. In the example above, you would point the DNS record api.<cluster_name>.<base_domain> to the public-lb-ip-v4 and *.apps.<cluster_name>.<base_domain> to the random character IP address. In the extreme case that your customer does not allow wildcard DNS entries on their Enterprise DNS servers, the following DNS records MUST be created, pointing to the apps Loadbalancer, instead of *.apps.<cluster_name>.<base_domain> . The cluster will not finish deployment until these records are created. oauth-openshift.apps.<cluster_name>.<base_domain> console-openshift-console.apps.<cluster_name>.<base_domain> downloads-openshift-console.apps.<cluster_name>.<base_domain> canary-openshift-ingress-canary.apps.<cluster_name>.<base_domain> alertmanager-main-openshift-monitoring.apps.<cluster_name>.<base_domain> grafana-openshift-monitoring.apps.<cluster_name>.<base_domain> prometheus-k8s-openshift-monitoring.apps.<cluster_name>.<base_domain> thanos-querier-openshift-monitoring.apps.<cluster_name>.<base_domain> User Provided Infrastructure \u00b6 If your customer has any requirements that can't be met by the above solutions, we've provided an implementation in terraform . This implementation is meant to be modified to meet your customer needs, and requires working knowledge of terraform. Deploying this implementation with no modifications is the equivalent of running openshift-install create cluster , and should be avoided.","title":"Microsoft Azure"},{"location":"infrastructure/azure/#production-deployment-guide-for-openshift-on-azure","text":"","title":"Production Deployment Guide for OpenShift on Azure"},{"location":"infrastructure/azure/#golden-topology","text":"Before reading this document, familiarize yourself with the Golden Topology section of this Production Deployment Guide. Unless otherwise stated, all the items in that section apply to the Cluster Infrastructure we will provision on Azure. Read Red Hat's documentation on Installing a cluster quickly on Azure to deploy a cluster with no modifications. Deploying a cluster with openshift-install will provide the following topology.","title":"Golden Topology"},{"location":"infrastructure/azure/#machinesets","text":"The following templates can be used to generate your infrastructure, cloudpak and storage MachineSets MachineSets Note Click on each tab above for sample yamls to create your infrastructure Infrastructure Infrastructure MachineConfigPool apiVersion : machineconfiguration.openshift.io/v1 kind : MachineConfigPool metadata : name : infra spec : machineConfigSelector : matchExpressions : - key : machineconfiguration.openshift.io/role operator : In values : - worker - infra nodeSelector : matchLabels : node-role.kubernetes.io/infra : \"\" Infrastructure MachineSet Create 3 Infrastructure MachineSets, the $region, $zone and $clusterid variables in the sample below. zone should be 1 2 and 3, respectively, on each machineset yaml file. The default node type is Standard_D4s_v3 (4CPU, 16GB mem), if your infrastructure needs are bigger, you can increase it to a bigger node type, or increase the replica count per Availability Zone. To obtain your Cluster ID, you can run the following command. $ oc get -o jsonpath = '{.status.infrastructureName}{\"\\n\"}' infrastructure cluster If you Bring Your Own Network , update lines 48, 57 and 61 If you Bring Your Own Resource Group , update lines 41 and 56. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 apiVersion : machine.openshift.io/v1beta1 kind : MachineSet metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : infra machine.openshift.io/cluster-api-machine-type : infra name : $clusterid-infra-$region$zone namespace : openshift-machine-api spec : replicas : 1 selector : matchLabels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machineset : $clusterid-infra-$region$zone template : metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : infra machine.openshift.io/cluster-api-machine-type : infra machine.openshift.io/cluster-api-machineset : $clusterid-infra-$region$zone spec : taints : - effect : NoSchedule key : infra value : \"\" metadata : labels : node-role.kubernetes.io/infra : \"\" providerSpec : value : apiVersion : azureproviderconfig.openshift.io/v1beta1 kind : AzureMachineProviderSpec credentialsSecret : name : azure-cloud-credentials namespace : openshift-machine-api image : offer : \"\" publisher : \"\" resourceID : /resourceGroups/$clusterid-rg/providers/Microsoft.Compute/images/$clusterid sku : \"\" version : \"\" location : $region managedIdentity : $clusterid-identity metadata : creationTimestamp : null networkResourceGroup : $clusterid-rg osDisk : diskSizeGB : 128 managedDisk : storageAccountType : Premium_LRS osType : Linux publicIP : false publicLoadBalancer : $clusterid resourceGroup : $clusterid-rg subnet : $clusterid-worker-subnet userDataSecret : name : worker-user-data vmSize : Standard_D4s_v3 vnet : $clusterid-vnet zone : \"$zone\" Infrastructure Component Placement Follow the Golden Topology Guidelines to control placement of infrastructure components. Storage Storage MachineConfigPool apiVersion : machineconfiguration.openshift.io/v1 kind : MachineConfigPool metadata : name : storage spec : machineConfigSelector : matchExpressions : - key : machineconfiguration.openshift.io/role operator : In values : - worker - storage nodeSelector : matchLabels : cluster.ocs.openshift.io/openshift-storage : \"\" node-role.kubernetes.io/storage : \"\" Storage MachineSet Create 3 Storage MachineSets, replacing the region, $zone and $clusterid variables in the sample below. zone should be 1 2 and 3, respectively, on each machineset yaml file. The default node type is Standard_D16s_v3 (16CPU, 64GB mem), To obtain your Cluster ID, you can run the following command. $ oc get -o jsonpath = '{.status.infrastructureName}{\"\\n\"}' infrastructure cluster If you Bring Your Own Network , update lines 49, 58 and 62 If you Bring Your Own Resource Group , update lines 42 and 57. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 apiVersion : machine.openshift.io/v1beta1 kind : MachineSet metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : storage machine.openshift.io/cluster-api-machine-type : storage name : $clusterid-storage-$region$zone namespace : openshift-machine-api spec : replicas : 1 selector : matchLabels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machineset : $clusterid-storage-$region$zone template : metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : storage machine.openshift.io/cluster-api-machine-type : storage machine.openshift.io/cluster-api-machineset : $clusterid-storage-$region$zone spec : taints : - effect : NoSchedule key : node.ocs.openshift.io/storage value : \"true\" metadata : labels : cluster.ocs.openshift.io/openshift-storage : \"\" node-role.kubernetes.io/storage : \"\" providerSpec : value : apiVersion : azureproviderconfig.openshift.io/v1beta1 kind : AzureMachineProviderSpec credentialsSecret : name : azure-cloud-credentials namespace : openshift-machine-api image : offer : \"\" publisher : \"\" resourceID : /resourceGroups/$clusterid-rg/providers/Microsoft.Compute/images/$clusterid sku : \"\" version : \"\" location : $region managedIdentity : $clusterid-identity metadata : creationTimestamp : null networkResourceGroup : $clusterid-rg osDisk : diskSizeGB : 128 managedDisk : storageAccountType : Premium_LRS osType : Linux publicIP : false publicLoadBalancer : $clusterid resourceGroup : $clusterid-rg subnet : $clusterid-worker-subnet userDataSecret : name : worker-user-data vmSize : Standard_D16s_v3 vnet : $clusterid-vnet zone : \"$zone\" CloudPak CloudPak MachineConfigPool apiVersion : machineconfiguration.openshift.io/v1 kind : MachineConfigPool metadata : name : cp4x spec : machineConfigSelector : matchExpressions : - key : machineconfiguration.openshift.io/role operator : In values : - worker - cp4x nodeSelector : matchLabels : node-role.kubernetes.io/cp4x : \"\" CloudPak Tuned Configuration apiVersion : tuned.openshift.io/v1 kind : Tuned metadata : name : cp4x-tuned namespace : openshift-cluster-node-tuning-operator spec : profile : - name : cp4x-tuned data : | [main] summary=Tune Kernel parameters on OpenShift Worker Nodes running CloudPak workloads [sysctl] kernel.shmall = 33554432 kernel.shmmax = 68719476736 kernel.shmmni = 16384 kernel.sem = 250 1024000 100 16384 kernel.msgmax = 65536 kernel.msgmnb = 65536 kernel.msgmni = 32768 vm.max_map_count = 262144 recommend : - match : - label : node-role.kubernetes.io/cp4x priority : 10 profile : cp4x-tuned CloudPak ContainerRuntimeConfig apiVersion : machineconfiguration.openshift.io/v1 kind : ContainerRuntimeConfig metadata : name : cp4x-containerruntimeconfig spec : machineConfigPoolSelector : matchLabels : limits-crio : cp4x-containerruntimeconfig containerRuntimeConfig : pidsLimit : 12288 CloudPak MachineSet Create 3 Storage MachineSets, replacing the region, $zone and $clusterid variables in the sample below. zone should be 1 2 and 3, respectively, on each machineset yaml file. The default node type is Standard_D8s_v3 (8CPU, 32GB mem) and can be modified on line 56. To obtain your Cluster ID, you can run the following command. $ oc get -o jsonpath = '{.status.infrastructureName}{\"\\n\"}' infrastructure cluster If you Bring Your Own Network , change lines 44, 53 and 57. If you Bring Your Own Resource Group , change line 37 and 52. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 apiVersion : machine.openshift.io/v1beta1 kind : MachineSet metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : cp4x machine.openshift.io/cluster-api-machine-type : cp4x name : $clusterid-cp4x-$region$zone namespace : openshift-machine-api spec : replicas : 0 selector : matchLabels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machineset : $clusterid-cp4x-$region$zone template : metadata : labels : machine.openshift.io/cluster-api-cluster : $clusterid machine.openshift.io/cluster-api-machine-role : worker machine.openshift.io/cluster-api-machine-type : worker machine.openshift.io/cluster-api-machineset : $clusterid-cp4x-$region$zone spec : metadata : labels : node-role.kubernetes.io/cp4x : \"\" providerSpec : value : apiVersion : azureproviderconfig.openshift.io/v1beta1 kind : AzureMachineProviderSpec credentialsSecret : name : azure-cloud-credentials namespace : openshift-machine-api image : offer : \"\" publisher : \"\" resourceID : /resourceGroups/$clusterid-rg/providers/Microsoft.Compute/images/$clusterid sku : \"\" version : \"\" location : $region managedIdentity : $clusterid-identity metadata : creationTimestamp : null networkResourceGroup : $clusterid-rg osDisk : diskSizeGB : 128 managedDisk : storageAccountType : Premium_LRS osType : Linux publicIP : false publicLoadBalancer : $clusterid resourceGroup : $clusterid-rg subnet : $clusterid-worker-subnet userDataSecret : name : worker-user-data vmSize : Standard_D8s_v3 vnet : $clusterid-vnet zone : \"$zone\"","title":"MachineSets"},{"location":"infrastructure/azure/#storage","text":"In a default OpenShift deployment on Azure, you will have a single storage class called managed-premium which only provides ReadWriteOnce (RWO) access modes. $ oc get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE managed-premium ( default ) kubernetes.io/azure-disk Delete WaitForFirstConsumer true 34h This storage class will consume azure-disk resources from a Storage Account in the cluster resource group. If your CloudPak workloads require a ReadWriteMany (RWX) compatible storage class, the recommended solution is to use OpenShift Container Storage .","title":"Storage"},{"location":"infrastructure/azure/#installation-scenarios","text":"The recommended method for installing OpenShift into Azure is using an Installer Provisioned Infrastructure Deployment . It is a solid foundation that encompass most of the best practices and recommendations outlined in the Golden Topology . The installation process is fully supported by IBM and Red Hat, and you won't need to manage and maintain additional code assets to deploy your cluster. Customers requirements may prevent you from using an unmodified OpenShift installation in Azure. Below are some of the most common customer requests, as well as ways of handling. All these methods begin with the same process $ openshift-install create install-config ? SSH Public Key /Users/ncolon/.ssh/openshift_rsa.pub ? Platform azure ? azure subscription id ceb9d1a9-d0e8-46f9-a404-f2635fba6829 ? azure tenant id fcf67057-50c9-4ad4-98f3-ffca64add9e9 ? azure service principal client id a10be0dd-d46f-4592-9a2c-31d1c0bbecbe ? azure service principal client secret [ ? for help ] ********************************** INFO Saving user credentials to \"/Users/ncolon/.azure/osServicePrincipal.json\" INFO Credentials loaded from file \"/Users/ncolon/.azure/osServicePrincipal.json\" ? Region centralus ? Base Domain azure.ibm-gtmaa.dev ? Cluster Name mycluster ? Pull Secret [ ? for help ] *************************** INFO Install-Config created in : . Once you've modified your install-config.yaml to fit your needs, create your manifests files. $ openshift-install create manifests INFO Credentials loaded from file \"/Users/ncolon/.azure/osServicePrincipal.json\" INFO Consuming Install Config from target directory INFO Manifests created in : manifests and openshift Once you've modified your manifests files, you can proceed to create your OpenShift Cluster. $ openshift-install create cluster Below is a sample install-config.yaml file for an Azure OpenShift Cluster. apiVersion : v1 baseDomain : azure.ibm-gtmaa.dev compute : - architecture : amd64 hyperthreading : Enabled name : worker platform : {} replicas : 3 controlPlane : architecture : amd64 hyperthreading : Enabled name : master platform : {} replicas : 3 metadata : creationTimestamp : null name : mycluster networking : clusterNetwork : - cidr : 10.128.0.0/14 hostPrefix : 23 machineNetwork : - cidr : 10.0.0.0/16 networkType : OpenShiftSDN serviceNetwork : - 172.30.0.0/16 platform : azure : baseDomainResourceGroupName : ncolon-openshift4-rg cloudName : AzurePublicCloud outboundType : Loadbalancer region : centralus publish : External pullSecret : '{\"auths\":{...}}' sshKey : | ssh-rsa AAAAB3N...","title":"Installation Scenarios"},{"location":"infrastructure/azure/#node-sizing","text":"","title":"Node Sizing"},{"location":"infrastructure/azure/#private-endpoints","text":"An unmodified OpenShift installation will provision publicly facing LoadBalancers for api and *.apps ( api-int is ALWAYS private). If your customer requires private endpoints, modify your install-config.yaml by appending publish: Internal to it. The api and *.apps LoadBalancers will be created within your VNET CIDR range on the master and worker subnets ... sshKey : | ssh-rsa AAAAB3N... publish : Internal Warning There are limitations on how Azure VMs can reach the internet. They either have to Have a Public IP address attached to them Your VNET provides alternate ways of reaching the internet The VMs are connected to a LoadBalancer with a public IP address(although no traffic is send their way) When you deploy a cluster with publish: Internal , OpenShift operators will create a LoadBalancer with a public IP address (scenario 3 above) listening on port 27627. The loadbalancer is created as a service called outbound-provider in the openshift-config-managed Namespace.","title":"Private Endpoints"},{"location":"infrastructure/azure/#bring-your-own-resource-group","text":"Customers may wish to use a pre-existing resource group as the target for OpenShift components. This can be achieved by setting the platform.azure.resourceGroupName configuration parameter in install-config.yaml ... platform : azure : baseDomainResourceGroupName : os4-common cloudName : AzurePublicCloud outboundType : Loadbalancer region : centralus resourceGroupName : example-rg ... Warning The resource group used MUST be empty, and will be deleted when the cluster is destroyed","title":"Bring Your Own Resource Group"},{"location":"infrastructure/azure/#bring-your-own-network","text":"Customers may wish to use their pre-existing VNET infrastructure as the target for OpenShift networking environment. This can be achieved by setting the networkResourceGroupName , virtualNetwork , controlPlaneSubnet and computeSubnet configuration parameters in the platform.azure section of install-config.yaml ... platform : azure : baseDomainResourceGroupName : ncolon-openshift4-rg cloudName : AzurePublicCloud outboundType : Loadbalancer region : centralus networkResourceGroupName : my-cluster-vnet-rg virtualNetwork : my-vnet controlPlaneSubnet : my-master-subnet computeSubnet : my-worker-subnet ... Warning While deploying into existing VNET and Subnets is a supported scenario, it is the customer's responsibility that there are no policies that prevent OpenShift from deploying, any security groups that block traffic, etc Only one virtualNetwork is supported per cluster. Note You can set your controlPlaneSubnet and computeSubnet to the same if your customer requires a flat network in their VNET.","title":"Bring Your Own Network"},{"location":"infrastructure/azure/#user-defined-routing","text":"If absolutely no public endpoints can be present in the deployment of OpenShift on the customer's VNET, the scenario described on Private Endpoints is not enough. You also need to set the platform.azure.outboundType flag to UserDefinedRouting . This will instruct the OpenShift operators not to create the Public LoadBalancer listening on port 27627. ... platform : azure : baseDomainResourceGroupName : ncolon-openshift4-rg cloudName : AzurePublicCloud outboundType : UserDefinedRouting region : centralus ... Warning This configuration parameter assumes that connectivity to the internet is somehow provided by the VNET infrastructure, via proxies, routing through customer on-prem network connected via ExpressRoute, or any other mechanisms in place. It is the customer's responsibility to provide the necessary connectivity to the internet so OpenShift can download its required installation artifacts.","title":"User Defined Routing"},{"location":"infrastructure/azure/#bring-your-own-dns","text":"Customers may not wish to use Azure DNS Zones (public or private), opting instead to use their existing Enterprise DNS servers. This can be achieved by modifying the manifests/cluster-dns-02-config.yml DNS configuration manifest. Remove the spec.privateZone and spec.publicZone sections in the file. When done editing, the file should look like this: apiVersion : config.openshift.io/v1 kind : DNS metadata : creationTimestamp : null name : cluster spec : baseDomain : mycluster.azure.ibm-gtmaa.dev status : {} Warning By disabling this feature of the DNS Operator, the cluster will no longer be able to update DNS for you as LoadBalancers are created during installation. As the installation progresses, you will need to pay VERY close attention to the loadbalancers created in your cluster resource group. The first loadbalancer that gets created is <cluster_id>-internal . You need to update your DNS records so that api-int.<cluster_name>.<base_domain> points to this LoadBalancer. This happens very early in the deployment process, and the OpenShift master nodes will not boot up until this record is created. The next loadbalancer created is <cluster_id> . It hosts the endpoints for api and *.apps . During the first phase of OpenShift deployment (pre-bootstrapping), the LoadBalancer will consist of a single IP address, public or private. During the second phase of OpenShift deployment (post-bootstrapping) a 2nd IP address will be created by the OpenShift Ingress Operator, and it will have a long random character name. In the example above, you would point the DNS record api.<cluster_name>.<base_domain> to the public-lb-ip-v4 and *.apps.<cluster_name>.<base_domain> to the random character IP address. In the extreme case that your customer does not allow wildcard DNS entries on their Enterprise DNS servers, the following DNS records MUST be created, pointing to the apps Loadbalancer, instead of *.apps.<cluster_name>.<base_domain> . The cluster will not finish deployment until these records are created. oauth-openshift.apps.<cluster_name>.<base_domain> console-openshift-console.apps.<cluster_name>.<base_domain> downloads-openshift-console.apps.<cluster_name>.<base_domain> canary-openshift-ingress-canary.apps.<cluster_name>.<base_domain> alertmanager-main-openshift-monitoring.apps.<cluster_name>.<base_domain> grafana-openshift-monitoring.apps.<cluster_name>.<base_domain> prometheus-k8s-openshift-monitoring.apps.<cluster_name>.<base_domain> thanos-querier-openshift-monitoring.apps.<cluster_name>.<base_domain>","title":"Bring Your Own DNS"},{"location":"infrastructure/azure/#user-provided-infrastructure","text":"If your customer has any requirements that can't be met by the above solutions, we've provided an implementation in terraform . This implementation is meant to be modified to meet your customer needs, and requires working knowledge of terraform. Deploying this implementation with no modifications is the equivalent of running openshift-install create cluster , and should be avoided.","title":"User Provided Infrastructure"},{"location":"infrastructure/cluster/","text":"Cluster creation \u00b6 So you needed a RedHat OpenShift cluster. How you create that depends a lot on the decision point that you make. Some of these architectural decisions are listed in here . The following is a generic simplification of the cluster creation and setup. You have to decide the following: Where will you stand the cluster? On premises, cloud providers, and other decision points there. How would you build the cluster? You can do a managed or unmanaged cluster. For most of the practical reason, we recommend that you use either a Managed cluster or an IPI cluster. UPI cluster has a lot more risk in installation and maintenance, so unless you have a very valid reason ... do not venture there. Decide on whether you can and wanted to do a machineset for infrastructure and storage nodes. The gitops resources that we have currently only supports aws , azure and vsphere platforms. Further more managed OpenShift has the infrastructure nodes created OOTB while they are not yet support OpenShift Data Foundation (ODF). The detailed steps for installing Managed OpenShift are listed in: RedHat OpenShift on IBM Cloud (ROKS) RedHat OpenShift on AWS (ROSA) Azure RedHat OpenShift (ARO) For a unmanaged OpenShift using Installer Provisioned Infrastructure (IPI) there are some use-cases explained in: Azure AWS VMware Implementing openshift-gitops operator is discussed in TBA. Creating infrastructure nodes and storage nodes for aws , azure and vsphere is explained in infrastructure GitOps .","title":"Cluster"},{"location":"infrastructure/cluster/#cluster-creation","text":"So you needed a RedHat OpenShift cluster. How you create that depends a lot on the decision point that you make. Some of these architectural decisions are listed in here . The following is a generic simplification of the cluster creation and setup. You have to decide the following: Where will you stand the cluster? On premises, cloud providers, and other decision points there. How would you build the cluster? You can do a managed or unmanaged cluster. For most of the practical reason, we recommend that you use either a Managed cluster or an IPI cluster. UPI cluster has a lot more risk in installation and maintenance, so unless you have a very valid reason ... do not venture there. Decide on whether you can and wanted to do a machineset for infrastructure and storage nodes. The gitops resources that we have currently only supports aws , azure and vsphere platforms. Further more managed OpenShift has the infrastructure nodes created OOTB while they are not yet support OpenShift Data Foundation (ODF). The detailed steps for installing Managed OpenShift are listed in: RedHat OpenShift on IBM Cloud (ROKS) RedHat OpenShift on AWS (ROSA) Azure RedHat OpenShift (ARO) For a unmanaged OpenShift using Installer Provisioned Infrastructure (IPI) there are some use-cases explained in: Azure AWS VMware Implementing openshift-gitops operator is discussed in TBA. Creating infrastructure nodes and storage nodes for aws , azure and vsphere is explained in infrastructure GitOps .","title":"Cluster creation"},{"location":"infrastructure/golden-topology-aro/","text":"OpenShift Golden Topology on ARO \u00b6 Overview \u00b6 Before reading this document, familiarize yourself with the Golden Topology section of this Production Deployment Guide. Unless otherwise stated, all the items in that section apply to the Cluster Infrastructure we will provision on Azure. Try It Out! \u00b6 We've included an ARO Hands-on Deployment Guide that will allow you to deploy the following Cluster Reference Architecture into your Azure Subscription. Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net","title":"Azure Managed (ARO)"},{"location":"infrastructure/golden-topology-aro/#openshift-golden-topology-on-aro","text":"","title":"OpenShift Golden Topology on ARO"},{"location":"infrastructure/golden-topology-aro/#overview","text":"Before reading this document, familiarize yourself with the Golden Topology section of this Production Deployment Guide. Unless otherwise stated, all the items in that section apply to the Cluster Infrastructure we will provision on Azure.","title":"Overview"},{"location":"infrastructure/golden-topology-aro/#try-it-out","text":"We've included an ARO Hands-on Deployment Guide that will allow you to deploy the following Cluster Reference Architecture into your Azure Subscription. Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net","title":"Try It Out!"},{"location":"infrastructure/golden-topology-aws/","text":"OpenShift Golden Topology on AWS \u00b6 Overview \u00b6 Before reading this document, familiarize yourself with the Golden Topology section of this Production Deployment Guide. Unless otherwise stated, all the items in that section apply to the Cluster Infrastructure we will provision on Azure. Try It Out! \u00b6 We've included an AWS Hands-on Guide that will allow you to deploy the following Cluster Reference Architecture into your AWS Subscription. Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net","title":"AWS Unmanaged"},{"location":"infrastructure/golden-topology-aws/#openshift-golden-topology-on-aws","text":"","title":"OpenShift Golden Topology on AWS"},{"location":"infrastructure/golden-topology-aws/#overview","text":"Before reading this document, familiarize yourself with the Golden Topology section of this Production Deployment Guide. Unless otherwise stated, all the items in that section apply to the Cluster Infrastructure we will provision on Azure.","title":"Overview"},{"location":"infrastructure/golden-topology-aws/#try-it-out","text":"We've included an AWS Hands-on Guide that will allow you to deploy the following Cluster Reference Architecture into your AWS Subscription. Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net","title":"Try It Out!"},{"location":"infrastructure/golden-topology-azure/","text":"OpenShift Golden Topology on Azure \u00b6 Overview \u00b6 Before reading this document, familiarize yourself with the Golden Topology section of this Production Deployment Guide. Unless otherwise stated, all the items in that section apply to the Cluster Infrastructure we will provision on Azure. Try It Out! \u00b6 We've included an Azure Hands-on Guide that will allow you to deploy the following Cluster Reference Architecture into your Azure Subscription. Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net","title":"Azure Unmanaged"},{"location":"infrastructure/golden-topology-azure/#openshift-golden-topology-on-azure","text":"","title":"OpenShift Golden Topology on Azure"},{"location":"infrastructure/golden-topology-azure/#overview","text":"Before reading this document, familiarize yourself with the Golden Topology section of this Production Deployment Guide. Unless otherwise stated, all the items in that section apply to the Cluster Infrastructure we will provision on Azure.","title":"Overview"},{"location":"infrastructure/golden-topology-azure/#try-it-out","text":"We've included an Azure Hands-on Guide that will allow you to deploy the following Cluster Reference Architecture into your Azure Subscription. Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net","title":"Try It Out!"},{"location":"infrastructure/golden-topology-roks/","text":"OpenShift Golden Topology on ROKS \u00b6 Overview \u00b6 Before reading this document, familiarize yourself with the Golden Topology section of this Production Deployment Guide. Unless otherwise stated, all the items in that section apply to the Cluster Infrastructure we will provision on IBM Cloud. Try It Out! \u00b6 We've included an ROKS Hands-on Deployment Guide that will allow you to deploy the following Cluster Reference Architecture into your IBM Cloud Subscription. Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net","title":"IBM Cloud Managed (ROKS)"},{"location":"infrastructure/golden-topology-roks/#openshift-golden-topology-on-roks","text":"","title":"OpenShift Golden Topology on ROKS"},{"location":"infrastructure/golden-topology-roks/#overview","text":"Before reading this document, familiarize yourself with the Golden Topology section of this Production Deployment Guide. Unless otherwise stated, all the items in that section apply to the Cluster Infrastructure we will provision on IBM Cloud.","title":"Overview"},{"location":"infrastructure/golden-topology-roks/#try-it-out","text":"We've included an ROKS Hands-on Deployment Guide that will allow you to deploy the following Cluster Reference Architecture into your IBM Cloud Subscription. Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net","title":"Try It Out!"},{"location":"infrastructure/golden-topology-rosa/","text":"OpenShift Golden Topology on AWS \u00b6 Overview \u00b6 Before reading this document, familiarize yourself with the Golden Topology section of this Production Deployment Guide. Unless otherwise stated, all the items in that section apply to the Cluster Infrastructure we will provision on Azure. Try It Out! \u00b6 We've included an ROSA Hands-on Guide that will allow you to deploy the following Cluster Reference Architecture into your AWS Subscription. Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net","title":"AWS Managed (ROSA)"},{"location":"infrastructure/golden-topology-rosa/#openshift-golden-topology-on-aws","text":"","title":"OpenShift Golden Topology on AWS"},{"location":"infrastructure/golden-topology-rosa/#overview","text":"Before reading this document, familiarize yourself with the Golden Topology section of this Production Deployment Guide. Unless otherwise stated, all the items in that section apply to the Cluster Infrastructure we will provision on Azure.","title":"Overview"},{"location":"infrastructure/golden-topology-rosa/#try-it-out","text":"We've included an ROSA Hands-on Guide that will allow you to deploy the following Cluster Reference Architecture into your AWS Subscription. Right-click here and select \"Save Link As\" to download the DrawIO file used to generate these diagrams or open the diagram using diagrams.net","title":"Try It Out!"},{"location":"infrastructure/golden-topology/","text":"OpenShift Golden Topology \u00b6 Overview \u00b6 A highly available, resilient deployment starts with a solid physical infrastructure foundation that embeds all these principles. The topology described bellow provides prescriptive guidance on how to achieve these principles at the Network, Compute and Storage layer of your cluster. All implementation details are handled with YAML so we can leverage Infrastructure as Code/GitOps processes to automate the experience. Networking \u00b6 We start by deploying our cluster into a single region with multiple availability zones . By deploying into multiple AZs, we provide physical redundancy of our infrastructure into physically separated resources. We depend on external Network Services like DHCP and DNS to provide initial cluster connectivity and configuration. Finally, Load Balancers are used to provide access to the management and application endpoints of our cluster. Regions \u00b6 A region is defined as an actual real-life geographical location where your cloud resources are located. They could be public cloud regions or your on-prem datacenters. An OpenShift cluster should only span a single region. Stretch clusters are not recommended, no matter the latency between these regions, to avoid ETCD database corruption. Note If you need to spread your application workload across multiple regions, the use of a Geographical LoadBalancer is recommended. Availability Zones \u00b6 An availability zone (AZ) is defined as physical locations in your region that are physically separated and connected with private, low latency, high throughput and redundant network connections. Note You should deploy your OpenShift cluster to at least 3 AZs, one master per zone and workers spread across them. Note Create a per-zone MachineSets for each type of server you want to have. This increases your Cluster redundancy. If you're not using MachineSets, make sure you create one master and one worker per zone. Network Services \u00b6 When deploying OpenShift, certain network services need to pre in place prior to deployment DHCP : DHCP is required to provide initial network connectivity to your nodes so it can download its ignition configuration file DNS : DNS is required to provide information on LoadBalancer endpoints for configuration NTP : Time Synchronization across all cluster nodes is critical due to the use of TLS certificates across the platform. By default they will reach out to Red Hat NTP servers, but can be configured to use internal enterprise NTP servers. Container Image Registry : If you're deploying in an environment that has connectivity to the internet, the cluster will use public container image registries provided by Red Hat (quay.io) or IBM (cp.icr.io). If you're deploying in a Restricted Network environment, you need to mirror the platform, operatorhub and cloudpak container images to a local image repository. In a public cloud environment, DHCP and DNS are for the most part handled automatically by your cloud provider, and are configured when deploying your OpenShift Cluster with IPI . When deploying On-Prem, you need to provide these services in your infrastructure. Warning Avoid creating a bastion server that also serves as a DHCP/DNS server to provide these services for the cluster. Production level clusters require production-level backing services. LoadBalancers \u00b6 To spread workload and API traffic to your nodes, there are 3 endpoints to consider API : LoadBalances Cluster API traffic to all masters on port 6443 DNS CNAME or A record Points api. cluster_name . base_domain to the IP Address of this LoadBalancer API-INT : LoadBalances Cluster API traffic to all masters and bootstrap server on port 6443 and 22623 DNS CNAME or A record Points api-int. cluster_name . base_domain to the IP Address this LoadBalancer APPS : LoadBalances HTTP/HTTPS traffic to all worker nodes on port 80 and 443. DNS WILDCARD CNAME or A record Points *.apps. cluster_name . base_domain to the IP Address this LoadBalancer Warning The API-INT endpoint contains sensitive cluster information. Split API and API-INT into separate load balancers, and place adequate traffic filters on API-INT so its only accessible from the cluster members CIDR range. OpenShift SDN \u00b6 When sizing your cluster, be aware of the networking section in your install-config.yaml cluster definition. The default values are shown bellow: networking : clusterNetwork : - cidr : 10.128.0.0/14 hostPrefix : 23 machineNetwork : - cidr : 192.168.100.0/24 networkType : OpenShiftSDN serviceNetwork : - 172.30.0.0/16 Error These values CAN NOT be changed after deployment. A cluster rebuild is necessary if these values need to be changed, so plan ahead. Warning Make sure there's no overlap between any of the CIDR ranges and external resources (databases, mainframes, etc) that need to be accessed from within the cluster. networking.clusterNetwork.cidr : The CIDR range for pods in the OpenShift SDN. networking.clusterNetwork.hostPrefix : Defines the mask for cluster network for pods within a node. Controls the maximum number of pods that can be placed on a single node networking.machineNetwork.cidr : The CIDR range for OpenShift nodes in your network. networking.serviceNetwork : The CIDR range for services in the OpenShift SDN networking.networkType : The CNI plugin to use for the OpenShift SDN. The networking.clusterNetwork parameters control how many nodes per cluster and pods per node you can have. With the default values, you can host up to 512 nodes and 510 pods per node. nodesPerCluster: 2^(hostPrefix - cidrMask) podsPerNode: 2^(32 - hostPrefix) - 2 hostPrefix: 23 = 512 nodes in cluster node1: 10.128.0.0/23 = 510 pods node2: 10.128.2.0/23 node3: 10.128.4.0/23 ... clusterNetwork.cidr clusterNetwork.hostPrefix nodesPerCluster podsPerNode 10.128.0.0/14 23 512 510 10.128.0.0/14 24 1024 254 10.128.0.0/12 23 2048 510 Compute \u00b6 Control Plane \u00b6 A good initial size for your masters is 3 nodes with 8CPU and 32GB memory. Since master nodes are deployed as static Machines objects, replacing them down the line is a complex task. The following outlines Red Hat's recommended Control Plane sizing. Oversizing them at deployment will ensure you have a cluster that can scale past your original estimates if needed. Number of worker nodes CPU cores Memory (GB) Storage (GB) 25 4 16 120 100 8 32 120 250 16 96 120 Note Spread your Control Plane nodes across multiple Availability Zones in your Region to provide resiliency to your cluster. Control Plane nodes should be deployed on a separate subnet within the machineNetwork CIDR range. Warning Storage for Control Plane nodes should provide at least 500 IOPS to minimize etcd latency. Compute Nodes \u00b6 Any node that is not a Control Plane node is considered a Compute (or Worker) node. They should be deployed as MachineSets to ensure High Availability and scalability Note Compute nodes should be deployed on a separate subnet within the machineNetwork CIDR range. Create a MachineSet per Availability Zone per Compute Node type (Infrastructure, Storage, CloudPak). Infrastructure Nodes \u00b6 Warning If you're running on a MANAGED platform like ROKS , ROSA or ARO , it is the Cloud Providers responsibility to manage these resources for you. Do not deploy Infrastructure Nodes on Managed Platforms. Skip to Storage You should deploy at least 3 nodes to host the OpenShift infrastructure components with 4CPU and 16GB memory. They are deployed on the worker subnet. Infrastructure Nodes allow customers to isolate infrastructure workloads for 2 primary purposes: To prevent incurring billing costs against subscription counts To separate maintenance and management The following outlines Red Hat's recommended Infrastructure Node sizing Compute Nodes CPU Memory (GB) CPU (Cluster Logging Enabled) Memory (GB) (Cluster Logging Enabled) 25 4 16 4 64 100 8 32 8 128 250 16 128 16 128 500 32 128 32 192 They are deployed as MachineSets with one MachineSet per Availability Zone. The MachineSet definition should include the following taints to ensure no non-infrastructure component is deployed on these nodes. Key Value Effect infra \"\" NoSchedule Note Since MachineSets can be modified, you can start with smaller sized nodes and scale as your cluster grows. Warning Infrastructure Nodes do not draw against your OpenShift Licensing Subscription. The use of taints ensures that only infrastructure components run on these nodes. The following components are considered infrastructure components Image Registry Ingress Controller Monitoring Metrics Cluster Logging Service Brokers Red Hat Quay Red Hat OpenShift Data Foundation (previously Red Hat OpenShift Container Storage) Red Hat Advanced Cluster Management Red Hat Advanced Cluster Security Red Hat OpenShift Gitops Red Hat OpenShift Pipelines Warning Cluster Logging is not deployed by default. If you're deploying it, take into account the increased capacity requirements outlined on the sizing table Error RedHat OpenShift Container Storage should not be deployed on infrastructure nodes. The use of dedicated storage nodes is recommended. Placement of these components is controlled with a combination of nodeSelectors and tolerations for each of the above deployments. Image Registry \u00b6 oc patch configs.imageregistry.operator.openshift.io/cluster --type = merge \\ -p '{\"spec\": {\"nodeSelector\": {\"node-role.kubernetes.io/infra\": \"\"},\"tolerations\": [{\"effect\":\"NoSchedule\", \"key\": \"infra\", \"value\": \"\"}]}}' Ingress Controller \u00b6 oc patch ingresscontroller/default -n openshift-ingress-operator --type = merge \\ -p ' { \"spec\" : { \"nodePlacement\" : { \"nodeSelector\" : { \"matchLabels\" : { \"node-role.kubernetes.io/infra\" : \"\" }} , \"tolerations\" : [{ \"effect\" : \"NoSchedule\" , \"key\" : \"infra\" , \"value\" : \"\" }]}}} \u2019 Monitoring \u00b6 Create a openshift-monitoring-configmap.yaml file with the following ConfigMap and apply it to your cluster. apiVersion : v1 kind : ConfigMap metadata : name : cluster-monitoring-config namespace : openshift-monitoring data : config.yaml : |+ alertmanagerMain: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule prometheusK8s: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule prometheusOperator: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule grafana: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule k8sPrometheusAdapter: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule kubeStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule telemeterClient: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule openshiftStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule thanosQuerier: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule oc apply -f openshift-monitoring-config.yaml Cluster Logging \u00b6 oc patch ClusterLogging instance --type = merge \\ -p '{\"spec\": {\"curation\": {\"curator\": {\"nodeSelector\": {\"node-role.kubernetes.io/infra\": \"\"}}},\"logStore\": {\"elasticsearch\": {\"nodeSelector\": {\"node-role.kubernetes.io/infra\": \"\"}}},\"visualization\": {\"kibana\": {\"nodeSelector\": { \"node-role.kubernetes.io/infra\": \"\"}}}}}' Storage \u00b6 By default, OpenShift will provide a ReadWriteOnce storage class leveraging the Cloud Provider storage infrastructure. For any storage requirements that can't be met with the cloud provider native storage, the recommended solution is to deploy OpenShift Container Storage/OpenShift Data Foundation. OCS handles data replication between multiple storage nodes, so your data will always be available regardless of any Availability Zone issues. If a ReadWriteMany storage class is required for your workloads, the use of OpenShift Container Storage is recommended. You should deploy at least 3 nodes with 16CPU and 64GB memory. Watch this video for an introduction to the various types of storage available in a Kubernetes cluster. Storage Nodes \u00b6 They are deployed on the worker subnet as MachineSets with one MachineSet per Availability Zone, and one replica per AZ. The MachineSet definition should include the following taints and labels to ensure no non-storage component is deployed on these nodes. If additional storage is needed in the future, you can Storage MachineSets as needed. Taints \u00b6 Key Value Effect node.ocs.openshift.io/storage true NoSchedule Labels \u00b6 Label Value cluster.ocs.openshift.io/openshift-storage \"\" Note Add taints and labels in the MachineSet definition to minimize manual configuration steps OpenShift Container Storage will provide the following StorageClasses Storage Class ReadWriteOnce ReadWriteMany ObjectBucket ocs-storagecluster-ceph-rbd \u2705 \u274c \u274c ocs-storagecluster-cephfs \u2705 \u2705 \u274c openshift-storage.noobaa.io \u274c \u274c \u2705 Deploying OpenShift Container Storage \u00b6 Create the openshift-storage namespace. apiVersion : v1 kind : Namespace metadata : labels : openshift.io/cluster-monitoring : \"true\" name : openshift-storage spec : {} Create an OperatorGroup that will grant necessary RBAC permissions. apiVersion : operators.coreos.com/v1 kind : OperatorGroup metadata : annotations : olm.providedAPIs : \"BackingStore.v1alpha1.noobaa.io,BucketClass.v1alpha1.noobaa.io,CephBlockPool.v1.ceph.rook.io,CephClient.v1.ceph.rook.io,CephCluster.v1.ceph.rook.io,CephFilesystem.v1.ceph.rook.io,CephNFS.v1.ceph.rook.io,CephObjectRealm.v1.ceph.rook.io,CephObjectStore.v1.ceph.rook.io,CephObjectStoreUser.v1.ceph.rook.io,CephObjectZone.v1.ceph.rook.io,CephObjectZoneGroup.v1.ceph.rook.io,CephRBDMirror.v1.ceph.rook.io,NooBaa.v1alpha1.noobaa.io,OCSInitialization.v1.ocs.openshift.io,ObjectBucket.v1alpha1.objectbucket.io,ObjectBucketClaim.v1alpha1.objectbucket.io,StorageCluster.v1.ocs.openshift.io\" name : openshift-storage-operatorgroup spec : targetNamespaces : - openshift-storage Create the OCS subscription in the openshift-storage namespace apiVersion : operators.coreos.com/v1alpha1 kind : Subscription metadata : name : ocs-operator labels : operators.coreos.com/ocs-operator.openshift-storage : \"\" spec : channel : stable-4.6 installPlanApproval : Automatic name : ocs-operator source : redhat-operators sourceNamespace : openshift-marketplace Deploy a StorageCluster instance. Modify spec.storageDeviceSets.dataPVCTemplate.spec.resources.requests.storage to the size that you need, and spec.storageDeviceSets.dataPVCTemplate.spec.storageClassName to the default RWO storage class available in your cluster. apiVersion : ocs.openshift.io/v1 kind : StorageCluster metadata : name : ocs-storagecluster spec : externalStorage : {} storageDeviceSets : - config : {} count : 1 dataPVCTemplate : metadata : creationTimestamp : null spec : accessModes : - ReadWriteOnce resources : requests : storage : 512Gi storageClassName : thin volumeMode : Block status : {} name : ocs-deviceset placement : {} portable : true replica : 3 resources : {} Since we added additional taints to the Infrastructure and Storage Nodes, we need to configure some OpenShift Container Storage components to tolerate those taints. kind : ConfigMap apiVersion : v1 metadata : name : rook-ceph-operator-config namespace : openshift-storage data : CSI_LOG_LEVEL : \"5\" CSI_PLUGIN_TOLERATIONS : | - effect: NoSchedule key: infra operator: Equal value: \"\" - effect: NoSchedule key: node.ocs.openshift.io/storage operator: Exists CSI_PROVISIONER_TOLERATIONS : |2- - key: node.ocs.openshift.io/storage operator: Equal value: \"true\" effect: NoSchedule","title":"Overview"},{"location":"infrastructure/golden-topology/#openshift-golden-topology","text":"","title":"OpenShift Golden Topology"},{"location":"infrastructure/golden-topology/#overview","text":"A highly available, resilient deployment starts with a solid physical infrastructure foundation that embeds all these principles. The topology described bellow provides prescriptive guidance on how to achieve these principles at the Network, Compute and Storage layer of your cluster. All implementation details are handled with YAML so we can leverage Infrastructure as Code/GitOps processes to automate the experience.","title":"Overview"},{"location":"infrastructure/golden-topology/#networking","text":"We start by deploying our cluster into a single region with multiple availability zones . By deploying into multiple AZs, we provide physical redundancy of our infrastructure into physically separated resources. We depend on external Network Services like DHCP and DNS to provide initial cluster connectivity and configuration. Finally, Load Balancers are used to provide access to the management and application endpoints of our cluster.","title":"Networking"},{"location":"infrastructure/golden-topology/#regions","text":"A region is defined as an actual real-life geographical location where your cloud resources are located. They could be public cloud regions or your on-prem datacenters. An OpenShift cluster should only span a single region. Stretch clusters are not recommended, no matter the latency between these regions, to avoid ETCD database corruption. Note If you need to spread your application workload across multiple regions, the use of a Geographical LoadBalancer is recommended.","title":"Regions"},{"location":"infrastructure/golden-topology/#availability-zones","text":"An availability zone (AZ) is defined as physical locations in your region that are physically separated and connected with private, low latency, high throughput and redundant network connections. Note You should deploy your OpenShift cluster to at least 3 AZs, one master per zone and workers spread across them. Note Create a per-zone MachineSets for each type of server you want to have. This increases your Cluster redundancy. If you're not using MachineSets, make sure you create one master and one worker per zone.","title":"Availability Zones"},{"location":"infrastructure/golden-topology/#network-services","text":"When deploying OpenShift, certain network services need to pre in place prior to deployment DHCP : DHCP is required to provide initial network connectivity to your nodes so it can download its ignition configuration file DNS : DNS is required to provide information on LoadBalancer endpoints for configuration NTP : Time Synchronization across all cluster nodes is critical due to the use of TLS certificates across the platform. By default they will reach out to Red Hat NTP servers, but can be configured to use internal enterprise NTP servers. Container Image Registry : If you're deploying in an environment that has connectivity to the internet, the cluster will use public container image registries provided by Red Hat (quay.io) or IBM (cp.icr.io). If you're deploying in a Restricted Network environment, you need to mirror the platform, operatorhub and cloudpak container images to a local image repository. In a public cloud environment, DHCP and DNS are for the most part handled automatically by your cloud provider, and are configured when deploying your OpenShift Cluster with IPI . When deploying On-Prem, you need to provide these services in your infrastructure. Warning Avoid creating a bastion server that also serves as a DHCP/DNS server to provide these services for the cluster. Production level clusters require production-level backing services.","title":"Network Services"},{"location":"infrastructure/golden-topology/#loadbalancers","text":"To spread workload and API traffic to your nodes, there are 3 endpoints to consider API : LoadBalances Cluster API traffic to all masters on port 6443 DNS CNAME or A record Points api. cluster_name . base_domain to the IP Address of this LoadBalancer API-INT : LoadBalances Cluster API traffic to all masters and bootstrap server on port 6443 and 22623 DNS CNAME or A record Points api-int. cluster_name . base_domain to the IP Address this LoadBalancer APPS : LoadBalances HTTP/HTTPS traffic to all worker nodes on port 80 and 443. DNS WILDCARD CNAME or A record Points *.apps. cluster_name . base_domain to the IP Address this LoadBalancer Warning The API-INT endpoint contains sensitive cluster information. Split API and API-INT into separate load balancers, and place adequate traffic filters on API-INT so its only accessible from the cluster members CIDR range.","title":"LoadBalancers"},{"location":"infrastructure/golden-topology/#openshift-sdn","text":"When sizing your cluster, be aware of the networking section in your install-config.yaml cluster definition. The default values are shown bellow: networking : clusterNetwork : - cidr : 10.128.0.0/14 hostPrefix : 23 machineNetwork : - cidr : 192.168.100.0/24 networkType : OpenShiftSDN serviceNetwork : - 172.30.0.0/16 Error These values CAN NOT be changed after deployment. A cluster rebuild is necessary if these values need to be changed, so plan ahead. Warning Make sure there's no overlap between any of the CIDR ranges and external resources (databases, mainframes, etc) that need to be accessed from within the cluster. networking.clusterNetwork.cidr : The CIDR range for pods in the OpenShift SDN. networking.clusterNetwork.hostPrefix : Defines the mask for cluster network for pods within a node. Controls the maximum number of pods that can be placed on a single node networking.machineNetwork.cidr : The CIDR range for OpenShift nodes in your network. networking.serviceNetwork : The CIDR range for services in the OpenShift SDN networking.networkType : The CNI plugin to use for the OpenShift SDN. The networking.clusterNetwork parameters control how many nodes per cluster and pods per node you can have. With the default values, you can host up to 512 nodes and 510 pods per node. nodesPerCluster: 2^(hostPrefix - cidrMask) podsPerNode: 2^(32 - hostPrefix) - 2 hostPrefix: 23 = 512 nodes in cluster node1: 10.128.0.0/23 = 510 pods node2: 10.128.2.0/23 node3: 10.128.4.0/23 ... clusterNetwork.cidr clusterNetwork.hostPrefix nodesPerCluster podsPerNode 10.128.0.0/14 23 512 510 10.128.0.0/14 24 1024 254 10.128.0.0/12 23 2048 510","title":"OpenShift SDN"},{"location":"infrastructure/golden-topology/#compute","text":"","title":"Compute"},{"location":"infrastructure/golden-topology/#control-plane","text":"A good initial size for your masters is 3 nodes with 8CPU and 32GB memory. Since master nodes are deployed as static Machines objects, replacing them down the line is a complex task. The following outlines Red Hat's recommended Control Plane sizing. Oversizing them at deployment will ensure you have a cluster that can scale past your original estimates if needed. Number of worker nodes CPU cores Memory (GB) Storage (GB) 25 4 16 120 100 8 32 120 250 16 96 120 Note Spread your Control Plane nodes across multiple Availability Zones in your Region to provide resiliency to your cluster. Control Plane nodes should be deployed on a separate subnet within the machineNetwork CIDR range. Warning Storage for Control Plane nodes should provide at least 500 IOPS to minimize etcd latency.","title":"Control Plane"},{"location":"infrastructure/golden-topology/#compute-nodes","text":"Any node that is not a Control Plane node is considered a Compute (or Worker) node. They should be deployed as MachineSets to ensure High Availability and scalability Note Compute nodes should be deployed on a separate subnet within the machineNetwork CIDR range. Create a MachineSet per Availability Zone per Compute Node type (Infrastructure, Storage, CloudPak).","title":"Compute Nodes"},{"location":"infrastructure/golden-topology/#infrastructure-nodes","text":"Warning If you're running on a MANAGED platform like ROKS , ROSA or ARO , it is the Cloud Providers responsibility to manage these resources for you. Do not deploy Infrastructure Nodes on Managed Platforms. Skip to Storage You should deploy at least 3 nodes to host the OpenShift infrastructure components with 4CPU and 16GB memory. They are deployed on the worker subnet. Infrastructure Nodes allow customers to isolate infrastructure workloads for 2 primary purposes: To prevent incurring billing costs against subscription counts To separate maintenance and management The following outlines Red Hat's recommended Infrastructure Node sizing Compute Nodes CPU Memory (GB) CPU (Cluster Logging Enabled) Memory (GB) (Cluster Logging Enabled) 25 4 16 4 64 100 8 32 8 128 250 16 128 16 128 500 32 128 32 192 They are deployed as MachineSets with one MachineSet per Availability Zone. The MachineSet definition should include the following taints to ensure no non-infrastructure component is deployed on these nodes. Key Value Effect infra \"\" NoSchedule Note Since MachineSets can be modified, you can start with smaller sized nodes and scale as your cluster grows. Warning Infrastructure Nodes do not draw against your OpenShift Licensing Subscription. The use of taints ensures that only infrastructure components run on these nodes. The following components are considered infrastructure components Image Registry Ingress Controller Monitoring Metrics Cluster Logging Service Brokers Red Hat Quay Red Hat OpenShift Data Foundation (previously Red Hat OpenShift Container Storage) Red Hat Advanced Cluster Management Red Hat Advanced Cluster Security Red Hat OpenShift Gitops Red Hat OpenShift Pipelines Warning Cluster Logging is not deployed by default. If you're deploying it, take into account the increased capacity requirements outlined on the sizing table Error RedHat OpenShift Container Storage should not be deployed on infrastructure nodes. The use of dedicated storage nodes is recommended. Placement of these components is controlled with a combination of nodeSelectors and tolerations for each of the above deployments.","title":"Infrastructure Nodes"},{"location":"infrastructure/golden-topology/#image-registry","text":"oc patch configs.imageregistry.operator.openshift.io/cluster --type = merge \\ -p '{\"spec\": {\"nodeSelector\": {\"node-role.kubernetes.io/infra\": \"\"},\"tolerations\": [{\"effect\":\"NoSchedule\", \"key\": \"infra\", \"value\": \"\"}]}}'","title":"Image Registry"},{"location":"infrastructure/golden-topology/#ingress-controller","text":"oc patch ingresscontroller/default -n openshift-ingress-operator --type = merge \\ -p ' { \"spec\" : { \"nodePlacement\" : { \"nodeSelector\" : { \"matchLabels\" : { \"node-role.kubernetes.io/infra\" : \"\" }} , \"tolerations\" : [{ \"effect\" : \"NoSchedule\" , \"key\" : \"infra\" , \"value\" : \"\" }]}}} \u2019","title":"Ingress Controller"},{"location":"infrastructure/golden-topology/#monitoring","text":"Create a openshift-monitoring-configmap.yaml file with the following ConfigMap and apply it to your cluster. apiVersion : v1 kind : ConfigMap metadata : name : cluster-monitoring-config namespace : openshift-monitoring data : config.yaml : |+ alertmanagerMain: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule prometheusK8s: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule prometheusOperator: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule grafana: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule k8sPrometheusAdapter: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule kubeStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule telemeterClient: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule openshiftStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule thanosQuerier: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - key: infra value: \"\" effect: NoSchedule oc apply -f openshift-monitoring-config.yaml","title":"Monitoring"},{"location":"infrastructure/golden-topology/#cluster-logging","text":"oc patch ClusterLogging instance --type = merge \\ -p '{\"spec\": {\"curation\": {\"curator\": {\"nodeSelector\": {\"node-role.kubernetes.io/infra\": \"\"}}},\"logStore\": {\"elasticsearch\": {\"nodeSelector\": {\"node-role.kubernetes.io/infra\": \"\"}}},\"visualization\": {\"kibana\": {\"nodeSelector\": { \"node-role.kubernetes.io/infra\": \"\"}}}}}'","title":"Cluster Logging"},{"location":"infrastructure/golden-topology/#storage","text":"By default, OpenShift will provide a ReadWriteOnce storage class leveraging the Cloud Provider storage infrastructure. For any storage requirements that can't be met with the cloud provider native storage, the recommended solution is to deploy OpenShift Container Storage/OpenShift Data Foundation. OCS handles data replication between multiple storage nodes, so your data will always be available regardless of any Availability Zone issues. If a ReadWriteMany storage class is required for your workloads, the use of OpenShift Container Storage is recommended. You should deploy at least 3 nodes with 16CPU and 64GB memory. Watch this video for an introduction to the various types of storage available in a Kubernetes cluster.","title":"Storage"},{"location":"infrastructure/golden-topology/#storage-nodes","text":"They are deployed on the worker subnet as MachineSets with one MachineSet per Availability Zone, and one replica per AZ. The MachineSet definition should include the following taints and labels to ensure no non-storage component is deployed on these nodes. If additional storage is needed in the future, you can Storage MachineSets as needed.","title":"Storage Nodes"},{"location":"infrastructure/golden-topology/#deploying-openshift-container-storage","text":"Create the openshift-storage namespace. apiVersion : v1 kind : Namespace metadata : labels : openshift.io/cluster-monitoring : \"true\" name : openshift-storage spec : {} Create an OperatorGroup that will grant necessary RBAC permissions. apiVersion : operators.coreos.com/v1 kind : OperatorGroup metadata : annotations : olm.providedAPIs : \"BackingStore.v1alpha1.noobaa.io,BucketClass.v1alpha1.noobaa.io,CephBlockPool.v1.ceph.rook.io,CephClient.v1.ceph.rook.io,CephCluster.v1.ceph.rook.io,CephFilesystem.v1.ceph.rook.io,CephNFS.v1.ceph.rook.io,CephObjectRealm.v1.ceph.rook.io,CephObjectStore.v1.ceph.rook.io,CephObjectStoreUser.v1.ceph.rook.io,CephObjectZone.v1.ceph.rook.io,CephObjectZoneGroup.v1.ceph.rook.io,CephRBDMirror.v1.ceph.rook.io,NooBaa.v1alpha1.noobaa.io,OCSInitialization.v1.ocs.openshift.io,ObjectBucket.v1alpha1.objectbucket.io,ObjectBucketClaim.v1alpha1.objectbucket.io,StorageCluster.v1.ocs.openshift.io\" name : openshift-storage-operatorgroup spec : targetNamespaces : - openshift-storage Create the OCS subscription in the openshift-storage namespace apiVersion : operators.coreos.com/v1alpha1 kind : Subscription metadata : name : ocs-operator labels : operators.coreos.com/ocs-operator.openshift-storage : \"\" spec : channel : stable-4.6 installPlanApproval : Automatic name : ocs-operator source : redhat-operators sourceNamespace : openshift-marketplace Deploy a StorageCluster instance. Modify spec.storageDeviceSets.dataPVCTemplate.spec.resources.requests.storage to the size that you need, and spec.storageDeviceSets.dataPVCTemplate.spec.storageClassName to the default RWO storage class available in your cluster. apiVersion : ocs.openshift.io/v1 kind : StorageCluster metadata : name : ocs-storagecluster spec : externalStorage : {} storageDeviceSets : - config : {} count : 1 dataPVCTemplate : metadata : creationTimestamp : null spec : accessModes : - ReadWriteOnce resources : requests : storage : 512Gi storageClassName : thin volumeMode : Block status : {} name : ocs-deviceset placement : {} portable : true replica : 3 resources : {} Since we added additional taints to the Infrastructure and Storage Nodes, we need to configure some OpenShift Container Storage components to tolerate those taints. kind : ConfigMap apiVersion : v1 metadata : name : rook-ceph-operator-config namespace : openshift-storage data : CSI_LOG_LEVEL : \"5\" CSI_PLUGIN_TOLERATIONS : | - effect: NoSchedule key: infra operator: Equal value: \"\" - effect: NoSchedule key: node.ocs.openshift.io/storage operator: Exists CSI_PROVISIONER_TOLERATIONS : |2- - key: node.ocs.openshift.io/storage operator: Equal value: \"true\" effect: NoSchedule","title":"Deploying OpenShift Container Storage"},{"location":"infrastructure/ibm-cloud/","text":"IBM Cloud \u00b6 Coming soon...","title":"IBM Cloud"},{"location":"infrastructure/ibm-cloud/#ibm-cloud","text":"Coming soon...","title":"IBM Cloud"},{"location":"infrastructure/infra-gitops/","text":"Infrastructure creation automation \u00b6 This document explains the steps for configuring infrastructure and storage resources using GitOps. The procedure is only applies when you want to customize the GitOps features manually. This procedure does not apply to the cluster bootstrapping method from the Quick-Start guide. In the Quick-Start guide, you must put ADD_INFRA=\"yes\" when you invoke bootstrap.sh . Requirements \u00b6 The following are needed: A working OpenShift 4.7 cluster on aws/azure/vsphere Logged in connection to the cluster you want to work on The oc command client (preferably v4.7) Run from under the multi-tenancy-gitops git structure Running \u00b6 The infra-mod.sh script performs the following tasks: Enable the YAML files under 1-infra argoCD application for machinesets , infraconfig , namespace-openshift-storage and storage Customize the content of 1-infra/argocd/machinesets.yaml using values from your cluster Customize the content of 1-infra/argocd/infraconfig.yaml using values from your cluster Customize the content of 1-infra/argocd/storage.yaml using values from your cluster Change the content for all branches of the customization Perform a GIT commit and push the changes to the GIT server Running this modification is simple: cd multi-tenancy-gitops ./scripts/infra-mod.sh The following shows the script running Since the repository is under argoCD control, these changes will be synchronized automatically if or when the bootstrap application is deployed in the cluster.","title":"Infra gitops"},{"location":"infrastructure/infra-gitops/#infrastructure-creation-automation","text":"This document explains the steps for configuring infrastructure and storage resources using GitOps. The procedure is only applies when you want to customize the GitOps features manually. This procedure does not apply to the cluster bootstrapping method from the Quick-Start guide. In the Quick-Start guide, you must put ADD_INFRA=\"yes\" when you invoke bootstrap.sh .","title":"Infrastructure creation automation"},{"location":"infrastructure/infra-gitops/#requirements","text":"The following are needed: A working OpenShift 4.7 cluster on aws/azure/vsphere Logged in connection to the cluster you want to work on The oc command client (preferably v4.7) Run from under the multi-tenancy-gitops git structure","title":"Requirements"},{"location":"infrastructure/infra-gitops/#running","text":"The infra-mod.sh script performs the following tasks: Enable the YAML files under 1-infra argoCD application for machinesets , infraconfig , namespace-openshift-storage and storage Customize the content of 1-infra/argocd/machinesets.yaml using values from your cluster Customize the content of 1-infra/argocd/infraconfig.yaml using values from your cluster Customize the content of 1-infra/argocd/storage.yaml using values from your cluster Change the content for all branches of the customization Perform a GIT commit and push the changes to the GIT server Running this modification is simple: cd multi-tenancy-gitops ./scripts/infra-mod.sh The following shows the script running Since the repository is under argoCD control, these changes will be synchronized automatically if or when the bootstrap application is deployed in the cluster.","title":"Running"},{"location":"infrastructure/infra-overview/","text":"Cluster Infrastructure Hands-On \u00b6 Cluster Infrastructure \u00b6 Deployment of IBM Cloud Pak requires RedHat OpenShift cluster. You can read more about the infrastructure layers and the implementation options for the different cloud platform providers in Golden Topology . In general, you can have one of the following clusters: Managed OpenShift clusters , on which some operation, monitoring and billing are integrated with the cloud-platform provider. These clusters are build and managed by the cloud-provider's interface: RedHat OpenShift on IBM Cloud (ROKS) RedHat OpenShift on AWS (ROSA) Azure RedHat OpenShift (ARO) Installer Provisioned Infrastructure (IPI) clusters which are created using the openshift-install command. These IPI clusters can typically be enhanced to accommodate infrastructure and storage nodes using the gitops scaffolding. Azure AWS VMware User Provisioned Infrastructure (UPI) clusters, which are built manually to accommodate an unsupported environment. Although we did not prefer this path, we understand that there are valid customer situation that may warrant the UPI implementation. However, with UPI, you are responsible on building the additional structure so that the cluster is production ready. Several customer may also request that the cluster would not have internet access (i.e restricted network or airgapped ). Although most can be fulfilled with IPI airgapped cluster, some specific deployment may need to be UPI.","title":"Overview"},{"location":"infrastructure/infra-overview/#cluster-infrastructure-hands-on","text":"","title":"Cluster Infrastructure Hands-On"},{"location":"infrastructure/infra-overview/#cluster-infrastructure","text":"Deployment of IBM Cloud Pak requires RedHat OpenShift cluster. You can read more about the infrastructure layers and the implementation options for the different cloud platform providers in Golden Topology . In general, you can have one of the following clusters: Managed OpenShift clusters , on which some operation, monitoring and billing are integrated with the cloud-platform provider. These clusters are build and managed by the cloud-provider's interface: RedHat OpenShift on IBM Cloud (ROKS) RedHat OpenShift on AWS (ROSA) Azure RedHat OpenShift (ARO) Installer Provisioned Infrastructure (IPI) clusters which are created using the openshift-install command. These IPI clusters can typically be enhanced to accommodate infrastructure and storage nodes using the gitops scaffolding. Azure AWS VMware User Provisioned Infrastructure (UPI) clusters, which are built manually to accommodate an unsupported environment. Although we did not prefer this path, we understand that there are valid customer situation that may warrant the UPI implementation. However, with UPI, you are responsible on building the additional structure so that the cluster is production ready. Several customer may also request that the cluster would not have internet access (i.e restricted network or airgapped ). Although most can be fulfilled with IPI airgapped cluster, some specific deployment may need to be UPI.","title":"Cluster Infrastructure"},{"location":"infrastructure/restricted-networks/","text":"Restricted Networks \u00b6 OpenShift in Restricted Network Environments \u00b6 When your OpenShift cluster is unable to reach the internet directly, its considered to be in a Restricted Network Environment . One common scenario is a cluster that is completely disconnected from the internet, commonly referred as an AirGapped or Disconnected Environment . Another type of restricted network occurs when access to the internet is restricted via an Enterprise Proxy . Certain configuration steps need to be taken in both scenarios. Warning This guide assumes you have an existing container image registry, protected with a TLS certificate signed by a Custom CA. The process of creating your own container image registry is outside of the scope of this document. Disconnected Environments \u00b6 This video describes automation developed by the Production Deployment Guides team to make it easier to deploy in restricted network environments. Preparing your OpenShift Container Platform Local Container Registry \u00b6 Before mirroring your OpenShift Container Platform components, please create the following registry namespaces/projects in your Local Container Registry ocp4 olm Mirroring of the OpenShift Platform components \u00b6 Note These steps were obtained from Red Hat's official documentation on mirroring images for a disconnected installation Update your local pull-secret with your registry credentials \u00b6 Download your registry.redhat.io pull secret from the Pull Secret page on the Red Hat OpenShift Cluster Manager site and save it to a .json file. Generate the base64-encoded user name and password or token for your mirror registry: $ echo -n username:password | base64 dXNlcm5hbWU6cGFzc3dvcmQ = Make a copy of your pull secret in JSON format: cat pull-secret | jq . > /tmp/pull-secret The contents of the file resemble the following example: { \"auths\" : { \"cloud.openshift.com\" : { \"auth\" : \"b3BlbnNo...\" , \"email\" : \"you@example.com\" }, \"quay.io\" : { \"auth\" : \"b3BlbnNo...\" , \"email\" : \"you@example.com\" }, \"registry.connect.redhat.com\" : { \"auth\" : \"NTE3Njg5Nj...\" , \"email\" : \"you@example.com\" }, \"registry.redhat.io\" : { \"auth\" : \"NTE3Njg5Nj...\" , \"email\" : \"you@example.com\" } } } Edit the new file and add a section that describes your registry to it: \"auths\" : { \"myimageregistry.example.com:8443\" : { \"auth\" : \"dXNlcm5hbWU6cGFzc3dvcmQ=\" , # fr om S te p 2 \"email\" : \"you@example.com\" }, Warning If no port is specified, port 443 is assumed. Error Do NOT include https://, docker:// or http:// in the registry name The file should now resemble the following example: { \"auths\" : { \"myimageregistry.example.com:8443\" : { \"auth\" : \"dXNlcm5hbWU6cGFzc3dvcmQ=\" , # fr om S te p 2 \"email\" : \"you@example.com\" }, \"cloud.openshift.com\" : { \"auth\" : \"b3BlbnNo...\" , \"email\" : \"you@example.com\" }, \"quay.io\" : { \"auth\" : \"b3BlbnNo...\" , \"email\" : \"you@example.com\" }, \"registry.connect.redhat.com\" : { \"auth\" : \"NTE3Njg5Nj...\" , \"email\" : \"you@example.com\" }, \"registry.redhat.io\" : { \"auth\" : \"NTE3Njg5Nj...\" , \"email\" : \"you@example.com\" } } } Mirror OpenShift Container Platform \u00b6 $ LOCAL_SECRET_JSON = '/path/to/pull-secret.txt' $ PRODUCT_REPO = 'openshift-release-dev' $ RELEASE_NAME = \"ocp-release\" $ OCP_RELEASE = 4 .6.17 $ ARCHITECTURE = x86_64 $ LOCAL_REGISTRY = myimageregistry.example.com:8443 $ LOCAL_REPOSITORY = ocp4/openshift4 $ oc adm release mirror -a ${ LOCAL_SECRET_JSON } --insecure \\ --from = quay.io/ ${ PRODUCT_REPO } / ${ RELEASE_NAME } : ${ OCP_RELEASE } - ${ ARCHITECTURE } \\ --to = ${ LOCAL_REGISTRY } / ${ LOCAL_REPOSITORY } \\ --to-release-image = ${ LOCAL_REGISTRY } / ${ LOCAL_REPOSITORY } : ${ OCP_RELEASE } - ${ ARCHITECTURE } Error Do NOT include https://, docker:// or http:// in the LOCAL_REGISTRY variable string The output will resemble something similar to this: info: Mirroring 121 images to myimageregistry.example.com:8443/ocp4/openshift4 ... myimageregistry.example.com:8443/ ocp4/openshift4 manifests: sha256:011b6819853dd51f795ac875b6ab338f6d85e9b31fa4fb8b5c7b2d580d9266bf -> 4.6.17-ironic-static-ip-manager ... Success Update image: myimageregistry.example.com:8443/ocp4/openshift4:4.6.17-x86_64 Mirror prefix: myimageregistry.example.com:8443/ocp4/openshift4 To use the new mirrored repository to install, add the following section to the install-config.yaml: imageContentSources: - mirrors: - myimageregistry.example.com:8443/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-release - mirrors: - myimageregistry.example.com:8443/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-v4.0-art-dev To use the new mirrored repository for upgrades, use the following to create an ImageContentSourcePolicy: apiVersion: operator.openshift.io/v1alpha1 kind: ImageContentSourcePolicy metadata: name: example spec: repositoryDigestMirrors: - mirrors: - myimageregistry.example.com:8443/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-release - mirrors: - myimageregistry.example.com:8443/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-v4.0-art-dev Deploy your Cluster \u00b6 Obtain installation program \u00b6 To create the installation program that is based on the content that you mirrored, extract it and pin it to the release: If your mirror host does not have Internet access, run the following command: $ oc adm release extract -a ${ LOCAL_SECRET_JSON } \\ --command = openshift-install \\ \" ${ LOCAL_REGISTRY } / ${ LOCAL_REPOSITORY } : ${ OCP_RELEASE } \" If the local container registry is connected to the mirror host, run the following command: $ oc adm release extract -a ${ LOCAL_SECRET_JSON } \\ --command = openshift-install \\ \" ${ LOCAL_REGISTRY } / ${ LOCAL_REPOSITORY } : ${ OCP_RELEASE } - ${ ARCHITECTURE } \" Validate version of the extracted openshift-install binary $ ./openshift-install version openshift-install 4 .6.17 built from commit 8a1ec01353e68cb6ebb1dd890d684f885c33145a release image quay.io/openshift-release-dev/ocp-release@sha256:a7b23f38d1e5be975a6b516739689673011bdfa59a7158dc6ca36cefae169c18 Create your install-config.yaml file \u00b6 $ openshift-install create install-config --dir = mycluster ... INFO Install-Config created in : mycluster Update your install-config.yaml, add the following at the bottom, as instructed by the oc adm release mirror command from previous steps. ... imageContentSources : - mirrors : - myimageregistry.example.com:8443/ocp4/openshift4 source : quay.io/openshift-release-dev/ocp-release - mirrors : - myimageregistry.example.com:8443/ocp4/openshift4 source : quay.io/openshift-release-dev/ocp-v4.0-art-dev ... If your container registry is using certificates signed by an internal CustomCA, you will need to include the CustomCA certificate as well under additionalTrustBundle: ... additionalTrustBundle : | -----BEGIN CERTIFICATE----- MIIGBzCCA++gAwIBAgIUZs95kGNRFr+cK+RoJG0PPhDwYP4wDQYJKoZIhvcNAQEL BQAwgZIxCzAJBgNVBAYTAlVTMRcwFQYDVQQIDA5Ob3J0aCBDYXJvbGluYTEQMA4G A1UEBwwHUmFsZWlnaDEMMAoGA1UECgwDSUJNMSQwIgYDVQQLDBtHVE0gQXNzZXRz IGFuZCBBcmNoaXRlY3R1cmUxJDAiBgNVBAMMG0dUTSBBc3NldHMgYW5kIEFyY2hp dGVjdHVyZTAeFw0yMTAzMTAxOTI5NDdaFw0yMzEyMjkxOTI5NDdaMIGSMQswCQYD VQQGEwJVUzEXMBUGA1UECAwOTm9ydGggQ2Fyb2xpbmExEDAOBgNVBAcMB1JhbGVp Z2gxDDAKBgNVBAoMA0lCTTEkMCIGA1UECwwbR1RNIEFzc2V0cyBhbmQgQXJjaGl0 ZWN0dXJlMSQwIgYDVQQDDBtHVE0gQXNzZXRzIGFuZCBBcmNoaXRlY3R1cmUwggIi MA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQC20LOBFQA/hUCmOXTSZ5HZYBnP lV41KJCya22FkkbpIyP59eABaaKHDoItqz3DDXf5fCcq0TZFVd3LjiItTusB3cE9 mvvJqs89NLsMAXa7KTCwGpWObokFrhHP4/dFhUN3RBfj1HLr/F1hQg3XKSzto+OL ZRzqASYVvWsjyXWByKRb1fZJorMk7JbzRQbM7yzUFZwAc+T4sQXRUwctRm5e/mtw 8uUkAzxUaj70mpeTkq6ijjj55yULK9F1LYj33pKPCLrRznRDKFFHy84W3xQmbbf6 wbcPxbaZ3njlaur9b/7S7KZAFjEhig7H27PnCEgLXmQI6OQj8DaQFcMI/DqTti3P QgDV8c9sdpGX4xk/d+yL+B+RyY/Z3vCF1bDVaftLdySXWDBS6D5354L5Qkh1Xkeh 1otwkqaIC0DF7mjmRmCiujR7YIYDiJIQ0QTGZxyvbZJvd+RmTyLJx+jK5YO7VAY8 dZ/7A9G/1G0FRsnb4pF8Rxy+5SxPjC+UOamraYesHdthkgy5Cq0T8wj7SDSg2Psr H8W5v5lfEF0NXdOtoRKpA4f5eY//WBz08X+xfJUmqF3tE38PuheRRUYeAc+4pPqP /Qtd4gZKWZ+kmorq9YuYejfbkcO0NgrAkDVOiUBWYiZ6COhnBcsitKo6LjCTwP6D N3531Ly/lhPXgox4swIDAQABo1MwUTAdBgNVHQ4EFgQUxdok6FNYGgwwFJ32XjSr 41NIcZ4wHwYDVR0jBBgwFoAUxdok6FNYGgwwFJ32XjSr41NIcZ4wDwYDVR0TAQH/ BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAgEAhEDxuczzQQCH0cXIFzc/Ljfsrm+k tMZlltYV+1FQ4Xy2sngAbMkOxpswgOf1LqFT687vxecxWskugA6GHj6N0LjPZjtf fgM5GnVqk8F/q2EsTBDQcvJTQj9JDr4OC50FvFwReOvwg/ivVTucvQfxVCOKRzBN g9TNUbCIPzHXrxm6pUR2iHLktcQaVmqNX9JaV9RrrjZKD/VqyMU3wNmlKHOm3pk3 HfbycNkOmZBVVMjpR5U8DdQmjQSntA2niDvS3WYOIJpIAzeTiwCuWuAZgr1F4lMV qXpuTA1GywYg36/qBB+KfQ2gVgifBYu2vzDq7ZqrC5IIkWOXmetbgST2TzCoRVHm dRlA2ajXe78F36RuiVQQwGZIQfbZDI4mTlGnGNgHcMUZFlruAZOjDGI14/ZBFX7B TI0uJOc6n8KAInJx2Anr04fjUvYOrqq6QMvpBbfQeHg/eDt6Xqo8s6AHgptBvB2g 5TAArFeBC/HOj8oihPVo+LhJG7T6HV/DjoY4swo9p7wfX7oVVNHNqZGDTQgqdPn5 QR4eMkZDycnHmPYzMouUyGLgS/nGNgDhGuwYMudxRRY0bf5cR3vJu/p65Y7iEA5L 08+f7KOxtj7LCfykKGbC97flS2WxMiV8w79eSUFMxIoI+oMsL7H28frgGzq6zak5 rveE8YHbgb5i6CE= -----END CERTIFICATE----- imageContentSources : - mirrors : - myimageregistry.example.com:8443/ocp4/openshift4 source : quay.io/openshift-release-dev/ocp-release - mirrors : - myimageregistry.example.com:8443/ocp4/openshift4 source : quay.io/openshift-release-dev/ocp-v4.0-art-dev ... Warning Pay attention to the indentation of the certificate in additionalTrustBundle Create your cluster \u00b6 $ openshift-install create cluster --dir mycluster ... INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc' , run 'export KUBECONFIG=/home/user/mycluster/refarch/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.mycluster.example.com INFO Login to the console with user: \"kubeadmin\" , and password: \"xxxxx-xxxxx-xxxxx-xxxxx\" ... Mirror RedHat Operator Catalog (OperatorHub) \u00b6 Build the catalog for redhat-operators . Match the tag of the redhat-operator-index in the --from flag to the major and minor versions of the OpenShift Container Platform cluster (for example, 4.6 ). oc adm catalog build --appregistry-org redhat-operators \\ --from = registry.redhat.io/redhat/redhat-operator-index:v4.6 \\ --to = ${ LOCAL_REGISTRY } /olm/redhat-operators:v4.6 \\ --registry-config = ${ LOCAL_SECRET_JSON } \\ --filter-by-os = \"linux/amd64\" --insecure using registry.redhat.io/redhat/redhat-operator-index:v4.6 as a base image for building ... Uploading ... 8 .094MB/s Pushed sha256:a392de24e5b294d15b3ceedcc1779a5cc5e81db7f007dc414351c6c18b38fff4 to myimageregistry.example.com:8443/olm/redhat-operators:v4.6 Mirror the catalog for redhat-operators . This process can take anywhere from 1-5 hours. Make sure you have at least 300Gb of storage available for the operator catalog in your image registry. oc adm catalog mirror ${ LOCAL_REGISTRY } /olm/redhat-operators:v4.6 ${ LOCAL_REGISTRY } /olm \\ --registry-config = ${ LOCAL_SECRET_JSON } --insecure using database path mapping: /:/tmp/737754181 wrote database to /tmp/737754181 using database at: /tmp/737754181/bundles.db ... info: Planning completed in 10m52.67s sha256:a392de24e5b294d15b3ceedcc1779a5cc5e81db7f007dc414351c6c18b38fff4 myimageregistry.example.com:8443/olm/redhat-operators:v4.6 ... wrote mirroring manifests to manifests-redhat-operators-1615353468 2.1 Optionally , you can choose to prune the Operator Catalog to mirror only specific operators. To do so, you need to podman , gprcurl and opm installed on your system. Run the source index image that you want to prune in a container. For example: podman run -p50051:50051 -it registry.redhat.io/redhat/redhat-operator-index:v4.6 In a separate terminal session, use the gprcurl command to get a list of the packages provided by the index: grpcurl -plaintext localhost:50051 api.Registry/ListPackages > packages.out Inspect the packages.out file and identify which package names from this list you want to keep in your pruned index. Run the following command to prune the source index of all but the specified packages opm index prune -f registry.redhat.io/redhat/redhat-operator-index:v4.6 \\ -p advanced-cluster-management,jaeger-product,quay-operator \\ -t myimageregistry.example.com:8443/olm/redhat-operator-index:v4.6 And finally, push your index image to your registry podman push <target_registry>:<port>/<namespace>/redhat-operator-index:v4.6 Disable the default OperatorSources by adding disableAllDefaultSources:true to the spec file for the Operator Hub. oc login https://api.mycluster.example.com:6443 -u kubeadmin -p xxxx-xxxx-xxxx-xxxx oc patch OperatorHub cluster --type json -p '[{\"op\": \"add\", \"path\": \"/spec/disableAllDefaultSources\", \"value\": true}]' If you're using an insecure registry, add it to the list of insecure registries. oc edit image.config.openshift.io/cluster Add the following to the spec section. spec : registrySources : insecureRegistries : - myimageregistry.example.com:8443 Error Do NOT include https://, docker:// or http:// in the registry name. Apply the imageContentSourcePolicy.yaml and imageCatalog.yaml file created from step 2 to your cluster and validate the results. This will restart all nodes to pick up the configuration change. $ cd manifests-redhat-operators-1615353468 $ oc create -f imageContentSourcePolicy.yaml -f catalogSource.yaml imagecontentsourcepolicy.operator.openshift.io/redhat-operators created catalogsource.operators.coreos.com/redhat-operators created $ oc get pods,catalogsource -n openshift-markerplace NAME READY STATUS RESTARTS AGE pod/marketplace-operator-5d59977875-wgbdd 1 /1 Running 0 110m pod/redhat-operators-4zdl5 1 /1 Running 0 38s NAME DISPLAY TYPE PUBLISHER AGE catalogsource.operators.coreos.com/redhat-operators grpc 38s Upgrading your Cluster \u00b6 To upgrade your cluster, first determine what your target upgrades should be. Follow Red Hat's documentation to determine what your next version is. Follow a similar process as creating your original platform mirror, and set OCP_RELEASE to your determined target version. You also need to pass the --apply-release-image-signature flag so the cluster can validate the hashes of the new version. If your cluster is completely airgapped, you can instead pass the --release-image-signature-to-dir flag which will create a ConfigMap for you that you can apply directly to the cluster. Mirror OpenShift Container Platform (Upgrade) \u00b6 $ LOCAL_SECRET_JSON = '/path/to/pull-secret.txt' $ PRODUCT_REPO = 'openshift-release-dev' $ RELEASE_NAME = \"ocp-release\" $ OCP_RELEASE = 4 .6.32 $ ARCHITECTURE = x86_64 $ LOCAL_REGISTRY = myimageregistry.example.com:8443 $ LOCAL_REPOSITORY = ocp4/openshift4 $ oc adm release mirror -a ${ LOCAL_SECRET_JSON } --insecure \\ --from = quay.io/ ${ PRODUCT_REPO } / ${ RELEASE_NAME } : ${ OCP_RELEASE } - ${ ARCHITECTURE } \\ --to = ${ LOCAL_REGISTRY } / ${ LOCAL_REPOSITORY } \\ --to-release-image = ${ LOCAL_REGISTRY } / ${ LOCAL_REPOSITORY } : ${ OCP_RELEASE } - ${ ARCHITECTURE } \\ --apply-release-image-signature --release-image-signature-to-dir ./release-manifest- ${ OCP_RELEASE } info: Mirroring 121 images to myimageregistry.example.com:8443/ocp4/openshift4 ... myimageregistry.example.com:8443/ ocp4/openshift4 manifests: ... Success Update image: myimageregistry.example.com:8443/ocp4/openshift4:4.6.32-x86_64 Mirror prefix: myimageregistry.example.com:8443/ocp4/openshift4 To use the new mirrored repository for upgrades, use the following to create an ImageContentSourcePolicy: apiVersion: operator.openshift.io/v1alpha1 kind: ImageContentSourcePolicy metadata: name: example spec: repositoryDigestMirrors: - mirrors: - myimageregistry.example.com:8443/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-release - mirrors: - myimageregistry.example.com:8443/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-v4.0-art-dev configmap/sha256-47df4bfe1cfd6d63dd2e880f00075ed1d37f997fd54884ed823ded9f5d96abfc created Upgrade to target version \u00b6 Apply the ImageContentSourcePolicy from the previous command and update your cluster. apiVersion : operator.openshift.io/v1alpha1 kind : ImageContentSourcePolicy metadata : name : myimageregistry-mirror spec : repositoryDigestMirrors : - mirrors : - myimageregistry.example.com:8443/ocp4/openshift4 source : quay.io/openshift-release-dev/ocp-release - mirrors : - myimageregistry.example.com:8443/ocp4/openshift4 source : quay.io/openshift-release-dev/ocp-v4.0-art-dev DIGEST = $( jq .metadata.name < release-manifest- ${ OCP_RELEASE } /signature-sha256-*.yaml | awk -F \\- '{print $1\":\"$2}' ) oc adm upgrade --allow-explicit-upgrade --to-image = quay.io/openshift-release-dev/ocp-release@ ${ DIGEST } Upgrade process can take a while. Validate cluster is ready by running checking the cluster ClusterVersion $ oc get clusterversion NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4 .6.32 True False 6d17h Cluster version is 4 .6.32 Post Deployment Configuration \u00b6 Disable the Insights Operator \u00b6 oc extract secret/pull-secret -n openshift-config --to = . This will create a .dockerconfigfile with your Pull Secret. Remove the cloud.redhat.com entry from the file \"cloud.openshift.com\" :{ \"auth\" : \"<hash>\" , \"email\" : \"<email_address>\" } Update the Global Pull Secret in your cluster oc set data secret/pull-secret -n openshift-config --from-file = .dockerconfigjson = .dockerconfigjson This update is rolled out to all nodes, which can take some time depending on the size of your cluster. During this time, nodes are drained and pods are rescheduled on the remaining nodes. Configure NTP Servers \u00b6 Your cluster is unable to reach the default RedHat NTP servers to synchronize its time. If you deploy OpenShift Container Storage, you may see the operator in a Warning state on the OpenShift main Dashboard. Patch OCS to deploy the Ceph Tools pod. On inspection, it will complain about clock skew on ceph mon pods. You will need to configure your cluster to talk to your local NTP servers. $ oc patch OCSInitialization ocsinit -n openshift-storage --type json --patch '[{ \"op\": \"replace\", \"path\": \"/spec/enableCephTools\", \"value\": true }]' $ oc exec -it rook-ceph-tools-7ddd664854-j8ccj -- ceph -s cluster: id: e05bb2fc-c757-4496-b9ad-a064e7c0b815 health: HEALTH_WARN clock skew detected on mon.b, mon.c . . . Create a chrony.conf file with your NTP Server. Replace vistime.rtp.raleigh.ibm.com with your NTP server. server vistime.rtp.raleigh.ibm.com iburst driftfile /var/lib/chrony/drift makestep 1.0 3 rtcsync logdir /var/log/chrony base64 encode it $ cat chrony.conf | base64 -w0 ICAgIHNlcnZlciB2aXN0aW1lLnJ0cC5yYWxlaWdoLmlibS5jb20gaWJ1cnN0CiAgICBkcmlmdGZpbGUgL3Zhci9saWIvY2hyb255L2RyaWZ0CiAgICBtYWtlc3RlcCAxLjAgMwogICAgcnRjc3luYwogICAgbG9nZGlyIC92YXIvbG9nL2Nocm9ueQo = Create a chrony-master-machineconfig.yaml and a chrony-worker-machineconfig.yaml apiVersion : machineconfiguration.openshift.io/v1 kind : MachineConfig metadata : labels : machineconfiguration.openshift.io/role : master ## << replace this with \"worker\" for the worker machineconfig name : 99-masters-chrony-configuration spec : config : ignition : config : {} security : tls : {} timeouts : {} version : 3.1.0 networkd : {} passwd : {} storage : files : - contents : source : data:text/plain;charset=utf-8;base64,ICAgIHNlcnZlciB2aXN0aW1lLnJ0cC5yYWxlaWdoLmlibS5jb20gaWJ1cnN0CiAgICBkcmlmdGZpbGUgL3Zhci9saWIvY2hyb255L2RyaWZ0CiAgICBtYWtlc3RlcCAxLjAgMwogICAgcnRjc3luYwogICAgbG9nZGlyIC92YXIvbG9nL2Nocm9ueQo= ## << the base64 encoded chrony.conf from step2 mode : 420 overwrite : true path : /etc/chrony.conf osImageURL : \"\" Create the MachineConfigs. Your cluster nodes will restart one by one until they all have the new configuration. $ oc create -f chrony-master-machineconfig.yaml -f chrony-worker-machineconfig.yaml machineconfig.machineconfiguration.openshift.io/99-masters-chrony-configuration created machineconfig.machineconfiguration.openshift.io/99-workers-chrony-configuration created Your storage cluster is now healthy $ oc exec -it rook-ceph-tools-7ddd664854-8nl82 -- ceph -s cluster: id: e05bb2fc-c757-4496-b9ad-a064e7c0b815 health: HEALTH_OK ... Mirroring CloudPak Container Images \u00b6 Create Registry Namespaces cp cpopen opencloudio Set up Environment Variables export CASE_NAME = ibm-cp-integration export CASE_VERSION = 2 .3.0 export CASE_INVENTORY_SETUP = operator export OFFLINEDIR = /path/to/offline export CASE_REPO_PATH = https://github.com/IBM/cloud-pak/raw/master/repo/case export ENTITLEMENT_KEY = \"ey...\" Note To obtain the CASE_NAME, CASE_VERSION and CASE_INVENTORY_SETUP for each CloudPak, please refer to each CloudPak documentation Warning Ensure you have adequate storage in your OFFLINEDIR Download Specific Components (if required) If you want to install a specific component of a CloudPak, override the CASE_ARCHIVE and CASE_INVENTORY_SETUP variables for your specific component. Example: export CASE_ARCHIVE = ibm-integration-platform-navigator-1.2.0.tgz export CASE_INVENTORY_SETUP = platformNavigatorOperator Download CloudPak Components $ cloudctl case save \\ --repo $CASE_REPO_PATH \\ --case $CASE_NAME \\ --version $CASE_VERSION \\ --outputdir $OFFLINEDIR Downloading and extracting the CASE ... - Success Retrieving CASE version ... - Success Validating the CASE ... [ warn ] - Validation warning found in inventory/operator/resources.yaml: Multiple media types specifed for single-platform images. The media types for images in a single resources.yaml should match. Found media types: [ application/vnd.docker.distribution.manifest.v2 application/vnd.oci.image.manifest.v1 ] Validating the signature for the ibm-cp-integration CASE... - Success Creating inventory ... - Success Finding inventory items - Success Resolving inventory items ... Parsing inventory items Validating the signature for the ibm-cp-common-services CASE... Validating the signature for the ibm-auditlogging CASE... Validating the signature for the ibm-cert-manager CASE... Validating the signature for the ibm-cs-commonui CASE... Validating the signature for the ibm-events-operator CASE... Validating the signature for the ibm-cs-healthcheck CASE... Validating the signature for the ibm-cs-iam CASE... Validating the signature for the ibm-zen CASE... Validating the signature for the ibm-licensing CASE... Validating the signature for the ibm-management-ingress CASE... Validating the signature for the ibm-cs-mongodb CASE... Validating the signature for the ibm-cs-monitoring CASE... Validating the signature for the ibm-platform-api-operator CASE... Validating the signature for the ibm-automation-foundation-core CASE... Validating the signature for the ibm-datapower-operator-cp4i CASE... Validating the signature for the ibm-apiconnect CASE... Validating the signature for the ibm-datapower-operator-prod CASE... Validating the signature for the ibm-appconnect CASE... Validating the signature for the ibm-couchdb CASE... Validating the signature for the ibm-aspera-hsts-operator CASE... Validating the signature for the ibm-cloud-databases-redis CASE... Validating the signature for the ibm-mq CASE... Validating the signature for the ibm-integration-asset-repository CASE... Validating the signature for the ibm-integration-platform-navigator CASE... Validating the signature for the ibm-integration-operations-dashboard CASE... - Success Configure Registry Credentials You need to configure credentials for your local registry and cp.icr.io Credentials for Local Registry cloudctl case launch \\ --case $OFFLINEDIR / ${ CASE_NAME } - ${ CASE_VERSION } .tgz \\ --inventory $CASE_INVENTORY_SETUP \\ --action configure-creds-airgap \\ --args \"--registry ${ LOCAL_REGISTRY } --user username --pass password\" Welcome to the CASE launcher Attempting to retrieve and extract the CASE from the specified location [ \u2713 ] CASE has been retrieved and extracted Attempting to validate the CASE [ \u2713 ] CASE has been successfully validated Attempting to locate the launch inventory item, script, and action in the specified CASE [ \u2713 ] Found the specified launch inventory item, action, and script for the CASE Attempting to check the cluster and machine for required prerequisites for launching the item [ WARNING ] : Unable to create a REST API connection to the cluster due to the following error: Unable to get cluster auth tokens due to the following error: Error: stat ./auth/kubeconfig: no such file or directory ( found in getting the kubernetes config ) . Ensure that you are logged into the cluster with the appropriate role to perform this launch action. Checking for required prereqs... No requires section specified. Required prereqs result: OK Checking user permissions... No user rules specified. [ \u2713 ] Cluster and Client Prerequisites have been met for the CASE Running the CASE operator launch script with the following action context: configureCredsAirgap Executing inventory item operator, action configureCredsAirgap : launch.sh -------------Configuring authentication secret------------- [ INFO ] Creating registry authencation secret for myimageregistry.example.com:8443 [ INFO ] Registry secret created in /home/ncolon/.airgap/secrets/myimageregistry.example.com:8443.json [ INFO ] Done [ \u2713 ] CASE launch script completed successfully OK Credentials for IBM Registry cloudctl case launch \\ --case $OFFLINEDIR / ${ CASE_NAME } - ${ CASE_VERSION } .tgz \\ --inventory $CASE_INVENTORY_SETUP \\ --action configure-creds-airgap \\ --args \"--registry cp.icr.io --user cp --pass $ENTITLEMENT_KEY \" Warning Its ok to get [WARNING]: Unable to create a REST API connection in the commands above Authenticate to your airgapped cluster oc login https://api.mycluster.example.com:6443 -u kubeadmin -p xxxx-xxxx-xxxx-xxxx oc login --token = sha256~AAAAA....BBBBB --server = https://api.mycluster.example.com:6443 Warning do not use export KUBECONFIG=path/to/kubeconfig as some items may not replicate correctly. Create a namespace for your CloudPak export NAMESPACE = cp4i oc create namespace $NAMESPACE Mirror CloudPak images to Local Repository Create environment variables with the local Docker registry connection information export CASE_NAME = ibm-cp-integration export CASE_VERSION = 2 .2.2 export CASE_ARCHIVE = ${ CASE_NAME } - ${ CASE_VERSION } .tgz export CASE_INVENTORY_SETUP = operator export OFFLINEDIR = /path/to/offline export OFFLINEDIR_ARCHIVE = offline.tgz export CASE_REPO_PATH = https://github.com/IBM/cloud-pak/raw/master/repo/case export CASE_LOCAL_PATH = $OFFLINEDIR / $CASE_ARCHIVE export LOCAL_DOCKER_REGISTRY_HOST = myimageregistry.example.com:8443 export LOCAL_DOCKER_REGISTRY_PORT = 443 export LOCAL_DOCKER_REGISTRY = $LOCAL_DOCKER_REGISTRY_HOST : $LOCAL_DOCKER_REGISTRY_PORT export LOCAL_DOCKER_USER = username export LOCAL_DOCKER_PASSWORD = password Warning If you are installing only a single capability, you must run commands similar to the example below to adjust the environment variables for the capability you need to install below (in this example, only Platform Navigator). Warning if your registry runs on port 443, you don't need to include it on $LOCAL_DOCKER_REGISTRY_HOST export CASE_ARCHIVE = ibm-integration-platform-navigator-2.3.0.tgz export CASE_INVENTORY_SETUP = platformNavigatorOperator export CASE_LOCAL_PATH = $OFFLINEDIR / ${ CASE_NAME } - ${ CASE_VERSION } .tgzexport Mirror contents cloudctl case launch \\ --case $OFFLINEDIR / ${ CASE_NAME } - ${ CASE_VERSION } .tgz \\ --inventory $CASE_INVENTORY_SETUP \\ --action mirror-images \\ --args \"--registry $LOCAL_DOCKER_REGISTRY --inputDir $OFFLINEDIR \" ... info: Planning completed in 130ms sha256:9df1f3d5b16df5d8753a78d8f7bd60f72d47135e139185464cbe7c8edb9ded61 myimageregistry.example.com:8443/cpopen/ibm-common-service-catalog sha256:31d1b593d7601c69c8a02985b7326766aec68a6379dfea69fbc2c07a6251020a myimageregistry.example.com:8443/cpopen/ibm-common-service-catalog sha256:27c45813f3dfe62aa654db36c4f01a34ed189b330c52823fcddbd605ee890a87 myimageregistry.example.com:8443/cpopen/ibm-common-service-catalog sha256:cc5f6fa46af38278f565ca22ff13ae8a2cd0ae60303d5d3be5dd9395bf4b3771 myimageregistry.example.com:8443/cpopen/ibm-common-service-catalog:latest info: Mirroring completed in 110ms ( 0B/s ) [ \u2713 ] CASE launch script completed successfully Create ImageCatalogSourcePolicy $ cloudctl case launch \\ --case $OFFLINEDIR / ${ CASE_NAME } - ${ CASE_VERSION } .tgz \\ --inventory $CASE_INVENTORY_SETUP \\ --action configure-cluster-airgap \\ --namespace $NAMESPACE \\ --args \"--registry $LOCAL_DOCKER_REGISTRY --user $LOCAL_DOCKER_USER --pass $LOCAL_DOCKER_PASSWORD --inputDir $OFFLINEDIR \" ... [ INFO ] Generating image content source policy ... --- apiVersion: operator.openshift.io/v1alpha1 kind: ImageContentSourcePolicy metadata: name: ibm-cp-integration spec: repositoryDigestMirrors: - mirrors: - myimageregistry.example.com:8443/cp/apic source: cp.icr.io/cp/apic - mirrors: - myimageregistry.example.com:8443/cp/appc source: cp.icr.io/cp/appc - mirrors: - myimageregistry.example.com:8443/cp/datapower source: cp.icr.io/cp/datapower - mirrors: - myimageregistry.example.com:8443/cp/icp4i/aspera source: cp.icr.io/cp/icp4i/aspera - mirrors: - myimageregistry.example.com:8443/cp/icp4i source: cp.icr.io/cp/icp4i - mirrors: - myimageregistry.example.com:8443/cp/icp4i/od source: cp.icr.io/cp/icp4i/od - mirrors: - myimageregistry.example.com:8443/cp source: cp.icr.io/cp - mirrors: - myimageregistry.example.com:8443/ibmcom source: docker.io/ibmcom - mirrors: - myimageregistry.example.com:8443/cpopen source: icr.io/cpopen - mirrors: - myimageregistry.example.com:8443/opencloudio source: quay.io/opencloudio --- [ INFO ] Applying image content source policy oc apply -f \"/tmp/airgap_image_policy_z2Hgmp9a8\" imagecontentsourcepolicy.operator.openshift.io/ibm-cp-integration created [ \u2713 ] CASE launch script completed successfully OK Install Catalog Sources cloudctl case launch \\ --case $OFFLINEDIR / ${ CASE_ARCHIVE } \\ --inventory ${ CASE_INVENTORY_SETUP } \\ --action install-catalog \\ --namespace ${ NAMESPACE } \\ --args \"--registry ${ LOCAL_DOCKER_REGISTRY } --inputDir $OFFLINEDIR --recursive\" $ oc get pod -n openshift-marketplace NAME READY STATUS RESTARTS AGE appconnect-operator-catalogsource-cwzn7 1 /1 Running 0 7m20s aspera-operators-nvqvf 1 /1 Running 0 96m couchdb-operator-catalog-jtmfz 1 /1 Running 0 96m ibm-apiconnect-catalog-wm5xd 1 /1 Running 0 96m ibm-automation-foundation-core-catalog-wz5jq 1 /1 Running 0 7m39s ibm-cloud-databases-redis-operator-catalog-zs2qd 1 /1 Running 0 96m ibm-cp-integration-catalog-bw64z 1 /1 Running 0 6m35s ibm-datapower-operator-catalog-nl95s 1 /1 Running 0 72s ibm-integration-asset-repository-catalog-g94xt 1 /1 Running 0 6m52s ibm-integration-operations-dashboard-catalog-z62p6 1 /1 Running 0 6m37s ibm-integration-platform-navigator-catalog-pfvhg 1 /1 Running 0 6m41s ibmmq-operator-catalogsource-2hfz6 1 /1 Running 0 96m marketplace-operator-67cb5b6d45-pqmcz 1 /1 Running 0 99m opencloud-operators-5hjfp 1 /1 Running 0 96m redhat-operators-pnlh5 1 /1 Running 0 103m Proxy Environments \u00b6 When installing OpenShift in an environment where an Enterprise Proxy controls access to public resources, you need to make the installer proxy aware. The proxy needs to allow traffic to the following endpoints https://quay.io https://registry.connect.redhat.com https://registry.redhat.io Additionally, if you're deploying in a Public Cloud Provider, the proxy needs to allow access to multiple cloud management endpoints so that the OpenShift operators can interact with your cloud provider. Configure Proxy at Install Time \u00b6 To configure your proxy at install time, first generate your install-config.yaml file $ openshift-install create install-config --dir = mycluster ... INFO Install-Config created in : mycluster Add the following proxy information at the bottom proxy : httpProxy : http://useranme:password@myproxy.example.com:3128 httpsProxy : http://useranme:password@myproxy.example.com:3128 noProxy : CIDR,IPAddress,hostname,.internal.example.com If the customer Enterprise Proxy solution also intercepts SSL/TLS traffic and injects its own TLS certificate on any outbound https request (also known as a Man-In-The-Middle SSL Proxy), you also need to add the CustomCA certificate used by the proxy TLS certificate ... proxy : httpProxy : http://myproxy.example.com:3128 httpsProxy : http://myproxy.example.com:3128 noProxy : CIDR,IPAddress,hostname,.internal.example.com additionalTrustBundle : | -----BEGIN CERTIFICATE----- MIIGBzCCA++gAwIBAgIUZs95kGNRFr+cK+RoJG0PPhDwYP4wDQYJKoZIhvcNAQEL BQAwgZIxCzAJBgNVBAYTAlVTMRcwFQYDVQQIDA5Ob3J0aCBDYXJvbGluYTEQMA4G A1UEBwwHUmFsZWlnaDEMMAoGA1UECgwDSUJNMSQwIgYDVQQLDBtHVE0gQXNzZXRz IGFuZCBBcmNoaXRlY3R1cmUxJDAiBgNVBAMMG0dUTSBBc3NldHMgYW5kIEFyY2hp dGVjdHVyZTAeFw0yMTAzMTAxOTI5NDdaFw0yMzEyMjkxOTI5NDdaMIGSMQswCQYD VQQGEwJVUzEXMBUGA1UECAwOTm9ydGggQ2Fyb2xpbmExEDAOBgNVBAcMB1JhbGVp Z2gxDDAKBgNVBAoMA0lCTTEkMCIGA1UECwwbR1RNIEFzc2V0cyBhbmQgQXJjaGl0 ZWN0dXJlMSQwIgYDVQQDDBtHVE0gQXNzZXRzIGFuZCBBcmNoaXRlY3R1cmUwggIi MA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQC20LOBFQA/hUCmOXTSZ5HZYBnP lV41KJCya22FkkbpIyP59eABaaKHDoItqz3DDXf5fCcq0TZFVd3LjiItTusB3cE9 mvvJqs89NLsMAXa7KTCwGpWObokFrhHP4/dFhUN3RBfj1HLr/F1hQg3XKSzto+OL ZRzqASYVvWsjyXWByKRb1fZJorMk7JbzRQbM7yzUFZwAc+T4sQXRUwctRm5e/mtw 8uUkAzxUaj70mpeTkq6ijjj55yULK9F1LYj33pKPCLrRznRDKFFHy84W3xQmbbf6 wbcPxbaZ3njlaur9b/7S7KZAFjEhig7H27PnCEgLXmQI6OQj8DaQFcMI/DqTti3P QgDV8c9sdpGX4xk/d+yL+B+RyY/Z3vCF1bDVaftLdySXWDBS6D5354L5Qkh1Xkeh 1otwkqaIC0DF7mjmRmCiujR7YIYDiJIQ0QTGZxyvbZJvd+RmTyLJx+jK5YO7VAY8 dZ/7A9G/1G0FRsnb4pF8Rxy+5SxPjC+UOamraYesHdthkgy5Cq0T8wj7SDSg2Psr H8W5v5lfEF0NXdOtoRKpA4f5eY//WBz08X+xfJUmqF3tE38PuheRRUYeAc+4pPqP /Qtd4gZKWZ+kmorq9YuYejfbkcO0NgrAkDVOiUBWYiZ6COhnBcsitKo6LjCTwP6D N3531Ly/lhPXgox4swIDAQABo1MwUTAdBgNVHQ4EFgQUxdok6FNYGgwwFJ32XjSr 41NIcZ4wHwYDVR0jBBgwFoAUxdok6FNYGgwwFJ32XjSr41NIcZ4wDwYDVR0TAQH/ BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAgEAhEDxuczzQQCH0cXIFzc/Ljfsrm+k tMZlltYV+1FQ4Xy2sngAbMkOxpswgOf1LqFT687vxecxWskugA6GHj6N0LjPZjtf fgM5GnVqk8F/q2EsTBDQcvJTQj9JDr4OC50FvFwReOvwg/ivVTucvQfxVCOKRzBN g9TNUbCIPzHXrxm6pUR2iHLktcQaVmqNX9JaV9RrrjZKD/VqyMU3wNmlKHOm3pk3 HfbycNkOmZBVVMjpR5U8DdQmjQSntA2niDvS3WYOIJpIAzeTiwCuWuAZgr1F4lMV qXpuTA1GywYg36/qBB+KfQ2gVgifBYu2vzDq7ZqrC5IIkWOXmetbgST2TzCoRVHm dRlA2ajXe78F36RuiVQQwGZIQfbZDI4mTlGnGNgHcMUZFlruAZOjDGI14/ZBFX7B TI0uJOc6n8KAInJx2Anr04fjUvYOrqq6QMvpBbfQeHg/eDt6Xqo8s6AHgptBvB2g 5TAArFeBC/HOj8oihPVo+LhJG7T6HV/DjoY4swo9p7wfX7oVVNHNqZGDTQgqdPn5 QR4eMkZDycnHmPYzMouUyGLgS/nGNgDhGuwYMudxRRY0bf5cR3vJu/p65Y7iEA5L 08+f7KOxtj7LCfykKGbC97flS2WxMiV8w79eSUFMxIoI+oMsL7H28frgGzq6zak5 rveE8YHbgb5i6CE= -----END CERTIFICATE----- ... Warning Pay attention to the indentation of the certificate in additionalTrustBundle Warning The httpProxy and httpsProxy parameters always begin with http://; https:// is not supported. Note The Proxy object status.noProxy field is populated with the values of the networking.machineNetwork[].cidr , networking.clusterNetwork[].cidr , and networking.serviceNetwork[] fields from your installation configuration. For installations on Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, and Red Hat OpenStack Platform (RHOSP), the Proxy object status.noProxy field is also populated with the instance metadata endpoint (169.254.169.254). Configure or Update the Proxy on an Existing Cluster \u00b6 If you didn't configure the proxy at install time, your Cluster Wide Proxy object will have an empty configuration apiVersion : config.openshift.io/v1 kind : Proxy metadata : name : cluster spec : trustedCA : name : \"\" status : If your proxy uses a certificate signed by a customCA, you will first need to create a ConfigMap with the proxy certificate apiVersion : v1 data : ca-bundle.crt : | <MY_PEM_ENCODED_CERTS> kind : ConfigMap metadata : name : user-ca-bundle namespace : openshift-config Note You can skip this step if the proxy certificate is signed by a known authority Once created, you can then edit your Cluster Wide Proxy and configure any necessary fields oc edit proxy/cluster apiVersion : config.openshift.io/v1 kind : Proxy metadata : name : cluster spec : httpProxy : http://<username>:<pswd>@<ip>:<port> httpsProxy : http://<username>:<pswd>@<ip>:<port> noProxy : 192.168.100.0/24,10.0.0.1,mydatabase.example.com,.mydomain.example.com readinessEndpoints : - http://site1.example.com - https://site2.example.com - https://www.google.com trustedCA : name : user-ca-bundle","title":"Restricted Networks"},{"location":"infrastructure/restricted-networks/#restricted-networks","text":"","title":"Restricted Networks"},{"location":"infrastructure/restricted-networks/#openshift-in-restricted-network-environments","text":"When your OpenShift cluster is unable to reach the internet directly, its considered to be in a Restricted Network Environment . One common scenario is a cluster that is completely disconnected from the internet, commonly referred as an AirGapped or Disconnected Environment . Another type of restricted network occurs when access to the internet is restricted via an Enterprise Proxy . Certain configuration steps need to be taken in both scenarios. Warning This guide assumes you have an existing container image registry, protected with a TLS certificate signed by a Custom CA. The process of creating your own container image registry is outside of the scope of this document.","title":"OpenShift in Restricted Network Environments"},{"location":"infrastructure/restricted-networks/#disconnected-environments","text":"This video describes automation developed by the Production Deployment Guides team to make it easier to deploy in restricted network environments.","title":"Disconnected Environments"},{"location":"infrastructure/restricted-networks/#preparing-your-openshift-container-platform-local-container-registry","text":"Before mirroring your OpenShift Container Platform components, please create the following registry namespaces/projects in your Local Container Registry ocp4 olm","title":"Preparing your OpenShift Container Platform Local Container Registry"},{"location":"infrastructure/restricted-networks/#mirroring-of-the-openshift-platform-components","text":"Note These steps were obtained from Red Hat's official documentation on mirroring images for a disconnected installation","title":"Mirroring of the OpenShift Platform components"},{"location":"infrastructure/restricted-networks/#deploy-your-cluster","text":"","title":"Deploy your Cluster"},{"location":"infrastructure/restricted-networks/#obtain-installation-program","text":"To create the installation program that is based on the content that you mirrored, extract it and pin it to the release: If your mirror host does not have Internet access, run the following command: $ oc adm release extract -a ${ LOCAL_SECRET_JSON } \\ --command = openshift-install \\ \" ${ LOCAL_REGISTRY } / ${ LOCAL_REPOSITORY } : ${ OCP_RELEASE } \" If the local container registry is connected to the mirror host, run the following command: $ oc adm release extract -a ${ LOCAL_SECRET_JSON } \\ --command = openshift-install \\ \" ${ LOCAL_REGISTRY } / ${ LOCAL_REPOSITORY } : ${ OCP_RELEASE } - ${ ARCHITECTURE } \" Validate version of the extracted openshift-install binary $ ./openshift-install version openshift-install 4 .6.17 built from commit 8a1ec01353e68cb6ebb1dd890d684f885c33145a release image quay.io/openshift-release-dev/ocp-release@sha256:a7b23f38d1e5be975a6b516739689673011bdfa59a7158dc6ca36cefae169c18","title":"Obtain installation program"},{"location":"infrastructure/restricted-networks/#create-your-install-configyaml-file","text":"$ openshift-install create install-config --dir = mycluster ... INFO Install-Config created in : mycluster Update your install-config.yaml, add the following at the bottom, as instructed by the oc adm release mirror command from previous steps. ... imageContentSources : - mirrors : - myimageregistry.example.com:8443/ocp4/openshift4 source : quay.io/openshift-release-dev/ocp-release - mirrors : - myimageregistry.example.com:8443/ocp4/openshift4 source : quay.io/openshift-release-dev/ocp-v4.0-art-dev ... If your container registry is using certificates signed by an internal CustomCA, you will need to include the CustomCA certificate as well under additionalTrustBundle: ... additionalTrustBundle : | -----BEGIN CERTIFICATE----- MIIGBzCCA++gAwIBAgIUZs95kGNRFr+cK+RoJG0PPhDwYP4wDQYJKoZIhvcNAQEL BQAwgZIxCzAJBgNVBAYTAlVTMRcwFQYDVQQIDA5Ob3J0aCBDYXJvbGluYTEQMA4G A1UEBwwHUmFsZWlnaDEMMAoGA1UECgwDSUJNMSQwIgYDVQQLDBtHVE0gQXNzZXRz IGFuZCBBcmNoaXRlY3R1cmUxJDAiBgNVBAMMG0dUTSBBc3NldHMgYW5kIEFyY2hp dGVjdHVyZTAeFw0yMTAzMTAxOTI5NDdaFw0yMzEyMjkxOTI5NDdaMIGSMQswCQYD VQQGEwJVUzEXMBUGA1UECAwOTm9ydGggQ2Fyb2xpbmExEDAOBgNVBAcMB1JhbGVp Z2gxDDAKBgNVBAoMA0lCTTEkMCIGA1UECwwbR1RNIEFzc2V0cyBhbmQgQXJjaGl0 ZWN0dXJlMSQwIgYDVQQDDBtHVE0gQXNzZXRzIGFuZCBBcmNoaXRlY3R1cmUwggIi MA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQC20LOBFQA/hUCmOXTSZ5HZYBnP lV41KJCya22FkkbpIyP59eABaaKHDoItqz3DDXf5fCcq0TZFVd3LjiItTusB3cE9 mvvJqs89NLsMAXa7KTCwGpWObokFrhHP4/dFhUN3RBfj1HLr/F1hQg3XKSzto+OL ZRzqASYVvWsjyXWByKRb1fZJorMk7JbzRQbM7yzUFZwAc+T4sQXRUwctRm5e/mtw 8uUkAzxUaj70mpeTkq6ijjj55yULK9F1LYj33pKPCLrRznRDKFFHy84W3xQmbbf6 wbcPxbaZ3njlaur9b/7S7KZAFjEhig7H27PnCEgLXmQI6OQj8DaQFcMI/DqTti3P QgDV8c9sdpGX4xk/d+yL+B+RyY/Z3vCF1bDVaftLdySXWDBS6D5354L5Qkh1Xkeh 1otwkqaIC0DF7mjmRmCiujR7YIYDiJIQ0QTGZxyvbZJvd+RmTyLJx+jK5YO7VAY8 dZ/7A9G/1G0FRsnb4pF8Rxy+5SxPjC+UOamraYesHdthkgy5Cq0T8wj7SDSg2Psr H8W5v5lfEF0NXdOtoRKpA4f5eY//WBz08X+xfJUmqF3tE38PuheRRUYeAc+4pPqP /Qtd4gZKWZ+kmorq9YuYejfbkcO0NgrAkDVOiUBWYiZ6COhnBcsitKo6LjCTwP6D N3531Ly/lhPXgox4swIDAQABo1MwUTAdBgNVHQ4EFgQUxdok6FNYGgwwFJ32XjSr 41NIcZ4wHwYDVR0jBBgwFoAUxdok6FNYGgwwFJ32XjSr41NIcZ4wDwYDVR0TAQH/ BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAgEAhEDxuczzQQCH0cXIFzc/Ljfsrm+k tMZlltYV+1FQ4Xy2sngAbMkOxpswgOf1LqFT687vxecxWskugA6GHj6N0LjPZjtf fgM5GnVqk8F/q2EsTBDQcvJTQj9JDr4OC50FvFwReOvwg/ivVTucvQfxVCOKRzBN g9TNUbCIPzHXrxm6pUR2iHLktcQaVmqNX9JaV9RrrjZKD/VqyMU3wNmlKHOm3pk3 HfbycNkOmZBVVMjpR5U8DdQmjQSntA2niDvS3WYOIJpIAzeTiwCuWuAZgr1F4lMV qXpuTA1GywYg36/qBB+KfQ2gVgifBYu2vzDq7ZqrC5IIkWOXmetbgST2TzCoRVHm dRlA2ajXe78F36RuiVQQwGZIQfbZDI4mTlGnGNgHcMUZFlruAZOjDGI14/ZBFX7B TI0uJOc6n8KAInJx2Anr04fjUvYOrqq6QMvpBbfQeHg/eDt6Xqo8s6AHgptBvB2g 5TAArFeBC/HOj8oihPVo+LhJG7T6HV/DjoY4swo9p7wfX7oVVNHNqZGDTQgqdPn5 QR4eMkZDycnHmPYzMouUyGLgS/nGNgDhGuwYMudxRRY0bf5cR3vJu/p65Y7iEA5L 08+f7KOxtj7LCfykKGbC97flS2WxMiV8w79eSUFMxIoI+oMsL7H28frgGzq6zak5 rveE8YHbgb5i6CE= -----END CERTIFICATE----- imageContentSources : - mirrors : - myimageregistry.example.com:8443/ocp4/openshift4 source : quay.io/openshift-release-dev/ocp-release - mirrors : - myimageregistry.example.com:8443/ocp4/openshift4 source : quay.io/openshift-release-dev/ocp-v4.0-art-dev ... Warning Pay attention to the indentation of the certificate in additionalTrustBundle","title":"Create your install-config.yaml file"},{"location":"infrastructure/restricted-networks/#create-your-cluster","text":"$ openshift-install create cluster --dir mycluster ... INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc' , run 'export KUBECONFIG=/home/user/mycluster/refarch/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.mycluster.example.com INFO Login to the console with user: \"kubeadmin\" , and password: \"xxxxx-xxxxx-xxxxx-xxxxx\" ...","title":"Create your cluster"},{"location":"infrastructure/restricted-networks/#mirror-redhat-operator-catalog-operatorhub","text":"Build the catalog for redhat-operators . Match the tag of the redhat-operator-index in the --from flag to the major and minor versions of the OpenShift Container Platform cluster (for example, 4.6 ). oc adm catalog build --appregistry-org redhat-operators \\ --from = registry.redhat.io/redhat/redhat-operator-index:v4.6 \\ --to = ${ LOCAL_REGISTRY } /olm/redhat-operators:v4.6 \\ --registry-config = ${ LOCAL_SECRET_JSON } \\ --filter-by-os = \"linux/amd64\" --insecure using registry.redhat.io/redhat/redhat-operator-index:v4.6 as a base image for building ... Uploading ... 8 .094MB/s Pushed sha256:a392de24e5b294d15b3ceedcc1779a5cc5e81db7f007dc414351c6c18b38fff4 to myimageregistry.example.com:8443/olm/redhat-operators:v4.6 Mirror the catalog for redhat-operators . This process can take anywhere from 1-5 hours. Make sure you have at least 300Gb of storage available for the operator catalog in your image registry. oc adm catalog mirror ${ LOCAL_REGISTRY } /olm/redhat-operators:v4.6 ${ LOCAL_REGISTRY } /olm \\ --registry-config = ${ LOCAL_SECRET_JSON } --insecure using database path mapping: /:/tmp/737754181 wrote database to /tmp/737754181 using database at: /tmp/737754181/bundles.db ... info: Planning completed in 10m52.67s sha256:a392de24e5b294d15b3ceedcc1779a5cc5e81db7f007dc414351c6c18b38fff4 myimageregistry.example.com:8443/olm/redhat-operators:v4.6 ... wrote mirroring manifests to manifests-redhat-operators-1615353468 2.1 Optionally , you can choose to prune the Operator Catalog to mirror only specific operators. To do so, you need to podman , gprcurl and opm installed on your system. Run the source index image that you want to prune in a container. For example: podman run -p50051:50051 -it registry.redhat.io/redhat/redhat-operator-index:v4.6 In a separate terminal session, use the gprcurl command to get a list of the packages provided by the index: grpcurl -plaintext localhost:50051 api.Registry/ListPackages > packages.out Inspect the packages.out file and identify which package names from this list you want to keep in your pruned index. Run the following command to prune the source index of all but the specified packages opm index prune -f registry.redhat.io/redhat/redhat-operator-index:v4.6 \\ -p advanced-cluster-management,jaeger-product,quay-operator \\ -t myimageregistry.example.com:8443/olm/redhat-operator-index:v4.6 And finally, push your index image to your registry podman push <target_registry>:<port>/<namespace>/redhat-operator-index:v4.6 Disable the default OperatorSources by adding disableAllDefaultSources:true to the spec file for the Operator Hub. oc login https://api.mycluster.example.com:6443 -u kubeadmin -p xxxx-xxxx-xxxx-xxxx oc patch OperatorHub cluster --type json -p '[{\"op\": \"add\", \"path\": \"/spec/disableAllDefaultSources\", \"value\": true}]' If you're using an insecure registry, add it to the list of insecure registries. oc edit image.config.openshift.io/cluster Add the following to the spec section. spec : registrySources : insecureRegistries : - myimageregistry.example.com:8443 Error Do NOT include https://, docker:// or http:// in the registry name. Apply the imageContentSourcePolicy.yaml and imageCatalog.yaml file created from step 2 to your cluster and validate the results. This will restart all nodes to pick up the configuration change. $ cd manifests-redhat-operators-1615353468 $ oc create -f imageContentSourcePolicy.yaml -f catalogSource.yaml imagecontentsourcepolicy.operator.openshift.io/redhat-operators created catalogsource.operators.coreos.com/redhat-operators created $ oc get pods,catalogsource -n openshift-markerplace NAME READY STATUS RESTARTS AGE pod/marketplace-operator-5d59977875-wgbdd 1 /1 Running 0 110m pod/redhat-operators-4zdl5 1 /1 Running 0 38s NAME DISPLAY TYPE PUBLISHER AGE catalogsource.operators.coreos.com/redhat-operators grpc 38s","title":"Mirror RedHat Operator Catalog (OperatorHub)"},{"location":"infrastructure/restricted-networks/#upgrading-your-cluster","text":"To upgrade your cluster, first determine what your target upgrades should be. Follow Red Hat's documentation to determine what your next version is. Follow a similar process as creating your original platform mirror, and set OCP_RELEASE to your determined target version. You also need to pass the --apply-release-image-signature flag so the cluster can validate the hashes of the new version. If your cluster is completely airgapped, you can instead pass the --release-image-signature-to-dir flag which will create a ConfigMap for you that you can apply directly to the cluster.","title":"Upgrading your Cluster"},{"location":"infrastructure/restricted-networks/#post-deployment-configuration","text":"","title":"Post Deployment Configuration"},{"location":"infrastructure/restricted-networks/#mirroring-cloudpak-container-images","text":"Create Registry Namespaces cp cpopen opencloudio Set up Environment Variables export CASE_NAME = ibm-cp-integration export CASE_VERSION = 2 .3.0 export CASE_INVENTORY_SETUP = operator export OFFLINEDIR = /path/to/offline export CASE_REPO_PATH = https://github.com/IBM/cloud-pak/raw/master/repo/case export ENTITLEMENT_KEY = \"ey...\" Note To obtain the CASE_NAME, CASE_VERSION and CASE_INVENTORY_SETUP for each CloudPak, please refer to each CloudPak documentation Warning Ensure you have adequate storage in your OFFLINEDIR Download Specific Components (if required) If you want to install a specific component of a CloudPak, override the CASE_ARCHIVE and CASE_INVENTORY_SETUP variables for your specific component. Example: export CASE_ARCHIVE = ibm-integration-platform-navigator-1.2.0.tgz export CASE_INVENTORY_SETUP = platformNavigatorOperator Download CloudPak Components $ cloudctl case save \\ --repo $CASE_REPO_PATH \\ --case $CASE_NAME \\ --version $CASE_VERSION \\ --outputdir $OFFLINEDIR Downloading and extracting the CASE ... - Success Retrieving CASE version ... - Success Validating the CASE ... [ warn ] - Validation warning found in inventory/operator/resources.yaml: Multiple media types specifed for single-platform images. The media types for images in a single resources.yaml should match. Found media types: [ application/vnd.docker.distribution.manifest.v2 application/vnd.oci.image.manifest.v1 ] Validating the signature for the ibm-cp-integration CASE... - Success Creating inventory ... - Success Finding inventory items - Success Resolving inventory items ... Parsing inventory items Validating the signature for the ibm-cp-common-services CASE... Validating the signature for the ibm-auditlogging CASE... Validating the signature for the ibm-cert-manager CASE... Validating the signature for the ibm-cs-commonui CASE... Validating the signature for the ibm-events-operator CASE... Validating the signature for the ibm-cs-healthcheck CASE... Validating the signature for the ibm-cs-iam CASE... Validating the signature for the ibm-zen CASE... Validating the signature for the ibm-licensing CASE... Validating the signature for the ibm-management-ingress CASE... Validating the signature for the ibm-cs-mongodb CASE... Validating the signature for the ibm-cs-monitoring CASE... Validating the signature for the ibm-platform-api-operator CASE... Validating the signature for the ibm-automation-foundation-core CASE... Validating the signature for the ibm-datapower-operator-cp4i CASE... Validating the signature for the ibm-apiconnect CASE... Validating the signature for the ibm-datapower-operator-prod CASE... Validating the signature for the ibm-appconnect CASE... Validating the signature for the ibm-couchdb CASE... Validating the signature for the ibm-aspera-hsts-operator CASE... Validating the signature for the ibm-cloud-databases-redis CASE... Validating the signature for the ibm-mq CASE... Validating the signature for the ibm-integration-asset-repository CASE... Validating the signature for the ibm-integration-platform-navigator CASE... Validating the signature for the ibm-integration-operations-dashboard CASE... - Success Configure Registry Credentials You need to configure credentials for your local registry and cp.icr.io Credentials for Local Registry cloudctl case launch \\ --case $OFFLINEDIR / ${ CASE_NAME } - ${ CASE_VERSION } .tgz \\ --inventory $CASE_INVENTORY_SETUP \\ --action configure-creds-airgap \\ --args \"--registry ${ LOCAL_REGISTRY } --user username --pass password\" Welcome to the CASE launcher Attempting to retrieve and extract the CASE from the specified location [ \u2713 ] CASE has been retrieved and extracted Attempting to validate the CASE [ \u2713 ] CASE has been successfully validated Attempting to locate the launch inventory item, script, and action in the specified CASE [ \u2713 ] Found the specified launch inventory item, action, and script for the CASE Attempting to check the cluster and machine for required prerequisites for launching the item [ WARNING ] : Unable to create a REST API connection to the cluster due to the following error: Unable to get cluster auth tokens due to the following error: Error: stat ./auth/kubeconfig: no such file or directory ( found in getting the kubernetes config ) . Ensure that you are logged into the cluster with the appropriate role to perform this launch action. Checking for required prereqs... No requires section specified. Required prereqs result: OK Checking user permissions... No user rules specified. [ \u2713 ] Cluster and Client Prerequisites have been met for the CASE Running the CASE operator launch script with the following action context: configureCredsAirgap Executing inventory item operator, action configureCredsAirgap : launch.sh -------------Configuring authentication secret------------- [ INFO ] Creating registry authencation secret for myimageregistry.example.com:8443 [ INFO ] Registry secret created in /home/ncolon/.airgap/secrets/myimageregistry.example.com:8443.json [ INFO ] Done [ \u2713 ] CASE launch script completed successfully OK Credentials for IBM Registry cloudctl case launch \\ --case $OFFLINEDIR / ${ CASE_NAME } - ${ CASE_VERSION } .tgz \\ --inventory $CASE_INVENTORY_SETUP \\ --action configure-creds-airgap \\ --args \"--registry cp.icr.io --user cp --pass $ENTITLEMENT_KEY \" Warning Its ok to get [WARNING]: Unable to create a REST API connection in the commands above Authenticate to your airgapped cluster oc login https://api.mycluster.example.com:6443 -u kubeadmin -p xxxx-xxxx-xxxx-xxxx oc login --token = sha256~AAAAA....BBBBB --server = https://api.mycluster.example.com:6443 Warning do not use export KUBECONFIG=path/to/kubeconfig as some items may not replicate correctly. Create a namespace for your CloudPak export NAMESPACE = cp4i oc create namespace $NAMESPACE Mirror CloudPak images to Local Repository Create environment variables with the local Docker registry connection information export CASE_NAME = ibm-cp-integration export CASE_VERSION = 2 .2.2 export CASE_ARCHIVE = ${ CASE_NAME } - ${ CASE_VERSION } .tgz export CASE_INVENTORY_SETUP = operator export OFFLINEDIR = /path/to/offline export OFFLINEDIR_ARCHIVE = offline.tgz export CASE_REPO_PATH = https://github.com/IBM/cloud-pak/raw/master/repo/case export CASE_LOCAL_PATH = $OFFLINEDIR / $CASE_ARCHIVE export LOCAL_DOCKER_REGISTRY_HOST = myimageregistry.example.com:8443 export LOCAL_DOCKER_REGISTRY_PORT = 443 export LOCAL_DOCKER_REGISTRY = $LOCAL_DOCKER_REGISTRY_HOST : $LOCAL_DOCKER_REGISTRY_PORT export LOCAL_DOCKER_USER = username export LOCAL_DOCKER_PASSWORD = password Warning If you are installing only a single capability, you must run commands similar to the example below to adjust the environment variables for the capability you need to install below (in this example, only Platform Navigator). Warning if your registry runs on port 443, you don't need to include it on $LOCAL_DOCKER_REGISTRY_HOST export CASE_ARCHIVE = ibm-integration-platform-navigator-2.3.0.tgz export CASE_INVENTORY_SETUP = platformNavigatorOperator export CASE_LOCAL_PATH = $OFFLINEDIR / ${ CASE_NAME } - ${ CASE_VERSION } .tgzexport Mirror contents cloudctl case launch \\ --case $OFFLINEDIR / ${ CASE_NAME } - ${ CASE_VERSION } .tgz \\ --inventory $CASE_INVENTORY_SETUP \\ --action mirror-images \\ --args \"--registry $LOCAL_DOCKER_REGISTRY --inputDir $OFFLINEDIR \" ... info: Planning completed in 130ms sha256:9df1f3d5b16df5d8753a78d8f7bd60f72d47135e139185464cbe7c8edb9ded61 myimageregistry.example.com:8443/cpopen/ibm-common-service-catalog sha256:31d1b593d7601c69c8a02985b7326766aec68a6379dfea69fbc2c07a6251020a myimageregistry.example.com:8443/cpopen/ibm-common-service-catalog sha256:27c45813f3dfe62aa654db36c4f01a34ed189b330c52823fcddbd605ee890a87 myimageregistry.example.com:8443/cpopen/ibm-common-service-catalog sha256:cc5f6fa46af38278f565ca22ff13ae8a2cd0ae60303d5d3be5dd9395bf4b3771 myimageregistry.example.com:8443/cpopen/ibm-common-service-catalog:latest info: Mirroring completed in 110ms ( 0B/s ) [ \u2713 ] CASE launch script completed successfully Create ImageCatalogSourcePolicy $ cloudctl case launch \\ --case $OFFLINEDIR / ${ CASE_NAME } - ${ CASE_VERSION } .tgz \\ --inventory $CASE_INVENTORY_SETUP \\ --action configure-cluster-airgap \\ --namespace $NAMESPACE \\ --args \"--registry $LOCAL_DOCKER_REGISTRY --user $LOCAL_DOCKER_USER --pass $LOCAL_DOCKER_PASSWORD --inputDir $OFFLINEDIR \" ... [ INFO ] Generating image content source policy ... --- apiVersion: operator.openshift.io/v1alpha1 kind: ImageContentSourcePolicy metadata: name: ibm-cp-integration spec: repositoryDigestMirrors: - mirrors: - myimageregistry.example.com:8443/cp/apic source: cp.icr.io/cp/apic - mirrors: - myimageregistry.example.com:8443/cp/appc source: cp.icr.io/cp/appc - mirrors: - myimageregistry.example.com:8443/cp/datapower source: cp.icr.io/cp/datapower - mirrors: - myimageregistry.example.com:8443/cp/icp4i/aspera source: cp.icr.io/cp/icp4i/aspera - mirrors: - myimageregistry.example.com:8443/cp/icp4i source: cp.icr.io/cp/icp4i - mirrors: - myimageregistry.example.com:8443/cp/icp4i/od source: cp.icr.io/cp/icp4i/od - mirrors: - myimageregistry.example.com:8443/cp source: cp.icr.io/cp - mirrors: - myimageregistry.example.com:8443/ibmcom source: docker.io/ibmcom - mirrors: - myimageregistry.example.com:8443/cpopen source: icr.io/cpopen - mirrors: - myimageregistry.example.com:8443/opencloudio source: quay.io/opencloudio --- [ INFO ] Applying image content source policy oc apply -f \"/tmp/airgap_image_policy_z2Hgmp9a8\" imagecontentsourcepolicy.operator.openshift.io/ibm-cp-integration created [ \u2713 ] CASE launch script completed successfully OK Install Catalog Sources cloudctl case launch \\ --case $OFFLINEDIR / ${ CASE_ARCHIVE } \\ --inventory ${ CASE_INVENTORY_SETUP } \\ --action install-catalog \\ --namespace ${ NAMESPACE } \\ --args \"--registry ${ LOCAL_DOCKER_REGISTRY } --inputDir $OFFLINEDIR --recursive\" $ oc get pod -n openshift-marketplace NAME READY STATUS RESTARTS AGE appconnect-operator-catalogsource-cwzn7 1 /1 Running 0 7m20s aspera-operators-nvqvf 1 /1 Running 0 96m couchdb-operator-catalog-jtmfz 1 /1 Running 0 96m ibm-apiconnect-catalog-wm5xd 1 /1 Running 0 96m ibm-automation-foundation-core-catalog-wz5jq 1 /1 Running 0 7m39s ibm-cloud-databases-redis-operator-catalog-zs2qd 1 /1 Running 0 96m ibm-cp-integration-catalog-bw64z 1 /1 Running 0 6m35s ibm-datapower-operator-catalog-nl95s 1 /1 Running 0 72s ibm-integration-asset-repository-catalog-g94xt 1 /1 Running 0 6m52s ibm-integration-operations-dashboard-catalog-z62p6 1 /1 Running 0 6m37s ibm-integration-platform-navigator-catalog-pfvhg 1 /1 Running 0 6m41s ibmmq-operator-catalogsource-2hfz6 1 /1 Running 0 96m marketplace-operator-67cb5b6d45-pqmcz 1 /1 Running 0 99m opencloud-operators-5hjfp 1 /1 Running 0 96m redhat-operators-pnlh5 1 /1 Running 0 103m","title":"Mirroring CloudPak Container Images"},{"location":"infrastructure/restricted-networks/#proxy-environments","text":"When installing OpenShift in an environment where an Enterprise Proxy controls access to public resources, you need to make the installer proxy aware. The proxy needs to allow traffic to the following endpoints https://quay.io https://registry.connect.redhat.com https://registry.redhat.io Additionally, if you're deploying in a Public Cloud Provider, the proxy needs to allow access to multiple cloud management endpoints so that the OpenShift operators can interact with your cloud provider.","title":"Proxy Environments"},{"location":"infrastructure/restricted-networks/#configure-or-update-the-proxy-on-an-existing-cluster","text":"If you didn't configure the proxy at install time, your Cluster Wide Proxy object will have an empty configuration apiVersion : config.openshift.io/v1 kind : Proxy metadata : name : cluster spec : trustedCA : name : \"\" status : If your proxy uses a certificate signed by a customCA, you will first need to create a ConfigMap with the proxy certificate apiVersion : v1 data : ca-bundle.crt : | <MY_PEM_ENCODED_CERTS> kind : ConfigMap metadata : name : user-ca-bundle namespace : openshift-config Note You can skip this step if the proxy certificate is signed by a known authority Once created, you can then edit your Cluster Wide Proxy and configure any necessary fields oc edit proxy/cluster apiVersion : config.openshift.io/v1 kind : Proxy metadata : name : cluster spec : httpProxy : http://<username>:<pswd>@<ip>:<port> httpsProxy : http://<username>:<pswd>@<ip>:<port> noProxy : 192.168.100.0/24,10.0.0.1,mydatabase.example.com,.mydomain.example.com readinessEndpoints : - http://site1.example.com - https://site2.example.com - https://www.google.com trustedCA : name : user-ca-bundle","title":"Configure or Update the Proxy on an Existing Cluster"},{"location":"infrastructure/roks-1/","text":"IBM Red Hat OpenShift Kubernetes Service (ROKS) \u00b6 Overview \u00b6 This repository document experiences on working with ROKS cluster on OpenShift 4.6 environment 2Q 2021 deployed on a VPC Gen2. For additional information, refer to ROKS documentation . 1. Create a Resource Group for your cluster and vpc \u00b6 On the top menu, go to Manage > Accounts then select Resource groups on the left hand side menu. Click on Create button and provide a name for your resource group. 2. Create a VPC for your cluster \u00b6 Navigate to the VPC Infrastructure overview page . Click on the hamburger menu on the top left corner, and go to VPC Infrastructure > VPCs. Click on Create button. To create a cluster that meets all the Golden Topology requirements, provide the following values to the VPC form. Name : Provide a name for your cluster Resource Group : Select the resource group created in Step #1 Region : Select the region you would like to use. Default security group : (Optional) Select whether the VPC's default security group allows inbound SSH and ping traffic. You can modify the default security group later. Classic access : (Optional) Enable this VPC to have private network connectivity to classic resources. Only one VPC per region can communicate with classic resources. Default address prefixes : (Optional) Each zone of your VPC is assigned a default address prefix that specifies the address range in which subnets are created. If the default address scheme doesn't suit your requirements, disable this option. After you create the VPC, go to its details page and set your own address prefixes. Then, you can create subnets. Subnets : By default, it will create 3 subnets, one per Availability Zone on your region. Once the VPC is ready, create a public gateway for each of the subnets in your VPC. Navigate to the VPC Infrastructure overview page and select your cluster. Under Subnets in this VPC select the first subnet, and click on the Detached toggle to attach a public gateway for your subnet. Repeat for all other subnets. Order the VPC on the right hand summary by clicking on Create virtual private cloud Warning Failure to attach public gateways to your subnets may result in unavailable worker nodes in your Availability Zones 3. Create a Cloud Object Storage instance for your cluster \u00b6 Navigate to the Cloud Object Storage create page . Search for Cloud Object Storage on the top menu search bar to create a COS instance. Cloud Object Storage is used to back the OpenShift Internal Image Registry. To create a COS instance that meets all the Golden Topology requirements, provide the following values to the Create COS Instance form. Plan : Standard Service Name : Provide a name for your COS Instance Resource Group : Select the resource group created in Step #1 4. Create your ROKS cluster \u00b6 Navigate to the OpenShift Cluster overview page . Click on the hamburger menu on the top left corner, and go to Openshift > Clusters. Click on Create cluster button. To create a cluster that meets all the Golden Topology requirements, provide the following values to the Cluster form. Orchestration Service : Select the OpenShift version. As of 2Q 2021, options are available for OpenShift 4.5, 4.6 and 4.7 OCP Entitlement : If you have OCP Entitlements from your CloudPak, select Apply my Cloud Pak OCP entitlement to this worker pool , otherwise select Purchase additional licenses for this worker pool Infrastructure : Select VPC Virtual private cloud : Select the VPC created on Step #2 Cloud Object Storage : Select the COS instance created on Step #3 Location : Select the resource group created on Step #1 Worker zones : Use all the subnets created when you deployed your VPC in Step #2 Worker pool : The default worker node consists of 4CPU by 16GB. If your application requirement are different, select a new VM profile by clicking the Change flavor button. By default each worker pool will have 3 nodes for a total of 9 worker nodes in your cluster. The minimum you can provide is 2 workers per zone. Warning Infrastructure nodes are not required on a Managed Platform like ROKS Worker pool name : provide a name for your worker pool. Master service endpoint : Select Both private & public endpoints if your Cluster API endpoint needs to be publicly exposed. Otherwise select Private endpoint only . Cluster name : Provide a name for your cluster Order the cluster on the right hand summary by clicking on Create After some time, your control plane, worker nodes and ingress controller will be ready.","title":"IBM Red Hat OpenShift Kubernetes Service (ROKS)"},{"location":"infrastructure/roks-1/#ibm-red-hat-openshift-kubernetes-service-roks","text":"","title":"IBM Red Hat OpenShift Kubernetes Service (ROKS)"},{"location":"infrastructure/roks-1/#overview","text":"This repository document experiences on working with ROKS cluster on OpenShift 4.6 environment 2Q 2021 deployed on a VPC Gen2. For additional information, refer to ROKS documentation .","title":"Overview"},{"location":"infrastructure/roks-1/#1-create-a-resource-group-for-your-cluster-and-vpc","text":"On the top menu, go to Manage > Accounts then select Resource groups on the left hand side menu. Click on Create button and provide a name for your resource group.","title":"1. Create a Resource Group for your cluster and vpc"},{"location":"infrastructure/roks-1/#2-create-a-vpc-for-your-cluster","text":"Navigate to the VPC Infrastructure overview page . Click on the hamburger menu on the top left corner, and go to VPC Infrastructure > VPCs. Click on Create button. To create a cluster that meets all the Golden Topology requirements, provide the following values to the VPC form. Name : Provide a name for your cluster Resource Group : Select the resource group created in Step #1 Region : Select the region you would like to use. Default security group : (Optional) Select whether the VPC's default security group allows inbound SSH and ping traffic. You can modify the default security group later. Classic access : (Optional) Enable this VPC to have private network connectivity to classic resources. Only one VPC per region can communicate with classic resources. Default address prefixes : (Optional) Each zone of your VPC is assigned a default address prefix that specifies the address range in which subnets are created. If the default address scheme doesn't suit your requirements, disable this option. After you create the VPC, go to its details page and set your own address prefixes. Then, you can create subnets. Subnets : By default, it will create 3 subnets, one per Availability Zone on your region. Once the VPC is ready, create a public gateway for each of the subnets in your VPC. Navigate to the VPC Infrastructure overview page and select your cluster. Under Subnets in this VPC select the first subnet, and click on the Detached toggle to attach a public gateway for your subnet. Repeat for all other subnets. Order the VPC on the right hand summary by clicking on Create virtual private cloud Warning Failure to attach public gateways to your subnets may result in unavailable worker nodes in your Availability Zones","title":"2. Create a VPC for your cluster"},{"location":"infrastructure/roks-1/#3-create-a-cloud-object-storage-instance-for-your-cluster","text":"Navigate to the Cloud Object Storage create page . Search for Cloud Object Storage on the top menu search bar to create a COS instance. Cloud Object Storage is used to back the OpenShift Internal Image Registry. To create a COS instance that meets all the Golden Topology requirements, provide the following values to the Create COS Instance form. Plan : Standard Service Name : Provide a name for your COS Instance Resource Group : Select the resource group created in Step #1","title":"3. Create a Cloud Object Storage instance for your cluster"},{"location":"infrastructure/roks-1/#4-create-your-roks-cluster","text":"Navigate to the OpenShift Cluster overview page . Click on the hamburger menu on the top left corner, and go to Openshift > Clusters. Click on Create cluster button. To create a cluster that meets all the Golden Topology requirements, provide the following values to the Cluster form. Orchestration Service : Select the OpenShift version. As of 2Q 2021, options are available for OpenShift 4.5, 4.6 and 4.7 OCP Entitlement : If you have OCP Entitlements from your CloudPak, select Apply my Cloud Pak OCP entitlement to this worker pool , otherwise select Purchase additional licenses for this worker pool Infrastructure : Select VPC Virtual private cloud : Select the VPC created on Step #2 Cloud Object Storage : Select the COS instance created on Step #3 Location : Select the resource group created on Step #1 Worker zones : Use all the subnets created when you deployed your VPC in Step #2 Worker pool : The default worker node consists of 4CPU by 16GB. If your application requirement are different, select a new VM profile by clicking the Change flavor button. By default each worker pool will have 3 nodes for a total of 9 worker nodes in your cluster. The minimum you can provide is 2 workers per zone. Warning Infrastructure nodes are not required on a Managed Platform like ROKS Worker pool name : provide a name for your worker pool. Master service endpoint : Select Both private & public endpoints if your Cluster API endpoint needs to be publicly exposed. Otherwise select Private endpoint only . Cluster name : Provide a name for your cluster Order the cluster on the right hand summary by clicking on Create After some time, your control plane, worker nodes and ingress controller will be ready.","title":"4. Create your ROKS cluster"},{"location":"infrastructure/roks/","text":"IBM Red Hat OpenShift Kubernetes Service (ROKS) \u00b6 Overview \u00b6 There are 4 proposed topologies depending on the deployment requirements, choose the one that adapts better to your needs. All topologies deploy Foundational Services as the Cloud Paks requires them to run, and some OpenShift infrastructure elements are deployed over these nodes. ROKS-split-1 \u00b6 Foundational Services Cloud Paks ROKS-split-2 \u00b6 Foundational Services Cloud Paks Workers (for non Cloud Paks workloads) ROKS-split-3 \u00b6 Foundational Services Cloud Paks Storage (Openshift Data Foundation using IBM Cloud CRDs) ROKS-split-4 \u00b6 Foundational Services Cloud Paks Storage (Openshift Data Foundation using IBM Cloud CRDs) Workers (for non Cloud Paks workloads) Deploy VPC \u00b6 Select an IBM Cloud region ($region) To get the available regions for VPC run ibmcloud is regions ibmcloud target -r $region Create a resource group or use one already created To get the resource groups available ibmcloud resource groups . Create a group if necessary: ibmcloud resource group-create $name Select the group: ibmcloud target -g $name Create a VPC ($vpc_name) Note You can use an already provisioned VPC, just need the VPC ID. ibmcloud is vpc-create $vpc_name --resource-group-name $resourcegroup --address-prefix-management manual Create the IP address network ranges. It is needed to create one per node type and zone: infra, storage, cloudpaks, and workers. For example, for infra nodes, it is needed three IP address network ranges, one network range address per zone. The network ranges must be big enough to allocate IPs for all nodes of each type in each zone. $range must differ from one network range to other, it can be a created different ones if it creates the minimum number of IPs needed for the nodes. Note To get available zones on the VPC region targeted run ibmcloud is zones , and to get the $vpc_id ibmcloud is vpcs : ibmcloud is vpc-address-prefix-create subnet-infra-1 $vpc_id $zone_1 192 .168. $range_1 .0/28 ibmcloud is vpc-address-prefix-create subnet-infra-2 $vpc_id $zone_3 192 .168. $range_2 .0/28 ibmcloud is vpc-address-prefix-create subnet-infra-3 $vpc_id $zone_3 192 .168. $range_3 .0/28 Repeat the process for the rest of the cloudpak node type: ibmcloud is vpc-address-prefix-create subnet-cloud-paks-1 $vpc_id $zone_1 192 .168. $range_7 .0/27 ibmcloud is vpc-address-prefix-create subnet-cloud-paks-2 $vpc_id $zone_3 192 .168. $range_8 .0/27 ibmcloud is vpc-address-prefix-create subnet-cloud-paks-3 $vpc_id $zone_3 192 .168. $range_9 .0/27 If you are deploying the architecture topology ROKS-3 or ROKS-4, where the cluster will host OpenShift Storage Container repeat the process for the storage node type: ibmcloud is vpc-address-prefix-create subnet-storage-1 $vpc_id $zone_1 192 .168. $range_4 .0/28 ibmcloud is vpc-address-prefix-create subnet-storage-2 $vpc_id $zone_3 192 .168. $range_5 .0/28 ibmcloud is vpc-address-prefix-create subnet-storage-3 $vpc_id $zone_3 192 .168. $range_6 .0/28 If you are deploying the architecture topology ROKS-2 or ROKS-4, where the cluster will host applications repeat the process for the worker node type: ibmcloud is vpc-address-prefix-create subnet-workers-1 $vpc_id $zone_1 192 .168. $range_10 .0/27 ibmcloud is vpc-address-prefix-create subnet-workers-2 $vpc_id $zone_3 192 .168. $range_11 .0/27 ibmcloud is vpc-address-prefix-create subnet-workers-3 $vpc_id $zone_3 192 .168. $range_12 .0/27 Create an IP address network range for the subnet where the VPN Gateway will be deployed: ibmcloud is vpc-address-prefix-create subnet-vpn-gw $vpc_id $zone_1 192 .168. $range_13 .0/29 Create the subnets. It is needed to create one per node type and zone: infra, storage, cloudpaks, and workers. For example, for infra nodes, it is needed three subnets, one subnet per zone. It is needed to apply the IP address range to each subnet. For example, for infra nodes: ibmcloud is subnet-create subnet-infra-1 $vpc_id --ipv4-cidr-block 192 .168. $range_1 .0/28 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-infra-2 $vpc_id --ipv4-cidr-block 192 .168. $range_2 .0/28 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-infra-3 $vpc_id --ipv4-cidr-block 192 .168. $range_3 .0/28 --resource-group-name $resourcegroup Repeat the process for the cloudpak node type: ibmcloud is subnet-create subnet-cloud-paks-1 $vpc_id --ipv4-cidr-block 192 .168. $range_7 .0/27 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-cloud-paks-2 $vpc_id --ipv4-cidr-block 192 .168. $range_8 .0/27 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-cloud-paks-3 $vpc_id --ipv4-cidr-block 192 .168. $range_9 .0/27 --resource-group-name $resourcegroup If you are deploying the architecture topology ROKS-3 or ROKS-4, where the cluster will host OpenShift Storage Container repeat the process for the storage node type: ibmcloud is subnet-create subnet-storage-1 $vpc_id --ipv4-cidr-block 192 .168. $range_4 .0/28 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-storage-2 $vpc_id --ipv4-cidr-block 192 .168. $range_5 .0/28 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-storage-3 $vpc_id --ipv4-cidr-block 192 .168. $range_6 .0/28 --resource-group-name $resourcegroup If you are deploying the architecture topology ROKS-2 or ROKS-4, where the cluster will host applications repeat the process for the worker node type: ibmcloud is subnet-create subnet-workers-1 $vpc_id --ipv4-cidr-block 192 .168. $range_10 .0/27 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-workers-2 $vpc_id --ipv4-cidr-block 192 .168. $range_11 .0/27 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-worker-3 $vpc_id --ipv4-cidr-block 192 .168. $range_12 .0/27 --resource-group-name $resourcegroup Create the subnet to place the VPN Gateway: ibmcloud is subnet-create subnet-vpn-gw $vpc_id --ipv4-cidr-block 192 .168. $range_13 .0/29 --resource-group-name $resourcegroup Create Public Gateways for outbound connectivity. It is needed to create one per zone. ibmcloud is public-gateway-create gw-subnet-zone-1 $vpc_id $zone_1 ibmcloud is public-gateway-create gw-subnet-zone-2 $vpc_id $zone_2 ibmcloud is public-gateway-create gw-subnet-zone-3 $vpc_id $zone_3 Attach the public gateways on all the subnets that need outbound connectivity to the Internet. Note Run ibmcloud is subnets to get the subnets ID, and ibmcloud is public-gateways to get the public gateway ID: ibmcloud is subnet-update $subnet -infra-1_id --public-gateway-id $gw -subnet-zone-1_id ibmcloud is subnet-update $subnet -infra-2_id --public-gateway-id $gw -subnet-zone-2_id ibmcloud is subnet-update $subnet -infra-3_id --public-gateway-id $gw -subnet-zone-3_id ibmcloud is subnet-update $subnet -cloud-paks-1_id --public-gateway-id $gw -subnet-zone-1_id ibmcloud is subnet-update $subnet -cloud-paks-2_id --public-gateway-id $gw -subnet-zone-2_id ibmcloud is subnet-update $subnet -cloud-paks-3_id --public-gateway-id $gw -subnet-zone-3_id If you are deploying the architecture topology ROKS-3 or ROKS-4, where the cluster will host OpenShift Storage Container repeat the process for the storage node type: ibmcloud is subnet-update $subnet -storage-1_id --public-gateway-id $gw -subnet-zone-1_id ibmcloud is subnet-update $subnet -storage-2_id --public-gateway-id $gw -subnet-zone-2_id ibmcloud is subnet-update $subnet -storage-3_id --public-gateway-id $gw -subnet-zone-3_id If you are deploying the architecture topology ROKS-2 or ROKS-4, where the cluster will host applications repeat the process for the worker node type: ibmcloud is subnet-update $subnet -workers-1_id --public-gateway-id $gw -subnet-zone-1_id ibmcloud is subnet-update $subnet -workers-2_id --public-gateway-id $gw -subnet-zone-2_id ibmcloud is subnet-update $subnet -workers-3_id --public-gateway-id $gw -subnet-zone-3_id Create a VPN tunnel to connect to the VPC \u00b6 The documentation from IBM Cloud explains each parameter needed to establish the IPsec tunnel that will connect the location of our premises with the VPC on IBM Cloud. https://cloud.ibm.com/docs/vpc?topic=vpc-using-vpn Note IBM Cloud offers a set of recommendation parameters for a certain set of devices here: https://cloud.ibm.com/docs/vpc?topic=vpc-vpn-onprem-example Warning VMware is not on the list but this article describes how to get it on VMWare environments: \"https://medium.com/ibm-garage/extending-on-premise-vmware-environment-with-an-ipsec-tunnel-to-consume-securely-ibm-cloud-13cc9326dce1\" The next resources need to be created according to the previous recommendations or the requirements needed to establish the IPsec tunnel. IKE-Policy: The options available for this parameter can be seen running this command ibmcloud is ike-policy-create --help ibmcloud is ike-policy-create $vpc_name $auth_algorithm_ike $dh_group_ike $encryption_ike $ike_version --key-lifetime $key_lifetime_ike --resource-group-name $resourcegroup IPsec-Policy: The options available for this parameter can be seen running this command ibmcloud is ipsec-policy-create --help ibmcloud is ipsec-policy-create $vpc_name -ipsec $auth_algorithm_ipsec $encryption_ipsec $pfs_ipsec --key-lifetime $key_lifetime_ipsec --resource-group-name $resourcegroup Create the VPN-Gateway: $vpn_subnet is the subnet where the VPN gateway will be deployed. It can be a dedicated subnet or one of the infra subnets ibmcloud is vpn-gateway-create $vpc_name $subnet -vpn-gw_ID --mode $policy --resource-group-name $resourcegroup Create an IPsec connection: To check the parameters needed ibmcloud is vpn-gateway-connection-create --help . To get the id of the IKE policies run ibmcloud is ike-policies and to get the IPsec policies ibmcloud is ipsec-policies , to get the vpn gateway ID run ibmcloud is vpn-gateways : ibmcloud is vpn-gateway-connection-create $vpc_name $vpn_id $peer_address $pre_shared_key --ike-policy $ike_id --ipsec-policy $ipsec_id If the policy selected was route based when the VPN Gateway was created, a route table is needed to be created: Note As an example here is described how to connect to a VMware environment as is described in the article shown above ibmcloud is vpc-routing-table-create $vpc_id --vpc-zone-ingress false --name $vpc_name Attach all the subnets to the route table, to get the ID of the route table run ibmcloud is vpc-routing-tables : ibmcloud is subnet-update $subnet -infra-1_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -infra-2_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -infra-3_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -storage-1_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -storage-2_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -storage-3_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -cloud-paks-1_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -cloud-paks-2_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -cloud-paks-3_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -workers-1_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -workers-2_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -workers-3_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -vpn-gw_ID --routing-table-id $route_table_ID * To add the routes: Each subnet needs a route to the remote network address range. $network_remote is the network address range of the remote network from which ROKS will be accessed. To get the connection ID run ibmcloud is vpn-gateway-connections $vpn-gw_ID For example for infra subnets: ibmcloud is vpc-routing-table-route-create $vpc_id $route_table_id --name subnet-infra-1 --zone $zone_1 --destination $network_remote --action deliver --next-hop $connection_id ibmcloud is vpc-routing-table-route-create $vpc_id $route_table_id --name subnet-infra-2 --zone $zone_2 --destination $network_remote --action deliver --next-hop $connection_id ibmcloud is vpc-routing-table-route-create $vpc_id $route_table_id --name subnet-infra-3 --zone $zone_3 --destination $network_remote --action deliver --next-hop $connection_id Deploy ROKS \u00b6 Create a Cloud Object Storage instance to host the images of the registry among others: ibmcloud resource service-instance-create $cos_name cloud-object-storage standard global -g $resourcegroup Create the ROKS cluster: There is a CLI limitation that does not allow the creation of several zones deployment for nodes from the start, it needs to be added after, either change the default worker pool from default , the default worker pool is deleted right after the creation of the cluster. The public endpoint is disabled for security, public outbound connectivity is enabled through the public gateways though. Note To get the flavors available on the zone/region ibmcloud oc flavors --zone $zone_1 . Use minimum flavor as the default pool will be deleted right after the cluster is created. To get the Openshift versions available ibmcloud oc versions . The recommended version at the moment that this guide was written is 4.7.19_openshift . To get the CRN (ID of the storage instance) of the Object Storage instance, run ibmcloud resource service-instance $cos_name . ibmcloud oc cluster create vpc-gen2 --name $cluster_name --vpc-id $vpc_id --workers 2 --version $version --flavor $flavor --zone $zone_1 --subnet-id $subnet -infra-1_id --cos-instance $cos_id --disable-public-service-endpoint --entitlement cloud_pak To create the Foundational services worker pool: Note The flavor recommended to deploy a minimum version of Foundational Services in the Infra nodes is bx2.8x32 To get the ROKS cluster ID run ibmcloud oc clusters ibmcloud oc worker-pool create vpc-gen2 --name foundational-services --cluster $roks_id --flavor $flavor_infra --size-per-zone 1 --vpc-id $vpc_id --entitlement cloud_pak --label node-role.kubernetes.io/foundational-services = true ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -infra-1_id --cluster $roks_id --zone $zone_1 --worker-pool foundational-services ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -infra-2_id --cluster $roks_id --zone $zone_2 --worker-pool foundational-services ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -infra-3_id --cluster $roks_id --zone $zone_3 --worker-pool foundational-services Note If you are deploying topologies ROKS-1 or ROKS-3. Once there is another worker pool, we can delete the default worker-pool ibmcloud oc worker-pool rm --worker-pool default --cluster $roks_id To create the Cloud Pak worker pool: If you are bringing your own license use the flag cloud_pak for the entitlement, in the case you want to consume the entitlement directly from IBM Cloud remove the flag. Note The flavor for these nodes will depend on the cloudpak that will be installed on them. ibmcloud oc worker-pool create vpc-gen2 --name cloud-paks --cluster $roks_id --flavor $flavor_cloud -paks --size-per-zone 1 --vpc-id $vpc_id --entitlement cloud_pak --label node-role.kubernetes.io/cloud-paks = true Extending the worker pool across the three zones: ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -cloud-paks-1_id --cluster $roks_id --zone $zone_1 --worker-pool cloud-paks ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -cloud-paks-2_id --cluster $roks_id --zone $zone_2 --worker-pool cloud-paks ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -cloud-paks-3_id --cluster $roks_id --zone $zone_3 --worker-pool cloud-paks * To create the Storage worker pool: If you are deploying topologies ROKS-3 or ROKS-4, you need to create the Storage worker pool: Note The recommended storage class to run Openshift Data Foundation (previously Openshift Container Storage) is bx2.16x64 If you are bringing your own license use the flag cloud_pak for the entitlement, in the case you want to consume the entitlement directly from IBM Cloud remove the flag. ibmcloud oc worker-pool create vpc-gen2 --name storage --cluster $roks_id --flavor $flavor_storage --size-per-zone 1 --vpc-id $vpc_id --entitlement cloud_pak --label node-role.kubernetes.io/storage = true It is needed to apply the Openshift Data Foundation addon from IBM Cloud to the cluster, that will deploy the specific CRD for Openshift Data Foundation from IBM Cloud: ibmcloud oc cluster addon enable openshift-container-storage --cluster $roks_id --version 4 .7.0 Extending the worker pool across the three zones: ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -storage-1_id --cluster $roks_id --zone $zone_1 --worker-pool storage ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -storage-2_id --cluster $roks_id --zone $zone_2 --worker-pool storage ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -storage-3_id --cluster $roks_id --zone $zone_3 --worker-pool storage * To create the Storage worker pool: If you are deploying topologies ROKS-2 or ROKS-4, you need to create the Workers worker pool: The default worker pool can be used to host applications that don`t fit with the infra workloads, storage workloads, or Cloud Pak workloads. To label the worker pool run: Adjust roles and labels The default roles assigned by IBM Cloud need to be changed, to remove them: Retrieve the admin credentials for the cluster: ibmcloud oc cluster config --cluster $roks_name --admin Note Run oc get nodes to check you are logged in the cluster. Remove the master and worker role labels from nodes, to apply the correct labels to each worker-pool: oc get nodes | awk '{print $1}' | awk '{if(NR>1)print}' | xargs -I {} bash -c 'oc label node {} node-role.kubernetes.io/master-' oc get nodes | awk '{print $1}' | awk '{if(NR>1)print}' | xargs -I {} bash -c 'oc label node {} node-role.kubernetes.io/worker-' Apply the label to the foundational services nodes (if not added when the worker pool was created): ```bash ibmcloud oc worker-pool label set --cluster $roks_name --worker-pool foundational-services --label node-role.kubernetes.io/foundational-services=true -f ibmcloud oc worker-pool label set --cluster $roks_name --worker-pool foundational-services --label node-role.kubernetes.io/master=\"\" -f Apply the label to the cloud-paks nodes (if not added when the worker pool was created): ```bash ibmcloud oc worker-pool label set --cluster $roks_name --worker-pool cloud-paks --label node-role.kubernetes.io/cloud-paks=true -f Apply the label to the worker nodes: ibmcloud oc worker-pool label set --cluster $roks_name --worker-pool default --label node-role.kubernetes.io/worker = true -f Apply the label to the Storage nodes: ibmcloud oc worker-pool label set --cluster $roks_name --worker-pool default --label cluster.ocs.openshift.io/openshift-storage = true -f ibmcloud oc worker-pool label set --cluster $roks_name --worker-pool default --label node-role.kubernetes.io/storage = true -f Deploy GitOps \u00b6 Log into the ROKS cluster: ibmcloud oc cluster config --cluster $roks_name --admin Installing GitOps operator: Fork and clone the standard repo GitOps for IBM Cloud: git clone https://github.com/chechuironman/roks-splits.git Install the GitOps operator oc apply -f setup/ocp47/openshift-gitops-operator.yaml Add the ClusterRoles needed by ArgoCD (GitOps) to Bootstrap the cluster oc apply -f setup/ocp47/custom-argocd-app-controller-clusterrole.yaml oc apply -f setup/ocp47/custom-argocd-app-controller-clusterrolebinding.yaml Apply the 3-layer GitOps structure Wait until the GitOps operator is successfully deployed before deploy the Bootstrap ArgoCD application. Modify the bootstrap.yaml on the root folder file to point to the newly forked repo: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-roks-split-X namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/argocd/$YOUR_ROKS_FLAVOR_TOPOLOGY repoURL : $YOUR_REPO_URL targetRevision : master syncPolicy : automated : prune : true selfHeal : true Apply the taints to the nodes to avoid workloads not related to that node role to be placed in: Note Before applying taints you should confirm all pods are running. For the cloud-paks nodes. ibmcloud oc worker-pool taint set --cluster $roks_name --worker-pool cloud-paks --taint node-role.kubernetes.io/cloud-paks = true:NoExecute -f For the Foundational Services nodes. ibmcloud oc worker-pool taint set --cluster $roks_name --worker-pool foundational-services --taint node-role.kubernetes.io/foundational-services = true:NoExecute -f If you are deploying topologies ROKS-3 or ROKS-4, you need to apply the Storage nodes taints: ibmcloud oc worker-pool taint set --cluster $roks_name --worker-pool storage --taint node-role.kubernetes.io/storage = true:NoExecute -f Depending on the cluster topology to deploy choose between the next bootstrap ArgoCD apps: - 0-bootstrap/argocd/roks-split-1/bootstrap.yaml - 0-bootstrap/argocd/roks-split-2/bootstrap.yaml - 0-bootstrap/argocd/roks-split-3/bootstrap.yaml - 0-bootstrap/argocd/roks-split-4/bootstrap.yaml To grab the ArgoCD credentials and route to access to the dashboard run: password: passwd_argocd = $( oc get secret openshift-gitops-cluster -n openshift-gitops -o json | jq -r .data | jq -r . [ 'admin.password' ] | tail -1 | base64 -d ) route: argo_route = $( oc get route openshift-gitops-server -n openshift-gitops -o json | jq -r '.spec.host' ) If using Github Enterprise it is needed to authenticate against the repo. Follow the next steps: argocd login ${ argo_route } --username admin --password ${ passwd_argocd } --insecure Add the following repos to the argoCD instance, $github_password is a token created in Github Enterprise with permissions to read the repo: ```bash argocd repo add https://github.com/chechuironman/roks-splits --type git --name roks-splits argocd repo add https://github.com/chechuironman/roks-infra --type git --name roks-infra argocd repo add https://github.com/chechuironman/roks-services --type git --name roks-services To deploy the bootstrap app that will roll out the cluster components installation run: ```bash oc apply -f 0-bootstrap/argocd/roks-split-$X/bootstrap.yaml -n openshift-gitops Deploy Observability \u00b6 Log into the ROKS cluster: ibmcloud oc cluster config --cluster $roks_name --admin Monitoring: \u00b6 Create the monitoring instance to use: Note It is possible to use an already provisioned instance Note You can find the service tiers for monitoring here . You can find the locations here ibmcloud resource service-instance-create $monitoring_instance_name sysdig-monitor $service_tier $location -p '{\"default_receiver\": false,\"external_api_auth\": \"API_AUTH\"}' Once the instance is created, we need to create the key to access the instance from Openshift with \"Writer\" access: ibmcloud resource service-key-create $monitoring_instance_access_key_name Writer --instance-name $monitoring_instance_name The output of the key creation should give us two parameters we need to monitor the Openshift cluster with this Sysdig instance: Sysdig Access Key and Sysdig Collector Endpoint. Note If you missed these parameters you can run the script get_monitoring_parameters.sh from teh monkey-army folder ```bash ./monkey-army/get_observability_parameters.sh $monitoring_instance_access_key_name Deploy the Sysdig monitoring agents on the Openshift cluster Note The Sysdig pods need priviledge to access the metrics of the node. Run the following command to add the privileged 'security context constraints' oc adm policy add-scc-to-user privileged -n ibm-observe -z sysdig-agent curl -sL https://raw.githubusercontent.com/draios/sysdig-cloud-scripts/master/agent_deploy/IBMCloud-Kubernetes-Service/install-agent-k8s.sh | bash -s -- -a $Sysdig_Access_Key -c $Sysdig_Collector_Endpoint -t TAG_DATA -ac 'sysdig_capture_enabled: false' Wait until all the pods are deployed on the 'ibm-observe' namespace: oc get pods -n ibm-observe --watch Access to the Monitoring Dashboard through the IBM Cloud UI or the url you can obtain with this command: ibmcloud resource service-instance $monitoring_instance_name --output JSON | jq -r . [ 0 ] .dashboard_url More information about monitoring in IBM Cloud Openshift clusters Logging \u00b6 Create the Logging instance to use: Note It is possible to use an already provisioned instance Note The service plan availables are: 'lite', '7-day', '14-day', '30-day'. You can find the regions available to deploy Logging here ibmcloud resource service-instance-create $logging_instance_name logdna $logging_plan $location Once the instance is created, we need to create the key to access the instance from Openshift with \"Manager\" access: ibmcloud resource service-key-create $logging_instance_access_key_name Manager --instance-name $logging_instance_name The output of the key creation should give us one parameter we need to monitor the Openshift cluster with this LogDNA instance: ingestion_key. Note If you missed this parameter you can run the script get_logging_parameter.sh from the monkey-army folder ./monkey-army/get_observability_parameters.sh $logging_instance_access_key_name Create the LogDNA secret with the logging ingestion key to send the logs to the LogDNA instance: oc create secret generic logdna-agent-key --from-literal = logdna-agent-key = $log_dna_ingestion_key -n ibm-observe * Deploy the LogDNA agents on the Openshift cluster. Note The LogDNA pods need hostmount-anyuid permissions. Run the following command to add the hostmount-anyuid 'security context constraints' oc adm policy add-scc-to-user hostmount-anyuid -n ibm-observe -z default Download the daemon set definition: wget https://assets.us-south.logging.cloud.ibm.com/clients/logdna-agent-ds.yaml Edit the yaml definition and change the following values to point to the right region where the logging instance is deployed, by default is pointing to us-south: - 'LDAPIHOST' - 'LDLOGHOST' Aply the new values: oc apply -f logdna-agent-ds.yaml -n ibm-observe Wait until all the pods are deployed on the 'ibm-observe' namespace: oc get pods -n ibm-observe --watch Access to the Logging Dashboard through the IBM Cloud UI or the url you can obtain with this command: ibmcloud resource service-instance $logging_instance_name --output JSON | jq -r . [ 0 ] .dashboard_url More information about monitoring in IBM Cloud Openshift clusters Establish Security Perimeter \u00b6 To establish a network security perimeter in the VPC, we will use Security groups. The security perimeter is split into two security groups: workers and load balancer. Note The deployment of Openshift Data Foundation triggers the deployment of two public load balancers that are not needed, this is because Red Hat does not support the Openshift Data Foundation install in an air-gapped environment, but as we deployed public gateways for outbound connectivity, we can remove it with the following commands: List the load balancers: ibmcloud is load-balancers Delete the public load balancers: ibmcloud is load-balancer-pools $public_lb_id | awk 'NR > 2 {print $1}' | xargs ibmcloud is load-balancer-delete $public_lb_id -f Workers Security Groups This security group is automatically created when the cluster is provisioned with the name \"kube-\"$roks_id. This security group is used to set the rules for the workers, as they are not visible in the IBM Cloud account. There are three inbound rules and one outbound rule. We just need to keep the rules that connect with the masters: (INBOUND) All protocols from 172.17.0.0/18 to All ports -> This is used to connect from masters to workers. (INBOUND) TCP from All to 30000-32767 ports - This is used to connect to services exposed via nodeports. (INBOUND) UDP from All to 30000-32767 ports - This is used to connect to services exposed via nodeports. (OUTBOUND) All protocols to 172.17.0.0/18 to All ports -> This is used to connect from workers to masters. We should remove rules 2 and 3 as the Load Balancer will connect to the nodeports via the routes created on Openshift. The commands to remove the rules are: ibmcloud is security-groups Copy the security group with the name $kube-roks_id, and use that id to get the rules on that security group: ibmcloud is security-group-rules $sg_id Delete the inbound rules pointing to the nodeports, the rules 2 and 3 from the previous list on this doc: ibmcloud is security-group-rule-delete $sg_id $rule_to_delete_id -f Load Balancer Security Group We assume the apps will be exposed through routes via the load balancer automatically provisioned by the default router or provisioned with another app service. Load Balancer for the default Openshift router Note 192.168.250.0/24 is the CIDR of our on-premise network in the diagram. 192.168.0.0/22 is the range of our networks hosting the infra nodes with the router pods where we want to send the traffic. There are three inbound rules and one outbound rule: (INBOUND) TCP protocol from $on_premise_network to 443 port -> This is used to expose the https routes (Openshift dashboard or other apps exposed through this LB) (INBOUND) TCP protocol from $on_premise_network to 80 port -> This is used to expose the http routes (INBOUND) ICMP from All - This is used from the IBM Cloud platform DNS to check the LB is healthy and send traffic (OUTBOUND) TCP protocol to $infra_nodes to Nodeports (30000:32767) -> This is used to connect to the services exposed on the nodeports. (OUTBOUND) UDP protocol to $infra_nodes to Nodeports (30000:32767) -> This is used to connect to the services exposed on the nodeports. Note It can be more granular and just create the outbound rule to the specific nodeports where the apps are published, app service by app service. We need to create a new security group for the load balancer: ibmcloud is security-group-create $LB -default-route-name $vpc_id With the id from the output from the previous command ($security_group_id) we add the rules as they are shown in the picture: ibmcloud is security-group-rule-add $security_group_id inbound tcp --port-min 443 --port-max 443 --remote $on_premise_network ibmcloud is security-group-rule-add $security_group_id inbound tcp --port-min 80 --port-max 80 --remote $on_premise_network ibmcloud is security-group-rule-add $security_group_id inbound icmp ibmcloud is security-group-rule-add $security_group_id outbound tcp --port-min 30000 --port-max 32767 --remote 192 .168.0.0/22 ibmcloud is security-group-rule-add $security_group_id outbound udp --port-min 30000 --port-max 32767 --remote 192 .168.0.0/22","title":"IBM ROKS"},{"location":"infrastructure/roks/#ibm-red-hat-openshift-kubernetes-service-roks","text":"","title":"IBM Red Hat OpenShift Kubernetes Service (ROKS)"},{"location":"infrastructure/roks/#overview","text":"There are 4 proposed topologies depending on the deployment requirements, choose the one that adapts better to your needs. All topologies deploy Foundational Services as the Cloud Paks requires them to run, and some OpenShift infrastructure elements are deployed over these nodes.","title":"Overview"},{"location":"infrastructure/roks/#roks-split-1","text":"Foundational Services Cloud Paks","title":"ROKS-split-1"},{"location":"infrastructure/roks/#roks-split-2","text":"Foundational Services Cloud Paks Workers (for non Cloud Paks workloads)","title":"ROKS-split-2"},{"location":"infrastructure/roks/#roks-split-3","text":"Foundational Services Cloud Paks Storage (Openshift Data Foundation using IBM Cloud CRDs)","title":"ROKS-split-3"},{"location":"infrastructure/roks/#roks-split-4","text":"Foundational Services Cloud Paks Storage (Openshift Data Foundation using IBM Cloud CRDs) Workers (for non Cloud Paks workloads)","title":"ROKS-split-4"},{"location":"infrastructure/roks/#deploy-vpc","text":"Select an IBM Cloud region ($region) To get the available regions for VPC run ibmcloud is regions ibmcloud target -r $region Create a resource group or use one already created To get the resource groups available ibmcloud resource groups . Create a group if necessary: ibmcloud resource group-create $name Select the group: ibmcloud target -g $name Create a VPC ($vpc_name) Note You can use an already provisioned VPC, just need the VPC ID. ibmcloud is vpc-create $vpc_name --resource-group-name $resourcegroup --address-prefix-management manual Create the IP address network ranges. It is needed to create one per node type and zone: infra, storage, cloudpaks, and workers. For example, for infra nodes, it is needed three IP address network ranges, one network range address per zone. The network ranges must be big enough to allocate IPs for all nodes of each type in each zone. $range must differ from one network range to other, it can be a created different ones if it creates the minimum number of IPs needed for the nodes. Note To get available zones on the VPC region targeted run ibmcloud is zones , and to get the $vpc_id ibmcloud is vpcs : ibmcloud is vpc-address-prefix-create subnet-infra-1 $vpc_id $zone_1 192 .168. $range_1 .0/28 ibmcloud is vpc-address-prefix-create subnet-infra-2 $vpc_id $zone_3 192 .168. $range_2 .0/28 ibmcloud is vpc-address-prefix-create subnet-infra-3 $vpc_id $zone_3 192 .168. $range_3 .0/28 Repeat the process for the rest of the cloudpak node type: ibmcloud is vpc-address-prefix-create subnet-cloud-paks-1 $vpc_id $zone_1 192 .168. $range_7 .0/27 ibmcloud is vpc-address-prefix-create subnet-cloud-paks-2 $vpc_id $zone_3 192 .168. $range_8 .0/27 ibmcloud is vpc-address-prefix-create subnet-cloud-paks-3 $vpc_id $zone_3 192 .168. $range_9 .0/27 If you are deploying the architecture topology ROKS-3 or ROKS-4, where the cluster will host OpenShift Storage Container repeat the process for the storage node type: ibmcloud is vpc-address-prefix-create subnet-storage-1 $vpc_id $zone_1 192 .168. $range_4 .0/28 ibmcloud is vpc-address-prefix-create subnet-storage-2 $vpc_id $zone_3 192 .168. $range_5 .0/28 ibmcloud is vpc-address-prefix-create subnet-storage-3 $vpc_id $zone_3 192 .168. $range_6 .0/28 If you are deploying the architecture topology ROKS-2 or ROKS-4, where the cluster will host applications repeat the process for the worker node type: ibmcloud is vpc-address-prefix-create subnet-workers-1 $vpc_id $zone_1 192 .168. $range_10 .0/27 ibmcloud is vpc-address-prefix-create subnet-workers-2 $vpc_id $zone_3 192 .168. $range_11 .0/27 ibmcloud is vpc-address-prefix-create subnet-workers-3 $vpc_id $zone_3 192 .168. $range_12 .0/27 Create an IP address network range for the subnet where the VPN Gateway will be deployed: ibmcloud is vpc-address-prefix-create subnet-vpn-gw $vpc_id $zone_1 192 .168. $range_13 .0/29 Create the subnets. It is needed to create one per node type and zone: infra, storage, cloudpaks, and workers. For example, for infra nodes, it is needed three subnets, one subnet per zone. It is needed to apply the IP address range to each subnet. For example, for infra nodes: ibmcloud is subnet-create subnet-infra-1 $vpc_id --ipv4-cidr-block 192 .168. $range_1 .0/28 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-infra-2 $vpc_id --ipv4-cidr-block 192 .168. $range_2 .0/28 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-infra-3 $vpc_id --ipv4-cidr-block 192 .168. $range_3 .0/28 --resource-group-name $resourcegroup Repeat the process for the cloudpak node type: ibmcloud is subnet-create subnet-cloud-paks-1 $vpc_id --ipv4-cidr-block 192 .168. $range_7 .0/27 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-cloud-paks-2 $vpc_id --ipv4-cidr-block 192 .168. $range_8 .0/27 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-cloud-paks-3 $vpc_id --ipv4-cidr-block 192 .168. $range_9 .0/27 --resource-group-name $resourcegroup If you are deploying the architecture topology ROKS-3 or ROKS-4, where the cluster will host OpenShift Storage Container repeat the process for the storage node type: ibmcloud is subnet-create subnet-storage-1 $vpc_id --ipv4-cidr-block 192 .168. $range_4 .0/28 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-storage-2 $vpc_id --ipv4-cidr-block 192 .168. $range_5 .0/28 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-storage-3 $vpc_id --ipv4-cidr-block 192 .168. $range_6 .0/28 --resource-group-name $resourcegroup If you are deploying the architecture topology ROKS-2 or ROKS-4, where the cluster will host applications repeat the process for the worker node type: ibmcloud is subnet-create subnet-workers-1 $vpc_id --ipv4-cidr-block 192 .168. $range_10 .0/27 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-workers-2 $vpc_id --ipv4-cidr-block 192 .168. $range_11 .0/27 --resource-group-name $resourcegroup ibmcloud is subnet-create subnet-worker-3 $vpc_id --ipv4-cidr-block 192 .168. $range_12 .0/27 --resource-group-name $resourcegroup Create the subnet to place the VPN Gateway: ibmcloud is subnet-create subnet-vpn-gw $vpc_id --ipv4-cidr-block 192 .168. $range_13 .0/29 --resource-group-name $resourcegroup Create Public Gateways for outbound connectivity. It is needed to create one per zone. ibmcloud is public-gateway-create gw-subnet-zone-1 $vpc_id $zone_1 ibmcloud is public-gateway-create gw-subnet-zone-2 $vpc_id $zone_2 ibmcloud is public-gateway-create gw-subnet-zone-3 $vpc_id $zone_3 Attach the public gateways on all the subnets that need outbound connectivity to the Internet. Note Run ibmcloud is subnets to get the subnets ID, and ibmcloud is public-gateways to get the public gateway ID: ibmcloud is subnet-update $subnet -infra-1_id --public-gateway-id $gw -subnet-zone-1_id ibmcloud is subnet-update $subnet -infra-2_id --public-gateway-id $gw -subnet-zone-2_id ibmcloud is subnet-update $subnet -infra-3_id --public-gateway-id $gw -subnet-zone-3_id ibmcloud is subnet-update $subnet -cloud-paks-1_id --public-gateway-id $gw -subnet-zone-1_id ibmcloud is subnet-update $subnet -cloud-paks-2_id --public-gateway-id $gw -subnet-zone-2_id ibmcloud is subnet-update $subnet -cloud-paks-3_id --public-gateway-id $gw -subnet-zone-3_id If you are deploying the architecture topology ROKS-3 or ROKS-4, where the cluster will host OpenShift Storage Container repeat the process for the storage node type: ibmcloud is subnet-update $subnet -storage-1_id --public-gateway-id $gw -subnet-zone-1_id ibmcloud is subnet-update $subnet -storage-2_id --public-gateway-id $gw -subnet-zone-2_id ibmcloud is subnet-update $subnet -storage-3_id --public-gateway-id $gw -subnet-zone-3_id If you are deploying the architecture topology ROKS-2 or ROKS-4, where the cluster will host applications repeat the process for the worker node type: ibmcloud is subnet-update $subnet -workers-1_id --public-gateway-id $gw -subnet-zone-1_id ibmcloud is subnet-update $subnet -workers-2_id --public-gateway-id $gw -subnet-zone-2_id ibmcloud is subnet-update $subnet -workers-3_id --public-gateway-id $gw -subnet-zone-3_id","title":"Deploy VPC"},{"location":"infrastructure/roks/#create-a-vpn-tunnel-to-connect-to-the-vpc","text":"The documentation from IBM Cloud explains each parameter needed to establish the IPsec tunnel that will connect the location of our premises with the VPC on IBM Cloud. https://cloud.ibm.com/docs/vpc?topic=vpc-using-vpn Note IBM Cloud offers a set of recommendation parameters for a certain set of devices here: https://cloud.ibm.com/docs/vpc?topic=vpc-vpn-onprem-example Warning VMware is not on the list but this article describes how to get it on VMWare environments: \"https://medium.com/ibm-garage/extending-on-premise-vmware-environment-with-an-ipsec-tunnel-to-consume-securely-ibm-cloud-13cc9326dce1\" The next resources need to be created according to the previous recommendations or the requirements needed to establish the IPsec tunnel. IKE-Policy: The options available for this parameter can be seen running this command ibmcloud is ike-policy-create --help ibmcloud is ike-policy-create $vpc_name $auth_algorithm_ike $dh_group_ike $encryption_ike $ike_version --key-lifetime $key_lifetime_ike --resource-group-name $resourcegroup IPsec-Policy: The options available for this parameter can be seen running this command ibmcloud is ipsec-policy-create --help ibmcloud is ipsec-policy-create $vpc_name -ipsec $auth_algorithm_ipsec $encryption_ipsec $pfs_ipsec --key-lifetime $key_lifetime_ipsec --resource-group-name $resourcegroup Create the VPN-Gateway: $vpn_subnet is the subnet where the VPN gateway will be deployed. It can be a dedicated subnet or one of the infra subnets ibmcloud is vpn-gateway-create $vpc_name $subnet -vpn-gw_ID --mode $policy --resource-group-name $resourcegroup Create an IPsec connection: To check the parameters needed ibmcloud is vpn-gateway-connection-create --help . To get the id of the IKE policies run ibmcloud is ike-policies and to get the IPsec policies ibmcloud is ipsec-policies , to get the vpn gateway ID run ibmcloud is vpn-gateways : ibmcloud is vpn-gateway-connection-create $vpc_name $vpn_id $peer_address $pre_shared_key --ike-policy $ike_id --ipsec-policy $ipsec_id If the policy selected was route based when the VPN Gateway was created, a route table is needed to be created: Note As an example here is described how to connect to a VMware environment as is described in the article shown above ibmcloud is vpc-routing-table-create $vpc_id --vpc-zone-ingress false --name $vpc_name Attach all the subnets to the route table, to get the ID of the route table run ibmcloud is vpc-routing-tables : ibmcloud is subnet-update $subnet -infra-1_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -infra-2_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -infra-3_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -storage-1_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -storage-2_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -storage-3_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -cloud-paks-1_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -cloud-paks-2_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -cloud-paks-3_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -workers-1_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -workers-2_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -workers-3_ID --routing-table-id $route_table_ID ibmcloud is subnet-update $subnet -vpn-gw_ID --routing-table-id $route_table_ID * To add the routes: Each subnet needs a route to the remote network address range. $network_remote is the network address range of the remote network from which ROKS will be accessed. To get the connection ID run ibmcloud is vpn-gateway-connections $vpn-gw_ID For example for infra subnets: ibmcloud is vpc-routing-table-route-create $vpc_id $route_table_id --name subnet-infra-1 --zone $zone_1 --destination $network_remote --action deliver --next-hop $connection_id ibmcloud is vpc-routing-table-route-create $vpc_id $route_table_id --name subnet-infra-2 --zone $zone_2 --destination $network_remote --action deliver --next-hop $connection_id ibmcloud is vpc-routing-table-route-create $vpc_id $route_table_id --name subnet-infra-3 --zone $zone_3 --destination $network_remote --action deliver --next-hop $connection_id","title":"Create a VPN  tunnel to connect to the VPC"},{"location":"infrastructure/roks/#deploy-roks","text":"Create a Cloud Object Storage instance to host the images of the registry among others: ibmcloud resource service-instance-create $cos_name cloud-object-storage standard global -g $resourcegroup Create the ROKS cluster: There is a CLI limitation that does not allow the creation of several zones deployment for nodes from the start, it needs to be added after, either change the default worker pool from default , the default worker pool is deleted right after the creation of the cluster. The public endpoint is disabled for security, public outbound connectivity is enabled through the public gateways though. Note To get the flavors available on the zone/region ibmcloud oc flavors --zone $zone_1 . Use minimum flavor as the default pool will be deleted right after the cluster is created. To get the Openshift versions available ibmcloud oc versions . The recommended version at the moment that this guide was written is 4.7.19_openshift . To get the CRN (ID of the storage instance) of the Object Storage instance, run ibmcloud resource service-instance $cos_name . ibmcloud oc cluster create vpc-gen2 --name $cluster_name --vpc-id $vpc_id --workers 2 --version $version --flavor $flavor --zone $zone_1 --subnet-id $subnet -infra-1_id --cos-instance $cos_id --disable-public-service-endpoint --entitlement cloud_pak To create the Foundational services worker pool: Note The flavor recommended to deploy a minimum version of Foundational Services in the Infra nodes is bx2.8x32 To get the ROKS cluster ID run ibmcloud oc clusters ibmcloud oc worker-pool create vpc-gen2 --name foundational-services --cluster $roks_id --flavor $flavor_infra --size-per-zone 1 --vpc-id $vpc_id --entitlement cloud_pak --label node-role.kubernetes.io/foundational-services = true ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -infra-1_id --cluster $roks_id --zone $zone_1 --worker-pool foundational-services ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -infra-2_id --cluster $roks_id --zone $zone_2 --worker-pool foundational-services ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -infra-3_id --cluster $roks_id --zone $zone_3 --worker-pool foundational-services Note If you are deploying topologies ROKS-1 or ROKS-3. Once there is another worker pool, we can delete the default worker-pool ibmcloud oc worker-pool rm --worker-pool default --cluster $roks_id To create the Cloud Pak worker pool: If you are bringing your own license use the flag cloud_pak for the entitlement, in the case you want to consume the entitlement directly from IBM Cloud remove the flag. Note The flavor for these nodes will depend on the cloudpak that will be installed on them. ibmcloud oc worker-pool create vpc-gen2 --name cloud-paks --cluster $roks_id --flavor $flavor_cloud -paks --size-per-zone 1 --vpc-id $vpc_id --entitlement cloud_pak --label node-role.kubernetes.io/cloud-paks = true Extending the worker pool across the three zones: ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -cloud-paks-1_id --cluster $roks_id --zone $zone_1 --worker-pool cloud-paks ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -cloud-paks-2_id --cluster $roks_id --zone $zone_2 --worker-pool cloud-paks ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -cloud-paks-3_id --cluster $roks_id --zone $zone_3 --worker-pool cloud-paks * To create the Storage worker pool: If you are deploying topologies ROKS-3 or ROKS-4, you need to create the Storage worker pool: Note The recommended storage class to run Openshift Data Foundation (previously Openshift Container Storage) is bx2.16x64 If you are bringing your own license use the flag cloud_pak for the entitlement, in the case you want to consume the entitlement directly from IBM Cloud remove the flag. ibmcloud oc worker-pool create vpc-gen2 --name storage --cluster $roks_id --flavor $flavor_storage --size-per-zone 1 --vpc-id $vpc_id --entitlement cloud_pak --label node-role.kubernetes.io/storage = true It is needed to apply the Openshift Data Foundation addon from IBM Cloud to the cluster, that will deploy the specific CRD for Openshift Data Foundation from IBM Cloud: ibmcloud oc cluster addon enable openshift-container-storage --cluster $roks_id --version 4 .7.0 Extending the worker pool across the three zones: ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -storage-1_id --cluster $roks_id --zone $zone_1 --worker-pool storage ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -storage-2_id --cluster $roks_id --zone $zone_2 --worker-pool storage ibmcloud oc zone add vpc-gen2 --subnet-id $subnet -storage-3_id --cluster $roks_id --zone $zone_3 --worker-pool storage * To create the Storage worker pool: If you are deploying topologies ROKS-2 or ROKS-4, you need to create the Workers worker pool: The default worker pool can be used to host applications that don`t fit with the infra workloads, storage workloads, or Cloud Pak workloads. To label the worker pool run: Adjust roles and labels The default roles assigned by IBM Cloud need to be changed, to remove them: Retrieve the admin credentials for the cluster: ibmcloud oc cluster config --cluster $roks_name --admin Note Run oc get nodes to check you are logged in the cluster. Remove the master and worker role labels from nodes, to apply the correct labels to each worker-pool: oc get nodes | awk '{print $1}' | awk '{if(NR>1)print}' | xargs -I {} bash -c 'oc label node {} node-role.kubernetes.io/master-' oc get nodes | awk '{print $1}' | awk '{if(NR>1)print}' | xargs -I {} bash -c 'oc label node {} node-role.kubernetes.io/worker-' Apply the label to the foundational services nodes (if not added when the worker pool was created): ```bash ibmcloud oc worker-pool label set --cluster $roks_name --worker-pool foundational-services --label node-role.kubernetes.io/foundational-services=true -f ibmcloud oc worker-pool label set --cluster $roks_name --worker-pool foundational-services --label node-role.kubernetes.io/master=\"\" -f Apply the label to the cloud-paks nodes (if not added when the worker pool was created): ```bash ibmcloud oc worker-pool label set --cluster $roks_name --worker-pool cloud-paks --label node-role.kubernetes.io/cloud-paks=true -f Apply the label to the worker nodes: ibmcloud oc worker-pool label set --cluster $roks_name --worker-pool default --label node-role.kubernetes.io/worker = true -f Apply the label to the Storage nodes: ibmcloud oc worker-pool label set --cluster $roks_name --worker-pool default --label cluster.ocs.openshift.io/openshift-storage = true -f ibmcloud oc worker-pool label set --cluster $roks_name --worker-pool default --label node-role.kubernetes.io/storage = true -f","title":"Deploy ROKS"},{"location":"infrastructure/roks/#deploy-gitops","text":"Log into the ROKS cluster: ibmcloud oc cluster config --cluster $roks_name --admin Installing GitOps operator: Fork and clone the standard repo GitOps for IBM Cloud: git clone https://github.com/chechuironman/roks-splits.git Install the GitOps operator oc apply -f setup/ocp47/openshift-gitops-operator.yaml Add the ClusterRoles needed by ArgoCD (GitOps) to Bootstrap the cluster oc apply -f setup/ocp47/custom-argocd-app-controller-clusterrole.yaml oc apply -f setup/ocp47/custom-argocd-app-controller-clusterrolebinding.yaml Apply the 3-layer GitOps structure Wait until the GitOps operator is successfully deployed before deploy the Bootstrap ArgoCD application. Modify the bootstrap.yaml on the root folder file to point to the newly forked repo: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-roks-split-X namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/argocd/$YOUR_ROKS_FLAVOR_TOPOLOGY repoURL : $YOUR_REPO_URL targetRevision : master syncPolicy : automated : prune : true selfHeal : true Apply the taints to the nodes to avoid workloads not related to that node role to be placed in: Note Before applying taints you should confirm all pods are running. For the cloud-paks nodes. ibmcloud oc worker-pool taint set --cluster $roks_name --worker-pool cloud-paks --taint node-role.kubernetes.io/cloud-paks = true:NoExecute -f For the Foundational Services nodes. ibmcloud oc worker-pool taint set --cluster $roks_name --worker-pool foundational-services --taint node-role.kubernetes.io/foundational-services = true:NoExecute -f If you are deploying topologies ROKS-3 or ROKS-4, you need to apply the Storage nodes taints: ibmcloud oc worker-pool taint set --cluster $roks_name --worker-pool storage --taint node-role.kubernetes.io/storage = true:NoExecute -f Depending on the cluster topology to deploy choose between the next bootstrap ArgoCD apps: - 0-bootstrap/argocd/roks-split-1/bootstrap.yaml - 0-bootstrap/argocd/roks-split-2/bootstrap.yaml - 0-bootstrap/argocd/roks-split-3/bootstrap.yaml - 0-bootstrap/argocd/roks-split-4/bootstrap.yaml To grab the ArgoCD credentials and route to access to the dashboard run: password: passwd_argocd = $( oc get secret openshift-gitops-cluster -n openshift-gitops -o json | jq -r .data | jq -r . [ 'admin.password' ] | tail -1 | base64 -d ) route: argo_route = $( oc get route openshift-gitops-server -n openshift-gitops -o json | jq -r '.spec.host' ) If using Github Enterprise it is needed to authenticate against the repo. Follow the next steps: argocd login ${ argo_route } --username admin --password ${ passwd_argocd } --insecure Add the following repos to the argoCD instance, $github_password is a token created in Github Enterprise with permissions to read the repo: ```bash argocd repo add https://github.com/chechuironman/roks-splits --type git --name roks-splits argocd repo add https://github.com/chechuironman/roks-infra --type git --name roks-infra argocd repo add https://github.com/chechuironman/roks-services --type git --name roks-services To deploy the bootstrap app that will roll out the cluster components installation run: ```bash oc apply -f 0-bootstrap/argocd/roks-split-$X/bootstrap.yaml -n openshift-gitops","title":"Deploy GitOps"},{"location":"infrastructure/roks/#deploy-observability","text":"Log into the ROKS cluster: ibmcloud oc cluster config --cluster $roks_name --admin","title":"Deploy Observability"},{"location":"infrastructure/roks/#monitoring","text":"Create the monitoring instance to use: Note It is possible to use an already provisioned instance Note You can find the service tiers for monitoring here . You can find the locations here ibmcloud resource service-instance-create $monitoring_instance_name sysdig-monitor $service_tier $location -p '{\"default_receiver\": false,\"external_api_auth\": \"API_AUTH\"}' Once the instance is created, we need to create the key to access the instance from Openshift with \"Writer\" access: ibmcloud resource service-key-create $monitoring_instance_access_key_name Writer --instance-name $monitoring_instance_name The output of the key creation should give us two parameters we need to monitor the Openshift cluster with this Sysdig instance: Sysdig Access Key and Sysdig Collector Endpoint. Note If you missed these parameters you can run the script get_monitoring_parameters.sh from teh monkey-army folder ```bash ./monkey-army/get_observability_parameters.sh $monitoring_instance_access_key_name Deploy the Sysdig monitoring agents on the Openshift cluster Note The Sysdig pods need priviledge to access the metrics of the node. Run the following command to add the privileged 'security context constraints' oc adm policy add-scc-to-user privileged -n ibm-observe -z sysdig-agent curl -sL https://raw.githubusercontent.com/draios/sysdig-cloud-scripts/master/agent_deploy/IBMCloud-Kubernetes-Service/install-agent-k8s.sh | bash -s -- -a $Sysdig_Access_Key -c $Sysdig_Collector_Endpoint -t TAG_DATA -ac 'sysdig_capture_enabled: false' Wait until all the pods are deployed on the 'ibm-observe' namespace: oc get pods -n ibm-observe --watch Access to the Monitoring Dashboard through the IBM Cloud UI or the url you can obtain with this command: ibmcloud resource service-instance $monitoring_instance_name --output JSON | jq -r . [ 0 ] .dashboard_url More information about monitoring in IBM Cloud Openshift clusters","title":"Monitoring:"},{"location":"infrastructure/roks/#logging","text":"Create the Logging instance to use: Note It is possible to use an already provisioned instance Note The service plan availables are: 'lite', '7-day', '14-day', '30-day'. You can find the regions available to deploy Logging here ibmcloud resource service-instance-create $logging_instance_name logdna $logging_plan $location Once the instance is created, we need to create the key to access the instance from Openshift with \"Manager\" access: ibmcloud resource service-key-create $logging_instance_access_key_name Manager --instance-name $logging_instance_name The output of the key creation should give us one parameter we need to monitor the Openshift cluster with this LogDNA instance: ingestion_key. Note If you missed this parameter you can run the script get_logging_parameter.sh from the monkey-army folder ./monkey-army/get_observability_parameters.sh $logging_instance_access_key_name Create the LogDNA secret with the logging ingestion key to send the logs to the LogDNA instance: oc create secret generic logdna-agent-key --from-literal = logdna-agent-key = $log_dna_ingestion_key -n ibm-observe * Deploy the LogDNA agents on the Openshift cluster. Note The LogDNA pods need hostmount-anyuid permissions. Run the following command to add the hostmount-anyuid 'security context constraints' oc adm policy add-scc-to-user hostmount-anyuid -n ibm-observe -z default Download the daemon set definition: wget https://assets.us-south.logging.cloud.ibm.com/clients/logdna-agent-ds.yaml Edit the yaml definition and change the following values to point to the right region where the logging instance is deployed, by default is pointing to us-south: - 'LDAPIHOST' - 'LDLOGHOST' Aply the new values: oc apply -f logdna-agent-ds.yaml -n ibm-observe Wait until all the pods are deployed on the 'ibm-observe' namespace: oc get pods -n ibm-observe --watch Access to the Logging Dashboard through the IBM Cloud UI or the url you can obtain with this command: ibmcloud resource service-instance $logging_instance_name --output JSON | jq -r . [ 0 ] .dashboard_url More information about monitoring in IBM Cloud Openshift clusters","title":"Logging"},{"location":"infrastructure/roks/#establish-security-perimeter","text":"To establish a network security perimeter in the VPC, we will use Security groups. The security perimeter is split into two security groups: workers and load balancer. Note The deployment of Openshift Data Foundation triggers the deployment of two public load balancers that are not needed, this is because Red Hat does not support the Openshift Data Foundation install in an air-gapped environment, but as we deployed public gateways for outbound connectivity, we can remove it with the following commands: List the load balancers: ibmcloud is load-balancers Delete the public load balancers: ibmcloud is load-balancer-pools $public_lb_id | awk 'NR > 2 {print $1}' | xargs ibmcloud is load-balancer-delete $public_lb_id -f Workers Security Groups This security group is automatically created when the cluster is provisioned with the name \"kube-\"$roks_id. This security group is used to set the rules for the workers, as they are not visible in the IBM Cloud account. There are three inbound rules and one outbound rule. We just need to keep the rules that connect with the masters: (INBOUND) All protocols from 172.17.0.0/18 to All ports -> This is used to connect from masters to workers. (INBOUND) TCP from All to 30000-32767 ports - This is used to connect to services exposed via nodeports. (INBOUND) UDP from All to 30000-32767 ports - This is used to connect to services exposed via nodeports. (OUTBOUND) All protocols to 172.17.0.0/18 to All ports -> This is used to connect from workers to masters. We should remove rules 2 and 3 as the Load Balancer will connect to the nodeports via the routes created on Openshift. The commands to remove the rules are: ibmcloud is security-groups Copy the security group with the name $kube-roks_id, and use that id to get the rules on that security group: ibmcloud is security-group-rules $sg_id Delete the inbound rules pointing to the nodeports, the rules 2 and 3 from the previous list on this doc: ibmcloud is security-group-rule-delete $sg_id $rule_to_delete_id -f Load Balancer Security Group We assume the apps will be exposed through routes via the load balancer automatically provisioned by the default router or provisioned with another app service. Load Balancer for the default Openshift router Note 192.168.250.0/24 is the CIDR of our on-premise network in the diagram. 192.168.0.0/22 is the range of our networks hosting the infra nodes with the router pods where we want to send the traffic. There are three inbound rules and one outbound rule: (INBOUND) TCP protocol from $on_premise_network to 443 port -> This is used to expose the https routes (Openshift dashboard or other apps exposed through this LB) (INBOUND) TCP protocol from $on_premise_network to 80 port -> This is used to expose the http routes (INBOUND) ICMP from All - This is used from the IBM Cloud platform DNS to check the LB is healthy and send traffic (OUTBOUND) TCP protocol to $infra_nodes to Nodeports (30000:32767) -> This is used to connect to the services exposed on the nodeports. (OUTBOUND) UDP protocol to $infra_nodes to Nodeports (30000:32767) -> This is used to connect to the services exposed on the nodeports. Note It can be more granular and just create the outbound rule to the specific nodeports where the apps are published, app service by app service. We need to create a new security group for the load balancer: ibmcloud is security-group-create $LB -default-route-name $vpc_id With the id from the output from the previous command ($security_group_id) we add the rules as they are shown in the picture: ibmcloud is security-group-rule-add $security_group_id inbound tcp --port-min 443 --port-max 443 --remote $on_premise_network ibmcloud is security-group-rule-add $security_group_id inbound tcp --port-min 80 --port-max 80 --remote $on_premise_network ibmcloud is security-group-rule-add $security_group_id inbound icmp ibmcloud is security-group-rule-add $security_group_id outbound tcp --port-min 30000 --port-max 32767 --remote 192 .168.0.0/22 ibmcloud is security-group-rule-add $security_group_id outbound udp --port-min 30000 --port-max 32767 --remote 192 .168.0.0/22","title":"Establish Security Perimeter"},{"location":"infrastructure/rosa/","text":"Amazon ROSA \u00b6 Overview \u00b6 This repository document experiences on working with ROSA cluster on OpenShift 4.6 environment 1Q 2021. For additional information, refer to ROSA documentation . To work with ROSA, you need the following: Enable ROSA in AWS Login to the AWS console , and search for ROSA service. Click Enable Openshift button in the ROSA services page. Download the CLI Rosa CLI can be downloaded from the ROSA page, once it is enabled. Click on Download CLI button or go to ROSA CLI page . Create a ROSA secret, it is retrieved from https://cloud.redhat.com/openshift/token/rosa Instructions \u00b6 Setup cluster \u00b6 ROSA cluster work is performed through the rosa cli. The CLI is typically supplemented by the oc command line. Login to ROSA: rosa login --token=<token> Once you login to ROSA, you can create a cluster, the following list summarizes the common options for creating a ROSA cluster: cluster-name: name of the cluster - this name is used for subsequent commands for manipulating the cluster. version: openshift version multi-az: create machinesets in all zones within a region or only on the first zone region: target region for deployment channel-group: openshift upgrade channel - typically use stable compute-machine-type, compute-nodes: machine types and number of compute nodes for this type. enable-autoscaling, min-replicas, max-replicas: the autoscaling properties of the cluster, max and min replicas only accepted when autoscaling is enabled machine-cidr, service-cidr, pod-cidr, host-prefix: network specifications for the cluster, see network private: whether the endpoints are exposed to the public network or not subnet-ids: pairs of subnet ids per az (one private, one public - attached to internet gateway) The following command creates a ROSA cluster: export AWS_ACCESS_KEY_ID=\"AKIAAAAAAAAAAAAAAAAAAA\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_DEFAULT_REGION=\"us-east-2\" rosa create cluster --cluster-name rosa-test-001 --version 4.6.27 --multi-az --region us-east-2 --compute-machine-type m5.xlarge --compute-nodes 3 The cluster status will be pending then installing and later ready . Once you have the cluster ready (it may take up to an hour) you can check it using the following command: rosa describe cluster --cluster=test-rosa-001 You can also go to the URL in the cluster description to get the progress log. With the ROSA cluster is ready, you can add the following features: admin: adding cluster-admin user idp: authentication method can be added, such as from github or ldap or other machinepool: adding a machineset or machinepool with different worker specifications addons: (codeready container, cluster logging addon etc) Authentication \u00b6 ROSA provides 2 methods for authentication: - A cluster-admin user rosa create admin --cluster <clustername> ![rosa admin](images/rosa/105-rosaadmin.png) Identity provider integration, the following example is for github integration: rosa create idp --cluster <clustername> --type github --name <name> --organization <github-org> The integration with github as the example show, requires a github token, which can be retrieved using the URL listed when the command is executed. when you click Register application , you are presented with the GitHub client Id and you can generate GitHub secret (which must be copied to the idp command). Once you have created an admin user and idp, your OpenShift login Web page shows those methods: OpenShift Container Storage \u00b6 OCS in ROSA can be installed from the Web UI, there are some validations in a managed OpenShift that restrict OCS installation. Create the machine pool for the storage nodes: rosa create machinepool --cluster rosa-test-001 --name storage-nodes --instance-type m5.4xlarge --replicas 3 \\ --taints 'node.ocs.openshift.io/storage=true:NoSchedule' \\ --labels 'node-role.kubernetes.io/storage=,cluster.ocs.openshift.io/openshift-storage=' Wait until the nodes becomes ready (notice that it creates one machinesets per zones and one machine per machineset). Go to the Web UI and login as cluster-admin , select Operators > Operator Hub and search for OCS . Click on the OpenShift Container Storage tile and you are presented with the Operator detail screen and click Install . In the OCS installation page, click Install again. After a couple of minutes, the Status is installed and proceed to the installed operator page. Click on the OpenShift Container Storage and select the Storage Cluster tab and click Create Storage Cluster . Specify the necessary argument (the nodes would have been preselected) and click Create . Once the storage cluster is available, your cluster is ready for use.","title":"Amazon ROSA"},{"location":"infrastructure/rosa/#amazon-rosa","text":"","title":"Amazon ROSA"},{"location":"infrastructure/rosa/#overview","text":"This repository document experiences on working with ROSA cluster on OpenShift 4.6 environment 1Q 2021. For additional information, refer to ROSA documentation . To work with ROSA, you need the following: Enable ROSA in AWS Login to the AWS console , and search for ROSA service. Click Enable Openshift button in the ROSA services page. Download the CLI Rosa CLI can be downloaded from the ROSA page, once it is enabled. Click on Download CLI button or go to ROSA CLI page . Create a ROSA secret, it is retrieved from https://cloud.redhat.com/openshift/token/rosa","title":"Overview"},{"location":"infrastructure/rosa/#instructions","text":"","title":"Instructions"},{"location":"infrastructure/rosa/#setup-cluster","text":"ROSA cluster work is performed through the rosa cli. The CLI is typically supplemented by the oc command line. Login to ROSA: rosa login --token=<token> Once you login to ROSA, you can create a cluster, the following list summarizes the common options for creating a ROSA cluster: cluster-name: name of the cluster - this name is used for subsequent commands for manipulating the cluster. version: openshift version multi-az: create machinesets in all zones within a region or only on the first zone region: target region for deployment channel-group: openshift upgrade channel - typically use stable compute-machine-type, compute-nodes: machine types and number of compute nodes for this type. enable-autoscaling, min-replicas, max-replicas: the autoscaling properties of the cluster, max and min replicas only accepted when autoscaling is enabled machine-cidr, service-cidr, pod-cidr, host-prefix: network specifications for the cluster, see network private: whether the endpoints are exposed to the public network or not subnet-ids: pairs of subnet ids per az (one private, one public - attached to internet gateway) The following command creates a ROSA cluster: export AWS_ACCESS_KEY_ID=\"AKIAAAAAAAAAAAAAAAAAAA\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_DEFAULT_REGION=\"us-east-2\" rosa create cluster --cluster-name rosa-test-001 --version 4.6.27 --multi-az --region us-east-2 --compute-machine-type m5.xlarge --compute-nodes 3 The cluster status will be pending then installing and later ready . Once you have the cluster ready (it may take up to an hour) you can check it using the following command: rosa describe cluster --cluster=test-rosa-001 You can also go to the URL in the cluster description to get the progress log. With the ROSA cluster is ready, you can add the following features: admin: adding cluster-admin user idp: authentication method can be added, such as from github or ldap or other machinepool: adding a machineset or machinepool with different worker specifications addons: (codeready container, cluster logging addon etc)","title":"Setup cluster"},{"location":"infrastructure/rosa/#authentication","text":"ROSA provides 2 methods for authentication: - A cluster-admin user rosa create admin --cluster <clustername> ![rosa admin](images/rosa/105-rosaadmin.png) Identity provider integration, the following example is for github integration: rosa create idp --cluster <clustername> --type github --name <name> --organization <github-org> The integration with github as the example show, requires a github token, which can be retrieved using the URL listed when the command is executed. when you click Register application , you are presented with the GitHub client Id and you can generate GitHub secret (which must be copied to the idp command). Once you have created an admin user and idp, your OpenShift login Web page shows those methods:","title":"Authentication"},{"location":"infrastructure/rosa/#openshift-container-storage","text":"OCS in ROSA can be installed from the Web UI, there are some validations in a managed OpenShift that restrict OCS installation. Create the machine pool for the storage nodes: rosa create machinepool --cluster rosa-test-001 --name storage-nodes --instance-type m5.4xlarge --replicas 3 \\ --taints 'node.ocs.openshift.io/storage=true:NoSchedule' \\ --labels 'node-role.kubernetes.io/storage=,cluster.ocs.openshift.io/openshift-storage=' Wait until the nodes becomes ready (notice that it creates one machinesets per zones and one machine per machineset). Go to the Web UI and login as cluster-admin , select Operators > Operator Hub and search for OCS . Click on the OpenShift Container Storage tile and you are presented with the Operator detail screen and click Install . In the OCS installation page, click Install again. After a couple of minutes, the Status is installed and proceed to the installed operator page. Click on the OpenShift Container Storage and select the Storage Cluster tab and click Create Storage Cluster . Specify the necessary argument (the nodes would have been preselected) and click Create . Once the storage cluster is available, your cluster is ready for use.","title":"OpenShift Container Storage"},{"location":"infrastructure/storage/","text":"Storage \u00b6 Coming soon...","title":"Storage"},{"location":"infrastructure/storage/#storage","text":"Coming soon...","title":"Storage"},{"location":"infrastructure/vmware/","text":"VMWare \u00b6 Coming soon...","title":"VMWare"},{"location":"infrastructure/vmware/#vmware","text":"Coming soon...","title":"VMWare"},{"location":"infrastructure/ad/","text":"Architecture decisions registry \u00b6 Decision Index \u00b6 Subject Area ID Topic Description Infra AD02 Cluster Design Considerations to segregate environments and applications using clusters and/or namespaces Topology AD03 Cluster Provisioning Use of OpenShift IPI deployments to automate cluster provisioning Topology AD04 MachineSets Use of MachineSets for compute node provisioning Topology AD05 Storage Use of OpenShift Data Foundation as a common storage provider","title":"Architectural Decisions"},{"location":"infrastructure/ad/#architecture-decisions-registry","text":"","title":"Architecture decisions registry"},{"location":"infrastructure/ad/#decision-index","text":"Subject Area ID Topic Description Infra AD02 Cluster Design Considerations to segregate environments and applications using clusters and/or namespaces Topology AD03 Cluster Provisioning Use of OpenShift IPI deployments to automate cluster provisioning Topology AD04 MachineSets Use of MachineSets for compute node provisioning Topology AD05 Storage Use of OpenShift Data Foundation as a common storage provider","title":"Decision Index"},{"location":"infrastructure/ad/ad02-clusters-design/","text":"AD02 - Clusters design \u00b6 < Back to architecture decision registry Clusters design \u00b6 Subject Area: Infra Issue or Problem Statement \u00b6 This architecture decision focuses on the boundaries of production and non-production environments. In traditional data centers, resources such as compute, storage, and network are partitioned into different hardware or zone to minimize the chance that workloads impact each other. This approach has also been useful to enforce segregation of duties between various stakeholders. In the context of OpenShift, partitioning can happen at the level of clusters or projects and namespace. A project can be assigned a quota on resources allocated and furthermore, the RBAC model can ensure that only a specific set of users can access resources contained within the project. For an on-premise data center, an OpenShift cluster offers a great opportunity to increase workload density. Workload density means running more workloads on a same set of compute. This is achieved thanks to the workload scheduler which will allocate workloads where capacity exists and where it meets the requirement stated by the deployment descriptor of your application. By avoiding the proliferation of master nodes (via discrete clusters), more nodes can be dedicated to run business workloads, helping reduce operational costs and improve the density. However, from a cloud provider perspective, this argument is not always true. Since some cloud provider do not charge for their master nodes, the cost and density arguments goes away. Finally, in planning the cluster and project strategy, there is the considerations of cluster level operations like OpenShift upgrades. Consolidating workloads on a single cluster, implicitly means that cluster upgrades become a riskier operation. In that context, what is the better option when it comes to managing and partitioning Dev, QA, Staging, Production environment? Givens & Assumptions \u00b6 Given The decision focusses only on the cloud-native application. Given Organization may decide to specialize clusters for a specific function (ie. Power System with GPU for AI). The considerations for this approach falls outside the scope of this decision. Given Segregation of duty is achieved in a similar fashion no matter which option is selected: User are assigned roles that allows them to access a subset of namespaces within the cluster. Assumption Additional sandbox clusters can be temporarily provision to assess new OpenShift versions. Motivation \u00b6 This architecture decision is critical as it can have a significant impact on costs, risks, operational complexity. It also has direct implication on the deployment process. Alternatives \u00b6 NOTE 1: The alternatives listed below could be combined together. For example, it is possible to combine option 3 (pre-prod/prod) with option 4 (cluster per app) to achieve a model with a pre-prod and a production cluster per application. Similar considerations would apply to the presented alternatives, so they are not included here. NOTE 2: In the diagrams across the various alternatives, the \"TOOLS\" refer to the Cloud Native Toolkit tools. 1. Single cluster for all environments \u00b6 Because all environments and applications are hosted on the same cluster, we rely on namespace naming convention to include a prefix or suffix that indicates the type of environment they host. Each application and services may have more then one namespace, but they should all have the prefix/suffix attached to the namespace. The implication of this standardization is that any reference from a component in one namespace to a different namespace will need to be configurable. This option leads to higher workload density since all applications and environment are hosted on the same cluster, however you should be mindful of the tested capacity limit . From a cluster upgrade perspective, it does require temporary sandbox to test new versions and organization adopting this approach need to take a \"fix-forward\" approach to their cluster management, since once the patch is applied, it can be impossible to return to previous state. NOTE: While this option is included here from a completeness perspective. We do not advocate such a strategy given the inherent risks. Considerations : + Great for workload density. + Simplify the cluster management day-to-day tasks. - Significant operational risks during cluster maintenance. - Cluster outages takes out all environment and applications. - Production workload segregation needs to be carefully planned. - Cluster sizing must take into account tested limits. 2. Single cluster for non-production environments, a separate one for production \u00b6 In this option, production workloads are deployed in a cluster dedicated to this environment. Pre-production workloads share a single cluster and environments are segregated by projects/namespaces. This option is in effect an extension of the first one but with the added production workload being segregated. As such, the namespace standardization still applies. From a cluster upgrade perspective, the infrastructure team may initially test a subset of workloads in a sandbox like environment and once ready have a single environment (the non-production cluster) to perform the final testing, assess the stability of the new version before proceeding with the production cluster. Considerations : + Good for workload density. + Provide ability to completely segregate production resources from other environments. + Keeps the cluster life-cycle management overhead to a minimum. + Minimizes the amount of resources allocated to control planes. - Requires that lookup between namespace be configurable. - Only a single environment to assess the impact of a new OpenShift version on existing workloads. 3. One cluster per environment \u00b6 As the name says, each environment are supported by a separate cluster. Within each environment, application teams deploy their application and services within the namespaces dictated by their solution design. Important to emphasize that the cluster represents the environment and that it can contain multiple independent applications and services. For a cluster upgrade perspective, each environment needs to be upgraded separately. As upgrade progress, the staging environment provides the replica of production, ensuring that until production is ready to be upgraded, at least a single non-production environment remains available for testing. The implication of that option is the associated additional resources that are dedicated to the control plane of each clusters and the associated operational costs of maintaining multiple clusters across the landscape. Considerations : + Highest degree of segregation between environments. + Provides most flexibility when it comes to cluster upgrades. - Increases costs associated with additional control plane resources. - Increases operations complexity. - Reduces workload density, since environments are not sharing the same resource pool. 4. Cluster per application \u00b6 By separating each cluster by application (or application domain), it enables organization to manage each cluster as an independent unit and cost and operational resources can be measured at the cluster level. Namespace standardization is again required to ensure that the application environments can be properly segregated. From a cluster upgrade perspective, the application faces similar challenges as the first option, meaning that sandbox cluster must be created to assess new version impacts. Considerations : + Highest degree of segregation between applications. + Simplest model to allocate costs to an application. + In situation of outages, blast radius tends to be simpler to identify. - Leads to proliferation of clusters. - Does not favour resource sharing; Many clusters running with low resource usage. - Risk of application clusters falling behind the upgrade life-cycle. - Higher risks of divergence across clusters, leading to operational and security risks. 5. Management and Resources plane clusters \u00b6 This approach separates concerns between a management and a resource plane. We define the two as being: Management plane: run key processes responsible for the applications life-cycle. These includes the CI and CD components. It can also run components like Red Hat Advanced Cluster Management to ensure consistency of configuration and operations. Resource plane: This is the set of clusters that are running the application and services components. This enables organization to track and automate the cluster upgrades via the management plane. Considerations : + Provides opportunity to federate all clusters under one view. + Management plane can drive automation of key activities. + Enforces consistency across clusters by leveraging a single source for all environments. - Requires additional resources to run the management plane. Justification \u00b6 N/A Implications \u00b6 N/A Derived requirements N/A Related decisions N/A","title":"AD02 - Clusters design"},{"location":"infrastructure/ad/ad02-clusters-design/#ad02-clusters-design","text":"< Back to architecture decision registry","title":"AD02 - Clusters design"},{"location":"infrastructure/ad/ad02-clusters-design/#clusters-design","text":"Subject Area: Infra","title":"Clusters design"},{"location":"infrastructure/ad/ad02-clusters-design/#issue-or-problem-statement","text":"This architecture decision focuses on the boundaries of production and non-production environments. In traditional data centers, resources such as compute, storage, and network are partitioned into different hardware or zone to minimize the chance that workloads impact each other. This approach has also been useful to enforce segregation of duties between various stakeholders. In the context of OpenShift, partitioning can happen at the level of clusters or projects and namespace. A project can be assigned a quota on resources allocated and furthermore, the RBAC model can ensure that only a specific set of users can access resources contained within the project. For an on-premise data center, an OpenShift cluster offers a great opportunity to increase workload density. Workload density means running more workloads on a same set of compute. This is achieved thanks to the workload scheduler which will allocate workloads where capacity exists and where it meets the requirement stated by the deployment descriptor of your application. By avoiding the proliferation of master nodes (via discrete clusters), more nodes can be dedicated to run business workloads, helping reduce operational costs and improve the density. However, from a cloud provider perspective, this argument is not always true. Since some cloud provider do not charge for their master nodes, the cost and density arguments goes away. Finally, in planning the cluster and project strategy, there is the considerations of cluster level operations like OpenShift upgrades. Consolidating workloads on a single cluster, implicitly means that cluster upgrades become a riskier operation. In that context, what is the better option when it comes to managing and partitioning Dev, QA, Staging, Production environment?","title":"Issue or Problem Statement"},{"location":"infrastructure/ad/ad02-clusters-design/#givens-assumptions","text":"Given The decision focusses only on the cloud-native application. Given Organization may decide to specialize clusters for a specific function (ie. Power System with GPU for AI). The considerations for this approach falls outside the scope of this decision. Given Segregation of duty is achieved in a similar fashion no matter which option is selected: User are assigned roles that allows them to access a subset of namespaces within the cluster. Assumption Additional sandbox clusters can be temporarily provision to assess new OpenShift versions.","title":"Givens &amp; Assumptions"},{"location":"infrastructure/ad/ad02-clusters-design/#motivation","text":"This architecture decision is critical as it can have a significant impact on costs, risks, operational complexity. It also has direct implication on the deployment process.","title":"Motivation"},{"location":"infrastructure/ad/ad02-clusters-design/#alternatives","text":"NOTE 1: The alternatives listed below could be combined together. For example, it is possible to combine option 3 (pre-prod/prod) with option 4 (cluster per app) to achieve a model with a pre-prod and a production cluster per application. Similar considerations would apply to the presented alternatives, so they are not included here. NOTE 2: In the diagrams across the various alternatives, the \"TOOLS\" refer to the Cloud Native Toolkit tools.","title":"Alternatives"},{"location":"infrastructure/ad/ad02-clusters-design/#justification","text":"N/A","title":"Justification"},{"location":"infrastructure/ad/ad02-clusters-design/#implications","text":"N/A Derived requirements N/A Related decisions N/A","title":"Implications"},{"location":"infrastructure/ad/ad03-cluster-provisioning/","text":"AD03 - Cluster Provisioning \u00b6 < Back to architecture decision registry Cluster Provisioning \u00b6 Subject Area: Topology Issue or Problem Statement \u00b6 The recommended approach to deploying OpenShift Container Platform is using the Installer Provisioned Infrastructure (IPI) method. Givens & Assumptions \u00b6 Given Production clusters will implement best practices to deploy in a highly available, resilient infrastructure Given Cluster needs to be easily reproducible Motivation \u00b6 ... Alternatives \u00b6 1. Deploy using terraform \u00b6 If the openshift-installer doesn't meet customer's constraints, then the following implementations are available to deploy OpenShift in different cloud providers. These terraform implementations are meant to be used as a base and modified by the OpenShift deployer to meet requirements. Some modification is expected. Azure: Cloud Provider Github Repository Azure https://github.com/ibm-cloud-architecture/terraform-openshift4-azure AWS https://github.com/ibm-cloud-architecture/terraform-openshift4-aws GCP https://github.com/ibm-cloud-architecture/terraform-openshift4-gcp vSphere https://github.com/ibm-cloud-architecture/terraform-openshift4-vmware VCD https://github.com/ibm-cloud-architecture/terraform-openshift4-vcd Considerations : + Pro's 1. Infrastructure as Code implementations increase repeatability, and minimize human errors 2. Automation drastically reduces deployment time when compared to manual configuration of cloud resources - Con's 1. Requires working knowledge of terraform 2. User Provided Infrastructure (UPI) \u00b6 In this option, we use cloud native automation to deploy into each cloud provider. Considerations : + Pro's 1. Infrastructure as Code implementations increase repeatability, and minimize human errors 2. Automation drastically reduces deployment time when compared to manual configuration of cloud resources - Con's 1. Requires working knowledge of cloud specific automation languages Justification \u00b6 N/A Implications \u00b6 N/A Derived requirements N/A Related decisions N/A","title":"AD03 - Cluster Provisioning"},{"location":"infrastructure/ad/ad03-cluster-provisioning/#ad03-cluster-provisioning","text":"< Back to architecture decision registry","title":"AD03 - Cluster Provisioning"},{"location":"infrastructure/ad/ad03-cluster-provisioning/#cluster-provisioning","text":"Subject Area: Topology","title":"Cluster Provisioning"},{"location":"infrastructure/ad/ad03-cluster-provisioning/#issue-or-problem-statement","text":"The recommended approach to deploying OpenShift Container Platform is using the Installer Provisioned Infrastructure (IPI) method.","title":"Issue or Problem Statement"},{"location":"infrastructure/ad/ad03-cluster-provisioning/#givens-assumptions","text":"Given Production clusters will implement best practices to deploy in a highly available, resilient infrastructure Given Cluster needs to be easily reproducible","title":"Givens &amp; Assumptions"},{"location":"infrastructure/ad/ad03-cluster-provisioning/#motivation","text":"...","title":"Motivation"},{"location":"infrastructure/ad/ad03-cluster-provisioning/#alternatives","text":"","title":"Alternatives"},{"location":"infrastructure/ad/ad03-cluster-provisioning/#justification","text":"N/A","title":"Justification"},{"location":"infrastructure/ad/ad03-cluster-provisioning/#implications","text":"N/A Derived requirements N/A Related decisions N/A","title":"Implications"},{"location":"infrastructure/ad/ad04-machinesets/","text":"AD04 - MachineSets \u00b6 < Back to architecture decision registry MachineSets \u00b6 Subject Area: Topology Issue or Problem Statement \u00b6 With OpenShift 4, RedHat moved to an operator-based platform. Think of these operators as code written to ease the operational complexity and capture the history and experience of managing another complex piece of software. The Machine Config Operator (MCO) is a cluster operator meant to abstract the complexities of managing the care and feed of the operating system, keeping cluster members in a consistent and well-known configuration. At its heart, MCO manages Machine objects. Think of a Machine as an abstraction of the compute, storage and network characteristics that make up your cluster nodes. The MachineSet object is a representation of similar types of Machines. It represents machines that have similar characteristics (compute, storage and network) as well as the desired number of machines of this type that a cluster should have. MachineSets are to Machines as ReplicaSets are to Pods. If you need to scale up or down your cluster, you change the replica count on your MachineSet and MCO handles all the provisioning or deprovisioning of cluster nodes against your cloud infrastructure. In a default installation (IPI), master or control plane nodes are created as Machines, and compute or worker nodes are created as MachineSets. Givens & Assumptions \u00b6 Your cluster is deployed using openshift-install leveraging Installer Provisioned Infrastructure (IPI) A key requirement for deploying your cluster with MachineSets in your cloud provider is an IP Address Management system. When deploying OpenShift clusters to cloud providers like Azure, AWS or GCP, the IP address allocation is handled by the underlying cloud provider network fabric. When deploying to OnPrem solutions like vSphere, DHCP is required to provide initial connectivity to the internal API endpoint so that your cluster nodes can obtain its ignition configuration. While deploying OpenShift clusters OnPrem without DHCP is possible, deploying MachineSets into these clusters is not. Motivation \u00b6 Similar to how ReplicaSets provide resiliency to Pods, MachineSets provide resiliency and scalability to their OpenShift infrastructure. As developers start deploying more and more workloads into their cluster, they might discover that tje initial cluster compute requirements no longer meet their enterprise needs, or experience burst events where their traffic load exceeds their initial expectations. MachineSets allow them to quickly add more compute resources to your cluster and release these resources when they're no longer need. Alternatives \u00b6 1. Manual Provisioning \u00b6 Considerations : No integration with cloud provider GitOps Automation is not possible Justification \u00b6 Let's consider the following three scenarios where MachineSets can be leveraged to quickly deploy and scale your clusters. Larger Capacity Nodes \u00b6 Like mentioned before, the default worker node size is 2 CPUs, 8GB of memory and 120GB disk storage. If you're deploying a well-behaved application into your OpenShift cluster, it will define requests and limits for CPU and memory usage. If these parameters are large enough, the cluster scheduler will not be able to \"fit\" your application in a single worker node. Creating nodes with a larger hardware footprint will solve this issue for you. Infrastructure Nodes \u00b6 A typical OpenShift 3.x cluster has 3 types of cluster nodes: masters, workers, and infrastructure. In OpenShift 4.x clusters, creating infrastructure nodes is no longer a Day-1 activity. In a production environment, you still want to create infrastructure machines to make sure all the infrastructure workloads have enough capacity to run in your environment. You don't want your ingress to contest for resources against your application workloads, as this could lead to performance issues with your router pods. You don't want your cluster monitoring solution to starve out your applications. Creating a MachineSet for infrastructure specific resources (ingress, monitoring, image registry and cluster logging) is the first step in resolving this issue. A good rule of thumb is to create a MachineSet with 3 replicas and 4 CPUs and 16GB of memory for its compute resources. Cluster AutoScaling \u00b6 The cluster autoscaler adjusts the size of an OpenShift Container Platform cluster to meet its current deployment needs. It uses declarative, Kubernetes-style arguments to provide infrastructure management that does not rely on objects of a specific cloud provider. The cluster autoscaler increases the size of the cluster when there are pods that failed to schedule on any of the current nodes due to insufficient resources or when another node is necessary to meet deployment needs. Similarly, the cluster autoscaler decreases the size of the cluster when some nodes are consistently not needed for a significant period, such as when it has low resource use and all of its important pods can fit on other nodes. In order to configure Cluster AutoScaling , MachineSets need to be provided so the cluster knows what type of nodes you want to scale. Implications \u00b6 N/A Derived requirements N/A Related decisions N/A","title":"Ad04 machinesets"},{"location":"infrastructure/ad/ad04-machinesets/#ad04-machinesets","text":"< Back to architecture decision registry","title":"AD04 - MachineSets"},{"location":"infrastructure/ad/ad04-machinesets/#machinesets","text":"Subject Area: Topology","title":"MachineSets"},{"location":"infrastructure/ad/ad04-machinesets/#issue-or-problem-statement","text":"With OpenShift 4, RedHat moved to an operator-based platform. Think of these operators as code written to ease the operational complexity and capture the history and experience of managing another complex piece of software. The Machine Config Operator (MCO) is a cluster operator meant to abstract the complexities of managing the care and feed of the operating system, keeping cluster members in a consistent and well-known configuration. At its heart, MCO manages Machine objects. Think of a Machine as an abstraction of the compute, storage and network characteristics that make up your cluster nodes. The MachineSet object is a representation of similar types of Machines. It represents machines that have similar characteristics (compute, storage and network) as well as the desired number of machines of this type that a cluster should have. MachineSets are to Machines as ReplicaSets are to Pods. If you need to scale up or down your cluster, you change the replica count on your MachineSet and MCO handles all the provisioning or deprovisioning of cluster nodes against your cloud infrastructure. In a default installation (IPI), master or control plane nodes are created as Machines, and compute or worker nodes are created as MachineSets.","title":"Issue or Problem Statement"},{"location":"infrastructure/ad/ad04-machinesets/#givens-assumptions","text":"Your cluster is deployed using openshift-install leveraging Installer Provisioned Infrastructure (IPI) A key requirement for deploying your cluster with MachineSets in your cloud provider is an IP Address Management system. When deploying OpenShift clusters to cloud providers like Azure, AWS or GCP, the IP address allocation is handled by the underlying cloud provider network fabric. When deploying to OnPrem solutions like vSphere, DHCP is required to provide initial connectivity to the internal API endpoint so that your cluster nodes can obtain its ignition configuration. While deploying OpenShift clusters OnPrem without DHCP is possible, deploying MachineSets into these clusters is not.","title":"Givens &amp; Assumptions"},{"location":"infrastructure/ad/ad04-machinesets/#motivation","text":"Similar to how ReplicaSets provide resiliency to Pods, MachineSets provide resiliency and scalability to their OpenShift infrastructure. As developers start deploying more and more workloads into their cluster, they might discover that tje initial cluster compute requirements no longer meet their enterprise needs, or experience burst events where their traffic load exceeds their initial expectations. MachineSets allow them to quickly add more compute resources to your cluster and release these resources when they're no longer need.","title":"Motivation"},{"location":"infrastructure/ad/ad04-machinesets/#alternatives","text":"","title":"Alternatives"},{"location":"infrastructure/ad/ad04-machinesets/#justification","text":"Let's consider the following three scenarios where MachineSets can be leveraged to quickly deploy and scale your clusters.","title":"Justification"},{"location":"infrastructure/ad/ad04-machinesets/#implications","text":"N/A Derived requirements N/A Related decisions N/A","title":"Implications"},{"location":"infrastructure/ad/ad05-storage/","text":"AD05 - Storage \u00b6 < Back to architecture decision registry Storage \u00b6 Subject Area: Topology Issue or Problem Statement \u00b6 When deploying CloudPak workloads on top of OpenShift, some components may require a storage class that provides ReadWriteMany (RWX) access mode: that is, a PersistentVolumeClaim (PVC) that can be mounted on multiple pods. By default, OpenShift cluster only provide storage classes with ReadWriteOnce (RWO) access modes. The use of OpenShift Container Storage/OpenShift Data Foundation is the recommended storage solution to provide a consistent hybrid story across all CloudPak components and Cloud Providers. Givens & Assumptions \u00b6 Your workload components require a ReadWriteMany or ObjectBucket PVC. Your cloud provider does not support a RWX storage class Motivation \u00b6 Provide a RWX-ready storage class to OpenShift Clusters when deploying components that depend on RWX storage classes. Alternatives \u00b6 1. NFS \u00b6 Considerations : + Pro 1: Widely understood + Pro 2: Integrates easily with OpenShift - Con 1: Tends to be slow - Con 2: Issues with file locking may arise 2. rook-ceph \u00b6 Considerations : + Pro 1: Available as an Operator in OperatorHub - Con 1: No enterprise support. Support only thru community channels. - Con 2: Can only provide a single RWX-enabled PVC unless experimental features are enabled. Justification \u00b6 OpenShift Data Foundation is a Software Defined Storage solution that leverages existing RWO storage classes on your OpenShift Cluster. Since the Storage Nodes are deployed as MachineSets , it is by default highly available. OpenShift Container Storage also replicates data across nodes in multiple Availability Zones, so data redundancy is built right on top of the solution. Implications \u00b6 N/A Derived requirements N/A Related decisions N/A","title":"Ad05 storage"},{"location":"infrastructure/ad/ad05-storage/#ad05-storage","text":"< Back to architecture decision registry","title":"AD05 - Storage"},{"location":"infrastructure/ad/ad05-storage/#storage","text":"Subject Area: Topology","title":"Storage"},{"location":"infrastructure/ad/ad05-storage/#issue-or-problem-statement","text":"When deploying CloudPak workloads on top of OpenShift, some components may require a storage class that provides ReadWriteMany (RWX) access mode: that is, a PersistentVolumeClaim (PVC) that can be mounted on multiple pods. By default, OpenShift cluster only provide storage classes with ReadWriteOnce (RWO) access modes. The use of OpenShift Container Storage/OpenShift Data Foundation is the recommended storage solution to provide a consistent hybrid story across all CloudPak components and Cloud Providers.","title":"Issue or Problem Statement"},{"location":"infrastructure/ad/ad05-storage/#givens-assumptions","text":"Your workload components require a ReadWriteMany or ObjectBucket PVC. Your cloud provider does not support a RWX storage class","title":"Givens &amp; Assumptions"},{"location":"infrastructure/ad/ad05-storage/#motivation","text":"Provide a RWX-ready storage class to OpenShift Clusters when deploying components that depend on RWX storage classes.","title":"Motivation"},{"location":"infrastructure/ad/ad05-storage/#alternatives","text":"","title":"Alternatives"},{"location":"infrastructure/ad/ad05-storage/#justification","text":"OpenShift Data Foundation is a Software Defined Storage solution that leverages existing RWO storage classes on your OpenShift Cluster. Since the Storage Nodes are deployed as MachineSets , it is by default highly available. OpenShift Container Storage also replicates data across nodes in multiple Availability Zones, so data redundancy is built right on top of the solution.","title":"Justification"},{"location":"infrastructure/ad/ad05-storage/#implications","text":"N/A Derived requirements N/A Related decisions N/A","title":"Implications"},{"location":"overview/overview/","text":"What are the Production Deployment Guides? \u00b6 The Cloud Pak Production Deployment Guides document and demonstrate how to design, deploy and operate cloud-native solutions leveraging IBM Cloud Pak capabilities in Red Hat OpenShift environments. The guides focus on Production level qualities of services such as High Availability, Security and Performance including how to design, install and manage your Cluster in the popular managed and unmanaged Cloud environments as well as tackling disconnected or air-gapped environments. Production Deployment Guide objectives \u00b6 There are a number of objectives behind providing the Production Deployment Guides. The three main goals of the Guides are provided below: Provide opinionated, validated Production topologies \u00b6 There are many different topology related options to consider when deploying Cloud Paks in Production and often our customers are looking for IBM to guide them to the right decisions for their environment. These guides provide an opinionated approach that has been validated in each of the major clouds by a team of IBMers including field practitioners, developers and thought leaders. More information can be read in golden-topology . Provide a GitOps driven automation framework \u00b6 What is GitOps? GitOps requires us to describe the entire system declaratively in Git where the desired system state is versioned, managed and applied by software agents. Automation is key for scale in the Cloud world and OpenShift and Cloud Paks are no different. The implementation code and artifacts provided along with these guides are based on a GitOps approach where infrastructure and everything deployed in to an OpenShift Cluster (including Cloud Paks) is described as code and is managed in the same way that application code would be. More information on the gitops structure can be read in gitops and the different personas that can benefit from this guide is listed in personas . Enable IBMers to setup Production topologies \u00b6 These guides and the assets provided along with them are designed for use by IBM field practitioners from Customer Success, Tech Sales, Tech Garage and Expert Labs. Before a customer is ready for Production, the Cloud Pak Quickstarts can be used to show a Customer how to run a given Cloud Pak capability on any cloud and to provide a foundation to discuss automation, GitOps and cloud-native concepts. Once the customer is ready to discuss their Production topology, the Infrastructure assets will be important in showing how to design a Cluster with the required qualities of service. Finally, once the topology is agreed, the automation provided by the guides can be used to quickly configure a Production cluster and deploy workloads. Note Before using this guide, check the following pre-requisites and important updates . Go to the Hands-on Guides section if you would like to try out some hands-on activities. Video Introduction \u00b6 This short video describes the motivation behind Production Deployment Guides, what its GitOps configurations can do, and the basic structure of the solution. Guided walkthrough \u00b6 If you'd like to have a guided walkthrough of what the Production Deployment Guides provide, check out this video demonstration of the Guides in action: Components of the Production Deployment Guides \u00b6 As the name suggests, the Production Deployment Guides provide a collection of guidance that can be used to deploy IBM Cloud Paks in Production Topologies on different cloud providers. The following provides a listing of the assets that make up the Production Deployment Guides: Infrastructure - best practice, validated, opinionated topologies for running OpenShift and Cloud Paks on each of the major clouds GitOps - a deep dive in to our opinionated GitOps approach and repository structure and a step by step guide to configure your cluster Quickstart Demos - a series of demonstrations that can be used to take a blank cluster and configure it to demonstrate GitOps and the cloud-native capabilities of the IBM Cloud Paks including: MQ App Connect Enterprise API Connect Process Mining Cloud Pak Guides - in-depth guides addressing Production level qualities of service for each of the Cloud Pak capabilities including: MQ App Connect Enterprise API Connect Process Mining Cloud Pak for Data Cloud Pak for Security To start deploying IBM Cloud Pak using this guide, read deployment paths .","title":"Objective"},{"location":"overview/overview/#what-are-the-production-deployment-guides","text":"The Cloud Pak Production Deployment Guides document and demonstrate how to design, deploy and operate cloud-native solutions leveraging IBM Cloud Pak capabilities in Red Hat OpenShift environments. The guides focus on Production level qualities of services such as High Availability, Security and Performance including how to design, install and manage your Cluster in the popular managed and unmanaged Cloud environments as well as tackling disconnected or air-gapped environments.","title":"What are the Production Deployment Guides?"},{"location":"overview/overview/#production-deployment-guide-objectives","text":"There are a number of objectives behind providing the Production Deployment Guides. The three main goals of the Guides are provided below:","title":"Production Deployment Guide objectives"},{"location":"overview/overview/#provide-opinionated-validated-production-topologies","text":"There are many different topology related options to consider when deploying Cloud Paks in Production and often our customers are looking for IBM to guide them to the right decisions for their environment. These guides provide an opinionated approach that has been validated in each of the major clouds by a team of IBMers including field practitioners, developers and thought leaders. More information can be read in golden-topology .","title":"Provide opinionated, validated Production topologies"},{"location":"overview/overview/#provide-a-gitops-driven-automation-framework","text":"What is GitOps? GitOps requires us to describe the entire system declaratively in Git where the desired system state is versioned, managed and applied by software agents. Automation is key for scale in the Cloud world and OpenShift and Cloud Paks are no different. The implementation code and artifacts provided along with these guides are based on a GitOps approach where infrastructure and everything deployed in to an OpenShift Cluster (including Cloud Paks) is described as code and is managed in the same way that application code would be. More information on the gitops structure can be read in gitops and the different personas that can benefit from this guide is listed in personas .","title":"Provide a GitOps driven automation framework"},{"location":"overview/overview/#enable-ibmers-to-setup-production-topologies","text":"These guides and the assets provided along with them are designed for use by IBM field practitioners from Customer Success, Tech Sales, Tech Garage and Expert Labs. Before a customer is ready for Production, the Cloud Pak Quickstarts can be used to show a Customer how to run a given Cloud Pak capability on any cloud and to provide a foundation to discuss automation, GitOps and cloud-native concepts. Once the customer is ready to discuss their Production topology, the Infrastructure assets will be important in showing how to design a Cluster with the required qualities of service. Finally, once the topology is agreed, the automation provided by the guides can be used to quickly configure a Production cluster and deploy workloads. Note Before using this guide, check the following pre-requisites and important updates . Go to the Hands-on Guides section if you would like to try out some hands-on activities.","title":"Enable IBMers to setup Production topologies"},{"location":"overview/overview/#video-introduction","text":"This short video describes the motivation behind Production Deployment Guides, what its GitOps configurations can do, and the basic structure of the solution.","title":"Video Introduction"},{"location":"overview/overview/#guided-walkthrough","text":"If you'd like to have a guided walkthrough of what the Production Deployment Guides provide, check out this video demonstration of the Guides in action:","title":"Guided walkthrough"},{"location":"overview/overview/#components-of-the-production-deployment-guides","text":"As the name suggests, the Production Deployment Guides provide a collection of guidance that can be used to deploy IBM Cloud Paks in Production Topologies on different cloud providers. The following provides a listing of the assets that make up the Production Deployment Guides: Infrastructure - best practice, validated, opinionated topologies for running OpenShift and Cloud Paks on each of the major clouds GitOps - a deep dive in to our opinionated GitOps approach and repository structure and a step by step guide to configure your cluster Quickstart Demos - a series of demonstrations that can be used to take a blank cluster and configure it to demonstrate GitOps and the cloud-native capabilities of the IBM Cloud Paks including: MQ App Connect Enterprise API Connect Process Mining Cloud Pak Guides - in-depth guides addressing Production level qualities of service for each of the Cloud Pak capabilities including: MQ App Connect Enterprise API Connect Process Mining Cloud Pak for Data Cloud Pak for Security To start deploying IBM Cloud Pak using this guide, read deployment paths .","title":"Components of the Production Deployment Guides"},{"location":"overview/prerequisites/","text":"Assumed Knowledge \u00b6 The Production Deployment Guides have been developed to help customers and IBMers deploy IBM Cloud Paks in to Production on any of the major clouds. The guides aim to demonstrate how to use Automation and the IBM Cloud Pak capabilities to achieve production level qualities of service, but assume a basic working knowledge of the following technologies: Git version control system Containers as the means of delivering applications and services Kubernetes as the container orchestration platform Helm as the kubernetes package manager Links to additional learning resources can be found in the resources section .","title":"Assumed Knowledge"},{"location":"overview/prerequisites/#assumed-knowledge","text":"The Production Deployment Guides have been developed to help customers and IBMers deploy IBM Cloud Paks in to Production on any of the major clouds. The guides aim to demonstrate how to use Automation and the IBM Cloud Pak capabilities to achieve production level qualities of service, but assume a basic working knowledge of the following technologies: Git version control system Containers as the means of delivering applications and services Kubernetes as the container orchestration platform Helm as the kubernetes package manager Links to additional learning resources can be found in the resources section .","title":"Assumed Knowledge"},{"location":"overview/using/","text":"Hands-on Guides \u00b6 Deploying IBM Cloud Pak using this Production Deployment Guide can follows one of these paths: Cluster Infrastructure : Build a Production Ready OpenShift cluster. Quick-start implementation : The goal is to get started quickly and stand up a cluster for enablement/demo with no need for customizations. The cluster is typically ready in 2-3 hours. Production Guide deployment : The goal is to customize the deploy a prescribed IBM Cloud Pak deployment step-by-step allowing a more customizable approach, while retaining the opinionated configuration from our guide. This approach is suitable for deploying a Production ready cluster, the cluster is typically ready in 1-2 days.","title":"Using"},{"location":"overview/using/#hands-on-guides","text":"Deploying IBM Cloud Pak using this Production Deployment Guide can follows one of these paths: Cluster Infrastructure : Build a Production Ready OpenShift cluster. Quick-start implementation : The goal is to get started quickly and stand up a cluster for enablement/demo with no need for customizations. The cluster is typically ready in 2-3 hours. Production Guide deployment : The goal is to customize the deploy a prescribed IBM Cloud Pak deployment step-by-step allowing a more customizable approach, while retaining the opinionated configuration from our guide. This approach is suitable for deploying a Production ready cluster, the cluster is typically ready in 1-2 days.","title":"Hands-on Guides"},{"location":"overview/whats-new/","text":"What's new \u00b6 Tip When in doubt, use the search feature Now 29, 2021 \u00b6 IBM Process Mining Production Considerations IBM Process Mining tutorial IBM Process Mining Quickstart Nov 16, 2021 \u00b6 IBM API Connect Multi-Cluster tutorial IBM API Connect Publish APIs pipeline Nov 14, 2021 \u00b6 Deploy Backend Service section, ACE tutorial Nov 11, 2021 \u00b6 IBM App Connect Deploying Service section . Nov 2, 2021 \u00b6 IBM MQ In depth Guide - Environment Promotion from Dev to Staging IBM MQ In depth Guide - Environment Promotion from Staging to Prod IBM MQ In depth Guide - Automate Promotion Pipeline Workflow Oct 29, 2021 \u00b6 The MQ walk-through guide is up-to-date with latest instructions. IBM MQ Promotion across environments - Production Considerations Oct 6, 2021 \u00b6 IBM API Connect configuration pipeline Oct 1, 2021 \u00b6 IBM API Connect high availability section IBM API Connect disaster recovery section IBM API Connect security section IBM API Connect QuickStart IBM API Connect walk-through guide Sept 22, 2021 \u00b6 Navigation tabs added The MQ walk-through guide is currently being updated but refer to the previous version of the guide in the meantime. Sept 17, 2021 \u00b6 Site navigation has re-structured based on user feedback August 23, 2021 \u00b6 Initial structure moved from Gatsby site","title":"What's New"},{"location":"overview/whats-new/#whats-new","text":"Tip When in doubt, use the search feature","title":"What's new"},{"location":"overview/whats-new/#now-29-2021","text":"IBM Process Mining Production Considerations IBM Process Mining tutorial IBM Process Mining Quickstart","title":"Now 29, 2021"},{"location":"overview/whats-new/#nov-16-2021","text":"IBM API Connect Multi-Cluster tutorial IBM API Connect Publish APIs pipeline","title":"Nov 16, 2021"},{"location":"overview/whats-new/#nov-14-2021","text":"Deploy Backend Service section, ACE tutorial","title":"Nov 14, 2021"},{"location":"overview/whats-new/#nov-11-2021","text":"IBM App Connect Deploying Service section .","title":"Nov 11, 2021"},{"location":"overview/whats-new/#nov-2-2021","text":"IBM MQ In depth Guide - Environment Promotion from Dev to Staging IBM MQ In depth Guide - Environment Promotion from Staging to Prod IBM MQ In depth Guide - Automate Promotion Pipeline Workflow","title":"Nov 2, 2021"},{"location":"overview/whats-new/#oct-29-2021","text":"The MQ walk-through guide is up-to-date with latest instructions. IBM MQ Promotion across environments - Production Considerations","title":"Oct 29, 2021"},{"location":"overview/whats-new/#oct-6-2021","text":"IBM API Connect configuration pipeline","title":"Oct 6, 2021"},{"location":"overview/whats-new/#oct-1-2021","text":"IBM API Connect high availability section IBM API Connect disaster recovery section IBM API Connect security section IBM API Connect QuickStart IBM API Connect walk-through guide","title":"Oct 1, 2021"},{"location":"overview/whats-new/#sept-22-2021","text":"Navigation tabs added The MQ walk-through guide is currently being updated but refer to the previous version of the guide in the meantime.","title":"Sept 22, 2021"},{"location":"overview/whats-new/#sept-17-2021","text":"Site navigation has re-structured based on user feedback","title":"Sept 17, 2021"},{"location":"overview/whats-new/#august-23-2021","text":"Initial structure moved from Gatsby site","title":"August 23, 2021"},{"location":"playbacks/playbacks/","text":"Project Playbacks \u00b6 Box folder with major project playbacks Scorecard showing completed and in-progress work Executive Playback #5 \u00b6 If the video doesn't appear, click here to log in to the IBM Media Center Executive Playback #4 \u00b6 If the video doesn't appear, click here to log in to the IBM Media Center Executive Playback #3 \u00b6 If the video doesn't appear, click here to log in to the IBM Media Center Executive Playback #2 \u00b6 If the video doesn't appear, click here to log in to the IBM Media Center Executive Playback #1 \u00b6 If the video doesn't appear, click here to log in to the IBM Media Center","title":"Project Playbacks"},{"location":"playbacks/playbacks/#project-playbacks","text":"Box folder with major project playbacks Scorecard showing completed and in-progress work","title":"Project Playbacks"},{"location":"playbacks/playbacks/#executive-playback-5","text":"If the video doesn't appear, click here to log in to the IBM Media Center","title":"Executive Playback #5"},{"location":"playbacks/playbacks/#executive-playback-4","text":"If the video doesn't appear, click here to log in to the IBM Media Center","title":"Executive Playback #4"},{"location":"playbacks/playbacks/#executive-playback-3","text":"If the video doesn't appear, click here to log in to the IBM Media Center","title":"Executive Playback #3"},{"location":"playbacks/playbacks/#executive-playback-2","text":"If the video doesn't appear, click here to log in to the IBM Media Center","title":"Executive Playback #2"},{"location":"playbacks/playbacks/#executive-playback-1","text":"If the video doesn't appear, click here to log in to the IBM Media Center","title":"Executive Playback #1"},{"location":"quickstart/create-new-cluster/","text":"Create an empty Red Hat OpenShift on IBM Cloud Cluster \u00b6 Overview \u00b6 IBM Technology Zone is the one-stop shop to get access to technical environments and software for demos, prototyping, and deployment. Here are two options to provision a Red Hat OpenShift on IBM Cloud managed cluster which can be used for the Production Reference Guide: Pre-requisites \u00b6 Create a custom Git Organization \u00b6 Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization. Create a Git Personal Access Token (PAT) \u00b6 Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later. OpenShift on IBM Cloud \u00b6 Navigate to the IBM Technology Zone In your browser, navigate to the IBM Technology Zone to create your cluster. You will need to sign-in with your IBM credentials if your browser has not authenticated. You will need to read and accept Terms and conditions to proceed to the IBM Technology Zone. Reserve a cluster You can reserve a cluster immediately on IBM Cloud. You'll see the following options: Click the Reserve now radio button. It takes between 30 and 60 minutes to provision a cluster. Moreover, once a cluster has been provisioned, by default, it will be deleted after 3 days -- unless you renew it. It can therefore be helpful to schedule the environment to be created later if you are not going to be using it for a while -- such as for a future demonstration or classroom session. Come back and explore the Schedule for later option another time. Complete reservation details - step 1 You will be asked to complete your reservations details in two steps. Firstly, when you make a cluster reservation, you can supply a set of details that describe: The cluster name The purpose of the reservation from a pre-defined set A description, which will appear for a Practice/Self-Education reservation Notes to help you identify this reservation In the following web page: Complete the details as follows: Type a helpful Name such as My cluster 1 Select Practice/Self-Education from the Purpose drop-down list Type a helpful Purpose Description such as Education Do not select Preferred Geography yet; we will do that in a moment Add some helpful Notes such as Learn how to build a cloud native production reference deployment using a CloudPak and OpenShift on IBM Cloud Complete reservation details - step 2 The second step in registration is to specify the physical details of the compute infrastructure for your cluster. When you make a cluster reservation, you can supply a set of details that describe: The cluster's geographical location The version of OpenShift Container Platform used by the cluster The number of CPUs in the cluster and their memory size The number of Kubernetes worker nodes in the cluster The NFS storage size for the cluster The End date and time for the cluster In the following web page: Complete the details as follows: Select your Preferred Geography such as London 5 , HongKong 2 or San Jose 04 Under End date and time , select a date and specify the time along with its time zone. Select Worker Node Count as 3 from the drop-down list Select Worker Node Flavor as 8CPU x 32GB from the drop-down list Select NFS Size as 500 GB from the drop-down list Select OpenShift Version as 4.8 from the drop-down-list Info By default, your cluster will be reserved for three days , after which time it will be deleted. This is more than enough time to complete this tutorial. If required, you can increase the cluster lifetime by configuring the End date and time . Don't do this unless you really need to do so; as you'll learn in the tutorial, the use of GitOps makes it quick and easy to provision a cluster when you need to do so. Create the cluster Check your reservation details and press Submit when you're ready to create the cluster: The cluster provisioning process will take between 30 and 60 minutes to complete on IBM Cloud. Provisioning email As the provisioning process starts, you will receive an email to confirm that the provisioning process has started: Checking the cluster provisioning status You can also view the status of your provisioning request on IBM Technology Zone. Click on My reservations to see: Note: The Status: field will be Ready when the cluster has been provisioned. The creation and expiry date-time for your cluster. You can return to the reservation at any time to get the details of your cluster. Cluster provisioned email You will receive another email once your cluster is ready: Your cluster is now provisioned and ready to use. Note the expiry date-time. Navigate to your cluster You can use the information in the notification email to access your cluster. Click on the Cluster URL link to login to IBM Cloud and locate your OpenShift cluster: Note: The Cluster ID that has been assigned by IBM Cloud. The Zone matches your requested geography. The OCP Version matches your requested version of OCP. The Node status identifies the 3 worker nodes in the Normal state. Feel free to explore this web page. Receiving cluster deletion email and extending your cluster's lifetime When your cluster is within 3 days of deletion, you will receive a daily email: Note: You can extend your cluster if required. This option may be limited to certain types of reservations. The My reservations page allows you to manage your cluster reservation Post cluster provisioning tasks \u00b6 Red Hat OpenShift cluster \u00b6 An OpenShift v4.7+ cluster is required. CLI tools \u00b6 Install the OpenShift CLI oc (version 4.7+) . The binary can be downloaded from the Help menu from the OpenShift Console. Download oc cli Log in from a terminal window. oc login --token = <token> --server = <server> IBM Entitlement Key \u00b6 The IBM Entitlement Key is required to pull IBM Cloud Pak specific container images from the IBM Entitled Registry. To get an entitlement key, Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. A Secret containing the entitlement key is created in the tools namespace. oc new-project tools || true oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io Select resources to deploy \u00b6 In this section you will: Clone the multi-tenancy-gitops repository from your Git Organization to your local machine Use one of the recipes to select which components from the infrastructure and services layers to deploy. ACE recipe MQ recipe Process Mining recipe Instructions \u00b6 Clone the multi-tenancy-gitops repository from your Git Organization to your local machine using a command like the one shown below: git clone https://github.com/xxxxxxx/multi-tenancy-gitops Review the contents of the 0-bootstrap folder and note the single-cluster folder and the other topologies are in an others folder. The contents of the others folder is provided for other scenarios and can be deleted. cd multi-tenancy-gitops/0-bootstrap/ tree -L 2 . \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Delete the others folder from the 0-bootstrap folder to ensure the single-cluster topology is used rm -rf others/ tree -L 2 . \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Using the recipe you selected above, modify the 0-bootstrap/single-cluster/1-infra/kustomization.yaml file to configure the Infrastructure layer by un-commenting the resources to deploy. Using the recipe you selected above, modify the 0-bootstrap/single-cluster/2-services/kustomization.yaml and un-comment the resources to deploy. Note that the recipe may show additional files to edit. If so, make those edits now. Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin If an IBM Cloud Pak is installed, retrieve the Platform Navigator console URL and admin password. # Verify the Common Services instance has been deployed successfully oc get commonservice common-service -n ibm-common-services -o = jsonpath = '{.status.phase}' # Expected output = Succeeded # Verify the Platform Navigator instance has been deployed successfully oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' # Expected output = True # oc get route -n tools integration-navigator-pn -o template --template = 'https://{{.spec.host}}' oc extract -n ibm-common-services secrets/platform-auth-idp-credentials --keys = admin_username,admin_password --to = -","title":"Create an empty Red Hat OpenShift on IBM Cloud Cluster"},{"location":"quickstart/create-new-cluster/#create-an-empty-red-hat-openshift-on-ibm-cloud-cluster","text":"","title":"Create an empty Red Hat OpenShift on IBM Cloud Cluster"},{"location":"quickstart/create-new-cluster/#overview","text":"IBM Technology Zone is the one-stop shop to get access to technical environments and software for demos, prototyping, and deployment. Here are two options to provision a Red Hat OpenShift on IBM Cloud managed cluster which can be used for the Production Reference Guide:","title":"Overview"},{"location":"quickstart/create-new-cluster/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"quickstart/create-new-cluster/#create-a-custom-git-organization","text":"Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization.","title":"Create a custom Git Organization"},{"location":"quickstart/create-new-cluster/#create-a-git-personal-access-token-pat","text":"Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later.","title":"Create a Git Personal Access Token (PAT)"},{"location":"quickstart/create-new-cluster/#openshift-on-ibm-cloud","text":"Navigate to the IBM Technology Zone In your browser, navigate to the IBM Technology Zone to create your cluster. You will need to sign-in with your IBM credentials if your browser has not authenticated. You will need to read and accept Terms and conditions to proceed to the IBM Technology Zone. Reserve a cluster You can reserve a cluster immediately on IBM Cloud. You'll see the following options: Click the Reserve now radio button. It takes between 30 and 60 minutes to provision a cluster. Moreover, once a cluster has been provisioned, by default, it will be deleted after 3 days -- unless you renew it. It can therefore be helpful to schedule the environment to be created later if you are not going to be using it for a while -- such as for a future demonstration or classroom session. Come back and explore the Schedule for later option another time. Complete reservation details - step 1 You will be asked to complete your reservations details in two steps. Firstly, when you make a cluster reservation, you can supply a set of details that describe: The cluster name The purpose of the reservation from a pre-defined set A description, which will appear for a Practice/Self-Education reservation Notes to help you identify this reservation In the following web page: Complete the details as follows: Type a helpful Name such as My cluster 1 Select Practice/Self-Education from the Purpose drop-down list Type a helpful Purpose Description such as Education Do not select Preferred Geography yet; we will do that in a moment Add some helpful Notes such as Learn how to build a cloud native production reference deployment using a CloudPak and OpenShift on IBM Cloud Complete reservation details - step 2 The second step in registration is to specify the physical details of the compute infrastructure for your cluster. When you make a cluster reservation, you can supply a set of details that describe: The cluster's geographical location The version of OpenShift Container Platform used by the cluster The number of CPUs in the cluster and their memory size The number of Kubernetes worker nodes in the cluster The NFS storage size for the cluster The End date and time for the cluster In the following web page: Complete the details as follows: Select your Preferred Geography such as London 5 , HongKong 2 or San Jose 04 Under End date and time , select a date and specify the time along with its time zone. Select Worker Node Count as 3 from the drop-down list Select Worker Node Flavor as 8CPU x 32GB from the drop-down list Select NFS Size as 500 GB from the drop-down list Select OpenShift Version as 4.8 from the drop-down-list Info By default, your cluster will be reserved for three days , after which time it will be deleted. This is more than enough time to complete this tutorial. If required, you can increase the cluster lifetime by configuring the End date and time . Don't do this unless you really need to do so; as you'll learn in the tutorial, the use of GitOps makes it quick and easy to provision a cluster when you need to do so. Create the cluster Check your reservation details and press Submit when you're ready to create the cluster: The cluster provisioning process will take between 30 and 60 minutes to complete on IBM Cloud. Provisioning email As the provisioning process starts, you will receive an email to confirm that the provisioning process has started: Checking the cluster provisioning status You can also view the status of your provisioning request on IBM Technology Zone. Click on My reservations to see: Note: The Status: field will be Ready when the cluster has been provisioned. The creation and expiry date-time for your cluster. You can return to the reservation at any time to get the details of your cluster. Cluster provisioned email You will receive another email once your cluster is ready: Your cluster is now provisioned and ready to use. Note the expiry date-time. Navigate to your cluster You can use the information in the notification email to access your cluster. Click on the Cluster URL link to login to IBM Cloud and locate your OpenShift cluster: Note: The Cluster ID that has been assigned by IBM Cloud. The Zone matches your requested geography. The OCP Version matches your requested version of OCP. The Node status identifies the 3 worker nodes in the Normal state. Feel free to explore this web page. Receiving cluster deletion email and extending your cluster's lifetime When your cluster is within 3 days of deletion, you will receive a daily email: Note: You can extend your cluster if required. This option may be limited to certain types of reservations. The My reservations page allows you to manage your cluster reservation","title":"OpenShift on IBM Cloud"},{"location":"quickstart/create-new-cluster/#post-cluster-provisioning-tasks","text":"","title":"Post cluster provisioning tasks"},{"location":"quickstart/create-new-cluster/#red-hat-openshift-cluster","text":"An OpenShift v4.7+ cluster is required.","title":"Red Hat OpenShift cluster"},{"location":"quickstart/create-new-cluster/#cli-tools","text":"Install the OpenShift CLI oc (version 4.7+) . The binary can be downloaded from the Help menu from the OpenShift Console. Download oc cli Log in from a terminal window. oc login --token = <token> --server = <server>","title":"CLI tools"},{"location":"quickstart/create-new-cluster/#ibm-entitlement-key","text":"The IBM Entitlement Key is required to pull IBM Cloud Pak specific container images from the IBM Entitled Registry. To get an entitlement key, Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. A Secret containing the entitlement key is created in the tools namespace. oc new-project tools || true oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io","title":"IBM Entitlement Key"},{"location":"quickstart/create-new-cluster/#select-resources-to-deploy","text":"In this section you will: Clone the multi-tenancy-gitops repository from your Git Organization to your local machine Use one of the recipes to select which components from the infrastructure and services layers to deploy. ACE recipe MQ recipe Process Mining recipe","title":"Select resources to deploy"},{"location":"quickstart/create-new-cluster/#instructions","text":"Clone the multi-tenancy-gitops repository from your Git Organization to your local machine using a command like the one shown below: git clone https://github.com/xxxxxxx/multi-tenancy-gitops Review the contents of the 0-bootstrap folder and note the single-cluster folder and the other topologies are in an others folder. The contents of the others folder is provided for other scenarios and can be deleted. cd multi-tenancy-gitops/0-bootstrap/ tree -L 2 . \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Delete the others folder from the 0-bootstrap folder to ensure the single-cluster topology is used rm -rf others/ tree -L 2 . \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Using the recipe you selected above, modify the 0-bootstrap/single-cluster/1-infra/kustomization.yaml file to configure the Infrastructure layer by un-commenting the resources to deploy. Using the recipe you selected above, modify the 0-bootstrap/single-cluster/2-services/kustomization.yaml and un-comment the resources to deploy. Note that the recipe may show additional files to edit. If so, make those edits now. Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin If an IBM Cloud Pak is installed, retrieve the Platform Navigator console URL and admin password. # Verify the Common Services instance has been deployed successfully oc get commonservice common-service -n ibm-common-services -o = jsonpath = '{.status.phase}' # Expected output = Succeeded # Verify the Platform Navigator instance has been deployed successfully oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' # Expected output = True # oc get route -n tools integration-navigator-pn -o template --template = 'https://{{.spec.host}}' oc extract -n ibm-common-services secrets/platform-auth-idp-credentials --keys = admin_username,admin_password --to = -","title":"Instructions"},{"location":"quickstart/overview/","text":"What are Quick-start Demos? \u00b6 Introduction \u00b6 The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers. Quick-start Demo Overview \u00b6 The overall process for running Quick-start Demos is as follows: Install the necessary CLI . This task is only performed once, regardless how many times you run a Quick-start Demos. Create a GitHub Personal Access Token that you can use to authenticate to GitHub. This task is only performed once. You can use the same Personal Access Token for different demos. Setup a Git organization , you can either create a new organization for each time you perform a demo or you clean up all the repositories in that organization before using it for a new demo. Prepare an OpenShift cluster, you perform this for each Demo, the instruction assumes that you start from a new cluster. The instruction here depends on the platform you choose. Implement the Demos scenario - this is typically done using the bootstrap.sh script. Explore the generated environment. Additional Considerations \u00b6 The Quick-start Demos is typically implemented on a single OpenShift cluster that has a sufficient compute and storage capabilities. Each demo scenario come with the list of compute and storage requirements. To setup your OpenShift cluster, see Cluster setup .","title":"Overview"},{"location":"quickstart/overview/#what-are-quick-start-demos","text":"","title":"What are Quick-start Demos?"},{"location":"quickstart/overview/#introduction","text":"The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers.","title":"Introduction"},{"location":"quickstart/overview/#quick-start-demo-overview","text":"The overall process for running Quick-start Demos is as follows: Install the necessary CLI . This task is only performed once, regardless how many times you run a Quick-start Demos. Create a GitHub Personal Access Token that you can use to authenticate to GitHub. This task is only performed once. You can use the same Personal Access Token for different demos. Setup a Git organization , you can either create a new organization for each time you perform a demo or you clean up all the repositories in that organization before using it for a new demo. Prepare an OpenShift cluster, you perform this for each Demo, the instruction assumes that you start from a new cluster. The instruction here depends on the platform you choose. Implement the Demos scenario - this is typically done using the bootstrap.sh script. Explore the generated environment.","title":"Quick-start Demo Overview"},{"location":"quickstart/overview/#additional-considerations","text":"The Quick-start Demos is typically implemented on a single OpenShift cluster that has a sufficient compute and storage capabilities. Each demo scenario come with the list of compute and storage requirements. To setup your OpenShift cluster, see Cluster setup .","title":"Additional Considerations"},{"location":"quickstart/quickstart-ace-bkup/","text":"App Connect Enterprise Quickstart \u00b6 Introduction \u00b6 The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers. The following instructions is from the README.md in this repository: https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-ace . Pre-requisites \u00b6 Create a custom Git Organization \u00b6 Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization. Create a Git Personal Access Token (PAT) \u00b6 Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later. OpenShift on IBM Cloud \u00b6 Navigate to the IBM Technology Zone In your browser, navigate to the IBM Technology Zone to create your cluster. You will need to sign-in with your IBM credentials if your browser has not authenticated. You will need to read and accept Terms and conditions to proceed to the IBM Technology Zone. Reserve a cluster You can reserve a cluster immediately on IBM Cloud. You'll see the following options: Click the Reserve now radio button. It takes between 30 and 60 minutes to provision a cluster. Moreover, once a cluster has been provisioned, by default, it will be deleted after 3 days -- unless you renew it. It can therefore be helpful to schedule the environment to be created later if you are not going to be using it for a while -- such as for a future demonstration or classroom session. Come back and explore the Schedule for later option another time. Complete reservation details - step 1 You will be asked to complete your reservations details in two steps. Firstly, when you make a cluster reservation, you can supply a set of details that describe: The cluster name The purpose of the reservation from a pre-defined set A description, which will appear for a Practice/Self-Education reservation Notes to help you identify this reservation In the following web page: Complete the details as follows: Type a helpful Name such as My cluster 1 Select Practice/Self-Education from the Purpose drop-down list Type a helpful Purpose Description such as Education Do not select Preferred Geography yet; we will do that in a moment Add some helpful Notes such as Learn how to build a cloud native production reference deployment using a CloudPak and OpenShift on IBM Cloud Complete reservation details - step 2 The second step in registration is to specify the physical details of the compute infrastructure for your cluster. When you make a cluster reservation, you can supply a set of details that describe: The cluster's geographical location The version of OpenShift Container Platform used by the cluster The number of CPUs in the cluster and their memory size The number of Kubernetes worker nodes in the cluster The NFS storage size for the cluster The End date and time for the cluster In the following web page: Complete the details as follows: Select your Preferred Geography such as London 5 , HongKong 2 or San Jose 04 Under End date and time , select a date and specify the time along with its time zone. Select Worker Node Count as 3 from the drop-down list Select Worker Node Flavor as 8CPU x 32GB from the drop-down list Select NFS Size as 500 GB from the drop-down list Select OpenShift Version as 4.8 from the drop-down-list Info By default, your cluster will be reserved for three days , after which time it will be deleted. This is more than enough time to complete this tutorial. If required, you can increase the cluster lifetime by configuring the End date and time . Don't do this unless you really need to do so; as you'll learn in the tutorial, the use of GitOps makes it quick and easy to provision a cluster when you need to do so. Create the cluster Check your reservation details and press Submit when you're ready to create the cluster: The cluster provisioning process will take between 30 and 60 minutes to complete on IBM Cloud. Provisioning email As the provisioning process starts, you will receive an email to confirm that the provisioning process has started: Checking the cluster provisioning status You can also view the status of your provisioning request on IBM Technology Zone. Click on My reservations to see: Note: The Status: field will be Ready when the cluster has been provisioned. The creation and expiry date-time for your cluster. You can return to the reservation at any time to get the details of your cluster. Cluster provisioned email You will receive another email once your cluster is ready: Your cluster is now provisioned and ready to use. Note the expiry date-time. Navigate to your cluster You can use the information in the notification email to access your cluster. Click on the Cluster URL link to login to IBM Cloud and locate your OpenShift cluster: Note: The Cluster ID that has been assigned by IBM Cloud. The Zone matches your requested geography. The OCP Version matches your requested version of OCP. The Node status identifies the 3 worker nodes in the Normal state. Feel free to explore this web page. Receiving cluster deletion email and extending your cluster's lifetime When your cluster is within 3 days of deletion, you will receive a daily email: Note: You can extend your cluster if required. This option may be limited to certain types of reservations. The My reservations page allows you to manage your cluster reservation Install required CLIs \u00b6 Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI Steps for setting up the Demo \u00b6 Bootstrap cluster \u00b6 Clone the repository and complete the instructions on the README.md in this repo -> https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-ace Login to the ArgoCD Console. A few things to note here: It will take approximately 20 - 30 minutes for all the Operators and instances to be deployed. Verify from the ArgoCD UI that all applications are successfully synced You might notice that ArgoCD Applications under \"Project: applications\" are out of sync. This is expected and they will be successfully synced once the pipelines are run. Validate Cloud Pak for Integration \u00b6 Check status of all the installed operators. From the OCP console, select Operators > Installed Operators for All Projects. All operators should show a status of \"Succeeded\". Check status of \"platformnavigator\" custom resource. From the OCP console, select Operators > Installed Operators for the \"tools\" project/namespace. Select the \"IBM Cloud Pak for Integration Platform Navigator\" operator and select the \"Platform Navigator\" tab. Click on the \"integration-navigator\" instance and at the bottom of the page, it should show the following: Status = True Message = Platform Navigator has been provisioned. Check status of the pods in the \"ibm-common-services\" namespace. From the OCP console, select Workloads > Pods for the \"ibm-common-services\" project/namespace. All pods should have a Status of \"Completed\" or \"Running\". Once all the operators and instances are successfully deployed, log in to the Platform Navigator UI and validate the ACE Operator capabilities are enabled. Platform Navigator URL: From the OCP console, select Operators > Installed Operators in the \"tools\" project/namespace. Select the \"IBM Cloud Pak for Integration Platform Navigator\" operator and click on the \"Platform Navigator\" tab. Select the \"integration-navigator\" instance and click the \"Platform Navigator UI\" link. Log in with OpenShift credentials. From the capabilities view, clicking the \"Create\" button should show the ability to create ACE operator provided capabilities (ie. ACE dashboard). If the operator and/or instances are not deployed correctly, the option will be greyed out. Run ACE pipelines \u00b6 From the OCP console, go to Pipelines -> Pipelines and select \"ci\" from the Project/Namespace dropdown and select \"ace-build-bar-promote-dev\" pipelines to build custom ACE image and write into the gitops-apps repo for dev environment Click Actions > Start. All the required \"Parameters\" are pre-populated. For \"Workspaces\", select the following: shared-workspace: PVC Select the \"ace-bar-pvc\" PVC Click Start to run the pipeline. NOTE: As no change has been made to the ACE flow application source repo, the \"git-ops\" step of this pipeline will fail per the logs from this task. The pipeline will have built an image of the ACE integrationserver with the bar file and pushed it into the internal registry. The \"create-customer-details-rest-is\" deployment pods in DEV should be bounced to pick up the newly built image. In order for that to happen, execute the following command in your terminal: oc rollout restart deploy/create-customer-details-rest-is -n dev In your OCP Console, go back to Pipelines > Pipelines and select \"ci\" Project/Namespace and select \"ace-promote-dev-stage\" pipelines to start pipeline to promote ACE IntegrationServer and Configuration resources to STAGING env if tests (ie. postman functional tests) pass. Click Actions > Start. Set the following \"Parameters\" For \"Workspaces\", select the following: shared-workspace: PVC Select the \"ace-test-pvc\" PVC Define the source to \"dev\" and target to \"staging\" Click Start to run the pipeline. View the pull request created by the pipeline for promotion to STAGING in this repo, select closed PRs to see an example PR, there will not be a PR created if the image tag on dev is the same in staging already (https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-apps/pulls). NOTE: The \"create-customer-details-rest-is\" deployment pods should be bounced to pick up the newly built image in STAGING. oc rollout restart deploy/create-customer-details-rest-is -n staging Back in your OCP console, go back to Pipelines > Pipelines and select \"ci\" Project/Namespace and select \"ace-promote-stage-prod\" pipeline to start the pipeline to promote ACE IntegrationServer and Configuration resources to PROD env if tests (ie. jmeter performance tests) pass. Click Actions > Start. Set the following \"Parameters\" For \"Workspaces\", select the following: shared-workspace: PVC Select the \"ace-test-pvc\" PVC Click Start to run the pipeline. View the pull request created by the pipeline for promotion to PROD in this repo. Select closed PRs to see an example PR, there will not be a PR created if the image tag on dev is the same in staging already (https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-apps/pulls). NOTE: The \"create-customer-details-rest-is\" deployment pods should be bounced to pick up the newly built image in PROD. oc rollout restart deploy/create-customer-details-rest-is -n prod ACE Dashboard \u00b6 You can open the ACE Dashboard running in the dev namespace and see the Integration Server and Integration running in dev. Go to Installed Operators Select dev namespace Select App Connect Operator Select App Connect Dashboard from the tabs Select the ace-dashboard instance Open the Admin UI url","title":"App Connect Enterprise Quickstart"},{"location":"quickstart/quickstart-ace-bkup/#app-connect-enterprise-quickstart","text":"","title":"App Connect Enterprise Quickstart"},{"location":"quickstart/quickstart-ace-bkup/#introduction","text":"The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers. The following instructions is from the README.md in this repository: https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-ace .","title":"Introduction"},{"location":"quickstart/quickstart-ace-bkup/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"quickstart/quickstart-ace-bkup/#create-a-custom-git-organization","text":"Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization.","title":"Create a custom Git Organization"},{"location":"quickstart/quickstart-ace-bkup/#create-a-git-personal-access-token-pat","text":"Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later.","title":"Create a Git Personal Access Token (PAT)"},{"location":"quickstart/quickstart-ace-bkup/#openshift-on-ibm-cloud","text":"Navigate to the IBM Technology Zone In your browser, navigate to the IBM Technology Zone to create your cluster. You will need to sign-in with your IBM credentials if your browser has not authenticated. You will need to read and accept Terms and conditions to proceed to the IBM Technology Zone. Reserve a cluster You can reserve a cluster immediately on IBM Cloud. You'll see the following options: Click the Reserve now radio button. It takes between 30 and 60 minutes to provision a cluster. Moreover, once a cluster has been provisioned, by default, it will be deleted after 3 days -- unless you renew it. It can therefore be helpful to schedule the environment to be created later if you are not going to be using it for a while -- such as for a future demonstration or classroom session. Come back and explore the Schedule for later option another time. Complete reservation details - step 1 You will be asked to complete your reservations details in two steps. Firstly, when you make a cluster reservation, you can supply a set of details that describe: The cluster name The purpose of the reservation from a pre-defined set A description, which will appear for a Practice/Self-Education reservation Notes to help you identify this reservation In the following web page: Complete the details as follows: Type a helpful Name such as My cluster 1 Select Practice/Self-Education from the Purpose drop-down list Type a helpful Purpose Description such as Education Do not select Preferred Geography yet; we will do that in a moment Add some helpful Notes such as Learn how to build a cloud native production reference deployment using a CloudPak and OpenShift on IBM Cloud Complete reservation details - step 2 The second step in registration is to specify the physical details of the compute infrastructure for your cluster. When you make a cluster reservation, you can supply a set of details that describe: The cluster's geographical location The version of OpenShift Container Platform used by the cluster The number of CPUs in the cluster and their memory size The number of Kubernetes worker nodes in the cluster The NFS storage size for the cluster The End date and time for the cluster In the following web page: Complete the details as follows: Select your Preferred Geography such as London 5 , HongKong 2 or San Jose 04 Under End date and time , select a date and specify the time along with its time zone. Select Worker Node Count as 3 from the drop-down list Select Worker Node Flavor as 8CPU x 32GB from the drop-down list Select NFS Size as 500 GB from the drop-down list Select OpenShift Version as 4.8 from the drop-down-list Info By default, your cluster will be reserved for three days , after which time it will be deleted. This is more than enough time to complete this tutorial. If required, you can increase the cluster lifetime by configuring the End date and time . Don't do this unless you really need to do so; as you'll learn in the tutorial, the use of GitOps makes it quick and easy to provision a cluster when you need to do so. Create the cluster Check your reservation details and press Submit when you're ready to create the cluster: The cluster provisioning process will take between 30 and 60 minutes to complete on IBM Cloud. Provisioning email As the provisioning process starts, you will receive an email to confirm that the provisioning process has started: Checking the cluster provisioning status You can also view the status of your provisioning request on IBM Technology Zone. Click on My reservations to see: Note: The Status: field will be Ready when the cluster has been provisioned. The creation and expiry date-time for your cluster. You can return to the reservation at any time to get the details of your cluster. Cluster provisioned email You will receive another email once your cluster is ready: Your cluster is now provisioned and ready to use. Note the expiry date-time. Navigate to your cluster You can use the information in the notification email to access your cluster. Click on the Cluster URL link to login to IBM Cloud and locate your OpenShift cluster: Note: The Cluster ID that has been assigned by IBM Cloud. The Zone matches your requested geography. The OCP Version matches your requested version of OCP. The Node status identifies the 3 worker nodes in the Normal state. Feel free to explore this web page. Receiving cluster deletion email and extending your cluster's lifetime When your cluster is within 3 days of deletion, you will receive a daily email: Note: You can extend your cluster if required. This option may be limited to certain types of reservations. The My reservations page allows you to manage your cluster reservation","title":"OpenShift on IBM Cloud"},{"location":"quickstart/quickstart-ace-bkup/#install-required-clis","text":"Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI","title":"Install required CLIs"},{"location":"quickstart/quickstart-ace-bkup/#steps-for-setting-up-the-demo","text":"","title":"Steps for setting up the Demo"},{"location":"quickstart/quickstart-ace-bkup/#bootstrap-cluster","text":"Clone the repository and complete the instructions on the README.md in this repo -> https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-ace Login to the ArgoCD Console. A few things to note here: It will take approximately 20 - 30 minutes for all the Operators and instances to be deployed. Verify from the ArgoCD UI that all applications are successfully synced You might notice that ArgoCD Applications under \"Project: applications\" are out of sync. This is expected and they will be successfully synced once the pipelines are run.","title":"Bootstrap cluster"},{"location":"quickstart/quickstart-ace-bkup/#validate-cloud-pak-for-integration","text":"Check status of all the installed operators. From the OCP console, select Operators > Installed Operators for All Projects. All operators should show a status of \"Succeeded\". Check status of \"platformnavigator\" custom resource. From the OCP console, select Operators > Installed Operators for the \"tools\" project/namespace. Select the \"IBM Cloud Pak for Integration Platform Navigator\" operator and select the \"Platform Navigator\" tab. Click on the \"integration-navigator\" instance and at the bottom of the page, it should show the following: Status = True Message = Platform Navigator has been provisioned. Check status of the pods in the \"ibm-common-services\" namespace. From the OCP console, select Workloads > Pods for the \"ibm-common-services\" project/namespace. All pods should have a Status of \"Completed\" or \"Running\". Once all the operators and instances are successfully deployed, log in to the Platform Navigator UI and validate the ACE Operator capabilities are enabled. Platform Navigator URL: From the OCP console, select Operators > Installed Operators in the \"tools\" project/namespace. Select the \"IBM Cloud Pak for Integration Platform Navigator\" operator and click on the \"Platform Navigator\" tab. Select the \"integration-navigator\" instance and click the \"Platform Navigator UI\" link. Log in with OpenShift credentials. From the capabilities view, clicking the \"Create\" button should show the ability to create ACE operator provided capabilities (ie. ACE dashboard). If the operator and/or instances are not deployed correctly, the option will be greyed out.","title":"Validate Cloud Pak for Integration"},{"location":"quickstart/quickstart-ace-bkup/#run-ace-pipelines","text":"From the OCP console, go to Pipelines -> Pipelines and select \"ci\" from the Project/Namespace dropdown and select \"ace-build-bar-promote-dev\" pipelines to build custom ACE image and write into the gitops-apps repo for dev environment Click Actions > Start. All the required \"Parameters\" are pre-populated. For \"Workspaces\", select the following: shared-workspace: PVC Select the \"ace-bar-pvc\" PVC Click Start to run the pipeline. NOTE: As no change has been made to the ACE flow application source repo, the \"git-ops\" step of this pipeline will fail per the logs from this task. The pipeline will have built an image of the ACE integrationserver with the bar file and pushed it into the internal registry. The \"create-customer-details-rest-is\" deployment pods in DEV should be bounced to pick up the newly built image. In order for that to happen, execute the following command in your terminal: oc rollout restart deploy/create-customer-details-rest-is -n dev In your OCP Console, go back to Pipelines > Pipelines and select \"ci\" Project/Namespace and select \"ace-promote-dev-stage\" pipelines to start pipeline to promote ACE IntegrationServer and Configuration resources to STAGING env if tests (ie. postman functional tests) pass. Click Actions > Start. Set the following \"Parameters\" For \"Workspaces\", select the following: shared-workspace: PVC Select the \"ace-test-pvc\" PVC Define the source to \"dev\" and target to \"staging\" Click Start to run the pipeline. View the pull request created by the pipeline for promotion to STAGING in this repo, select closed PRs to see an example PR, there will not be a PR created if the image tag on dev is the same in staging already (https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-apps/pulls). NOTE: The \"create-customer-details-rest-is\" deployment pods should be bounced to pick up the newly built image in STAGING. oc rollout restart deploy/create-customer-details-rest-is -n staging Back in your OCP console, go back to Pipelines > Pipelines and select \"ci\" Project/Namespace and select \"ace-promote-stage-prod\" pipeline to start the pipeline to promote ACE IntegrationServer and Configuration resources to PROD env if tests (ie. jmeter performance tests) pass. Click Actions > Start. Set the following \"Parameters\" For \"Workspaces\", select the following: shared-workspace: PVC Select the \"ace-test-pvc\" PVC Click Start to run the pipeline. View the pull request created by the pipeline for promotion to PROD in this repo. Select closed PRs to see an example PR, there will not be a PR created if the image tag on dev is the same in staging already (https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-apps/pulls). NOTE: The \"create-customer-details-rest-is\" deployment pods should be bounced to pick up the newly built image in PROD. oc rollout restart deploy/create-customer-details-rest-is -n prod","title":"Run ACE pipelines"},{"location":"quickstart/quickstart-ace-bkup/#ace-dashboard","text":"You can open the ACE Dashboard running in the dev namespace and see the Integration Server and Integration running in dev. Go to Installed Operators Select dev namespace Select App Connect Operator Select App Connect Dashboard from the tabs Select the ace-dashboard instance Open the Admin UI url","title":"ACE Dashboard"},{"location":"quickstart/quickstart-ace/","text":"App Connect Enterprise Quickstart \u00b6 Introduction \u00b6 The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers. Pre-requisites \u00b6 Create a custom Git Organization \u00b6 Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization. Create a Git Personal Access Token (PAT) \u00b6 Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later. Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration \u00b6 Create the cluster \u00b6 Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this IBM App Connect Enterprise Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 3 Worker Node Flavor = 8 CPU x 32 GB Use the cluster \u00b6 You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy. Install required CLIs \u00b6 Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI Select resources to deploy \u00b6 Clone the multi-tenancy-gitops repository in your Git Organization if you have not already done so and select the K8s resources to deploy in the infrastructure and services layers. Instructions \u00b6 Select a profile and delete the others from the 0-bootstrap directory. If this is your first usage of the gitops workflow, Use the single-cluster profile. GITOPS_PROFILE = \"0-bootstrap/single-cluster\" Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the IBM App Connect Enterprise recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the IBM App Connect Enterprise recipe . Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin Validation \u00b6 Check the status of the CommonService and PlatformNavigator custom resource. # Verify the Common Services instance has been deployed successfully oc get commonservice common-service -n ibm-common-services -o = jsonpath = '{.status.phase}' # Expected output = Succeeded # [Optional] If selected, verify the Platform Navigator instance has been deployed successfully oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' # Expected output = True Log in to the Platform Navigator console # Retrieve Platform Navigator Console URL oc get route -n tools integration-navigator-pn -o template --template = 'https://{{.spec.host}}' # Retrieve admin password oc extract -n ibm-common-services secrets/platform-auth-idp-credentials --keys = admin_username,admin_password --to = -","title":"ACE (CP4I)"},{"location":"quickstart/quickstart-ace/#app-connect-enterprise-quickstart","text":"","title":"App Connect Enterprise Quickstart"},{"location":"quickstart/quickstart-ace/#introduction","text":"The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers.","title":"Introduction"},{"location":"quickstart/quickstart-ace/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"quickstart/quickstart-ace/#create-a-custom-git-organization","text":"Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization.","title":"Create a custom Git Organization"},{"location":"quickstart/quickstart-ace/#create-a-git-personal-access-token-pat","text":"Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later.","title":"Create a Git Personal Access Token (PAT)"},{"location":"quickstart/quickstart-ace/#red-hat-openshift-on-ibm-cloud-cluster-with-gitops-configuration","text":"","title":"Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration"},{"location":"quickstart/quickstart-ace/#create-the-cluster","text":"Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this IBM App Connect Enterprise Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 3 Worker Node Flavor = 8 CPU x 32 GB","title":"Create the cluster"},{"location":"quickstart/quickstart-ace/#use-the-cluster","text":"You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy.","title":"Use the cluster"},{"location":"quickstart/quickstart-ace/#install-required-clis","text":"Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI","title":"Install required CLIs"},{"location":"quickstart/quickstart-ace/#select-resources-to-deploy","text":"Clone the multi-tenancy-gitops repository in your Git Organization if you have not already done so and select the K8s resources to deploy in the infrastructure and services layers.","title":"Select resources to deploy"},{"location":"quickstart/quickstart-ace/#instructions","text":"Select a profile and delete the others from the 0-bootstrap directory. If this is your first usage of the gitops workflow, Use the single-cluster profile. GITOPS_PROFILE = \"0-bootstrap/single-cluster\" Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the IBM App Connect Enterprise recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the IBM App Connect Enterprise recipe . Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin","title":"Instructions"},{"location":"quickstart/quickstart-ace/#validation","text":"Check the status of the CommonService and PlatformNavigator custom resource. # Verify the Common Services instance has been deployed successfully oc get commonservice common-service -n ibm-common-services -o = jsonpath = '{.status.phase}' # Expected output = Succeeded # [Optional] If selected, verify the Platform Navigator instance has been deployed successfully oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' # Expected output = True Log in to the Platform Navigator console # Retrieve Platform Navigator Console URL oc get route -n tools integration-navigator-pn -o template --template = 'https://{{.spec.host}}' # Retrieve admin password oc extract -n ibm-common-services secrets/platform-auth-idp-credentials --keys = admin_username,admin_password --to = -","title":"Validation"},{"location":"quickstart/quickstart-apic/","text":"IBM API Connect Quickstart \u00b6 Introduction \u00b6 The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers. By the end of this tutorial, you should have a highly available deployment of IBM API Connect on a Red Hat OpenShift Kubernetes Service on IBM Cloud as shown below. Pre-requisites \u00b6 Create a custom Git Organization \u00b6 Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization. Create a Git Personal Access Token (PAT) \u00b6 Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later. Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration \u00b6 Create the cluster \u00b6 Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this IBM API Connect quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 3 Worker Node Flavor = 16 CPU x 64 GB Use the cluster \u00b6 You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy. Install required CLIs \u00b6 Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI Select resources to deploy \u00b6 By now, you should already have a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration bootstrapped for you. If you open ArgoCD, which is the GitOps tool being installed by the Red Hat OpenShift GitOps Operator, using the Argo CD URL provided in the email shown in the previous section, you will see that your ArgoCD GitOps application has been bootstrapped to monitor the multi-tenancy-gitops repository that has been forked into the GitHub organization you provided when requesting the quickstart environment. As a result, anything you want to apply/do to your quickstart environment will be done through code changes on the aforementioned forked GitHub repository . You can see in the image above of your ArgoCD web console that the profile within the multi-tenancy-gitops repository ArgoCD has been bootstrapped with is the single-cluster . As a result, anything you want to apply/do to your quickstart environment will be done within that GitOps profile. You can also see that the ArgoCD applications for the infrastructure , services and applications layers are already created so that these will pick up any changes done, through code, at their respective layers. Instructions \u00b6 You can execute the following steps either locally by cloning the multi-tenancy-gitops GitHub repository the IBM Technology Zone environment request automation has forked into the GitHub organization you created in the Pre-requisites section above to your local workstation (and then make changes from you local workstation and commit and deliver those) or by using the new VSCode extension of GitHub Codespaces capability straight from your web browser (just open the multi-tenancy-gitops repository in your Git Organization in your browser and press the . key): Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the IBM API Connect recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the IBM API Connect recipe . Review the storage and high availability options for the IBM API Connect cluster definition explained in the IBM API Connect recipe . Commit and push changes to your git repository (the following code refers to the commands when you cloned the multi-tenancy-gitops repository locally on your workstation) git add . git commit -m \"initial bootstrap setup\" git push origin After 40 mins approximately: Make sure that the phase in which the IBM API Connect cluster is at is Ready oc get APIConnectCluster apic-cluster -n tools -o = jsonpath = '{.status.phase}' Expected output is Ready Make sure that the state in which the IBM API Connect cluster installation is at is 4/4 oc get APIConnectCluster apic-cluster -n tools -o = jsonpath = '{.status.state}' Expected output is 4/4 You can now access your IBM API Connect Cloud Manager oc get APIConnectCluster apic-cluster -n tools -o = jsonpath = '{.status.endpoints[?(@.name==\"admin\")].uri}' The credentials for logging into the IBM API Connect Cloud Manager are admin/<password> where password is stored in a secret. oc get secret apic-cluster-mgmt-admin-pass -n tools -o = jsonpath = '{.data.password}' | base64 -D Info The deployment process WILL NOT configure all of the IBM API Connect subsystems that this quickstart got deployed to work together straight away. Instead, you will need to manually complete the following IBM API Connect Cloud Manager configuration checklist or you can go to the Configure API section of the in depth IBM API Connect tutorial to create an OpenShift Pipeline that gets your recently deployed IBM API Connect instance configured automatically so that you can start working with it right away. Important Before going through the Configure API section aforementioned, you must fork the GitOps Application Repository into the GitHub organization that you created at the beginning of this IBM API Connect quickstart tutorial in the Pre-requisites section. Important The Configure API section of the in depth IBM API Connect tutorial that creates an OpenShift Pipeline that gets your recently deployed IBM API Connect instance configured automatically expects/assumes you have previously cloned the multi-tenancy-gitops GitHub repository the IBM Technology Zone environment request automation has forked into the GitHub organization you created in the Pre-requisites section above to your local workstation into $HOME/git . As a result, you will need to clone that multi-tenancy-gitops GitHub repository manually into such location or follow that Configure API section with precaution to replace such location where needed.","title":"APIC (CP4I)"},{"location":"quickstart/quickstart-apic/#ibm-api-connect-quickstart","text":"","title":"IBM API Connect Quickstart"},{"location":"quickstart/quickstart-apic/#introduction","text":"The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers. By the end of this tutorial, you should have a highly available deployment of IBM API Connect on a Red Hat OpenShift Kubernetes Service on IBM Cloud as shown below.","title":"Introduction"},{"location":"quickstart/quickstart-apic/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"quickstart/quickstart-apic/#create-a-custom-git-organization","text":"Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization.","title":"Create a custom Git Organization"},{"location":"quickstart/quickstart-apic/#create-a-git-personal-access-token-pat","text":"Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later.","title":"Create a Git Personal Access Token (PAT)"},{"location":"quickstart/quickstart-apic/#red-hat-openshift-on-ibm-cloud-cluster-with-gitops-configuration","text":"","title":"Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration"},{"location":"quickstart/quickstart-apic/#create-the-cluster","text":"Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this IBM API Connect quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 3 Worker Node Flavor = 16 CPU x 64 GB","title":"Create the cluster"},{"location":"quickstart/quickstart-apic/#use-the-cluster","text":"You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy.","title":"Use the cluster"},{"location":"quickstart/quickstart-apic/#install-required-clis","text":"Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI","title":"Install required CLIs"},{"location":"quickstart/quickstart-apic/#select-resources-to-deploy","text":"By now, you should already have a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration bootstrapped for you. If you open ArgoCD, which is the GitOps tool being installed by the Red Hat OpenShift GitOps Operator, using the Argo CD URL provided in the email shown in the previous section, you will see that your ArgoCD GitOps application has been bootstrapped to monitor the multi-tenancy-gitops repository that has been forked into the GitHub organization you provided when requesting the quickstart environment. As a result, anything you want to apply/do to your quickstart environment will be done through code changes on the aforementioned forked GitHub repository . You can see in the image above of your ArgoCD web console that the profile within the multi-tenancy-gitops repository ArgoCD has been bootstrapped with is the single-cluster . As a result, anything you want to apply/do to your quickstart environment will be done within that GitOps profile. You can also see that the ArgoCD applications for the infrastructure , services and applications layers are already created so that these will pick up any changes done, through code, at their respective layers.","title":"Select resources to deploy"},{"location":"quickstart/quickstart-apic/#instructions","text":"You can execute the following steps either locally by cloning the multi-tenancy-gitops GitHub repository the IBM Technology Zone environment request automation has forked into the GitHub organization you created in the Pre-requisites section above to your local workstation (and then make changes from you local workstation and commit and deliver those) or by using the new VSCode extension of GitHub Codespaces capability straight from your web browser (just open the multi-tenancy-gitops repository in your Git Organization in your browser and press the . key): Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the IBM API Connect recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the IBM API Connect recipe . Review the storage and high availability options for the IBM API Connect cluster definition explained in the IBM API Connect recipe . Commit and push changes to your git repository (the following code refers to the commands when you cloned the multi-tenancy-gitops repository locally on your workstation) git add . git commit -m \"initial bootstrap setup\" git push origin After 40 mins approximately: Make sure that the phase in which the IBM API Connect cluster is at is Ready oc get APIConnectCluster apic-cluster -n tools -o = jsonpath = '{.status.phase}' Expected output is Ready Make sure that the state in which the IBM API Connect cluster installation is at is 4/4 oc get APIConnectCluster apic-cluster -n tools -o = jsonpath = '{.status.state}' Expected output is 4/4 You can now access your IBM API Connect Cloud Manager oc get APIConnectCluster apic-cluster -n tools -o = jsonpath = '{.status.endpoints[?(@.name==\"admin\")].uri}' The credentials for logging into the IBM API Connect Cloud Manager are admin/<password> where password is stored in a secret. oc get secret apic-cluster-mgmt-admin-pass -n tools -o = jsonpath = '{.data.password}' | base64 -D Info The deployment process WILL NOT configure all of the IBM API Connect subsystems that this quickstart got deployed to work together straight away. Instead, you will need to manually complete the following IBM API Connect Cloud Manager configuration checklist or you can go to the Configure API section of the in depth IBM API Connect tutorial to create an OpenShift Pipeline that gets your recently deployed IBM API Connect instance configured automatically so that you can start working with it right away. Important Before going through the Configure API section aforementioned, you must fork the GitOps Application Repository into the GitHub organization that you created at the beginning of this IBM API Connect quickstart tutorial in the Pre-requisites section. Important The Configure API section of the in depth IBM API Connect tutorial that creates an OpenShift Pipeline that gets your recently deployed IBM API Connect instance configured automatically expects/assumes you have previously cloned the multi-tenancy-gitops GitHub repository the IBM Technology Zone environment request automation has forked into the GitHub organization you created in the Pre-requisites section above to your local workstation into $HOME/git . As a result, you will need to clone that multi-tenancy-gitops GitHub repository manually into such location or follow that Configure API section with precaution to replace such location where needed.","title":"Instructions"},{"location":"quickstart/quickstart-cntk/","text":"Cloud Native Toolkit Quickstart \u00b6 Quickstart Demonstration Video \u00b6 This video demonstrates how to deploy a demonstration cluster with the Quickstart configuration, following the instructions below. Introduction \u00b6 The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers. Pre-requisites \u00b6 Create a custom Git Organization \u00b6 Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization. Create a Git Personal Access Token (PAT) \u00b6 Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later. Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration \u00b6 Create the cluster \u00b6 Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this Cloud Native Toolkit Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 3 Worker Node Flavor = 8 CPU x 32 GB Use the cluster \u00b6 You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy. Install required CLIs \u00b6 Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI Select resources to deploy \u00b6 Clone the multi-tenancy-gitops repository in your Git Organization if you have not already done so and select the K8s resources to deploy in the infrastructure and services layers. Instructions \u00b6 Select a profile and delete the others from the 0-bootstrap directory. If this is your first usage of the gitops workflow, Use the single-cluster profile. GITOPS_PROFILE = \"0-bootstrap/single-cluster\" Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the Cloud Native Toolkit recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the Cloud Native Toolkit recipe . Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin","title":"Cloud Native Toolkit"},{"location":"quickstart/quickstart-cntk/#cloud-native-toolkit-quickstart","text":"","title":"Cloud Native Toolkit Quickstart"},{"location":"quickstart/quickstart-cntk/#quickstart-demonstration-video","text":"This video demonstrates how to deploy a demonstration cluster with the Quickstart configuration, following the instructions below.","title":"Quickstart Demonstration Video"},{"location":"quickstart/quickstart-cntk/#introduction","text":"The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers.","title":"Introduction"},{"location":"quickstart/quickstart-cntk/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"quickstart/quickstart-cntk/#create-a-custom-git-organization","text":"Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization.","title":"Create a custom Git Organization"},{"location":"quickstart/quickstart-cntk/#create-a-git-personal-access-token-pat","text":"Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later.","title":"Create a Git Personal Access Token (PAT)"},{"location":"quickstart/quickstart-cntk/#red-hat-openshift-on-ibm-cloud-cluster-with-gitops-configuration","text":"","title":"Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration"},{"location":"quickstart/quickstart-cntk/#create-the-cluster","text":"Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this Cloud Native Toolkit Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 3 Worker Node Flavor = 8 CPU x 32 GB","title":"Create the cluster"},{"location":"quickstart/quickstart-cntk/#use-the-cluster","text":"You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy.","title":"Use the cluster"},{"location":"quickstart/quickstart-cntk/#install-required-clis","text":"Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI","title":"Install required CLIs"},{"location":"quickstart/quickstart-cntk/#select-resources-to-deploy","text":"Clone the multi-tenancy-gitops repository in your Git Organization if you have not already done so and select the K8s resources to deploy in the infrastructure and services layers.","title":"Select resources to deploy"},{"location":"quickstart/quickstart-cntk/#instructions","text":"Select a profile and delete the others from the 0-bootstrap directory. If this is your first usage of the gitops workflow, Use the single-cluster profile. GITOPS_PROFILE = \"0-bootstrap/single-cluster\" Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the Cloud Native Toolkit recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the Cloud Native Toolkit recipe . Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin","title":"Instructions"},{"location":"quickstart/quickstart-cp4d-ws/","text":"Cloud Pak for Data - Watson Studio Quickstart \u00b6 Introduction \u00b6 The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers. Pre-requisites \u00b6 Create a custom Git Organization \u00b6 Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization. Create a Git Personal Access Token (PAT) \u00b6 Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later. Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration \u00b6 Create the cluster \u00b6 Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this Cloud Pak for Data - Watson Studio Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 5 Worker Node Flavor = 16 CPU x 64 GB NFS Size = 500 GB Use the cluster \u00b6 You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy. Install required CLIs \u00b6 Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI Select resources to deploy \u00b6 Clone the multi-tenancy-gitops repository in your Git Organization if you have not already done so and select the K8s resources to deploy in the infrastructure and services layers. Instructions \u00b6 Select a profile and delete the others from the 0-bootstrap directory. If this is your first usage of the gitops workflow, Use the single-cluster profile. GITOPS_PROFILE = \"0-bootstrap/single-cluster\" Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the Cloud Pak for Data - Watson Studio recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the Cloud Pak for Data - Watson Studio recipe . Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin Verifying the Installation \u00b6 Get the status of the control plane (lite-cr) oc get ZenService lite-cr -n tools -o jsonpath=\"{.status.zenStatus}{'\\n'}\" Cloud Pak for Data control plane is ready when the command returns Completed . If the command returns another status, wait for some more time and rerun the command. Get the status of Watson Studio (ws-cr) oc get WS ws-cr -n tools -o jsonpath=\"{.status.wsStatus} {'\\n'}\" Watson Studio is ready when the command returns Completed . Cloud Pak for Data UI \u00b6 Get the URL of the Cloud Pak for Data web client and open it in a browser. echo https://`oc get ZenService lite-cr -n tools -o jsonpath=\"{.status.url}{'\\n'}\"` The credentials for logging into the Cloud Pak for Data web client are admin/<password> where password is stored in a secret. oc extract secret/admin-user-details -n tools --keys=initial_admin_password --to=- Log into the IBM Cloud Pak for Data UI using the password from previous step. Click on the navigation menu icon on the top left corner. Click on Services menu option to expand it, then select Services catalog. Under Status, select \"Enabled\" to display only the services that are installed and enabled in Cloud Pak for Data. Notice the green colored text that says Enabled next to Watson Studio. There might be other enabled services as well.","title":"Watson Studio (CP4D)"},{"location":"quickstart/quickstart-cp4d-ws/#cloud-pak-for-data-watson-studio-quickstart","text":"","title":"Cloud Pak for Data - Watson Studio Quickstart"},{"location":"quickstart/quickstart-cp4d-ws/#introduction","text":"The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers.","title":"Introduction"},{"location":"quickstart/quickstart-cp4d-ws/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"quickstart/quickstart-cp4d-ws/#create-a-custom-git-organization","text":"Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization.","title":"Create a custom Git Organization"},{"location":"quickstart/quickstart-cp4d-ws/#create-a-git-personal-access-token-pat","text":"Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later.","title":"Create a Git Personal Access Token (PAT)"},{"location":"quickstart/quickstart-cp4d-ws/#red-hat-openshift-on-ibm-cloud-cluster-with-gitops-configuration","text":"","title":"Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration"},{"location":"quickstart/quickstart-cp4d-ws/#create-the-cluster","text":"Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this Cloud Pak for Data - Watson Studio Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 5 Worker Node Flavor = 16 CPU x 64 GB NFS Size = 500 GB","title":"Create the cluster"},{"location":"quickstart/quickstart-cp4d-ws/#use-the-cluster","text":"You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy.","title":"Use the cluster"},{"location":"quickstart/quickstart-cp4d-ws/#install-required-clis","text":"Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI","title":"Install required CLIs"},{"location":"quickstart/quickstart-cp4d-ws/#select-resources-to-deploy","text":"Clone the multi-tenancy-gitops repository in your Git Organization if you have not already done so and select the K8s resources to deploy in the infrastructure and services layers.","title":"Select resources to deploy"},{"location":"quickstart/quickstart-cp4d-ws/#instructions","text":"Select a profile and delete the others from the 0-bootstrap directory. If this is your first usage of the gitops workflow, Use the single-cluster profile. GITOPS_PROFILE = \"0-bootstrap/single-cluster\" Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the Cloud Pak for Data - Watson Studio recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the Cloud Pak for Data - Watson Studio recipe . Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin","title":"Instructions"},{"location":"quickstart/quickstart-cp4d-ws/#verifying-the-installation","text":"Get the status of the control plane (lite-cr) oc get ZenService lite-cr -n tools -o jsonpath=\"{.status.zenStatus}{'\\n'}\" Cloud Pak for Data control plane is ready when the command returns Completed . If the command returns another status, wait for some more time and rerun the command. Get the status of Watson Studio (ws-cr) oc get WS ws-cr -n tools -o jsonpath=\"{.status.wsStatus} {'\\n'}\" Watson Studio is ready when the command returns Completed .","title":"Verifying the Installation"},{"location":"quickstart/quickstart-cp4d-ws/#cloud-pak-for-data-ui","text":"Get the URL of the Cloud Pak for Data web client and open it in a browser. echo https://`oc get ZenService lite-cr -n tools -o jsonpath=\"{.status.url}{'\\n'}\"` The credentials for logging into the Cloud Pak for Data web client are admin/<password> where password is stored in a secret. oc extract secret/admin-user-details -n tools --keys=initial_admin_password --to=- Log into the IBM Cloud Pak for Data UI using the password from previous step. Click on the navigation menu icon on the top left corner. Click on Services menu option to expand it, then select Services catalog. Under Status, select \"Enabled\" to display only the services that are installed and enabled in Cloud Pak for Data. Notice the green colored text that says Enabled next to Watson Studio. There might be other enabled services as well.","title":"Cloud Pak for Data UI"},{"location":"quickstart/quickstart-cp4d/","text":"Cloud Pak for Data Quickstart \u00b6 Introduction \u00b6 The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers. Pre-requisites \u00b6 Create a custom Git Organization \u00b6 Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization. Create a Git Personal Access Token (PAT) \u00b6 Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later. Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration \u00b6 Create the cluster \u00b6 Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this Cloud Pak for Data Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 5 Worker Node Flavor = 16 CPU x 64 GB NFS Size = 500 GB Use the cluster \u00b6 You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy. Install required CLIs \u00b6 Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI Select resources to deploy \u00b6 Clone the multi-tenancy-gitops repository in your Git Organization if you have not already done so and select the K8s resources to deploy in the infrastructure and services layers. Instructions \u00b6 Select a profile and delete the others from the 0-bootstrap directory. If this is your first usage of the gitops workflow, Use the single-cluster profile. GITOPS_PROFILE = \"0-bootstrap/single-cluster\" Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the Cloud Pak for Data recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the Cloud Pak for Data recipe . Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin Verifying the Installation \u00b6 Get the status of the control plane (lite-cr) oc get ZenService lite-cr -n tools -o jsonpath=\"{.status.zenStatus}{'\\n'}\" Cloud Pak for Data control plane is ready when the command returns Completed . If the command returns another status, wait for some more time and rerun the command. Cloud Pak for Data UI \u00b6 Get the URL of the Cloud Pak for Data web client and open it in a browser. echo https://`oc get ZenService lite-cr -n tools -o jsonpath=\"{.status.url}{'\\n'}\"` The credentials for logging into the Cloud Pak for Data web client are admin/<password> where password is stored in a secret. oc extract secret/admin-user-details -n tools --keys=initial_admin_password --to=- Log into the IBM Cloud Pak for Data UI using the password from previous step. Click on the navigation menu icon on the top left corner. Click on Services menu option to expand it, then select Services catalog. The various services installed with IBM Cloud Pak for Data will be displayed.","title":"Cloud Pak for Data"},{"location":"quickstart/quickstart-cp4d/#cloud-pak-for-data-quickstart","text":"","title":"Cloud Pak for Data Quickstart"},{"location":"quickstart/quickstart-cp4d/#introduction","text":"The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers.","title":"Introduction"},{"location":"quickstart/quickstart-cp4d/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"quickstart/quickstart-cp4d/#create-a-custom-git-organization","text":"Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization.","title":"Create a custom Git Organization"},{"location":"quickstart/quickstart-cp4d/#create-a-git-personal-access-token-pat","text":"Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later.","title":"Create a Git Personal Access Token (PAT)"},{"location":"quickstart/quickstart-cp4d/#red-hat-openshift-on-ibm-cloud-cluster-with-gitops-configuration","text":"","title":"Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration"},{"location":"quickstart/quickstart-cp4d/#create-the-cluster","text":"Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this Cloud Pak for Data Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 5 Worker Node Flavor = 16 CPU x 64 GB NFS Size = 500 GB","title":"Create the cluster"},{"location":"quickstart/quickstart-cp4d/#use-the-cluster","text":"You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy.","title":"Use the cluster"},{"location":"quickstart/quickstart-cp4d/#install-required-clis","text":"Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI","title":"Install required CLIs"},{"location":"quickstart/quickstart-cp4d/#select-resources-to-deploy","text":"Clone the multi-tenancy-gitops repository in your Git Organization if you have not already done so and select the K8s resources to deploy in the infrastructure and services layers.","title":"Select resources to deploy"},{"location":"quickstart/quickstart-cp4d/#instructions","text":"Select a profile and delete the others from the 0-bootstrap directory. If this is your first usage of the gitops workflow, Use the single-cluster profile. GITOPS_PROFILE = \"0-bootstrap/single-cluster\" Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the Cloud Pak for Data recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the Cloud Pak for Data recipe . Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin","title":"Instructions"},{"location":"quickstart/quickstart-cp4d/#verifying-the-installation","text":"Get the status of the control plane (lite-cr) oc get ZenService lite-cr -n tools -o jsonpath=\"{.status.zenStatus}{'\\n'}\" Cloud Pak for Data control plane is ready when the command returns Completed . If the command returns another status, wait for some more time and rerun the command.","title":"Verifying the Installation"},{"location":"quickstart/quickstart-cp4d/#cloud-pak-for-data-ui","text":"Get the URL of the Cloud Pak for Data web client and open it in a browser. echo https://`oc get ZenService lite-cr -n tools -o jsonpath=\"{.status.url}{'\\n'}\"` The credentials for logging into the Cloud Pak for Data web client are admin/<password> where password is stored in a secret. oc extract secret/admin-user-details -n tools --keys=initial_admin_password --to=- Log into the IBM Cloud Pak for Data UI using the password from previous step. Click on the navigation menu icon on the top left corner. Click on Services menu option to expand it, then select Services catalog. The various services installed with IBM Cloud Pak for Data will be displayed.","title":"Cloud Pak for Data UI"},{"location":"quickstart/quickstart-cp4s/","text":"IBM CP4S Quickstart ON ROKS \u00b6 Introduction \u00b6 The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers. Pre-requisites \u00b6 Create a custom Git Organization \u00b6 Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization. Create a Git Personal Access Token (PAT) \u00b6 Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later. Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration \u00b6 Create the cluster \u00b6 Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this IBM MQ Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 4 Worker Node Flavor = 8 CPU x 32 GB Use the cluster \u00b6 You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy. Install required CLIs \u00b6 Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI Select resources to deploy \u00b6 Clone the multi-tenancy-gitops repository in your Git Organization if you have not already done so and select the K8s resources to deploy in the infrastructure and services layers. Instructions \u00b6 Select a profile and delete the others from the 0-bootstrap directory. If this is your first usage of the gitops workflow, Use the single-cluster profile. GITOPS_PROFILE = \"0-bootstrap/single-cluster\" Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the IBM CP4S recipe . Edit the CP4SThreatManagements custom resource instance and specify a block or file storage class (https://github.com/cloud-native-toolkit/multi-tenancy-gitops/blob/master/0-bootstrap/single-cluster/2-services/argocd/instances/ibm-cp4sthreatmanagements-instance.yaml). The default is set to managed-nfs-storage. Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the IBM MQ recipe . Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin Validation \u00b6 Check the status of the CommonService and PlatformNavigator custom resource. # Verify the Common Services instance has been deployed successfully oc get commonservice common-service -n ibm-common-services -o = jsonpath = '{.status.phase}' # Expected output = Succeeded # [Optional] If selected, verify the Platform Navigator instance has been deployed successfully oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' # Expected output = True Log in to the Platform Navigator console # Retrieve Platform Navigator Console URL oc get route -n tools integration-navigator-pn -o template --template = 'https://{{.spec.host}}' # Retrieve admin password oc extract -n ibm-common-services secrets/platform-auth-idp-credentials --keys = admin_username,admin_password --to = - Check the status of the CommonService and PlatformNavigator custom resource. oc get CP4SThreatManagement threatmgmt -n tools -o jsonpath = '{.status.conditions}' # Expected output = Cloudpak for Security Deployment is successful Before users can log in to the console for Cloud Pak for Security, an identity provider must be configured. The documentation provides further instructions. For DEMO purposes, OpenLDAP can be deployed and instructions are provided below. Download the cpctl utility #Log in to the OpenShift cluster oc login --token = <token> --server = <openshift_url> -n <namespace> #Retrieve the pod that contains the utility POD = $( oc get pod --no-headers -lrun = cp-serviceability | cut -d ' ' -f1 ) #Copy the utility locally oc cp $POD :/opt/bin/<operatingsystem>/cpctl ./cpctl && chmod +x ./cpctl Install OpenLDAP Start a session ./cpctl load Install OpenLDAP cpctl tools deploy_openldap --token $( oc whoami -t ) --ldap_usernames 'adminUser,user1,user2,user3' --ldap_password cloudpak Initial user log in Retrieve Cloud Pak for Security Console URL oc get route isc-route-default --no-headers -n <CP4S_NAMESPACE> | awk '{print $2}' Log in with the user ID and password specified (ie. adminUser / cloudpak ).","title":"Security (CP4S)"},{"location":"quickstart/quickstart-cp4s/#ibm-cp4s-quickstart-on-roks","text":"","title":"IBM CP4S Quickstart ON ROKS"},{"location":"quickstart/quickstart-cp4s/#introduction","text":"The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers.","title":"Introduction"},{"location":"quickstart/quickstart-cp4s/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"quickstart/quickstart-cp4s/#create-a-custom-git-organization","text":"Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization.","title":"Create a custom Git Organization"},{"location":"quickstart/quickstart-cp4s/#create-a-git-personal-access-token-pat","text":"Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later.","title":"Create a Git Personal Access Token (PAT)"},{"location":"quickstart/quickstart-cp4s/#red-hat-openshift-on-ibm-cloud-cluster-with-gitops-configuration","text":"","title":"Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration"},{"location":"quickstart/quickstart-cp4s/#create-the-cluster","text":"Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this IBM MQ Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 4 Worker Node Flavor = 8 CPU x 32 GB","title":"Create the cluster"},{"location":"quickstart/quickstart-cp4s/#use-the-cluster","text":"You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy.","title":"Use the cluster"},{"location":"quickstart/quickstart-cp4s/#install-required-clis","text":"Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI","title":"Install required CLIs"},{"location":"quickstart/quickstart-cp4s/#select-resources-to-deploy","text":"Clone the multi-tenancy-gitops repository in your Git Organization if you have not already done so and select the K8s resources to deploy in the infrastructure and services layers.","title":"Select resources to deploy"},{"location":"quickstart/quickstart-cp4s/#instructions","text":"Select a profile and delete the others from the 0-bootstrap directory. If this is your first usage of the gitops workflow, Use the single-cluster profile. GITOPS_PROFILE = \"0-bootstrap/single-cluster\" Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the IBM CP4S recipe . Edit the CP4SThreatManagements custom resource instance and specify a block or file storage class (https://github.com/cloud-native-toolkit/multi-tenancy-gitops/blob/master/0-bootstrap/single-cluster/2-services/argocd/instances/ibm-cp4sthreatmanagements-instance.yaml). The default is set to managed-nfs-storage. Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the IBM MQ recipe . Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin","title":"Instructions"},{"location":"quickstart/quickstart-cp4s/#validation","text":"Check the status of the CommonService and PlatformNavigator custom resource. # Verify the Common Services instance has been deployed successfully oc get commonservice common-service -n ibm-common-services -o = jsonpath = '{.status.phase}' # Expected output = Succeeded # [Optional] If selected, verify the Platform Navigator instance has been deployed successfully oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' # Expected output = True Log in to the Platform Navigator console # Retrieve Platform Navigator Console URL oc get route -n tools integration-navigator-pn -o template --template = 'https://{{.spec.host}}' # Retrieve admin password oc extract -n ibm-common-services secrets/platform-auth-idp-credentials --keys = admin_username,admin_password --to = - Check the status of the CommonService and PlatformNavigator custom resource. oc get CP4SThreatManagement threatmgmt -n tools -o jsonpath = '{.status.conditions}' # Expected output = Cloudpak for Security Deployment is successful Before users can log in to the console for Cloud Pak for Security, an identity provider must be configured. The documentation provides further instructions. For DEMO purposes, OpenLDAP can be deployed and instructions are provided below. Download the cpctl utility #Log in to the OpenShift cluster oc login --token = <token> --server = <openshift_url> -n <namespace> #Retrieve the pod that contains the utility POD = $( oc get pod --no-headers -lrun = cp-serviceability | cut -d ' ' -f1 ) #Copy the utility locally oc cp $POD :/opt/bin/<operatingsystem>/cpctl ./cpctl && chmod +x ./cpctl Install OpenLDAP Start a session ./cpctl load Install OpenLDAP cpctl tools deploy_openldap --token $( oc whoami -t ) --ldap_usernames 'adminUser,user1,user2,user3' --ldap_password cloudpak Initial user log in Retrieve Cloud Pak for Security Console URL oc get route isc-route-default --no-headers -n <CP4S_NAMESPACE> | awk '{print $2}' Log in with the user ID and password specified (ie. adminUser / cloudpak ).","title":"Validation"},{"location":"quickstart/quickstart-instana-agent/","text":"Instana Agent Quickstart \u00b6 Introduction \u00b6 The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers. Pre-requisites \u00b6 Create a custom Git Organization \u00b6 Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization. Create a Git Personal Access Token (PAT) \u00b6 Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later. Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration \u00b6 Create the cluster \u00b6 Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this Instana Agent Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 3 Worker Node Flavor = 8 CPU x 32 GB Use the cluster \u00b6 You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy. Install required CLIs \u00b6 Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI Select resources to deploy \u00b6 Clone the multi-tenancy-gitops repository in your Git Organization if you have not already done so and select the K8s resources to deploy in the infrastructure and services layers. Instructions \u00b6 Select a profile and delete the others from the 0-bootstrap directory. If this is your first usage of the gitops workflow, Use the single-cluster profile. GITOPS_PROFILE = \"0-bootstrap/single-cluster\" Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the Instana Agent recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the Instana Agent recipe . Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin","title":"Instana Agent"},{"location":"quickstart/quickstart-instana-agent/#instana-agent-quickstart","text":"","title":"Instana Agent Quickstart"},{"location":"quickstart/quickstart-instana-agent/#introduction","text":"The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers.","title":"Introduction"},{"location":"quickstart/quickstart-instana-agent/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"quickstart/quickstart-instana-agent/#create-a-custom-git-organization","text":"Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization.","title":"Create a custom Git Organization"},{"location":"quickstart/quickstart-instana-agent/#create-a-git-personal-access-token-pat","text":"Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later.","title":"Create a Git Personal Access Token (PAT)"},{"location":"quickstart/quickstart-instana-agent/#red-hat-openshift-on-ibm-cloud-cluster-with-gitops-configuration","text":"","title":"Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration"},{"location":"quickstart/quickstart-instana-agent/#create-the-cluster","text":"Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this Instana Agent Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 3 Worker Node Flavor = 8 CPU x 32 GB","title":"Create the cluster"},{"location":"quickstart/quickstart-instana-agent/#use-the-cluster","text":"You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy.","title":"Use the cluster"},{"location":"quickstart/quickstart-instana-agent/#install-required-clis","text":"Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI","title":"Install required CLIs"},{"location":"quickstart/quickstart-instana-agent/#select-resources-to-deploy","text":"Clone the multi-tenancy-gitops repository in your Git Organization if you have not already done so and select the K8s resources to deploy in the infrastructure and services layers.","title":"Select resources to deploy"},{"location":"quickstart/quickstart-instana-agent/#instructions","text":"Select a profile and delete the others from the 0-bootstrap directory. If this is your first usage of the gitops workflow, Use the single-cluster profile. GITOPS_PROFILE = \"0-bootstrap/single-cluster\" Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the Instana Agent recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the Instana Agent recipe . Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin","title":"Instructions"},{"location":"quickstart/quickstart-mq-bkup/","text":"IBM MQ Quickstart \u00b6 Introduction \u00b6 The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers. The following instructions is from the README.md in this repository: https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-mq . Pre-requisites \u00b6 Create a custom Git Organization \u00b6 Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization. Create a Git Personal Access Token (PAT) \u00b6 Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later. OpenShift on IBM Cloud \u00b6 Navigate to the IBM Technology Zone In your browser, navigate to the IBM Technology Zone to create your cluster. You will need to sign-in with your IBM credentials if your browser has not authenticated. You will need to read and accept Terms and conditions to proceed to the IBM Technology Zone. Reserve a cluster You can reserve a cluster immediately on IBM Cloud. You'll see the following options: Click the Reserve now radio button. It takes between 30 and 60 minutes to provision a cluster. Moreover, once a cluster has been provisioned, by default, it will be deleted after 3 days -- unless you renew it. It can therefore be helpful to schedule the environment to be created later if you are not going to be using it for a while -- such as for a future demonstration or classroom session. Come back and explore the Schedule for later option another time. Complete reservation details - step 1 You will be asked to complete your reservations details in two steps. Firstly, when you make a cluster reservation, you can supply a set of details that describe: The cluster name The purpose of the reservation from a pre-defined set A description, which will appear for a Practice/Self-Education reservation Notes to help you identify this reservation In the following web page: Complete the details as follows: Type a helpful Name such as My cluster 1 Select Practice/Self-Education from the Purpose drop-down list Type a helpful Purpose Description such as Education Do not select Preferred Geography yet; we will do that in a moment Add some helpful Notes such as Learn how to build a cloud native production reference deployment using a CloudPak and OpenShift on IBM Cloud Complete reservation details - step 2 The second step in registration is to specify the physical details of the compute infrastructure for your cluster. When you make a cluster reservation, you can supply a set of details that describe: The cluster's geographical location The version of OpenShift Container Platform used by the cluster The number of CPUs in the cluster and their memory size The number of Kubernetes worker nodes in the cluster The NFS storage size for the cluster The End date and time for the cluster In the following web page: Complete the details as follows: Select your Preferred Geography such as London 5 , HongKong 2 or San Jose 04 Under End date and time , select a date and specify the time along with its time zone. Select Worker Node Count as 3 from the drop-down list Select Worker Node Flavor as 8CPU x 32GB from the drop-down list Select NFS Size as 500 GB from the drop-down list Select OpenShift Version as 4.8 from the drop-down-list Info By default, your cluster will be reserved for three days , after which time it will be deleted. This is more than enough time to complete this tutorial. If required, you can increase the cluster lifetime by configuring the End date and time . Don't do this unless you really need to do so; as you'll learn in the tutorial, the use of GitOps makes it quick and easy to provision a cluster when you need to do so. Create the cluster Check your reservation details and press Submit when you're ready to create the cluster: The cluster provisioning process will take between 30 and 60 minutes to complete on IBM Cloud. Provisioning email As the provisioning process starts, you will receive an email to confirm that the provisioning process has started: Checking the cluster provisioning status You can also view the status of your provisioning request on IBM Technology Zone. Click on My reservations to see: Note: The Status: field will be Ready when the cluster has been provisioned. The creation and expiry date-time for your cluster. You can return to the reservation at any time to get the details of your cluster. Cluster provisioned email You will receive another email once your cluster is ready: Your cluster is now provisioned and ready to use. Note the expiry date-time. Navigate to your cluster You can use the information in the notification email to access your cluster. Click on the Cluster URL link to login to IBM Cloud and locate your OpenShift cluster: Note: The Cluster ID that has been assigned by IBM Cloud. The Zone matches your requested geography. The OCP Version matches your requested version of OCP. The Node status identifies the 3 worker nodes in the Normal state. Feel free to explore this web page. Receiving cluster deletion email and extending your cluster's lifetime When your cluster is within 3 days of deletion, you will receive a daily email: Note: You can extend your cluster if required. This option may be limited to certain types of reservations. The My reservations page allows you to manage your cluster reservation Install required CLIs \u00b6 Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI Deploy the IBM MQ operator and its pre-requisites \u00b6 Log in with the Github CLI gh auth login Setup a local git directory to clone all the git repositories mkdir -p mq-quickstart Download sealed-secrets-ibm-demo-key.yaml and save in the default location ~/Downloads/sealed-secrets-ibm-demo-key.yaml . You can override the location when running the script with SEALED_SECRET_KEY_FILE . Remember do not check this file to git. Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Run the bootstrap script, specify the git org GIT_ORG and the output directory to clone all repos OUTPUT_DIR . You can use DEBUG=true for verbose output. Note, the deployment of all selected resources will take 30 - 45 minutes. curl -sfL https://raw.githubusercontent.com/cloud-native-toolkit-demos/multi-tenancy-gitops-mq/ocp47-2021-2/scripts/bootstrap.sh | DEBUG = true GIT_ORG = <YOUR_GIT_ORG> OUTPUT_DIR = mq-quickstart bash Open the ArgoCD UI from the OpenShift Console, then use admin as the username and password should have printed in the previous command You can open the output directory with VSCode and the set of required git repositories has been forked into your GitHub Organization. code mq-quickstart At this point, you can already demonstrate the value of using a GitOps approach (OpenShift Pipelines) to declaratively deploy the IBM MQ operator and its dependencies. Execute pipelines to deploy a Queue Manager and Spring application to write messages to the queue. \u00b6 Before running the pipelines, verify the Platform Navigator and Common Services instances have been deployed successfully. oc get commonservice common-service -n ibm-common-services -o = jsonpath = '{.status.phase}' # Expected output = Succeeded oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' # Expected output = True Configure the cluster with your GitHub Personal Access Token (PAT), update the gitops-repo Configmap which will be used by the pipeline to populate the forked gitops repository and add the artifactory-access Secret to the ci namespace. Specify values for the GIT_USER , GIT_TOKEN and GIT_ORG environment variables. cd mq-quickstart/gitops-3-apps/scripts curl -sfL https://raw.githubusercontent.com/cloud-native-toolkit-demos/multi-tenancy-gitops-apps/ocp47-2021-2/scripts/mq-kubeseal.sh | DEBUG = true GIT_USER = <GIT_USER> GIT_TOKEN = <GIT_TOKEN> GIT_ORG = <GIT_ORG> bash As this script executes it will issue a git diff to allow you to review its customizations. - Type q when you're finished examining the changes; the script will continue to completion. Run a pipeline to build and deploy a Queue Manager Log in to the OpenShift Web Console. Select Pipelines > Pipelines view in the ci namespace. Click the mq-infra-dev pipeline and select Actions > Start. Provide the HTTPS URL for the mq-infra repository in your Git Organization. Run a pipeline to build and deploy a Spring application Log in to the OpenShift Web Console. Select Pipelines > Pipelines view in the ci namespace. Click the mq-spring-app-dev pipeline and select Actions > Start. Provide the HTTPS URL for the mq-spring-app repository in your Git Organization.","title":"IBM MQ Quickstart"},{"location":"quickstart/quickstart-mq-bkup/#ibm-mq-quickstart","text":"","title":"IBM MQ Quickstart"},{"location":"quickstart/quickstart-mq-bkup/#introduction","text":"The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers. The following instructions is from the README.md in this repository: https://github.com/cloud-native-toolkit-demos/multi-tenancy-gitops-mq .","title":"Introduction"},{"location":"quickstart/quickstart-mq-bkup/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"quickstart/quickstart-mq-bkup/#create-a-custom-git-organization","text":"Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization.","title":"Create a custom Git Organization"},{"location":"quickstart/quickstart-mq-bkup/#create-a-git-personal-access-token-pat","text":"Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later.","title":"Create a Git Personal Access Token (PAT)"},{"location":"quickstart/quickstart-mq-bkup/#openshift-on-ibm-cloud","text":"Navigate to the IBM Technology Zone In your browser, navigate to the IBM Technology Zone to create your cluster. You will need to sign-in with your IBM credentials if your browser has not authenticated. You will need to read and accept Terms and conditions to proceed to the IBM Technology Zone. Reserve a cluster You can reserve a cluster immediately on IBM Cloud. You'll see the following options: Click the Reserve now radio button. It takes between 30 and 60 minutes to provision a cluster. Moreover, once a cluster has been provisioned, by default, it will be deleted after 3 days -- unless you renew it. It can therefore be helpful to schedule the environment to be created later if you are not going to be using it for a while -- such as for a future demonstration or classroom session. Come back and explore the Schedule for later option another time. Complete reservation details - step 1 You will be asked to complete your reservations details in two steps. Firstly, when you make a cluster reservation, you can supply a set of details that describe: The cluster name The purpose of the reservation from a pre-defined set A description, which will appear for a Practice/Self-Education reservation Notes to help you identify this reservation In the following web page: Complete the details as follows: Type a helpful Name such as My cluster 1 Select Practice/Self-Education from the Purpose drop-down list Type a helpful Purpose Description such as Education Do not select Preferred Geography yet; we will do that in a moment Add some helpful Notes such as Learn how to build a cloud native production reference deployment using a CloudPak and OpenShift on IBM Cloud Complete reservation details - step 2 The second step in registration is to specify the physical details of the compute infrastructure for your cluster. When you make a cluster reservation, you can supply a set of details that describe: The cluster's geographical location The version of OpenShift Container Platform used by the cluster The number of CPUs in the cluster and their memory size The number of Kubernetes worker nodes in the cluster The NFS storage size for the cluster The End date and time for the cluster In the following web page: Complete the details as follows: Select your Preferred Geography such as London 5 , HongKong 2 or San Jose 04 Under End date and time , select a date and specify the time along with its time zone. Select Worker Node Count as 3 from the drop-down list Select Worker Node Flavor as 8CPU x 32GB from the drop-down list Select NFS Size as 500 GB from the drop-down list Select OpenShift Version as 4.8 from the drop-down-list Info By default, your cluster will be reserved for three days , after which time it will be deleted. This is more than enough time to complete this tutorial. If required, you can increase the cluster lifetime by configuring the End date and time . Don't do this unless you really need to do so; as you'll learn in the tutorial, the use of GitOps makes it quick and easy to provision a cluster when you need to do so. Create the cluster Check your reservation details and press Submit when you're ready to create the cluster: The cluster provisioning process will take between 30 and 60 minutes to complete on IBM Cloud. Provisioning email As the provisioning process starts, you will receive an email to confirm that the provisioning process has started: Checking the cluster provisioning status You can also view the status of your provisioning request on IBM Technology Zone. Click on My reservations to see: Note: The Status: field will be Ready when the cluster has been provisioned. The creation and expiry date-time for your cluster. You can return to the reservation at any time to get the details of your cluster. Cluster provisioned email You will receive another email once your cluster is ready: Your cluster is now provisioned and ready to use. Note the expiry date-time. Navigate to your cluster You can use the information in the notification email to access your cluster. Click on the Cluster URL link to login to IBM Cloud and locate your OpenShift cluster: Note: The Cluster ID that has been assigned by IBM Cloud. The Zone matches your requested geography. The OCP Version matches your requested version of OCP. The Node status identifies the 3 worker nodes in the Normal state. Feel free to explore this web page. Receiving cluster deletion email and extending your cluster's lifetime When your cluster is within 3 days of deletion, you will receive a daily email: Note: You can extend your cluster if required. This option may be limited to certain types of reservations. The My reservations page allows you to manage your cluster reservation","title":"OpenShift on IBM Cloud"},{"location":"quickstart/quickstart-mq-bkup/#install-required-clis","text":"Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI","title":"Install required CLIs"},{"location":"quickstart/quickstart-mq-bkup/#deploy-the-ibm-mq-operator-and-its-pre-requisites","text":"Log in with the Github CLI gh auth login Setup a local git directory to clone all the git repositories mkdir -p mq-quickstart Download sealed-secrets-ibm-demo-key.yaml and save in the default location ~/Downloads/sealed-secrets-ibm-demo-key.yaml . You can override the location when running the script with SEALED_SECRET_KEY_FILE . Remember do not check this file to git. Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Run the bootstrap script, specify the git org GIT_ORG and the output directory to clone all repos OUTPUT_DIR . You can use DEBUG=true for verbose output. Note, the deployment of all selected resources will take 30 - 45 minutes. curl -sfL https://raw.githubusercontent.com/cloud-native-toolkit-demos/multi-tenancy-gitops-mq/ocp47-2021-2/scripts/bootstrap.sh | DEBUG = true GIT_ORG = <YOUR_GIT_ORG> OUTPUT_DIR = mq-quickstart bash Open the ArgoCD UI from the OpenShift Console, then use admin as the username and password should have printed in the previous command You can open the output directory with VSCode and the set of required git repositories has been forked into your GitHub Organization. code mq-quickstart At this point, you can already demonstrate the value of using a GitOps approach (OpenShift Pipelines) to declaratively deploy the IBM MQ operator and its dependencies.","title":"Deploy the IBM MQ operator and its pre-requisites"},{"location":"quickstart/quickstart-mq-bkup/#execute-pipelines-to-deploy-a-queue-manager-and-spring-application-to-write-messages-to-the-queue","text":"Before running the pipelines, verify the Platform Navigator and Common Services instances have been deployed successfully. oc get commonservice common-service -n ibm-common-services -o = jsonpath = '{.status.phase}' # Expected output = Succeeded oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' # Expected output = True Configure the cluster with your GitHub Personal Access Token (PAT), update the gitops-repo Configmap which will be used by the pipeline to populate the forked gitops repository and add the artifactory-access Secret to the ci namespace. Specify values for the GIT_USER , GIT_TOKEN and GIT_ORG environment variables. cd mq-quickstart/gitops-3-apps/scripts curl -sfL https://raw.githubusercontent.com/cloud-native-toolkit-demos/multi-tenancy-gitops-apps/ocp47-2021-2/scripts/mq-kubeseal.sh | DEBUG = true GIT_USER = <GIT_USER> GIT_TOKEN = <GIT_TOKEN> GIT_ORG = <GIT_ORG> bash As this script executes it will issue a git diff to allow you to review its customizations. - Type q when you're finished examining the changes; the script will continue to completion. Run a pipeline to build and deploy a Queue Manager Log in to the OpenShift Web Console. Select Pipelines > Pipelines view in the ci namespace. Click the mq-infra-dev pipeline and select Actions > Start. Provide the HTTPS URL for the mq-infra repository in your Git Organization. Run a pipeline to build and deploy a Spring application Log in to the OpenShift Web Console. Select Pipelines > Pipelines view in the ci namespace. Click the mq-spring-app-dev pipeline and select Actions > Start. Provide the HTTPS URL for the mq-spring-app repository in your Git Organization.","title":"Execute pipelines to deploy a Queue Manager and Spring application to write messages to the queue."},{"location":"quickstart/quickstart-mq/","text":"IBM MQ Quickstart \u00b6 Introduction \u00b6 The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers. Pre-requisites \u00b6 Create a custom Git Organization \u00b6 Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization. Create a Git Personal Access Token (PAT) \u00b6 Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later. Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration \u00b6 Create the cluster \u00b6 Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this IBM MQ Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 3 Worker Node Flavor = 8 CPU x 32 GB Use the cluster \u00b6 You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy. Install required CLIs \u00b6 Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI Select resources to deploy \u00b6 Clone the multi-tenancy-gitops repository in your Git Organization if you have not already done so and select the K8s resources to deploy in the infrastructure and services layers. Instructions \u00b6 Select a profile and delete the others from the 0-bootstrap directory. If this is your first usage of the gitops workflow, Use the single-cluster profile. GITOPS_PROFILE = \"0-bootstrap/single-cluster\" Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the IBM MQ recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the IBM MQ recipe . Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin Validation \u00b6 Check the status of the CommonService and PlatformNavigator custom resource. # Verify the Common Services instance has been deployed successfully oc get commonservice common-service -n ibm-common-services -o = jsonpath = '{.status.phase}' # Expected output = Succeeded # [Optional] If selected, verify the Platform Navigator instance has been deployed successfully oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' # Expected output = True Log in to the Platform Navigator console # Retrieve Platform Navigator Console URL oc get route -n tools integration-navigator-pn -o template --template = 'https://{{.spec.host}}' # Retrieve admin password oc extract -n ibm-common-services secrets/platform-auth-idp-credentials --keys = admin_username,admin_password --to = -","title":"MQ (CP4I)"},{"location":"quickstart/quickstart-mq/#ibm-mq-quickstart","text":"","title":"IBM MQ Quickstart"},{"location":"quickstart/quickstart-mq/#introduction","text":"The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers.","title":"Introduction"},{"location":"quickstart/quickstart-mq/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"quickstart/quickstart-mq/#create-a-custom-git-organization","text":"Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization.","title":"Create a custom Git Organization"},{"location":"quickstart/quickstart-mq/#create-a-git-personal-access-token-pat","text":"Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later.","title":"Create a Git Personal Access Token (PAT)"},{"location":"quickstart/quickstart-mq/#red-hat-openshift-on-ibm-cloud-cluster-with-gitops-configuration","text":"","title":"Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration"},{"location":"quickstart/quickstart-mq/#create-the-cluster","text":"Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this IBM MQ Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 3 Worker Node Flavor = 8 CPU x 32 GB","title":"Create the cluster"},{"location":"quickstart/quickstart-mq/#use-the-cluster","text":"You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy.","title":"Use the cluster"},{"location":"quickstart/quickstart-mq/#install-required-clis","text":"Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI","title":"Install required CLIs"},{"location":"quickstart/quickstart-mq/#select-resources-to-deploy","text":"Clone the multi-tenancy-gitops repository in your Git Organization if you have not already done so and select the K8s resources to deploy in the infrastructure and services layers.","title":"Select resources to deploy"},{"location":"quickstart/quickstart-mq/#instructions","text":"Select a profile and delete the others from the 0-bootstrap directory. If this is your first usage of the gitops workflow, Use the single-cluster profile. GITOPS_PROFILE = \"0-bootstrap/single-cluster\" Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the IBM MQ recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the IBM MQ recipe . Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin","title":"Instructions"},{"location":"quickstart/quickstart-mq/#validation","text":"Check the status of the CommonService and PlatformNavigator custom resource. # Verify the Common Services instance has been deployed successfully oc get commonservice common-service -n ibm-common-services -o = jsonpath = '{.status.phase}' # Expected output = Succeeded # [Optional] If selected, verify the Platform Navigator instance has been deployed successfully oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' # Expected output = True Log in to the Platform Navigator console # Retrieve Platform Navigator Console URL oc get route -n tools integration-navigator-pn -o template --template = 'https://{{.spec.host}}' # Retrieve admin password oc extract -n ibm-common-services secrets/platform-auth-idp-credentials --keys = admin_username,admin_password --to = -","title":"Validation"},{"location":"quickstart/quickstart-process-mining/","text":"IBM Process Mining Quickstart \u00b6 Introduction \u00b6 The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers. Pre-requisites \u00b6 Create a custom Git Organization \u00b6 Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization. Create a Git Personal Access Token (PAT) \u00b6 Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later. Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration \u00b6 Create the cluster \u00b6 Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this IBM Process Mining Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 3 Worker Node Flavor = 16 CPU x 64 GB Use the cluster \u00b6 You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy. Install required CLIs \u00b6 Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI Select resources to deploy \u00b6 By now, you should already have a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration bootstrapped for you. If you open ArgoCD, which is the GitOps tool being installed by the Red Hat OpenShift GitOps Operator, using the Argo CD URL provided in the email shown in the previous section, you will see that your ArgoCD GitOps application has been bootstrapped to monitor the multi-tenancy-gitops repository that has been forked into the GitHub organization you provided when requesting the quickstart environment. As a result, anything you want to apply/do to your quickstart environment will be done through code changes on the aforementioned forked GitHub repository . You can see in the image above of your ArgoCD web console that the profile within the multi-tenancy-gitops repository ArgoCD has been bootstrapped with is the single-cluster . As a result, anything you want to apply/do to your quickstart environment will be done within that GitOps profile. You can also see that the ArgoCD applications for the infrastructure , services and applications layers are already created so that these will pick up any changes done, through code, at their respective layers. Instructions \u00b6 You can execute the following steps either locally by cloning the multi-tenancy-gitops GitHub repository the IBM Technology Zone environment request automation has forked into the GitHub organization you created in the Pre-requisites section above to your local workstation (and then make changes from you local workstation and commit and deliver those) or by using the new VSCode extension of GitHub Codespaces capability straight from your web browser (just open the multi-tenancy-gitops repository in your Git Organization in your browser and press the . key): Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the IBM Process Mining recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the IBM Process Mining recipe . Commit and push changes to your git repository (the following code refers to the commands when you cloned the multi-tenancy-gitops repository locally on your workstation) git add . git commit -m \"initial bootstrap setup\" git push origin You can go to the Deploy Process Mining section of the in-depth IBM Process Mining tutorial here to make sure your IBM Process Mining quickstart deployment is successful and see the instructions for accessing the IBM Process Mining User Interface.","title":"Process Mining (CP4BA)"},{"location":"quickstart/quickstart-process-mining/#ibm-process-mining-quickstart","text":"","title":"IBM Process Mining Quickstart"},{"location":"quickstart/quickstart-process-mining/#introduction","text":"The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers.","title":"Introduction"},{"location":"quickstart/quickstart-process-mining/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"quickstart/quickstart-process-mining/#create-a-custom-git-organization","text":"Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization.","title":"Create a custom Git Organization"},{"location":"quickstart/quickstart-process-mining/#create-a-git-personal-access-token-pat","text":"Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later.","title":"Create a Git Personal Access Token (PAT)"},{"location":"quickstart/quickstart-process-mining/#red-hat-openshift-on-ibm-cloud-cluster-with-gitops-configuration","text":"","title":"Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration"},{"location":"quickstart/quickstart-process-mining/#create-the-cluster","text":"Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit . Note For this IBM Process Mining Quickstart we recommend you to request the Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration with the following size: Worker Node Count = 3 Worker Node Flavor = 16 CPU x 64 GB","title":"Create the cluster"},{"location":"quickstart/quickstart-process-mining/#use-the-cluster","text":"You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy.","title":"Use the cluster"},{"location":"quickstart/quickstart-process-mining/#install-required-clis","text":"Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI","title":"Install required CLIs"},{"location":"quickstart/quickstart-process-mining/#select-resources-to-deploy","text":"By now, you should already have a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration bootstrapped for you. If you open ArgoCD, which is the GitOps tool being installed by the Red Hat OpenShift GitOps Operator, using the Argo CD URL provided in the email shown in the previous section, you will see that your ArgoCD GitOps application has been bootstrapped to monitor the multi-tenancy-gitops repository that has been forked into the GitHub organization you provided when requesting the quickstart environment. As a result, anything you want to apply/do to your quickstart environment will be done through code changes on the aforementioned forked GitHub repository . You can see in the image above of your ArgoCD web console that the profile within the multi-tenancy-gitops repository ArgoCD has been bootstrapped with is the single-cluster . As a result, anything you want to apply/do to your quickstart environment will be done within that GitOps profile. You can also see that the ArgoCD applications for the infrastructure , services and applications layers are already created so that these will pick up any changes done, through code, at their respective layers.","title":"Select resources to deploy"},{"location":"quickstart/quickstart-process-mining/#instructions","text":"You can execute the following steps either locally by cloning the multi-tenancy-gitops GitHub repository the IBM Technology Zone environment request automation has forked into the GitHub organization you created in the Pre-requisites section above to your local workstation (and then make changes from you local workstation and commit and deliver those) or by using the new VSCode extension of GitHub Codespaces capability straight from your web browser (just open the multi-tenancy-gitops repository in your Git Organization in your browser and press the . key): Review the Infrastructure layer kustomization.yaml and un-comment the resources to deploy to match the IBM Process Mining recipe . Review the Services layer kustomization.yaml and un-comment the resources to deploy to match the IBM Process Mining recipe . Commit and push changes to your git repository (the following code refers to the commands when you cloned the multi-tenancy-gitops repository locally on your workstation) git add . git commit -m \"initial bootstrap setup\" git push origin You can go to the Deploy Process Mining section of the in-depth IBM Process Mining tutorial here to make sure your IBM Process Mining quickstart deployment is successful and see the instructions for accessing the IBM Process Mining User Interface.","title":"Instructions"},{"location":"references/prerequisites/instructions/","text":"Installing Prerequisites \u00b6 Audience : Application developers, Administrators Overview \u00b6 The production reference guide uses many different command line tools to allow you interact with the cluster from your local machine. Tools such as oc , git and jq help you to understand how a cloud native deployment works in a hands-on manner. In this topic, we're going to: Learn how to install the prerequisite command line tools macOS Linux Windows Where appropriate, click on the macOS , Linux or Windows horizontal tab to see the corresponding operating system instructions. Installing a package manager \u00b6 A operating system specific package manager is used to install the prerequisite tools on the different platforms. macOS On macOS, we use the homebrew package manager to install the prerequisite tools On the terminal command line, issue the following command to see if homebrew is installed: brew --version You should see the following output if homebrew is installed: Homebrew 3 .3.5 Homebrew/homebrew-core ( git revision 63e2388f12d ; last commit 2021 -11-24 ) Homebrew/homebrew-cask ( git revision b8049f243b ; last commit 2021 -11-24 ) If it is not installed, use the Install Homebrew instructions. Linux TBD Windows TBD Installing git \u00b6 The git command line tool is used extensively throughout the tutorial to interact with git and GitHub. We perform GitOps operations using commands such as git push to make a change to a cluster. Install git using the following command: macOS brew install git Linux TBD Windows TBD Learn more about git . Installing node , nvm & npm \u00b6 This trio of technologies is used to install various node.js based utilities used in the tutorial. Install node using the following command: macOS brew install node Linux TBD Windows TBD It is advisable to use nvm (node version manager) to allow you to install and use different versions of node.js . Install nvm using the following command: macOS brew install nvm Linux TBD Windows TBD npm is the package manager used to install different node.js packages. Install npm using the following command: macOS brew install npm Linux TBD Windows TBD Learn more about npm . Installing jq \u00b6 jq is a utility used to manipulate the JSON output typically returned by the oc command when reading or updating Kubernetes resources. Install jq using the following command: macOS brew install jq Linux TBD Windows TBD Learn more about jq . Installing tree \u00b6 tree is a command-line tool display a file system structure by displaying folders and files in tree-like structure. Install tree using the following command: macOS brew install tree Linux TBD Windows TBD Learn more about tree . Installing oc \u00b6 The oc command is used to interact with an OpenShift cluster. It is effectively a synonym for the kubectl command with some additional options. Use these instructions to get the latest version of oc . You can use oc version to confirm that you have: Client Version: 4.6 or higher Server Version: 4.7 or higher The version of oc must be compatible with the version of the cluster you previously created.","title":"Prerequisites"},{"location":"references/prerequisites/instructions/#installing-prerequisites","text":"Audience : Application developers, Administrators","title":"Installing Prerequisites"},{"location":"references/prerequisites/instructions/#overview","text":"The production reference guide uses many different command line tools to allow you interact with the cluster from your local machine. Tools such as oc , git and jq help you to understand how a cloud native deployment works in a hands-on manner. In this topic, we're going to: Learn how to install the prerequisite command line tools macOS Linux Windows Where appropriate, click on the macOS , Linux or Windows horizontal tab to see the corresponding operating system instructions.","title":"Overview"},{"location":"references/prerequisites/instructions/#installing-a-package-manager","text":"A operating system specific package manager is used to install the prerequisite tools on the different platforms. macOS On macOS, we use the homebrew package manager to install the prerequisite tools On the terminal command line, issue the following command to see if homebrew is installed: brew --version You should see the following output if homebrew is installed: Homebrew 3 .3.5 Homebrew/homebrew-core ( git revision 63e2388f12d ; last commit 2021 -11-24 ) Homebrew/homebrew-cask ( git revision b8049f243b ; last commit 2021 -11-24 ) If it is not installed, use the Install Homebrew instructions. Linux TBD Windows TBD","title":"Installing a package manager"},{"location":"references/prerequisites/instructions/#installing-git","text":"The git command line tool is used extensively throughout the tutorial to interact with git and GitHub. We perform GitOps operations using commands such as git push to make a change to a cluster. Install git using the following command: macOS brew install git Linux TBD Windows TBD Learn more about git .","title":"Installing git"},{"location":"references/prerequisites/instructions/#installing-node-nvm-npm","text":"This trio of technologies is used to install various node.js based utilities used in the tutorial. Install node using the following command: macOS brew install node Linux TBD Windows TBD It is advisable to use nvm (node version manager) to allow you to install and use different versions of node.js . Install nvm using the following command: macOS brew install nvm Linux TBD Windows TBD npm is the package manager used to install different node.js packages. Install npm using the following command: macOS brew install npm Linux TBD Windows TBD Learn more about npm .","title":"Installing node, nvm &amp; npm"},{"location":"references/prerequisites/instructions/#installing-jq","text":"jq is a utility used to manipulate the JSON output typically returned by the oc command when reading or updating Kubernetes resources. Install jq using the following command: macOS brew install jq Linux TBD Windows TBD Learn more about jq .","title":"Installing jq"},{"location":"references/prerequisites/instructions/#installing-tree","text":"tree is a command-line tool display a file system structure by displaying folders and files in tree-like structure. Install tree using the following command: macOS brew install tree Linux TBD Windows TBD Learn more about tree .","title":"Installing tree"},{"location":"references/prerequisites/instructions/#installing-oc","text":"The oc command is used to interact with an OpenShift cluster. It is effectively a synonym for the kubectl command with some additional options. Use these instructions to get the latest version of oc . You can use oc version to confirm that you have: Client Version: 4.6 or higher Server Version: 4.7 or higher The version of oc must be compatible with the version of the cluster you previously created.","title":"Installing oc"},{"location":"references/training/training/","text":"These training videos explain the rationale and architecture behind the Production Deployment Guides, and walk you through how to create a cluster environment to learn and practice with the configurations. 1 - Introduction to Production Deployment Guides \u00b6 2 - Deploying a GitOps Cluster on IBM ROKS \u00b6 3 - Golden Topologies \u00b6 4 - GitOps Framework \u00b6 5 - Deploying a Cloud Pak Recipe \u00b6","title":"Training"},{"location":"references/training/training/#1-introduction-to-production-deployment-guides","text":"","title":"1 - Introduction to Production Deployment Guides"},{"location":"references/training/training/#2-deploying-a-gitops-cluster-on-ibm-roks","text":"","title":"2 - Deploying a GitOps Cluster on IBM ROKS"},{"location":"references/training/training/#3-golden-topologies","text":"","title":"3 - Golden Topologies"},{"location":"references/training/training/#4-gitops-framework","text":"","title":"4 - GitOps Framework"},{"location":"references/training/training/#5-deploying-a-cloud-pak-recipe","text":"","title":"5 - Deploying a Cloud Pak Recipe"},{"location":"resources/resources/","text":"Additional Resources \u00b6 Use the links below to find additional resources and learning content around Cloud-Native development, RedHat OpenShift and the IBM Cloud.","title":"Additional Resources"},{"location":"resources/resources/#additional-resources","text":"Use the links below to find additional resources and learning content around Cloud-Native development, RedHat OpenShift and the IBM Cloud.","title":"Additional Resources"},{"location":"snippets/cpd_intro_snippet/","text":"To deploy IBM Cloud Pak for Data on an OpenShift cluster, we will use the IBM Cloud Native Toolkit GitOps Framework . There are only five steps to do: Prereqs - Make sure you have a RedHat OpenShift cluster and you are able to use the RedHat OpenShift CLI against it. Sealed Secrets - Provide the private key used to seal the secrets provided with the API Connect GitOps repository. RedHat OpenShift GitOps Operator - Install the RedHat OpenShift GitOps operator which provides the GitOps tools needed for installing and managing IBM Cloud Pak for Data instances using the GitOps approach already explained. IBM Cloud Pak for Data - Deploy an instance of IBM Cloud Pak for Data on the RedHat OpenShift cluster. IBM Cloud Pak for Data UI - Validate the installation of your IBM Cloud Pak for Data instance by making sure you are able to log into the IBM Cloud Pak for Data user interface.","title":"Cpd intro snippet"},{"location":"snippets/cpd_sealed_secrets/","text":"Create the sealed-secrets project. This project will host the Sealed Secrets operator that will allow us to decrypt sealed secrets stored in GitHub. oc new-project sealed-secrets Download the private key sealed-secrets-ibm-demo-key.yaml used to seal any secret contained in this demonstration and apply it to the cluster. In our case, we have included a demo IBM Entitlement Key within the GitOps GitHub repository so that we are able to pull down IBM Software. oc apply -f sealed-secrets-ibm-demo-key.yaml Delete the pod oc delete pod -n sealed-secrets -l app.kubernetes.io/name=sealed-secrets IMPORTANT WARNING: DO NOT CHECK THE FILE INTO GIT The private key MUST NOT be checked into GitHub under any circumstances. Please, remove the private key from your workstation to avoid any issues. rm sealed-secrets-ibm-demo-key.yaml For Cloud Pak to consume the entitlement key, restart the Platform Navigator pods oc delete pod -n tools -l app.kubernetes.io/name=ibm-integration-platform-navigator","title":"Cpd sealed secrets"},{"location":"snippets/cpd_ui_snippet/","text":"Let's make sure that our IBM Cloud Pak for Data instance is up and running. Do that by logging into the IBM Cloud Pak for Data user interface. Obtain IBM Cloud Pak for Data console URL by executing echo https://`oc -n cloudpak get ZenService lite-cr -o jsonpath=\"{.status.url}{'\\n'}\"` Open the URL in a browser and you will be presented with the IBM Cloud Pak for Data user interface login option. Enter admin as username. Obtain admin password by executing oc -n cloudpak extract secret/admin-user-details --keys=initial_admin_password --to=- Log into the IBM Cloud Pak for Data UI using the password from previous step. Click on the navigation menu icon on the top left corner. Click on Services menu option to expand it, then select Services catalog. The various services installed with IBM Cloud Pak for Data will be displayed. That is it to get a working instance of IBM Cloud Pak for Data.","title":"Cpd ui snippet"},{"location":"snippets/gitops-argocd-cm-gov/","text":"ArgoCD change management and governance \u00b6 In this final section about managing infrastructure resources, let's explore how ArgoCD provides some advanced resource management features: Dynamic Monitored Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Customize the web console banner Examine the banner in the OpenShift web console: We're going to use GitOps to modify this banner dynamically. The banner YAML This banner properties are defined by the YAML in /0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml . This YAML is currently being used by the cntk-consolenotification ArgoCD application that was deployed earlier. We can examine the YAML with the following command: cat 0 -bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml which shows the banner properties are part of the ConsoleNotification custom resource: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : cntk-consolenotification labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" finalizers : - resources-finalizer.argocd.argoproj.io spec : syncPolicy : automated : prune : true selfHeal : true destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : path : consolenotification helm : values : | ocp-console-notification: ## The name of the ConsoleNotification resource in the cluster name: \"banner-env\" ## The background color that should be used for the banner backgroundColor: teal ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text: \"Cluster Description\" See how the banner at the top of the screen: * contains the text Cluster Description * is located at top of the screen * has the color Teal Modify the YAML for this banner Let's now change this YAML In your editor, modify this YAML and change the below fields as follows: ocp-console-notification : ## The name of the ConsoleNotification resource in the cluster name : \"banner-env\" ## The background color that should be used for the banner backgroundColor : red ## The color of the text that will appear in the banner color : \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location : BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text : \"Production reference guide\" It's clear that our intention is to modify the banner's backgroundColor:red and text: Production reference guide to the newly specified values. If you look at the diff : git diff you should see the following diff --git a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml index 30adf1a..596e821 100644 --- a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml +++ b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml @@ -26,10 +26,10 @@ spec: name: \"banner-env\" ## The background color that should be used for the banner - backgroundColor: teal + backgroundColor: red ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created - text: \"Cluster Description\" + text: \"Production reference guide\" Make the web console YAML change active Let's make these changes visible to the cntk-consolenotification ArgoCD application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Modify console banner\" git push origin $GIT_BRANCH You'll see the changes being pushed to GitHub: Enumerating objects: 13, done. Counting objects: 100% (13/13), done. Delta compression using up to 8 threads Compressing objects: 100% (7/7), done. Writing objects: 100% (7/7), 670 bytes | 670.00 KiB/s, done. Total 7 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a1e8292..b49dff5 master -> master Let's see what effect they have on the web console. A dynamic change to the web console You can either wait for ArgoCD to automatically sync the cntk-consolenotification application or manually Sync it yourself: Returning to the OpenShift web console, you'll notice changes. Notice the dynamic nature of these changes; we updated the console YAML, pushed our changes to our GitOps repository and everything else happened automatically. As a result, our OpenShift console has a new banner color and text. Governing changes to the dev namespace Let's now look at how ArgoCD monitors Kubernetes resources for configuration drift, and what happens if it detects an unexpected change to a monitored resource. Don't worry about the following command; it might seem drastic and even reckless, but as you'll see, everything will be OK. Let's delete the dev namespace from the cluster: oc get namespace dev oc delete namespace dev See how the active namespace: NAME STATUS AGE dev Active 2d18h is deleted: namespace \"dev\" deleted We can see that the dev namespace has been manually deleted from the cluster. GitOps repository as a source of truth If you switch back to the ArgoCD UI, you may see that ArgoCD has detected a configuration drift: a resource is Missing (the dev namespace) namespace-dev therefore is OutOfSync namespace-dev is therefore Syncing with the GitOps repository After a while we'll see that namespace-dev is Healthy and Synced : ArgoCD has detected a configuration drift, and resynched with GitOps repository, re-applying the dev namespace to the cluster. Note You may miss seeing the first screenshot if ArgoCD detects and corrects the missing dev namespace before you get a chance to switch to the ArgoCD UI. Don't worry, you can try this operator again! The restored dev namespace Issue the following command to determine the status of the dev namespace: oc get namespace dev which confirms that the dev namespace has been re-instated: NAME STATUS AGE dev Active 115s Note that it is a different instance of the dev namespace; as indicated by its AGE value. Notice the governed nature of these changes; GitOps is our source of truth about the resources deployed to the cluster. ArgoCD restores any resources that suffer from configuration drift to their GitOps-defined configuration.","title":"Gitops argocd cm gov"},{"location":"snippets/gitops-argocd-cm-gov/#argocd-change-management-and-governance","text":"In this final section about managing infrastructure resources, let's explore how ArgoCD provides some advanced resource management features: Dynamic Monitored Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Customize the web console banner Examine the banner in the OpenShift web console: We're going to use GitOps to modify this banner dynamically. The banner YAML This banner properties are defined by the YAML in /0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml . This YAML is currently being used by the cntk-consolenotification ArgoCD application that was deployed earlier. We can examine the YAML with the following command: cat 0 -bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml which shows the banner properties are part of the ConsoleNotification custom resource: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : cntk-consolenotification labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" finalizers : - resources-finalizer.argocd.argoproj.io spec : syncPolicy : automated : prune : true selfHeal : true destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : path : consolenotification helm : values : | ocp-console-notification: ## The name of the ConsoleNotification resource in the cluster name: \"banner-env\" ## The background color that should be used for the banner backgroundColor: teal ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text: \"Cluster Description\" See how the banner at the top of the screen: * contains the text Cluster Description * is located at top of the screen * has the color Teal Modify the YAML for this banner Let's now change this YAML In your editor, modify this YAML and change the below fields as follows: ocp-console-notification : ## The name of the ConsoleNotification resource in the cluster name : \"banner-env\" ## The background color that should be used for the banner backgroundColor : red ## The color of the text that will appear in the banner color : \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location : BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created text : \"Production reference guide\" It's clear that our intention is to modify the banner's backgroundColor:red and text: Production reference guide to the newly specified values. If you look at the diff : git diff you should see the following diff --git a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml index 30adf1a..596e821 100644 --- a/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml +++ b/0-bootstrap/single-cluster/1-infra/argocd/consolenotification.yaml @@ -26,10 +26,10 @@ spec: name: \"banner-env\" ## The background color that should be used for the banner - backgroundColor: teal + backgroundColor: red ## The color of the text that will appear in the banner color: \"'#fff'\" ## The location of the banner. Options: BannerTop, BannerBottom, BannerTopBottom location: BannerTop ## The text that should be displayed in the banner. This value is required for the banner to be created - text: \"Cluster Description\" + text: \"Production reference guide\" Make the web console YAML change active Let's make these changes visible to the cntk-consolenotification ArgoCD application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Modify console banner\" git push origin $GIT_BRANCH You'll see the changes being pushed to GitHub: Enumerating objects: 13, done. Counting objects: 100% (13/13), done. Delta compression using up to 8 threads Compressing objects: 100% (7/7), done. Writing objects: 100% (7/7), 670 bytes | 670.00 KiB/s, done. Total 7 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a1e8292..b49dff5 master -> master Let's see what effect they have on the web console. A dynamic change to the web console You can either wait for ArgoCD to automatically sync the cntk-consolenotification application or manually Sync it yourself: Returning to the OpenShift web console, you'll notice changes. Notice the dynamic nature of these changes; we updated the console YAML, pushed our changes to our GitOps repository and everything else happened automatically. As a result, our OpenShift console has a new banner color and text. Governing changes to the dev namespace Let's now look at how ArgoCD monitors Kubernetes resources for configuration drift, and what happens if it detects an unexpected change to a monitored resource. Don't worry about the following command; it might seem drastic and even reckless, but as you'll see, everything will be OK. Let's delete the dev namespace from the cluster: oc get namespace dev oc delete namespace dev See how the active namespace: NAME STATUS AGE dev Active 2d18h is deleted: namespace \"dev\" deleted We can see that the dev namespace has been manually deleted from the cluster. GitOps repository as a source of truth If you switch back to the ArgoCD UI, you may see that ArgoCD has detected a configuration drift: a resource is Missing (the dev namespace) namespace-dev therefore is OutOfSync namespace-dev is therefore Syncing with the GitOps repository After a while we'll see that namespace-dev is Healthy and Synced : ArgoCD has detected a configuration drift, and resynched with GitOps repository, re-applying the dev namespace to the cluster. Note You may miss seeing the first screenshot if ArgoCD detects and corrects the missing dev namespace before you get a chance to switch to the ArgoCD UI. Don't worry, you can try this operator again! The restored dev namespace Issue the following command to determine the status of the dev namespace: oc get namespace dev which confirms that the dev namespace has been re-instated: NAME STATUS AGE dev Active 115s Note that it is a different instance of the dev namespace; as indicated by its AGE value. Notice the governed nature of these changes; GitOps is our source of truth about the resources deployed to the cluster. ArgoCD restores any resources that suffer from configuration drift to their GitOps-defined configuration.","title":"ArgoCD change management and governance"},{"location":"snippets/gitops-argocd-features/","text":"Other important ArgoCD features \u00b6 In this final section of this chapter, let's explore ArgoCD features you may have noticed as you explored different YAML files in this chapter: SyncWave \u00b6 Using SyncWave to control deployment sequencing When ArgoCD uses a GitOps repository to apply ArgoCD applications to the cluster, it applies all ArgoCD applications concurrently. Even though Kubernetes use of eventual consistency means that resources which depend on each other can be deployed in any order, it often makes sense to help deploy certain resources in a certain order. This can avoid spurious error messages or wasted compute resources for example. Let's compare two ArgoCD YAML files. Firstly, let's examine the namespace-tools in the infrastructure layer: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-tools.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: tools labels: gitops.tier.layer: infra annotations: argocd.argoproj.io/sync-wave: \"100\" ... Then examine the artifactory in the services layer: cat 0 -bootstrap/single-cluster/2-services/argocd/instances/artifactory.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: artifactory annotations: argocd.argoproj.io/sync-wave: \"250\" labels: gitops.tier.group: cntk gitops.tier.layer: services finalizers: - resources-finalizer.argocd.argoproj.io ... Notice the use of sync-wave: \"100\" for tools and how it contrasts with sync-wave: \"250\" for artifactory . The lower 100 will be deployed before the higher sync-wave 250 because it only makes sense to deploy the pipelines into the tools namespace, after the tools namespace has been created. You are free to choose any number for sync-wave . In our deployment, we have chosen 100-199 for infrastructure, 200-299 for services and 300-399 for applications; it provides alignment with high level folder numbering such as 1-infra and so on. In our tutorial, we're going to incrementally add infrastructure, services and applications so that you can understand how everything fits together incrementally. This makes sync-wave less important than in a real-world system, where you might be making updates to each of these deployed layers simultaneously. In such cases the use of sync-wave provides a high degree of confidence in the effectiveness of a deployment change because all changes go through in the correct order.","title":"Gitops argocd features"},{"location":"snippets/gitops-argocd-features/#other-important-argocd-features","text":"In this final section of this chapter, let's explore ArgoCD features you may have noticed as you explored different YAML files in this chapter:","title":"Other important ArgoCD features"},{"location":"snippets/gitops-argocd-features/#syncwave","text":"Using SyncWave to control deployment sequencing When ArgoCD uses a GitOps repository to apply ArgoCD applications to the cluster, it applies all ArgoCD applications concurrently. Even though Kubernetes use of eventual consistency means that resources which depend on each other can be deployed in any order, it often makes sense to help deploy certain resources in a certain order. This can avoid spurious error messages or wasted compute resources for example. Let's compare two ArgoCD YAML files. Firstly, let's examine the namespace-tools in the infrastructure layer: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-tools.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: tools labels: gitops.tier.layer: infra annotations: argocd.argoproj.io/sync-wave: \"100\" ... Then examine the artifactory in the services layer: cat 0 -bootstrap/single-cluster/2-services/argocd/instances/artifactory.yaml which shows the ArgoCD application YAML: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: artifactory annotations: argocd.argoproj.io/sync-wave: \"250\" labels: gitops.tier.group: cntk gitops.tier.layer: services finalizers: - resources-finalizer.argocd.argoproj.io ... Notice the use of sync-wave: \"100\" for tools and how it contrasts with sync-wave: \"250\" for artifactory . The lower 100 will be deployed before the higher sync-wave 250 because it only makes sense to deploy the pipelines into the tools namespace, after the tools namespace has been created. You are free to choose any number for sync-wave . In our deployment, we have chosen 100-199 for infrastructure, 200-299 for services and 300-399 for applications; it provides alignment with high level folder numbering such as 1-infra and so on. In our tutorial, we're going to incrementally add infrastructure, services and applications so that you can understand how everything fits together incrementally. This makes sync-wave less important than in a real-world system, where you might be making updates to each of these deployed layers simultaneously. In such cases the use of sync-wave provides a high degree of confidence in the effectiveness of a deployment change because all changes go through in the correct order.","title":"SyncWave"},{"location":"snippets/gitops-cluster-post-tasks/","text":"Post cluster provisioning tasks \u00b6 Red Hat OpenShift cluster \u00b6 An OpenShift v4.7+ cluster is required. CLI tools \u00b6 Install the OpenShift CLI oc (version 4.7+) . The binary can be downloaded from the Help menu from the OpenShift Console. Download oc cli Log in from a terminal window. oc login --token = <token> --server = <server> IBM Entitlement Key \u00b6 The IBM Entitlement Key is required to pull IBM Cloud Pak specific container images from the IBM Entitled Registry. To get an entitlement key, Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. A Secret containing the entitlement key is created in the tools namespace. oc new-project tools || true oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io","title":"Gitops cluster post tasks"},{"location":"snippets/gitops-cluster-post-tasks/#post-cluster-provisioning-tasks","text":"","title":"Post cluster provisioning tasks"},{"location":"snippets/gitops-cluster-post-tasks/#red-hat-openshift-cluster","text":"An OpenShift v4.7+ cluster is required.","title":"Red Hat OpenShift cluster"},{"location":"snippets/gitops-cluster-post-tasks/#cli-tools","text":"Install the OpenShift CLI oc (version 4.7+) . The binary can be downloaded from the Help menu from the OpenShift Console. Download oc cli Log in from a terminal window. oc login --token = <token> --server = <server>","title":"CLI tools"},{"location":"snippets/gitops-cluster-post-tasks/#ibm-entitlement-key","text":"The IBM Entitlement Key is required to pull IBM Cloud Pak specific container images from the IBM Entitled Registry. To get an entitlement key, Log in to MyIBM Container Software Library with an IBMid and password associated with the entitled software. Select the View library option to verify your entitlement(s). Select the Get entitlement key to retrieve the key. A Secret containing the entitlement key is created in the tools namespace. oc new-project tools || true oc create secret docker-registry ibm-entitlement-key -n tools \\ --docker-username = cp \\ --docker-password = \"<entitlement_key>\" \\ --docker-server = cp.icr.io","title":"IBM Entitlement Key"},{"location":"snippets/gitops-cluster-prereq/","text":"Pre-requisites \u00b6 Create a custom Git Organization \u00b6 Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization. Create a Git Personal Access Token (PAT) \u00b6 Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later.","title":"Gitops cluster prereq"},{"location":"snippets/gitops-cluster-prereq/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"snippets/gitops-cluster-prereq/#create-a-custom-git-organization","text":"Create a new Git Organization to host the different GitOps repositories. Instructions: Log in to http://github.com and select the option to create a New organization . Click on Create a free organization plan. Complete the wizard by filling in the Organization account name and Contact email fields. Select the My personal account bullet and complete the verification step and click Next. Skip the step to add members to the Organization. Complete the Welcome to GitHub questionnaire and click Submit. Congratulations, you have successfully created a new Github Organization.","title":"Create a custom Git Organization"},{"location":"snippets/gitops-cluster-prereq/#create-a-git-personal-access-token-pat","text":"Create a new Git Personal Access Token with the appropriate scopes. This will be required to run the application pipelines or to set up webhooks. Instructions: Log in to http://github.com and click on Settings . Select Developer settings and click on Personal access tokens . Provide a name for the token, set the Expiration to 90 days ,set the following scopes and click Generate token . GitHub Personal Access Token scopes Copy and save the Personal Access Token . You will not be able to retrieve this value again later.","title":"Create a Git Personal Access Token (PAT)"},{"location":"snippets/gitops-deploy-ace-services/","text":"Deploy services to the cluster \u00b6 We've just had our first successful GitOps experience, using an ArgoCD application to create the ci , tools and dev namespaces in our cluster. There are few more components to create: IBM ACE, IBM Platform navigator, IBM foundations, IBM automation foundations, IBM Catalog and Sealed secrets. These components are part of the services layer in our architecture, and that requires us to access /0-bootstrap/single-cluster/2-services within our GitOps repository. How to deploy services As we saw earlier, the bootstrap-single-cluster application uses the contents of the /0-bootstrap/single-cluster/2-services folder to determine which Kubernetes resources should be deployed in the cluster. Issue the following command to see what's currently deployed in the cluster: tree 0 -bootstrap/single-cluster/2-services/ -L 3 The resources currently deployed to the cluster map directly to this folder structure: 0 -bootstrap/single-cluster/ \u251c\u2500\u2500 1 -infra \u2502 \u251c\u2500\u2500 1 -infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2 -services \u2502 \u251c\u2500\u2500 2 -services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3 -apps \u2502 \u251c\u2500\u2500 3 -apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml 13 directories, 23 files This structure shows how only infrastructure ArgoCD applications -- and therefore their managed resources such as ci , tools and dev namespaces -- are deployed in the cluster. Review ArgoCD services folder Let\u2019s examine the 0-bootstrap/single-cluster/2-services/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/kustomization.yaml We can see the contents of the kustomization.yaml : resources: # IBM Software #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml # IBM Foundations / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml # IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount #- argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml patches: - target: group: argoproj.io kind: Application labelSelector: \"gitops.tier.layer=services\" patch: | - - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master Add the services layer to the cluster Note The IBM Platform Navigator instance requires a RWX storageclass and it is set to managed-nfs-storage by default in the ArgoCD Application 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-platform-navigator-instance.yaml . This storageclass is available for Red Hat OpenShift on IBM Cloud cluster provisioned from IBM Technology Zone with NFS storage selected. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: - argocd/operators/ibm-ace-operator.yaml - argocd/operators/ibm-platform-navigator.yaml - argocd/instances/ibm-platform-navigator-instance.yaml # IBM Foundations / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml # IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml If you take a diff, and just look for the added services: git diff | grep \"^\\+\" | grep -v \"^\\+++\" You will have the following +- argocd/operators/ibm-ace-operator.yaml +- argocd/operators/ibm-platform-navigator.yaml +- argocd/instances/ibm-platform-navigator-instance.yaml +- argocd/operators/ibm-foundations.yaml +- argocd/instances/ibm-foundational-services-instance.yaml +- argocd/operators/ibm-automation-foundation-core-operator.yaml +- argocd/operators/ibm-catalogs.yaml +- argocd/instances/sealed-secrets.yaml which shows the resources to be deployed for services. Commit and push changes to your git repository: git add . git commit -s -m \"Initial boostrap setup for services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11 , done . Counting objects: 100 % ( 11 /11 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 6 /6 ) , done . Writing objects: 100 % ( 6 /6 ) , 564 bytes | 564 .00 KiB/s, done . Total 6 ( delta 5 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 5 /5 ) , completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git b49dff5..533602c master -> master The intention of this operation is to indicate that we'd like the resources declared in 0-bootstrap/single-cluster/2-services/kustomization.yaml to be deployed in the cluster. Like the infra ArgoCD application, the resources created by the services ArgoCD application will manage the Kubernetes relevant services resources applied to the cluster. Optional: Activate the services in the GitOps repo Run this step if you skip installing tekton in the previous section. If you did not skip that step go to Step 7 . Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources: - 1 -infra/1-infra.yaml - 2 -services/2-services.yaml # - 3-apps/3-apps.yaml patches: - target: group: argoproj.io kind: Application labelSelector: \"gitops.tier.layer=gitops\" patch: | - - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target: group: argoproj.io kind: AppProject labelSelector: \"gitops.tier.layer=infra\" patch: | - - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target: group: argoproj.io kind: AppProject labelSelector: \"gitops.tier.layer=services\" patch: | - - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target: group: argoproj.io kind: AppProject labelSelector: \"gitops.tier.layer=applications\" patch: | - - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9 , done . Counting objects: 100 % ( 9 /9 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 5 /5 ) , done . Writing objects: 100 % ( 5 /5 ) , 431 bytes | 431 .00 KiB/s, done . Total 5 ( delta 4 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 4 /4 ) , completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see lots of new ArgoCD applications have been created to manage the services we have deployed by modifying kustomization.yaml under 0-bootstrap/single-cluster/2-services folder: See how most ArgoCD applications are Synced almost immediately, but some spend time in Progressing . After a few minutes you'll see that all ArgoCD applications become Healthy and Synced : Notice how many more Synched ArgoCD applications are now in the cluster; these are as a result of the newly added services layer in our architecture. The services application Let's examine the ArgoCD application that manage the services in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the services application: We can see that the services ArgoCD application creates several ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It\u2019s the services ArgoCD application that watches the 0-bootstrap/single-cluster/2-services/argocd folder for ArgoCD applications that apply service resources to our cluster. It was the services application that created the artifactory ArgoCD application which manages the Artifactory instance that is used for application configuration management that we will be exploring later in this section of the tutorial. We\u2019ll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it\u2019s not necessary, as you\u2019ll get lots of practice as we proceed. The services ArgoCD project As we've seen in the ArgoCD UI, the services ArgoCD application is responsible for creating the ArgoCD applications that manage the services within the cluster. Let's examine their definitions to see how they do this. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/2-services.yaml The following YAML may initially look a little intimidating; we'll discuss the major elements below: --- apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: services labels: gitops.tier.layer: services spec: sourceRepos: [] # Populated by kustomize patches in 2-services/kustomization.yaml destinations: - namespace: tools server: https://kubernetes.default.svc - namespace: ibm-common-services server: https://kubernetes.default.svc - namespace: redhat-operators server: https://kubernetes.default.svc - namespace: openshift-operators server: https://kubernetes.default.svc - namespace: openshift-marketplace server: https://kubernetes.default.svc - namespace: ci server: https://kubernetes.default.svc - namespace: dev server: https://kubernetes.default.svc - namespace: staging server: https://kubernetes.default.svc - namespace: prod server: https://kubernetes.default.svc - namespace: sealed-secrets server: https://kubernetes.default.svc - namespace: istio-system server: https://kubernetes.default.svc - namespace: openldap server: https://kubernetes.default.svc - namespace: instana-agent server: https://kubernetes.default.svc - namespace: openshift-gitops server: https://kubernetes.default.svc clusterResourceWhitelist: # TODO: SCC needs to be moved to 1-infra, here for now for artifactory - group: \"security.openshift.io\" kind: SecurityContextConstraints - group: \"console.openshift.io\" kind: ConsoleLink - group: \"apps\" kind: statefulsets - group: \"apps\" kind: deployments - group: \"\" kind: services - group: \"\" kind: configmaps - group: \"\" kind: secrets - group: \"\" kind: serviceaccounts - group: \"batch\" kind: jobs - group: \"\" kind: roles - group: \"route.openshift.io\" kind: routes - group: \"\" kind: RoleBinding - group: \"rbac.authorization.k8s.io\" kind: ClusterRoleBinding - group: \"rbac.authorization.k8s.io\" kind: ClusterRole - group: apiextensions.k8s.io kind: CustomResourceDefinition roles: # A role which provides read-only access to all applications in the project - name: read-only description: Read-only privileges to my-project policies: - p, proj:my-project:read-only, applications, get, my-project/*, allow groups: - argocd-admins --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: services annotations: argocd.argoproj.io/sync-wave: \"200\" labels: gitops.tier.layer: gitops spec: destination: namespace: openshift-gitops server: https://kubernetes.default.svc project: services source: # repoURL and targetRevision populated by kustomize patches in 2-services/kustomization.yaml path: 0 -bootstrap/single-cluster/2-services syncPolicy: automated: prune: true selfHeal: true Notice how this YAML defines three ArgoCD resources: a services project which manages all the necessary services that are needed by the applications. Notice how the destinations for the services project are limited to the ci , tools and dev namespaces -- as well as a few others that we'll use in the tutorial. These destinations restrict the namespaces where ArgoCD applications in the services project can manage resources. The same is true for clusterResourceWhiteList . It limits the Kubernetes resources that can be managed to configmaps , deployments and rolebindings amongst others. In summary, we see that the service project is used to group all the ArgoCD applications that will manage the services in our cluster. These ArgoCD applications can only perform specific actions on specific resource types in specific namespaces. See how ArgoCD is acting as a well-governed administrator. The similar structure of services and infra ArgoCD applications Even though we didn't closely examine the infra ArgoCD application YAML in the previous topic, it has has a very similar structure to the ArgoCD services applications we've just examined. Type the following command to list the ArgoCD infra app YAML. cat 0 -bootstrap/single-cluster/1-infra/1-infra.yaml Again, although this YAML might look a little intimidating, the overall structure is the same as for services : --- apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: infra labels: gitops.tier.layer: infra spec: sourceRepos: [] # Populated by kustomize patches in 1-infra/kustomization.yaml destinations: - namespace: ci server: https://kubernetes.default.svc - namespace: dev server: https://kubernetes.default.svc - namespace: staging server: https://kubernetes.default.svc - namespace: prod server: https://kubernetes.default.svc - namespace: sealed-secrets server: https://kubernetes.default.svc - namespace: tools server: https://kubernetes.default.svc - namespace: ibm-common-services server: https://kubernetes.default.svc - namespace: istio-system server: https://kubernetes.default.svc - namespace: openldap server: https://kubernetes.default.svc - namespace: instana-agent server: https://kubernetes.default.svc - namespace: openshift-gitops server: https://kubernetes.default.svc clusterResourceWhitelist: - group: \"\" kind: Namespace - group: \"\" kind: RoleBinding - group: \"security.openshift.io\" kind: SecurityContextConstraints - group: \"console.openshift.io\" kind: ConsoleNotification - group: \"console.openshift.io\" kind: ConsoleLink roles: # A role which provides read-only access to all applications in the project - name: read-only description: Read-only privileges to my-project policies: - p, proj:my-project:read-only, applications, get, my-project/*, allow groups: - argocd-admins --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: infra annotations: argocd.argoproj.io/sync-wave: \"100\" labels: gitops.tier.layer: gitops spec: destination: namespace: openshift-gitops server: https://kubernetes.default.svc project: infra source: # repoURL and targetRevision populated by kustomize patches in 1-infra/kustomization.yaml path: 0 -bootstrap/single-cluster/1-infra syncPolicy: automated: prune: true selfHeal: true As with the 2-services.yaml , we can see An ArgoCD project called infra . ArgoCD applications defined in this project will be limited by the destinations: and clusterResourceWhitelist: specified in the YAML. An ArgoCD app called infra . This is the ArgoCD application that we used in the previous section of the tutorial. It watches the path: 0-bootstrap/single-cluster/1-infra folder for ArgoCD applications that it applied to the cluster. It was these applications that managed the ci , tools and dev namespaces for example. The installation of the operators will take approximately 30 - 45 minutes. Verify the deployment of IBM App Connect operator sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/ibm-appconnect -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/ibm-appconnect condition met If you see something like: Error from server ( NotFound ) : deployments.apps \"ibm-appconnect\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Validate the deployment of IBM Cloud Pak for Integration Platform Navigator operator sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/ibm-integration-platform-navigator-operator -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/ibm-integration-platform-navigator-operator condition met If you see something like: Error from server ( NotFound ) : deployments.apps \"ibm-integration-platform-navigator-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Verify the IBM Platform Navigator has been deployed successfully. oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' Expected output = True The Platform Navigator console has been installed as part of the IBM Cloud Pak for Integration. We use the Platform Navigator to view our App Connect Enterprise dashboard. Access the IBM Platform Navigator console Retrieve the URL for the IBM Platform Navigator console: oc get route integration-navigator-pn -n tools Copy the URL from the HOST/PORT column and paste into a browser, prefixed with https:// . Retrieve the admin password: oc get secret ibm-iam-bindinfo-platform-auth-idp-credentials -n dev -o json -n ibm-common-services | jq -r '.data.\"admin_password\"' | base64 -D Log in to the IBM Platform Navigator console with the admin credentials.","title":"Gitops deploy ace services"},{"location":"snippets/gitops-deploy-ace-services/#deploy-services-to-the-cluster","text":"We've just had our first successful GitOps experience, using an ArgoCD application to create the ci , tools and dev namespaces in our cluster. There are few more components to create: IBM ACE, IBM Platform navigator, IBM foundations, IBM automation foundations, IBM Catalog and Sealed secrets. These components are part of the services layer in our architecture, and that requires us to access /0-bootstrap/single-cluster/2-services within our GitOps repository. How to deploy services As we saw earlier, the bootstrap-single-cluster application uses the contents of the /0-bootstrap/single-cluster/2-services folder to determine which Kubernetes resources should be deployed in the cluster. Issue the following command to see what's currently deployed in the cluster: tree 0 -bootstrap/single-cluster/2-services/ -L 3 The resources currently deployed to the cluster map directly to this folder structure: 0 -bootstrap/single-cluster/ \u251c\u2500\u2500 1 -infra \u2502 \u251c\u2500\u2500 1 -infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2 -services \u2502 \u251c\u2500\u2500 2 -services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3 -apps \u2502 \u251c\u2500\u2500 3 -apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml 13 directories, 23 files This structure shows how only infrastructure ArgoCD applications -- and therefore their managed resources such as ci , tools and dev namespaces -- are deployed in the cluster. Review ArgoCD services folder Let\u2019s examine the 0-bootstrap/single-cluster/2-services/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/kustomization.yaml We can see the contents of the kustomization.yaml : resources: # IBM Software #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml # IBM Foundations / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml # IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount #- argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml patches: - target: group: argoproj.io kind: Application labelSelector: \"gitops.tier.layer=services\" patch: | - - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master Add the services layer to the cluster Note The IBM Platform Navigator instance requires a RWX storageclass and it is set to managed-nfs-storage by default in the ArgoCD Application 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-platform-navigator-instance.yaml . This storageclass is available for Red Hat OpenShift on IBM Cloud cluster provisioned from IBM Technology Zone with NFS storage selected. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: - argocd/operators/ibm-ace-operator.yaml - argocd/operators/ibm-platform-navigator.yaml - argocd/instances/ibm-platform-navigator-instance.yaml # IBM Foundations / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml # IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml If you take a diff, and just look for the added services: git diff | grep \"^\\+\" | grep -v \"^\\+++\" You will have the following +- argocd/operators/ibm-ace-operator.yaml +- argocd/operators/ibm-platform-navigator.yaml +- argocd/instances/ibm-platform-navigator-instance.yaml +- argocd/operators/ibm-foundations.yaml +- argocd/instances/ibm-foundational-services-instance.yaml +- argocd/operators/ibm-automation-foundation-core-operator.yaml +- argocd/operators/ibm-catalogs.yaml +- argocd/instances/sealed-secrets.yaml which shows the resources to be deployed for services. Commit and push changes to your git repository: git add . git commit -s -m \"Initial boostrap setup for services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11 , done . Counting objects: 100 % ( 11 /11 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 6 /6 ) , done . Writing objects: 100 % ( 6 /6 ) , 564 bytes | 564 .00 KiB/s, done . Total 6 ( delta 5 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 5 /5 ) , completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git b49dff5..533602c master -> master The intention of this operation is to indicate that we'd like the resources declared in 0-bootstrap/single-cluster/2-services/kustomization.yaml to be deployed in the cluster. Like the infra ArgoCD application, the resources created by the services ArgoCD application will manage the Kubernetes relevant services resources applied to the cluster. Optional: Activate the services in the GitOps repo Run this step if you skip installing tekton in the previous section. If you did not skip that step go to Step 7 . Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources: - 1 -infra/1-infra.yaml - 2 -services/2-services.yaml # - 3-apps/3-apps.yaml patches: - target: group: argoproj.io kind: Application labelSelector: \"gitops.tier.layer=gitops\" patch: | - - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target: group: argoproj.io kind: AppProject labelSelector: \"gitops.tier.layer=infra\" patch: | - - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target: group: argoproj.io kind: AppProject labelSelector: \"gitops.tier.layer=services\" patch: | - - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target: group: argoproj.io kind: AppProject labelSelector: \"gitops.tier.layer=applications\" patch: | - - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9 , done . Counting objects: 100 % ( 9 /9 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 5 /5 ) , done . Writing objects: 100 % ( 5 /5 ) , 431 bytes | 431 .00 KiB/s, done . Total 5 ( delta 4 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 4 /4 ) , completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see lots of new ArgoCD applications have been created to manage the services we have deployed by modifying kustomization.yaml under 0-bootstrap/single-cluster/2-services folder: See how most ArgoCD applications are Synced almost immediately, but some spend time in Progressing . After a few minutes you'll see that all ArgoCD applications become Healthy and Synced : Notice how many more Synched ArgoCD applications are now in the cluster; these are as a result of the newly added services layer in our architecture. The services application Let's examine the ArgoCD application that manage the services in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the services application: We can see that the services ArgoCD application creates several ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It\u2019s the services ArgoCD application that watches the 0-bootstrap/single-cluster/2-services/argocd folder for ArgoCD applications that apply service resources to our cluster. It was the services application that created the artifactory ArgoCD application which manages the Artifactory instance that is used for application configuration management that we will be exploring later in this section of the tutorial. We\u2019ll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it\u2019s not necessary, as you\u2019ll get lots of practice as we proceed. The services ArgoCD project As we've seen in the ArgoCD UI, the services ArgoCD application is responsible for creating the ArgoCD applications that manage the services within the cluster. Let's examine their definitions to see how they do this. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/2-services.yaml The following YAML may initially look a little intimidating; we'll discuss the major elements below: --- apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: services labels: gitops.tier.layer: services spec: sourceRepos: [] # Populated by kustomize patches in 2-services/kustomization.yaml destinations: - namespace: tools server: https://kubernetes.default.svc - namespace: ibm-common-services server: https://kubernetes.default.svc - namespace: redhat-operators server: https://kubernetes.default.svc - namespace: openshift-operators server: https://kubernetes.default.svc - namespace: openshift-marketplace server: https://kubernetes.default.svc - namespace: ci server: https://kubernetes.default.svc - namespace: dev server: https://kubernetes.default.svc - namespace: staging server: https://kubernetes.default.svc - namespace: prod server: https://kubernetes.default.svc - namespace: sealed-secrets server: https://kubernetes.default.svc - namespace: istio-system server: https://kubernetes.default.svc - namespace: openldap server: https://kubernetes.default.svc - namespace: instana-agent server: https://kubernetes.default.svc - namespace: openshift-gitops server: https://kubernetes.default.svc clusterResourceWhitelist: # TODO: SCC needs to be moved to 1-infra, here for now for artifactory - group: \"security.openshift.io\" kind: SecurityContextConstraints - group: \"console.openshift.io\" kind: ConsoleLink - group: \"apps\" kind: statefulsets - group: \"apps\" kind: deployments - group: \"\" kind: services - group: \"\" kind: configmaps - group: \"\" kind: secrets - group: \"\" kind: serviceaccounts - group: \"batch\" kind: jobs - group: \"\" kind: roles - group: \"route.openshift.io\" kind: routes - group: \"\" kind: RoleBinding - group: \"rbac.authorization.k8s.io\" kind: ClusterRoleBinding - group: \"rbac.authorization.k8s.io\" kind: ClusterRole - group: apiextensions.k8s.io kind: CustomResourceDefinition roles: # A role which provides read-only access to all applications in the project - name: read-only description: Read-only privileges to my-project policies: - p, proj:my-project:read-only, applications, get, my-project/*, allow groups: - argocd-admins --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: services annotations: argocd.argoproj.io/sync-wave: \"200\" labels: gitops.tier.layer: gitops spec: destination: namespace: openshift-gitops server: https://kubernetes.default.svc project: services source: # repoURL and targetRevision populated by kustomize patches in 2-services/kustomization.yaml path: 0 -bootstrap/single-cluster/2-services syncPolicy: automated: prune: true selfHeal: true Notice how this YAML defines three ArgoCD resources: a services project which manages all the necessary services that are needed by the applications. Notice how the destinations for the services project are limited to the ci , tools and dev namespaces -- as well as a few others that we'll use in the tutorial. These destinations restrict the namespaces where ArgoCD applications in the services project can manage resources. The same is true for clusterResourceWhiteList . It limits the Kubernetes resources that can be managed to configmaps , deployments and rolebindings amongst others. In summary, we see that the service project is used to group all the ArgoCD applications that will manage the services in our cluster. These ArgoCD applications can only perform specific actions on specific resource types in specific namespaces. See how ArgoCD is acting as a well-governed administrator. The similar structure of services and infra ArgoCD applications Even though we didn't closely examine the infra ArgoCD application YAML in the previous topic, it has has a very similar structure to the ArgoCD services applications we've just examined. Type the following command to list the ArgoCD infra app YAML. cat 0 -bootstrap/single-cluster/1-infra/1-infra.yaml Again, although this YAML might look a little intimidating, the overall structure is the same as for services : --- apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: infra labels: gitops.tier.layer: infra spec: sourceRepos: [] # Populated by kustomize patches in 1-infra/kustomization.yaml destinations: - namespace: ci server: https://kubernetes.default.svc - namespace: dev server: https://kubernetes.default.svc - namespace: staging server: https://kubernetes.default.svc - namespace: prod server: https://kubernetes.default.svc - namespace: sealed-secrets server: https://kubernetes.default.svc - namespace: tools server: https://kubernetes.default.svc - namespace: ibm-common-services server: https://kubernetes.default.svc - namespace: istio-system server: https://kubernetes.default.svc - namespace: openldap server: https://kubernetes.default.svc - namespace: instana-agent server: https://kubernetes.default.svc - namespace: openshift-gitops server: https://kubernetes.default.svc clusterResourceWhitelist: - group: \"\" kind: Namespace - group: \"\" kind: RoleBinding - group: \"security.openshift.io\" kind: SecurityContextConstraints - group: \"console.openshift.io\" kind: ConsoleNotification - group: \"console.openshift.io\" kind: ConsoleLink roles: # A role which provides read-only access to all applications in the project - name: read-only description: Read-only privileges to my-project policies: - p, proj:my-project:read-only, applications, get, my-project/*, allow groups: - argocd-admins --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: infra annotations: argocd.argoproj.io/sync-wave: \"100\" labels: gitops.tier.layer: gitops spec: destination: namespace: openshift-gitops server: https://kubernetes.default.svc project: infra source: # repoURL and targetRevision populated by kustomize patches in 1-infra/kustomization.yaml path: 0 -bootstrap/single-cluster/1-infra syncPolicy: automated: prune: true selfHeal: true As with the 2-services.yaml , we can see An ArgoCD project called infra . ArgoCD applications defined in this project will be limited by the destinations: and clusterResourceWhitelist: specified in the YAML. An ArgoCD app called infra . This is the ArgoCD application that we used in the previous section of the tutorial. It watches the path: 0-bootstrap/single-cluster/1-infra folder for ArgoCD applications that it applied to the cluster. It was these applications that managed the ci , tools and dev namespaces for example. The installation of the operators will take approximately 30 - 45 minutes. Verify the deployment of IBM App Connect operator sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/ibm-appconnect -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/ibm-appconnect condition met If you see something like: Error from server ( NotFound ) : deployments.apps \"ibm-appconnect\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Validate the deployment of IBM Cloud Pak for Integration Platform Navigator operator sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/ibm-integration-platform-navigator-operator -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/ibm-integration-platform-navigator-operator condition met If you see something like: Error from server ( NotFound ) : deployments.apps \"ibm-integration-platform-navigator-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Verify the IBM Platform Navigator has been deployed successfully. oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' Expected output = True The Platform Navigator console has been installed as part of the IBM Cloud Pak for Integration. We use the Platform Navigator to view our App Connect Enterprise dashboard. Access the IBM Platform Navigator console Retrieve the URL for the IBM Platform Navigator console: oc get route integration-navigator-pn -n tools Copy the URL from the HOST/PORT column and paste into a browser, prefixed with https:// . Retrieve the admin password: oc get secret ibm-iam-bindinfo-platform-auth-idp-credentials -n dev -o json -n ibm-common-services | jq -r '.data.\"admin_password\"' | base64 -D Log in to the IBM Platform Navigator console with the admin credentials.","title":"Deploy services to the cluster"},{"location":"snippets/gitops-deploy-selection/","text":"Select resources to deploy \u00b6 In this section you will: Clone the multi-tenancy-gitops repository from your Git Organization to your local machine Use one of the recipes to select which components from the infrastructure and services layers to deploy. ACE recipe MQ recipe Process Mining recipe Instructions \u00b6 Clone the multi-tenancy-gitops repository from your Git Organization to your local machine using a command like the one shown below: git clone https://github.com/xxxxxxx/multi-tenancy-gitops Review the contents of the 0-bootstrap folder and note the single-cluster folder and the other topologies are in an others folder. The contents of the others folder is provided for other scenarios and can be deleted. cd multi-tenancy-gitops/0-bootstrap/ tree -L 2 . \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Delete the others folder from the 0-bootstrap folder to ensure the single-cluster topology is used rm -rf others/ tree -L 2 . \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Using the recipe you selected above, modify the 0-bootstrap/single-cluster/1-infra/kustomization.yaml file to configure the Infrastructure layer by un-commenting the resources to deploy. Using the recipe you selected above, modify the 0-bootstrap/single-cluster/2-services/kustomization.yaml and un-comment the resources to deploy. Note that the recipe may show additional files to edit. If so, make those edits now. Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin If an IBM Cloud Pak is installed, retrieve the Platform Navigator console URL and admin password. # Verify the Common Services instance has been deployed successfully oc get commonservice common-service -n ibm-common-services -o = jsonpath = '{.status.phase}' # Expected output = Succeeded # Verify the Platform Navigator instance has been deployed successfully oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' # Expected output = True # oc get route -n tools integration-navigator-pn -o template --template = 'https://{{.spec.host}}' oc extract -n ibm-common-services secrets/platform-auth-idp-credentials --keys = admin_username,admin_password --to = -","title":"Gitops deploy selection"},{"location":"snippets/gitops-deploy-selection/#select-resources-to-deploy","text":"In this section you will: Clone the multi-tenancy-gitops repository from your Git Organization to your local machine Use one of the recipes to select which components from the infrastructure and services layers to deploy. ACE recipe MQ recipe Process Mining recipe","title":"Select resources to deploy"},{"location":"snippets/gitops-deploy-selection/#instructions","text":"Clone the multi-tenancy-gitops repository from your Git Organization to your local machine using a command like the one shown below: git clone https://github.com/xxxxxxx/multi-tenancy-gitops Review the contents of the 0-bootstrap folder and note the single-cluster folder and the other topologies are in an others folder. The contents of the others folder is provided for other scenarios and can be deleted. cd multi-tenancy-gitops/0-bootstrap/ tree -L 2 . \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Delete the others folder from the 0-bootstrap folder to ensure the single-cluster topology is used rm -rf others/ tree -L 2 . \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Using the recipe you selected above, modify the 0-bootstrap/single-cluster/1-infra/kustomization.yaml file to configure the Infrastructure layer by un-commenting the resources to deploy. Using the recipe you selected above, modify the 0-bootstrap/single-cluster/2-services/kustomization.yaml and un-comment the resources to deploy. Note that the recipe may show additional files to edit. If so, make those edits now. Commit and push changes to your git repository git add . git commit -m \"initial bootstrap setup\" git push origin If an IBM Cloud Pak is installed, retrieve the Platform Navigator console URL and admin password. # Verify the Common Services instance has been deployed successfully oc get commonservice common-service -n ibm-common-services -o = jsonpath = '{.status.phase}' # Expected output = Succeeded # Verify the Platform Navigator instance has been deployed successfully oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' # Expected output = True # oc get route -n tools integration-navigator-pn -o template --template = 'https://{{.spec.host}}' oc extract -n ibm-common-services secrets/platform-auth-idp-credentials --keys = admin_username,admin_password --to = -","title":"Instructions"},{"location":"snippets/gitops-deploy-services-cp4s/","text":"Deploy services to the cluster \u00b6 We've just had our first successful GitOps experience, using an ArgoCD application to create the ci , tools and dev namespaces in our cluster at the infrastructure layer level as well as using ArgoCD to deploy the Red Hat OpenShift Pipelines operator at the services layer level. There are few more components you need to create for this IBM CP4S tutorial. Edit the CP4SThreatManagements custom resource instance and specify a block or file storage class ${GITOPS_PROFILE}/2-services/argocd/instances/ibm-cp4sthreatmanagements-instance.yaml. The default is set to managed-nfs-storage. - name: spec.basicDeploymentConfiguration.storageClass value: managed-nfs-storage - name: spec.extendedDeploymentConfiguration.backupStorageClass value: managed-nfs-storage These components are part of the services layer in our architecture, and that requires us to access /0-bootstrap/single-cluster/2-services within our GitOps repository. Check services layer components As we saw earlier, the bootstrap-single-cluster application uses the contents of the /0-bootstrap/single-cluster/2-services folder to determine which Kubernetes resources are available to be deployed in the cluster. Issue the following command to see what could be deployed in the cluster: tree 0 -bootstrap/single-cluster/2-services/ -L 3 You should see a set of operators and instances of these available in the GitOps framework at the services layer: 0 -bootstrap/single-cluster/2-services/ \u251c\u2500\u2500 2 -services.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 artifactory.yaml \u2502 \u2502 \u251c\u2500\u2500 baas-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 cert-manager-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 chartmuseum.yaml \u2502 \u2502 \u251c\u2500\u2500 developer-dashboard.yaml \u2502 \u2502 \u251c\u2500\u2500 grafana-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-gateway-analytics-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-management-portal-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4sthreatmanagements-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cpd-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-foundational-services-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-platform-navigator-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-process-mining-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 oadp-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 openshift-service-mesh-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 pact-broker.yaml \u2502 \u2502 \u251c\u2500\u2500 sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 sonarqube.yaml \u2502 \u2502 \u251c\u2500\u2500 spp-instance.yaml \u2502 \u2502 \u2514\u2500\u2500 swaggereditor.yaml \u2502 \u2514\u2500\u2500 operators \u2502 \u251c\u2500\u2500 cert-manager.yaml \u2502 \u251c\u2500\u2500 elasticsearch.yaml \u2502 \u251c\u2500\u2500 grafana-operator.yaml \u2502 \u251c\u2500\u2500 ibm-ace-operator.yaml \u2502 \u251c\u2500\u2500 ibm-apic-operator.yaml \u2502 \u251c\u2500\u2500 ibm-aspera-operator.yaml \u2502 \u251c\u2500\u2500 ibm-assetrepository-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-core-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-operator.yaml \u2502 \u251c\u2500\u2500 ibm-catalogs.yaml \u2502 \u251c\u2500\u2500 ibm-cp4a-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4i-operators.yaml \u2502 \u251c\u2500\u2500 ibm-cp4s-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-platform-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-scheduling-operator.yaml \u2502 \u251c\u2500\u2500 ibm-datapower-operator.yaml \u2502 \u251c\u2500\u2500 ibm-db2u-operator.yaml \u2502 \u251c\u2500\u2500 ibm-eventstreams-operator.yaml \u2502 \u251c\u2500\u2500 ibm-foundations.yaml \u2502 \u251c\u2500\u2500 ibm-mq-operator.yaml \u2502 \u251c\u2500\u2500 ibm-opsdashboard-operator.yaml \u2502 \u251c\u2500\u2500 ibm-platform-navigator.yaml \u2502 \u251c\u2500\u2500 ibm-process-mining-operator.yaml \u2502 \u251c\u2500\u2500 jaeger.yaml \u2502 \u251c\u2500\u2500 kiali.yaml \u2502 \u251c\u2500\u2500 oadp-operator.yaml \u2502 \u251c\u2500\u2500 openshift-gitops.yaml \u2502 \u251c\u2500\u2500 openshift-pipelines.yaml \u2502 \u251c\u2500\u2500 openshift-service-mesh.yaml \u2502 \u251c\u2500\u2500 spp-catalog.yaml \u2502 \u2514\u2500\u2500 spp-operator.yaml \u2514\u2500\u2500 kustomization.yaml Review ArgoCD services folder Let\u2019s examine the 0-bootstrap/single-cluster/2-services/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/kustomization.yaml We can see the contents of the kustomization.yaml : resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Add the services to the cluster Note The IBM Platform Navigator instance requires a RWX storageclass and it is set to managed-nfs-storage by default in the ArgoCD Application 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-platform-navigator-instance.yaml . This storageclass is available for Red Hat OpenShift on IBM Cloud cluster provisioned from IBM Technology Zone with NFS storage selected. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: ## Cloud Pak for Security - argocd/operators/ibm-cp4s-operator.yaml - argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml - argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD - argocd/instances/artifactory.yaml - argocd/instances/sonarqube.yaml You will have the following resources deployed for services: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security - argocd/operators/ibm-cp4s-operator.yaml - argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml - argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml - argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 564 bytes | 564.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git b49dff5..533602c master -> master The intention of this operation is to indicate that we'd like the resources declared in 0-bootstrap/single-cluster/2-services/kustomization.yaml to be deployed in the cluster. Like the infra ArgoCD application, the resources created by the services ArgoCD application will manage the Kubernetes relevant services resources applied to the cluster. Optional: Activate the services in the GitOps repo Run this step if you skip installing tekton in the previous section. If you did not skip that step go to Step 7 . Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see lots of new ArgoCD applications have been created to manage the services we have deployed by modifying kustomization.yaml under 0-bootstrap/single-cluster/2-services folder: See how most ArgoCD applications are Synced almost immediately, but some spend time in Progressing . That's because the Artifactory and Sonarqube ArgoCD applications are more complex -- they create more Kubernetes resources than other ArgoCD applications and therefore take longer to sync . After a few minutes you'll see that all ArgoCD applications become Healthy and Synced : Notice how many more Synched ArgoCD applications are now in the cluster; these are as a result of the newly added services layer in our architecture. The services application Let's examine the ArgoCD application that manage the services in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the services application: We can see that the services ArgoCD application creates 10 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It\u2019s the services ArgoCD application that watches the 0-bootstrap/single-cluster/2-services/argocd folder for ArgoCD applications that apply service resources to our cluster. It was the services application that created the artifactory ArgoCD application which manages the Artifactory instance that is used for application configuration management that we will be exploring later in this section of the tutorial. We\u2019ll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it\u2019s not necessary, as you\u2019ll get lots of practice as we proceed. Examine Artifactory resources in detail You can see the Kubernetes resources that the Artifactory ArgoCD application has created for Artifactory. Click on the Artifactory : See how many different Kubernetes resources were created when the Artifactory ArgoCD application YAML was applied to the cluster. That's why it took the Artifactory ArgoCD application a little time to sync. If you'd like, have a look at some of the other ArgoCD applications, such as Sonarqube and the Kubernetes resources they created. The services ArgoCD project As we've seen in the ArgoCD UI, the services ArgoCD application is responsible for creating the ArgoCD applications that manage the services within the cluster. Let's examine their definitions to see how they do this. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/2-services.yaml The following YAML may initially look a little intimidating; we'll discuss the major elements below: --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : services labels : gitops.tier.layer : services spec : sourceRepos : [] # Populated by kustomize patches in 2-services/kustomization.yaml destinations : - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : redhat-operators server : https://kubernetes.default.svc - namespace : openshift-operators server : https://kubernetes.default.svc - namespace : openshift-marketplace server : https://kubernetes.default.svc - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : # TODO: SCC needs to be moved to 1-infra, here for now for artifactory - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleLink - group : \"apps\" kind : statefulsets - group : \"apps\" kind : deployments - group : \"\" kind : services - group : \"\" kind : configmaps - group : \"\" kind : secrets - group : \"\" kind : serviceaccounts - group : \"batch\" kind : jobs - group : \"\" kind : roles - group : \"route.openshift.io\" kind : routes - group : \"\" kind : RoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRole - group : apiextensions.k8s.io kind : CustomResourceDefinition roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : services annotations : argocd.argoproj.io/sync-wave : \"200\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : services source : # repoURL and targetRevision populated by kustomize patches in 2-services/kustomization.yaml path : 0-bootstrap/single-cluster/2-services syncPolicy : automated : prune : true selfHeal : true Notice how this YAML defines three ArgoCD resources: a services project which manages all the necessary services that are needed by the applications. Notice how the destinations for the services project are limited to the ci , tools and dev namespaces -- as well as a few others that we'll use in the tutorial. These destinations restrict the namespaces where ArgoCD applications in the services project can manage resources. The same is true for clusterResourceWhiteList . It limits the Kubernetes resources that can be managed to configmaps , deployments and rolebindings amongst others. In summary, we see that the service project is used to group all the ArgoCD applications that will manage the services in our cluster. These ArgoCD applications can only perform specific actions on specific resource types in specific namespaces. See how ArgoCD is acting as a well-governed administrator. The similar structure of services and infra ArgoCD applications Even though we didn't closely examine the infra ArgoCD application YAML in the previous topic, it has has a very similar structure to the ArgoCD services applications we've just examined. Type the following command to list the ArgoCD infra app YAML. cat 0 -bootstrap/single-cluster/1-infra/1-infra.yaml Again, although this YAML might look a little intimidating, the overall structure is the same as for services : --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : infra labels : gitops.tier.layer : infra spec : sourceRepos : [] # Populated by kustomize patches in 1-infra/kustomization.yaml destinations : - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : - group : \"\" kind : Namespace - group : \"\" kind : RoleBinding - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleNotification - group : \"console.openshift.io\" kind : ConsoleLink roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : infra annotations : argocd.argoproj.io/sync-wave : \"100\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : # repoURL and targetRevision populated by kustomize patches in 1-infra/kustomization.yaml path : 0-bootstrap/single-cluster/1-infra syncPolicy : automated : prune : true selfHeal : true As with the 2-services.yaml , we can see An ArgoCD project called infra . ArgoCD applications defined in this project will be limited by the destinations: and clusterResourceWhitelist: specified in the YAML. An ArgoCD app called infra . This is the ArgoCD application that we used in the previous section of the tutorial. It watches the path: 0-bootstrap/single-cluster/1-infra folder for ArgoCD applications that it applied to the cluster. It was these applications that managed the ci , tools and dev namespaces for example. The installation of the operators will take approximately 30 - 45 minutes. Monitor your installation in the workspace \u00b6 From this stage, the installation will take approximately 1.5 hours to complete. After you start the installation, you are brought to the Schematics workspace for your Cloud Pak. You can track progress by viewing the logs. Go to the Activity tab, and click View logs. To further track the installation, you can monitor the status of IBM Cloud Pak\u00ae for Security Threat Management: Log in to the OpenShift web console and ensure you are in the Administrator view. Go to Operators > Installed Operators and ensure that the Project is set to the namespace where IBM Cloud Pak\u00ae for Security was installed. In the list of installed operators, click IBM Cloud Pak for Security. On the Threat Management tab, select the threatmgmt instance. On the Details page, the following message is displayed in the Conditions section when installation is complete. Cloudpak for Security Deployment is successful. The installation is complete when you see the message Cloudpak for Security Deployment is successful. Retrieve foundational services login details \u00b6 Use the following commands to retrieve IBM Cloud Pak foundational services hostname, default username, and password: Hostname: oc -n ibm-common-services get route | grep cp-console | awk '{print $2}' Username: oc -n ibm-common-services get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_username}' | base64 --decode Password: oc -n ibm-common-services get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_password}' | base64 --decode These login details are required to access foundational services and configure your LDAP directory. Next steps \u00b6 If the adminUser you provided is a user ID that you added and authenticated by using the IBM Cloud account that is associated with the cluster and roksAuthentication was enabled, go to step 2. Otherwise, Configure LDAP authentication and ensure that the adminUser that you provided exists in the LDAP directory. Log in to Cloud Pak\u00ae for Security using the domain and the adminUser that you provided during installation. The domain, also known as application URL, can be retrieved by running the following command: oc get route isc-route-default --no-headers -n <CP4S_NAMESPACE> | awk '{print $2}' Select Enterprise LDAP in the login screen if you are logging in using an LDAP you connected to Foundational Services, otherwise use OpenShift Authentication if it is enabled. Add users to Cloud Pak\u00ae for Security Install the IBM\u00ae Security Orchestration & Automation license . If you choose Orchestration & Automation as part of your Cloud Pak\u00ae for Security bundle, you must install your Orchestration & Automation license to access the complete orchestration and automation capabilities that are provided by Orchestration & Automation. Configure data sources Validation \u00b6 Check the status of the CommonService and PlatformNavigator custom resource. oc get CP4SThreatManagement threatmgmt -n tools -o jsonpath = '{.status.conditions}' # Expected output = Cloudpak for Security Deployment is successful Before users can log in to the console for Cloud Pak for Security, an identity provider must be configured. The documentation provides further instructions. For DEMO purposes, OpenLDAP can be deployed and instructions are provided below. Download the cpctl utility Log in to the OpenShift cluster oc login --token = <token> --server = <openshift_url> -n <namespace> Retrieve the pod that contains the utility POD = $( oc get pod --no-headers -lrun = cp-serviceability | cut -d ' ' -f1 ) Copy the utility locally oc cp $POD :/opt/bin/<operatingsystem>/cpctl ./cpctl && chmod +x ./cpctl Install OpenLDAP Start a session ./cpctl load Install OpenLDAP cpctl tools deploy_openldap --token $( oc whoami -t ) --ldap_usernames 'adminUser,user1,user2,user3' --ldap_password cloudpak Initial user log in Retrieve Cloud Pak for Security Console URL oc get route isc-route-default --no-headers -n <CP4S_NAMESPACE> | awk '{print $2}' Log in with the user ID and password specified (ie. adminUser / cloudpak ).","title":"Gitops deploy services cp4s"},{"location":"snippets/gitops-deploy-services-cp4s/#deploy-services-to-the-cluster","text":"We've just had our first successful GitOps experience, using an ArgoCD application to create the ci , tools and dev namespaces in our cluster at the infrastructure layer level as well as using ArgoCD to deploy the Red Hat OpenShift Pipelines operator at the services layer level. There are few more components you need to create for this IBM CP4S tutorial. Edit the CP4SThreatManagements custom resource instance and specify a block or file storage class ${GITOPS_PROFILE}/2-services/argocd/instances/ibm-cp4sthreatmanagements-instance.yaml. The default is set to managed-nfs-storage. - name: spec.basicDeploymentConfiguration.storageClass value: managed-nfs-storage - name: spec.extendedDeploymentConfiguration.backupStorageClass value: managed-nfs-storage These components are part of the services layer in our architecture, and that requires us to access /0-bootstrap/single-cluster/2-services within our GitOps repository. Check services layer components As we saw earlier, the bootstrap-single-cluster application uses the contents of the /0-bootstrap/single-cluster/2-services folder to determine which Kubernetes resources are available to be deployed in the cluster. Issue the following command to see what could be deployed in the cluster: tree 0 -bootstrap/single-cluster/2-services/ -L 3 You should see a set of operators and instances of these available in the GitOps framework at the services layer: 0 -bootstrap/single-cluster/2-services/ \u251c\u2500\u2500 2 -services.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 artifactory.yaml \u2502 \u2502 \u251c\u2500\u2500 baas-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 cert-manager-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 chartmuseum.yaml \u2502 \u2502 \u251c\u2500\u2500 developer-dashboard.yaml \u2502 \u2502 \u251c\u2500\u2500 grafana-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-gateway-analytics-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-management-portal-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4sthreatmanagements-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cpd-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-foundational-services-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-platform-navigator-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-process-mining-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 oadp-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 openshift-service-mesh-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 pact-broker.yaml \u2502 \u2502 \u251c\u2500\u2500 sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 sonarqube.yaml \u2502 \u2502 \u251c\u2500\u2500 spp-instance.yaml \u2502 \u2502 \u2514\u2500\u2500 swaggereditor.yaml \u2502 \u2514\u2500\u2500 operators \u2502 \u251c\u2500\u2500 cert-manager.yaml \u2502 \u251c\u2500\u2500 elasticsearch.yaml \u2502 \u251c\u2500\u2500 grafana-operator.yaml \u2502 \u251c\u2500\u2500 ibm-ace-operator.yaml \u2502 \u251c\u2500\u2500 ibm-apic-operator.yaml \u2502 \u251c\u2500\u2500 ibm-aspera-operator.yaml \u2502 \u251c\u2500\u2500 ibm-assetrepository-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-core-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-operator.yaml \u2502 \u251c\u2500\u2500 ibm-catalogs.yaml \u2502 \u251c\u2500\u2500 ibm-cp4a-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4i-operators.yaml \u2502 \u251c\u2500\u2500 ibm-cp4s-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-platform-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-scheduling-operator.yaml \u2502 \u251c\u2500\u2500 ibm-datapower-operator.yaml \u2502 \u251c\u2500\u2500 ibm-db2u-operator.yaml \u2502 \u251c\u2500\u2500 ibm-eventstreams-operator.yaml \u2502 \u251c\u2500\u2500 ibm-foundations.yaml \u2502 \u251c\u2500\u2500 ibm-mq-operator.yaml \u2502 \u251c\u2500\u2500 ibm-opsdashboard-operator.yaml \u2502 \u251c\u2500\u2500 ibm-platform-navigator.yaml \u2502 \u251c\u2500\u2500 ibm-process-mining-operator.yaml \u2502 \u251c\u2500\u2500 jaeger.yaml \u2502 \u251c\u2500\u2500 kiali.yaml \u2502 \u251c\u2500\u2500 oadp-operator.yaml \u2502 \u251c\u2500\u2500 openshift-gitops.yaml \u2502 \u251c\u2500\u2500 openshift-pipelines.yaml \u2502 \u251c\u2500\u2500 openshift-service-mesh.yaml \u2502 \u251c\u2500\u2500 spp-catalog.yaml \u2502 \u2514\u2500\u2500 spp-operator.yaml \u2514\u2500\u2500 kustomization.yaml Review ArgoCD services folder Let\u2019s examine the 0-bootstrap/single-cluster/2-services/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/kustomization.yaml We can see the contents of the kustomization.yaml : resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Add the services to the cluster Note The IBM Platform Navigator instance requires a RWX storageclass and it is set to managed-nfs-storage by default in the ArgoCD Application 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-platform-navigator-instance.yaml . This storageclass is available for Red Hat OpenShift on IBM Cloud cluster provisioned from IBM Technology Zone with NFS storage selected. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: ## Cloud Pak for Security - argocd/operators/ibm-cp4s-operator.yaml - argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml - argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD - argocd/instances/artifactory.yaml - argocd/instances/sonarqube.yaml You will have the following resources deployed for services: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security - argocd/operators/ibm-cp4s-operator.yaml - argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml - argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml - argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 564 bytes | 564.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git b49dff5..533602c master -> master The intention of this operation is to indicate that we'd like the resources declared in 0-bootstrap/single-cluster/2-services/kustomization.yaml to be deployed in the cluster. Like the infra ArgoCD application, the resources created by the services ArgoCD application will manage the Kubernetes relevant services resources applied to the cluster. Optional: Activate the services in the GitOps repo Run this step if you skip installing tekton in the previous section. If you did not skip that step go to Step 7 . Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see lots of new ArgoCD applications have been created to manage the services we have deployed by modifying kustomization.yaml under 0-bootstrap/single-cluster/2-services folder: See how most ArgoCD applications are Synced almost immediately, but some spend time in Progressing . That's because the Artifactory and Sonarqube ArgoCD applications are more complex -- they create more Kubernetes resources than other ArgoCD applications and therefore take longer to sync . After a few minutes you'll see that all ArgoCD applications become Healthy and Synced : Notice how many more Synched ArgoCD applications are now in the cluster; these are as a result of the newly added services layer in our architecture. The services application Let's examine the ArgoCD application that manage the services in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the services application: We can see that the services ArgoCD application creates 10 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It\u2019s the services ArgoCD application that watches the 0-bootstrap/single-cluster/2-services/argocd folder for ArgoCD applications that apply service resources to our cluster. It was the services application that created the artifactory ArgoCD application which manages the Artifactory instance that is used for application configuration management that we will be exploring later in this section of the tutorial. We\u2019ll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it\u2019s not necessary, as you\u2019ll get lots of practice as we proceed. Examine Artifactory resources in detail You can see the Kubernetes resources that the Artifactory ArgoCD application has created for Artifactory. Click on the Artifactory : See how many different Kubernetes resources were created when the Artifactory ArgoCD application YAML was applied to the cluster. That's why it took the Artifactory ArgoCD application a little time to sync. If you'd like, have a look at some of the other ArgoCD applications, such as Sonarqube and the Kubernetes resources they created. The services ArgoCD project As we've seen in the ArgoCD UI, the services ArgoCD application is responsible for creating the ArgoCD applications that manage the services within the cluster. Let's examine their definitions to see how they do this. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/2-services.yaml The following YAML may initially look a little intimidating; we'll discuss the major elements below: --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : services labels : gitops.tier.layer : services spec : sourceRepos : [] # Populated by kustomize patches in 2-services/kustomization.yaml destinations : - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : redhat-operators server : https://kubernetes.default.svc - namespace : openshift-operators server : https://kubernetes.default.svc - namespace : openshift-marketplace server : https://kubernetes.default.svc - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : # TODO: SCC needs to be moved to 1-infra, here for now for artifactory - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleLink - group : \"apps\" kind : statefulsets - group : \"apps\" kind : deployments - group : \"\" kind : services - group : \"\" kind : configmaps - group : \"\" kind : secrets - group : \"\" kind : serviceaccounts - group : \"batch\" kind : jobs - group : \"\" kind : roles - group : \"route.openshift.io\" kind : routes - group : \"\" kind : RoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRole - group : apiextensions.k8s.io kind : CustomResourceDefinition roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : services annotations : argocd.argoproj.io/sync-wave : \"200\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : services source : # repoURL and targetRevision populated by kustomize patches in 2-services/kustomization.yaml path : 0-bootstrap/single-cluster/2-services syncPolicy : automated : prune : true selfHeal : true Notice how this YAML defines three ArgoCD resources: a services project which manages all the necessary services that are needed by the applications. Notice how the destinations for the services project are limited to the ci , tools and dev namespaces -- as well as a few others that we'll use in the tutorial. These destinations restrict the namespaces where ArgoCD applications in the services project can manage resources. The same is true for clusterResourceWhiteList . It limits the Kubernetes resources that can be managed to configmaps , deployments and rolebindings amongst others. In summary, we see that the service project is used to group all the ArgoCD applications that will manage the services in our cluster. These ArgoCD applications can only perform specific actions on specific resource types in specific namespaces. See how ArgoCD is acting as a well-governed administrator. The similar structure of services and infra ArgoCD applications Even though we didn't closely examine the infra ArgoCD application YAML in the previous topic, it has has a very similar structure to the ArgoCD services applications we've just examined. Type the following command to list the ArgoCD infra app YAML. cat 0 -bootstrap/single-cluster/1-infra/1-infra.yaml Again, although this YAML might look a little intimidating, the overall structure is the same as for services : --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : infra labels : gitops.tier.layer : infra spec : sourceRepos : [] # Populated by kustomize patches in 1-infra/kustomization.yaml destinations : - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : - group : \"\" kind : Namespace - group : \"\" kind : RoleBinding - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleNotification - group : \"console.openshift.io\" kind : ConsoleLink roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : infra annotations : argocd.argoproj.io/sync-wave : \"100\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : # repoURL and targetRevision populated by kustomize patches in 1-infra/kustomization.yaml path : 0-bootstrap/single-cluster/1-infra syncPolicy : automated : prune : true selfHeal : true As with the 2-services.yaml , we can see An ArgoCD project called infra . ArgoCD applications defined in this project will be limited by the destinations: and clusterResourceWhitelist: specified in the YAML. An ArgoCD app called infra . This is the ArgoCD application that we used in the previous section of the tutorial. It watches the path: 0-bootstrap/single-cluster/1-infra folder for ArgoCD applications that it applied to the cluster. It was these applications that managed the ci , tools and dev namespaces for example. The installation of the operators will take approximately 30 - 45 minutes.","title":"Deploy services to the cluster"},{"location":"snippets/gitops-deploy-services-cp4s/#monitor-your-installation-in-the-workspace","text":"From this stage, the installation will take approximately 1.5 hours to complete. After you start the installation, you are brought to the Schematics workspace for your Cloud Pak. You can track progress by viewing the logs. Go to the Activity tab, and click View logs. To further track the installation, you can monitor the status of IBM Cloud Pak\u00ae for Security Threat Management: Log in to the OpenShift web console and ensure you are in the Administrator view. Go to Operators > Installed Operators and ensure that the Project is set to the namespace where IBM Cloud Pak\u00ae for Security was installed. In the list of installed operators, click IBM Cloud Pak for Security. On the Threat Management tab, select the threatmgmt instance. On the Details page, the following message is displayed in the Conditions section when installation is complete. Cloudpak for Security Deployment is successful. The installation is complete when you see the message Cloudpak for Security Deployment is successful.","title":"Monitor your installation in the workspace"},{"location":"snippets/gitops-deploy-services-cp4s/#retrieve-foundational-services-login-details","text":"Use the following commands to retrieve IBM Cloud Pak foundational services hostname, default username, and password: Hostname: oc -n ibm-common-services get route | grep cp-console | awk '{print $2}' Username: oc -n ibm-common-services get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_username}' | base64 --decode Password: oc -n ibm-common-services get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_password}' | base64 --decode These login details are required to access foundational services and configure your LDAP directory.","title":"Retrieve foundational services login details"},{"location":"snippets/gitops-deploy-services-cp4s/#next-steps","text":"If the adminUser you provided is a user ID that you added and authenticated by using the IBM Cloud account that is associated with the cluster and roksAuthentication was enabled, go to step 2. Otherwise, Configure LDAP authentication and ensure that the adminUser that you provided exists in the LDAP directory. Log in to Cloud Pak\u00ae for Security using the domain and the adminUser that you provided during installation. The domain, also known as application URL, can be retrieved by running the following command: oc get route isc-route-default --no-headers -n <CP4S_NAMESPACE> | awk '{print $2}' Select Enterprise LDAP in the login screen if you are logging in using an LDAP you connected to Foundational Services, otherwise use OpenShift Authentication if it is enabled. Add users to Cloud Pak\u00ae for Security Install the IBM\u00ae Security Orchestration & Automation license . If you choose Orchestration & Automation as part of your Cloud Pak\u00ae for Security bundle, you must install your Orchestration & Automation license to access the complete orchestration and automation capabilities that are provided by Orchestration & Automation. Configure data sources","title":"Next steps"},{"location":"snippets/gitops-deploy-services-cp4s/#validation","text":"Check the status of the CommonService and PlatformNavigator custom resource. oc get CP4SThreatManagement threatmgmt -n tools -o jsonpath = '{.status.conditions}' # Expected output = Cloudpak for Security Deployment is successful Before users can log in to the console for Cloud Pak for Security, an identity provider must be configured. The documentation provides further instructions. For DEMO purposes, OpenLDAP can be deployed and instructions are provided below. Download the cpctl utility Log in to the OpenShift cluster oc login --token = <token> --server = <openshift_url> -n <namespace> Retrieve the pod that contains the utility POD = $( oc get pod --no-headers -lrun = cp-serviceability | cut -d ' ' -f1 ) Copy the utility locally oc cp $POD :/opt/bin/<operatingsystem>/cpctl ./cpctl && chmod +x ./cpctl Install OpenLDAP Start a session ./cpctl load Install OpenLDAP cpctl tools deploy_openldap --token $( oc whoami -t ) --ldap_usernames 'adminUser,user1,user2,user3' --ldap_password cloudpak Initial user log in Retrieve Cloud Pak for Security Console URL oc get route isc-route-default --no-headers -n <CP4S_NAMESPACE> | awk '{print $2}' Log in with the user ID and password specified (ie. adminUser / cloudpak ).","title":"Validation"},{"location":"snippets/gitops-deploy-services/","text":"Deploy services to the cluster \u00b6 We've just had our first successful GitOps experience, using an ArgoCD application to create the ci , tools and dev namespaces in our cluster at the infrastructure layer level as well as using ArgoCD to deploy the Red Hat OpenShift Pipelines operator at the services layer level. There are few more components you need to create for this IBM MQ tutorial: IBM MQ, IBM Platform navigator, IBM foundations, IBM automation foundations, IBM Catalog, Sealed secrets, Sonarqube and Artifactory. These components are part of the services layer in our architecture, and that requires us to access /0-bootstrap/single-cluster/2-services within our GitOps repository. Check services layer components As we saw earlier, the bootstrap-single-cluster application uses the contents of the /0-bootstrap/single-cluster/2-services folder to determine which Kubernetes resources are available to be deployed in the cluster. Issue the following command to see what could be deployed in the cluster: tree 0 -bootstrap/single-cluster/2-services/ -L 3 You should see a set of operators and instances of these available in the GitOps framework at the services layer: 0 -bootstrap/single-cluster/2-services/ \u251c\u2500\u2500 2 -services.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 artifactory.yaml \u2502 \u2502 \u251c\u2500\u2500 baas-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 cert-manager-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 chartmuseum.yaml \u2502 \u2502 \u251c\u2500\u2500 developer-dashboard.yaml \u2502 \u2502 \u251c\u2500\u2500 grafana-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-gateway-analytics-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-management-portal-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4sthreatmanagements-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cpd-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-foundational-services-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-platform-navigator-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-process-mining-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 oadp-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 openshift-service-mesh-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 pact-broker.yaml \u2502 \u2502 \u251c\u2500\u2500 sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 sonarqube.yaml \u2502 \u2502 \u251c\u2500\u2500 spp-instance.yaml \u2502 \u2502 \u2514\u2500\u2500 swaggereditor.yaml \u2502 \u2514\u2500\u2500 operators \u2502 \u251c\u2500\u2500 cert-manager.yaml \u2502 \u251c\u2500\u2500 elasticsearch.yaml \u2502 \u251c\u2500\u2500 grafana-operator.yaml \u2502 \u251c\u2500\u2500 ibm-ace-operator.yaml \u2502 \u251c\u2500\u2500 ibm-apic-operator.yaml \u2502 \u251c\u2500\u2500 ibm-aspera-operator.yaml \u2502 \u251c\u2500\u2500 ibm-assetrepository-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-core-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-operator.yaml \u2502 \u251c\u2500\u2500 ibm-catalogs.yaml \u2502 \u251c\u2500\u2500 ibm-cp4a-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4i-operators.yaml \u2502 \u251c\u2500\u2500 ibm-cp4s-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-platform-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-scheduling-operator.yaml \u2502 \u251c\u2500\u2500 ibm-datapower-operator.yaml \u2502 \u251c\u2500\u2500 ibm-db2u-operator.yaml \u2502 \u251c\u2500\u2500 ibm-eventstreams-operator.yaml \u2502 \u251c\u2500\u2500 ibm-foundations.yaml \u2502 \u251c\u2500\u2500 ibm-mq-operator.yaml \u2502 \u251c\u2500\u2500 ibm-opsdashboard-operator.yaml \u2502 \u251c\u2500\u2500 ibm-platform-navigator.yaml \u2502 \u251c\u2500\u2500 ibm-process-mining-operator.yaml \u2502 \u251c\u2500\u2500 jaeger.yaml \u2502 \u251c\u2500\u2500 kiali.yaml \u2502 \u251c\u2500\u2500 oadp-operator.yaml \u2502 \u251c\u2500\u2500 openshift-gitops.yaml \u2502 \u251c\u2500\u2500 openshift-pipelines.yaml \u2502 \u251c\u2500\u2500 openshift-service-mesh.yaml \u2502 \u251c\u2500\u2500 spp-catalog.yaml \u2502 \u2514\u2500\u2500 spp-operator.yaml \u2514\u2500\u2500 kustomization.yaml Review ArgoCD services folder Let\u2019s examine the 0-bootstrap/single-cluster/2-services/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/kustomization.yaml We can see the contents of the kustomization.yaml : resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Add the services to the cluster Note The IBM Platform Navigator instance requires a RWX storageclass and it is set to managed-nfs-storage by default in the ArgoCD Application 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-platform-navigator-instance.yaml . This storageclass is available for Red Hat OpenShift on IBM Cloud cluster provisioned from IBM Technology Zone with NFS storage selected. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: ## Cloud Pak for Integration - argocd/operators/ibm-mq-operator.yaml - argocd/operators/ibm-platform-navigator.yaml - argocd/instances/ibm-platform-navigator-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD - argocd/instances/artifactory.yaml - argocd/instances/sonarqube.yaml You will have the following resources deployed for services: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml - argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml - argocd/operators/ibm-platform-navigator.yaml - argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml - argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml - argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 564 bytes | 564.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git b49dff5..533602c master -> master The intention of this operation is to indicate that we'd like the resources declared in 0-bootstrap/single-cluster/2-services/kustomization.yaml to be deployed in the cluster. Like the infra ArgoCD application, the resources created by the services ArgoCD application will manage the Kubernetes relevant services resources applied to the cluster. Optional: Activate the services in the GitOps repo Run this step if you skip installing tekton in the previous section. If you did not skip that step go to Step 7 . Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see lots of new ArgoCD applications have been created to manage the services we have deployed by modifying kustomization.yaml under 0-bootstrap/single-cluster/2-services folder: See how most ArgoCD applications are Synced almost immediately, but some spend time in Progressing . That's because the Artifactory and Sonarqube ArgoCD applications are more complex -- they create more Kubernetes resources than other ArgoCD applications and therefore take longer to sync . After a few minutes you'll see that all ArgoCD applications become Healthy and Synced : Notice how many more Synched ArgoCD applications are now in the cluster; these are as a result of the newly added services layer in our architecture. The services application Let's examine the ArgoCD application that manage the services in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the services application: We can see that the services ArgoCD application creates 10 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It\u2019s the services ArgoCD application that watches the 0-bootstrap/single-cluster/2-services/argocd folder for ArgoCD applications that apply service resources to our cluster. It was the services application that created the artifactory ArgoCD application which manages the Artifactory instance that is used for application configuration management that we will be exploring later in this section of the tutorial. We\u2019ll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it\u2019s not necessary, as you\u2019ll get lots of practice as we proceed. Examine Artifactory resources in detail You can see the Kubernetes resources that the Artifactory ArgoCD application has created for Artifactory. Click on the Artifactory : See how many different Kubernetes resources were created when the Artifactory ArgoCD application YAML was applied to the cluster. That's why it took the Artifactory ArgoCD application a little time to sync. If you'd like, have a look at some of the other ArgoCD applications, such as Sonarqube and the Kubernetes resources they created. The services ArgoCD project As we've seen in the ArgoCD UI, the services ArgoCD application is responsible for creating the ArgoCD applications that manage the services within the cluster. Let's examine their definitions to see how they do this. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/2-services.yaml The following YAML may initially look a little intimidating; we'll discuss the major elements below: --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : services labels : gitops.tier.layer : services spec : sourceRepos : [] # Populated by kustomize patches in 2-services/kustomization.yaml destinations : - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : redhat-operators server : https://kubernetes.default.svc - namespace : openshift-operators server : https://kubernetes.default.svc - namespace : openshift-marketplace server : https://kubernetes.default.svc - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : # TODO: SCC needs to be moved to 1-infra, here for now for artifactory - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleLink - group : \"apps\" kind : statefulsets - group : \"apps\" kind : deployments - group : \"\" kind : services - group : \"\" kind : configmaps - group : \"\" kind : secrets - group : \"\" kind : serviceaccounts - group : \"batch\" kind : jobs - group : \"\" kind : roles - group : \"route.openshift.io\" kind : routes - group : \"\" kind : RoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRole - group : apiextensions.k8s.io kind : CustomResourceDefinition roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : services annotations : argocd.argoproj.io/sync-wave : \"200\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : services source : # repoURL and targetRevision populated by kustomize patches in 2-services/kustomization.yaml path : 0-bootstrap/single-cluster/2-services syncPolicy : automated : prune : true selfHeal : true Notice how this YAML defines three ArgoCD resources: a services project which manages all the necessary services that are needed by the applications. Notice how the destinations for the services project are limited to the ci , tools and dev namespaces -- as well as a few others that we'll use in the tutorial. These destinations restrict the namespaces where ArgoCD applications in the services project can manage resources. The same is true for clusterResourceWhiteList . It limits the Kubernetes resources that can be managed to configmaps , deployments and rolebindings amongst others. In summary, we see that the service project is used to group all the ArgoCD applications that will manage the services in our cluster. These ArgoCD applications can only perform specific actions on specific resource types in specific namespaces. See how ArgoCD is acting as a well-governed administrator. The similar structure of services and infra ArgoCD applications Even though we didn't closely examine the infra ArgoCD application YAML in the previous topic, it has has a very similar structure to the ArgoCD services applications we've just examined. Type the following command to list the ArgoCD infra app YAML. cat 0 -bootstrap/single-cluster/1-infra/1-infra.yaml Again, although this YAML might look a little intimidating, the overall structure is the same as for services : --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : infra labels : gitops.tier.layer : infra spec : sourceRepos : [] # Populated by kustomize patches in 1-infra/kustomization.yaml destinations : - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : - group : \"\" kind : Namespace - group : \"\" kind : RoleBinding - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleNotification - group : \"console.openshift.io\" kind : ConsoleLink roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : infra annotations : argocd.argoproj.io/sync-wave : \"100\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : # repoURL and targetRevision populated by kustomize patches in 1-infra/kustomization.yaml path : 0-bootstrap/single-cluster/1-infra syncPolicy : automated : prune : true selfHeal : true As with the 2-services.yaml , we can see An ArgoCD project called infra . ArgoCD applications defined in this project will be limited by the destinations: and clusterResourceWhitelist: specified in the YAML. An ArgoCD app called infra . This is the ArgoCD application that we used in the previous section of the tutorial. It watches the path: 0-bootstrap/single-cluster/1-infra folder for ArgoCD applications that it applied to the cluster. It was these applications that managed the ci , tools and dev namespaces for example. The installation of the operators will take approximately 30 - 45 minutes. Validate the deployment of IBM MQ and IBM Platform Navigator. Verify the IBM Platform Navigator has been deployed successfully. oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' Expected output = True The Platform Navigator console has been installed as part of the IBM Cloud Pak for Integration. We use the Platform Navigator to view our queue manager and its queues. The Spring application includes a Swagger interface which we can use to call methods on the Spring application. In the next few steps we will use these to interact with both the queue manager and Spring application. Access the IBM Platform Navigator console Retrieve the URL for the IBM Platform Navigator console: oc get route integration-navigator-pn -n tools -o jsonpath = '{.spec.host}' Copy the URL from the HOST/PORT column and paste into a browser, prefixed with https:// . Retrieve the admin password: oc get secret ibm-iam-bindinfo-platform-auth-idp-credentials -n dev -o json -n ibm-common-services | jq -r '.data.\"admin_password\"' | base64 -D Log in to the IBM Platform Navigator console with the admin credentials.","title":"Gitops deploy services"},{"location":"snippets/gitops-deploy-services/#deploy-services-to-the-cluster","text":"We've just had our first successful GitOps experience, using an ArgoCD application to create the ci , tools and dev namespaces in our cluster at the infrastructure layer level as well as using ArgoCD to deploy the Red Hat OpenShift Pipelines operator at the services layer level. There are few more components you need to create for this IBM MQ tutorial: IBM MQ, IBM Platform navigator, IBM foundations, IBM automation foundations, IBM Catalog, Sealed secrets, Sonarqube and Artifactory. These components are part of the services layer in our architecture, and that requires us to access /0-bootstrap/single-cluster/2-services within our GitOps repository. Check services layer components As we saw earlier, the bootstrap-single-cluster application uses the contents of the /0-bootstrap/single-cluster/2-services folder to determine which Kubernetes resources are available to be deployed in the cluster. Issue the following command to see what could be deployed in the cluster: tree 0 -bootstrap/single-cluster/2-services/ -L 3 You should see a set of operators and instances of these available in the GitOps framework at the services layer: 0 -bootstrap/single-cluster/2-services/ \u251c\u2500\u2500 2 -services.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 artifactory.yaml \u2502 \u2502 \u251c\u2500\u2500 baas-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 cert-manager-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 chartmuseum.yaml \u2502 \u2502 \u251c\u2500\u2500 developer-dashboard.yaml \u2502 \u2502 \u251c\u2500\u2500 grafana-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-gateway-analytics-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-apic-management-portal-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cp4sthreatmanagements-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-cpd-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-foundational-services-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-platform-navigator-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-process-mining-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 instana-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 oadp-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 openshift-service-mesh-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 pact-broker.yaml \u2502 \u2502 \u251c\u2500\u2500 sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 sonarqube.yaml \u2502 \u2502 \u251c\u2500\u2500 spp-instance.yaml \u2502 \u2502 \u2514\u2500\u2500 swaggereditor.yaml \u2502 \u2514\u2500\u2500 operators \u2502 \u251c\u2500\u2500 cert-manager.yaml \u2502 \u251c\u2500\u2500 elasticsearch.yaml \u2502 \u251c\u2500\u2500 grafana-operator.yaml \u2502 \u251c\u2500\u2500 ibm-ace-operator.yaml \u2502 \u251c\u2500\u2500 ibm-apic-operator.yaml \u2502 \u251c\u2500\u2500 ibm-aspera-operator.yaml \u2502 \u251c\u2500\u2500 ibm-assetrepository-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-core-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-operator.yaml \u2502 \u251c\u2500\u2500 ibm-catalogs.yaml \u2502 \u251c\u2500\u2500 ibm-cp4a-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4d-watson-studio-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cp4i-operators.yaml \u2502 \u251c\u2500\u2500 ibm-cp4s-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-platform-operator.yaml \u2502 \u251c\u2500\u2500 ibm-cpd-scheduling-operator.yaml \u2502 \u251c\u2500\u2500 ibm-datapower-operator.yaml \u2502 \u251c\u2500\u2500 ibm-db2u-operator.yaml \u2502 \u251c\u2500\u2500 ibm-eventstreams-operator.yaml \u2502 \u251c\u2500\u2500 ibm-foundations.yaml \u2502 \u251c\u2500\u2500 ibm-mq-operator.yaml \u2502 \u251c\u2500\u2500 ibm-opsdashboard-operator.yaml \u2502 \u251c\u2500\u2500 ibm-platform-navigator.yaml \u2502 \u251c\u2500\u2500 ibm-process-mining-operator.yaml \u2502 \u251c\u2500\u2500 jaeger.yaml \u2502 \u251c\u2500\u2500 kiali.yaml \u2502 \u251c\u2500\u2500 oadp-operator.yaml \u2502 \u251c\u2500\u2500 openshift-gitops.yaml \u2502 \u251c\u2500\u2500 openshift-pipelines.yaml \u2502 \u251c\u2500\u2500 openshift-service-mesh.yaml \u2502 \u251c\u2500\u2500 spp-catalog.yaml \u2502 \u2514\u2500\u2500 spp-operator.yaml \u2514\u2500\u2500 kustomization.yaml Review ArgoCD services folder Let\u2019s examine the 0-bootstrap/single-cluster/2-services/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/kustomization.yaml We can see the contents of the kustomization.yaml : resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Add the services to the cluster Note The IBM Platform Navigator instance requires a RWX storageclass and it is set to managed-nfs-storage by default in the ArgoCD Application 0-bootstrap/single-cluster/2-services/argocd/instances/ibm-platform-navigator-instance.yaml . This storageclass is available for Red Hat OpenShift on IBM Cloud cluster provisioned from IBM Technology Zone with NFS storage selected. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: ## Cloud Pak for Integration - argocd/operators/ibm-mq-operator.yaml - argocd/operators/ibm-platform-navigator.yaml - argocd/instances/ibm-platform-navigator-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD - argocd/instances/artifactory.yaml - argocd/instances/sonarqube.yaml You will have the following resources deployed for services: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml - argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml - argocd/operators/ibm-platform-navigator.yaml - argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services - argocd/operators/ibm-foundations.yaml - argocd/instances/ibm-foundational-services-instance.yaml - argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs - argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets - argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml - argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml - argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 564 bytes | 564.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git b49dff5..533602c master -> master The intention of this operation is to indicate that we'd like the resources declared in 0-bootstrap/single-cluster/2-services/kustomization.yaml to be deployed in the cluster. Like the infra ArgoCD application, the resources created by the services ArgoCD application will manage the Kubernetes relevant services resources applied to the cluster. Optional: Activate the services in the GitOps repo Run this step if you skip installing tekton in the previous section. If you did not skip that step go to Step 7 . Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see lots of new ArgoCD applications have been created to manage the services we have deployed by modifying kustomization.yaml under 0-bootstrap/single-cluster/2-services folder: See how most ArgoCD applications are Synced almost immediately, but some spend time in Progressing . That's because the Artifactory and Sonarqube ArgoCD applications are more complex -- they create more Kubernetes resources than other ArgoCD applications and therefore take longer to sync . After a few minutes you'll see that all ArgoCD applications become Healthy and Synced : Notice how many more Synched ArgoCD applications are now in the cluster; these are as a result of the newly added services layer in our architecture. The services application Let's examine the ArgoCD application that manage the services in our reference architecture. In the ArgoCD UI Applications view, click on the icon for the services application: We can see that the services ArgoCD application creates 10 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It\u2019s the services ArgoCD application that watches the 0-bootstrap/single-cluster/2-services/argocd folder for ArgoCD applications that apply service resources to our cluster. It was the services application that created the artifactory ArgoCD application which manages the Artifactory instance that is used for application configuration management that we will be exploring later in this section of the tutorial. We\u2019ll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it\u2019s not necessary, as you\u2019ll get lots of practice as we proceed. Examine Artifactory resources in detail You can see the Kubernetes resources that the Artifactory ArgoCD application has created for Artifactory. Click on the Artifactory : See how many different Kubernetes resources were created when the Artifactory ArgoCD application YAML was applied to the cluster. That's why it took the Artifactory ArgoCD application a little time to sync. If you'd like, have a look at some of the other ArgoCD applications, such as Sonarqube and the Kubernetes resources they created. The services ArgoCD project As we've seen in the ArgoCD UI, the services ArgoCD application is responsible for creating the ArgoCD applications that manage the services within the cluster. Let's examine their definitions to see how they do this. Issue the following command: cat 0 -bootstrap/single-cluster/2-services/2-services.yaml The following YAML may initially look a little intimidating; we'll discuss the major elements below: --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : services labels : gitops.tier.layer : services spec : sourceRepos : [] # Populated by kustomize patches in 2-services/kustomization.yaml destinations : - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : redhat-operators server : https://kubernetes.default.svc - namespace : openshift-operators server : https://kubernetes.default.svc - namespace : openshift-marketplace server : https://kubernetes.default.svc - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : # TODO: SCC needs to be moved to 1-infra, here for now for artifactory - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleLink - group : \"apps\" kind : statefulsets - group : \"apps\" kind : deployments - group : \"\" kind : services - group : \"\" kind : configmaps - group : \"\" kind : secrets - group : \"\" kind : serviceaccounts - group : \"batch\" kind : jobs - group : \"\" kind : roles - group : \"route.openshift.io\" kind : routes - group : \"\" kind : RoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRoleBinding - group : \"rbac.authorization.k8s.io\" kind : ClusterRole - group : apiextensions.k8s.io kind : CustomResourceDefinition roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : services annotations : argocd.argoproj.io/sync-wave : \"200\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : services source : # repoURL and targetRevision populated by kustomize patches in 2-services/kustomization.yaml path : 0-bootstrap/single-cluster/2-services syncPolicy : automated : prune : true selfHeal : true Notice how this YAML defines three ArgoCD resources: a services project which manages all the necessary services that are needed by the applications. Notice how the destinations for the services project are limited to the ci , tools and dev namespaces -- as well as a few others that we'll use in the tutorial. These destinations restrict the namespaces where ArgoCD applications in the services project can manage resources. The same is true for clusterResourceWhiteList . It limits the Kubernetes resources that can be managed to configmaps , deployments and rolebindings amongst others. In summary, we see that the service project is used to group all the ArgoCD applications that will manage the services in our cluster. These ArgoCD applications can only perform specific actions on specific resource types in specific namespaces. See how ArgoCD is acting as a well-governed administrator. The similar structure of services and infra ArgoCD applications Even though we didn't closely examine the infra ArgoCD application YAML in the previous topic, it has has a very similar structure to the ArgoCD services applications we've just examined. Type the following command to list the ArgoCD infra app YAML. cat 0 -bootstrap/single-cluster/1-infra/1-infra.yaml Again, although this YAML might look a little intimidating, the overall structure is the same as for services : --- apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : infra labels : gitops.tier.layer : infra spec : sourceRepos : [] # Populated by kustomize patches in 1-infra/kustomization.yaml destinations : - namespace : ci server : https://kubernetes.default.svc - namespace : dev server : https://kubernetes.default.svc - namespace : staging server : https://kubernetes.default.svc - namespace : prod server : https://kubernetes.default.svc - namespace : sealed-secrets server : https://kubernetes.default.svc - namespace : tools server : https://kubernetes.default.svc - namespace : ibm-common-services server : https://kubernetes.default.svc - namespace : istio-system server : https://kubernetes.default.svc - namespace : openldap server : https://kubernetes.default.svc - namespace : instana-agent server : https://kubernetes.default.svc - namespace : openshift-gitops server : https://kubernetes.default.svc clusterResourceWhitelist : - group : \"\" kind : Namespace - group : \"\" kind : RoleBinding - group : \"security.openshift.io\" kind : SecurityContextConstraints - group : \"console.openshift.io\" kind : ConsoleNotification - group : \"console.openshift.io\" kind : ConsoleLink roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - argocd-admins --- apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : infra annotations : argocd.argoproj.io/sync-wave : \"100\" labels : gitops.tier.layer : gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : infra source : # repoURL and targetRevision populated by kustomize patches in 1-infra/kustomization.yaml path : 0-bootstrap/single-cluster/1-infra syncPolicy : automated : prune : true selfHeal : true As with the 2-services.yaml , we can see An ArgoCD project called infra . ArgoCD applications defined in this project will be limited by the destinations: and clusterResourceWhitelist: specified in the YAML. An ArgoCD app called infra . This is the ArgoCD application that we used in the previous section of the tutorial. It watches the path: 0-bootstrap/single-cluster/1-infra folder for ArgoCD applications that it applied to the cluster. It was these applications that managed the ci , tools and dev namespaces for example. The installation of the operators will take approximately 30 - 45 minutes. Validate the deployment of IBM MQ and IBM Platform Navigator. Verify the IBM Platform Navigator has been deployed successfully. oc get platformnavigator -n tools -o = jsonpath = '{ .items[*].status.conditions[].status }' Expected output = True The Platform Navigator console has been installed as part of the IBM Cloud Pak for Integration. We use the Platform Navigator to view our queue manager and its queues. The Spring application includes a Swagger interface which we can use to call methods on the Spring application. In the next few steps we will use these to interact with both the queue manager and Spring application. Access the IBM Platform Navigator console Retrieve the URL for the IBM Platform Navigator console: oc get route integration-navigator-pn -n tools -o jsonpath = '{.spec.host}' Copy the URL from the HOST/PORT column and paste into a browser, prefixed with https:// . Retrieve the admin password: oc get secret ibm-iam-bindinfo-platform-auth-idp-credentials -n dev -o json -n ibm-common-services | jq -r '.data.\"admin_password\"' | base64 -D Log in to the IBM Platform Navigator console with the admin credentials.","title":"Deploy services to the cluster"},{"location":"snippets/gitops-install-argocd/","text":"Installing ArgoCD for GitOps \u00b6 Now that we've had an initial view of the GitOps repository, let's install ArgoCD to make use of it. We will install ArgoCD using the Red Hat OpenShift GitOps operator. We will also configure it with the appropriate access rights to limit the resources it can create in the cluster. In so doing, we ensure that ArgoCD acts like a well-governed administrator of the cluster, only doing what it needs to do. Also, we will customize our ArgoCD instance by adding necessary custom checks. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". On successful login, you'll see: Logged into \"https://c100-e.jp-tok.containers.cloud.ibm.com:30749\" as \"IAM#hperabat@in.ibm.com\" using the token provided. You have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\" . This shows some basic information about your user within this cluster. (This user is different to your GitHub user.) Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. The following command creates the subscription: oc apply -f setup/ocp47/ The response confirms that the below resources has been created: clusterrole.rbac.authorization.k8s.io/custom-argocd-cluster-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-cntk-argocd-application-controller created subscription.operators.coreos.com/openshift-gitops-operator created Wait for the ArgoCD installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/gitops-operator-controller-manager -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/gitops-operator condition met ArgoCD is now installed and ready to use. If you see something like: Error from server (NotFound): deployments.apps \"gitops-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Alternatively, you can also validate as follows: while ! oc wait crd applications.argoproj.io --timeout = -1s --for = condition = Established 2 >/dev/null ; do sleep 30 ; done while ! oc wait pod --timeout = -1s --for = condition = Ready -l '!job-name' -n openshift-gitops > /dev/null ; do sleep 30 ; done ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are also setup along with the subscription. oc get clusterrole custom-argocd-cluster-argocd-application-controller oc get clusterrolebinding openshift-gitops-argocd-application-controller oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller You'll see the resources are created in the cluster: $ oc get clusterrole custom-argocd-cluster-argocd-application-controller NAME CREATED AT custom-argocd-cluster-argocd-application-controller 2021-08-27T13:35:13Z $ oc get clusterrolebinding openshift-gitops-argocd-application-controller NAME ROLE AGE openshift-gitops-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m43s $ oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller NAME ROLE AGE openshift-gitops-cntk-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m45s The clusterrole custom-argocd-cluster-argocd-application-controller defines a specific set of specific resources that are required by the ArgoCD service account and access rights over them. The cluster role bindings openshift-gitops-argocd-application-controller and openshift-gitops-cntk-argocd-application-controller binds the ArgoCD service account to the cluster role above. In combination, these definitions limit ArgoCD to perform the minimum set of operations required to manage the cluster. This is important; it means that ArgoCD behaves as a well governed administrator of the cluster. We'll also see later in the tutorial how ArgoCD creates resource in the cluster. Delete default ArgoCD instance Let us now delete the default ArgoCD instance that is created earlier. oc delete gitopsservice cluster -n openshift-gitops || true You will see the resources being deleted as follows: warning: deleting cluster-scoped resources, not scoped to the provided namespace gitopsservice.pipelines.openshift.io \"cluster\" deleted We are deleting the existing default instance and planning to make use of a customized ArgoCD instance with additional capabilities. Create a custom ArgoCD instance This customized ArgoCD instance implements custom checks. By default ArgoCD provides some built-in health checks for validating standard kubernetes resources. However, these checks will not be sufficient to validate the custom resources that belong to IBM Cloud Pak for Integration. Argo CD supports custom health checks that are defined in Lua . If you want explore more about the ArgoCD Resource Health, check this link out. Let us create a custom ArgoCD instance as follows: oc apply -f setup/ocp47/argocd-instance/ -n openshift-gitops The response confirms that the below resources has been created: argocd.argoproj.io/openshift-gitops-cntk created Wait till the ArgoCD instance pod is up and running. Issue the below command to verify the same: while ! oc wait pod --timeout = -1s --for = condition = ContainersReady -l app.kubernetes.io/name = openshift-gitops-cntk-server -n openshift-gitops > /dev/null ; do sleep 30 ; done Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops | grep openshift-gitops-cntk-server | awk '{print \"https://\"$2}' This will list the route to the ArgoCD we've just installed, for example: https://openshift-gitops-cntk-server-openshift-gitops.ibmcloud-roks-xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/openshift-gitops-cntk-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Once the UI launches, you'll see: See how there are no ArgoCD applications active at the moment. In the next section of the tutorial, we'll configure ArgoCD to create the ArgoCD applications that will in turn spin up infrastructure , service , and application resources to apply to the cluster.","title":"Gitops install argocd"},{"location":"snippets/gitops-install-argocd/#installing-argocd-for-gitops","text":"Now that we've had an initial view of the GitOps repository, let's install ArgoCD to make use of it. We will install ArgoCD using the Red Hat OpenShift GitOps operator. We will also configure it with the appropriate access rights to limit the resources it can create in the cluster. In so doing, we ensure that ArgoCD acts like a well-governed administrator of the cluster, only doing what it needs to do. Also, we will customize our ArgoCD instance by adding necessary custom checks. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". On successful login, you'll see: Logged into \"https://c100-e.jp-tok.containers.cloud.ibm.com:30749\" as \"IAM#hperabat@in.ibm.com\" using the token provided. You have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\" . This shows some basic information about your user within this cluster. (This user is different to your GitHub user.) Install ArgoCD into the cluster We use the Red Hat GitOps operator to install ArgoCD into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. The following command creates the subscription: oc apply -f setup/ocp47/ The response confirms that the below resources has been created: clusterrole.rbac.authorization.k8s.io/custom-argocd-cluster-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-cntk-argocd-application-controller created subscription.operators.coreos.com/openshift-gitops-operator created Wait for the ArgoCD installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/gitops-operator-controller-manager -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/gitops-operator condition met ArgoCD is now installed and ready to use. If you see something like: Error from server (NotFound): deployments.apps \"gitops-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected. Alternatively, you can also validate as follows: while ! oc wait crd applications.argoproj.io --timeout = -1s --for = condition = Established 2 >/dev/null ; do sleep 30 ; done while ! oc wait pod --timeout = -1s --for = condition = Ready -l '!job-name' -n openshift-gitops > /dev/null ; do sleep 30 ; done ArgoCD clusterrole and clusterrolebinding ArgoCD runs under a dedicated service account. For good governance, we use a custom clusterrole and clusterrolebinding to control the specific operations this service account can perform on different resources in the cluster. The cluster role and the cluster role binding YAMLs are also setup along with the subscription. oc get clusterrole custom-argocd-cluster-argocd-application-controller oc get clusterrolebinding openshift-gitops-argocd-application-controller oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller You'll see the resources are created in the cluster: $ oc get clusterrole custom-argocd-cluster-argocd-application-controller NAME CREATED AT custom-argocd-cluster-argocd-application-controller 2021-08-27T13:35:13Z $ oc get clusterrolebinding openshift-gitops-argocd-application-controller NAME ROLE AGE openshift-gitops-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m43s $ oc get clusterrolebinding openshift-gitops-cntk-argocd-application-controller NAME ROLE AGE openshift-gitops-cntk-argocd-application-controller ClusterRole/custom-argocd-cluster-argocd-application-controller 8m45s The clusterrole custom-argocd-cluster-argocd-application-controller defines a specific set of specific resources that are required by the ArgoCD service account and access rights over them. The cluster role bindings openshift-gitops-argocd-application-controller and openshift-gitops-cntk-argocd-application-controller binds the ArgoCD service account to the cluster role above. In combination, these definitions limit ArgoCD to perform the minimum set of operations required to manage the cluster. This is important; it means that ArgoCD behaves as a well governed administrator of the cluster. We'll also see later in the tutorial how ArgoCD creates resource in the cluster. Delete default ArgoCD instance Let us now delete the default ArgoCD instance that is created earlier. oc delete gitopsservice cluster -n openshift-gitops || true You will see the resources being deleted as follows: warning: deleting cluster-scoped resources, not scoped to the provided namespace gitopsservice.pipelines.openshift.io \"cluster\" deleted We are deleting the existing default instance and planning to make use of a customized ArgoCD instance with additional capabilities. Create a custom ArgoCD instance This customized ArgoCD instance implements custom checks. By default ArgoCD provides some built-in health checks for validating standard kubernetes resources. However, these checks will not be sufficient to validate the custom resources that belong to IBM Cloud Pak for Integration. Argo CD supports custom health checks that are defined in Lua . If you want explore more about the ArgoCD Resource Health, check this link out. Let us create a custom ArgoCD instance as follows: oc apply -f setup/ocp47/argocd-instance/ -n openshift-gitops The response confirms that the below resources has been created: argocd.argoproj.io/openshift-gitops-cntk created Wait till the ArgoCD instance pod is up and running. Issue the below command to verify the same: while ! oc wait pod --timeout = -1s --for = condition = ContainersReady -l app.kubernetes.io/name = openshift-gitops-cntk-server -n openshift-gitops > /dev/null ; do sleep 30 ; done Launch ArgoCD ArgoCD can be accessed via an OpenShift route. Using a browser, navigate to the URL returned by following command: oc get route -n openshift-gitops | grep openshift-gitops-cntk-server | awk '{print \"https://\"$2}' This will list the route to the ArgoCD we've just installed, for example: https://openshift-gitops-cntk-server-openshift-gitops.ibmcloud-roks-xxxxx.containers.appdomain.cloud Copy the URL into your browser to launch the ArgoCD web console. ( You can safely ignore any browser certificate warnings. ) Login to ArgoCD Sign in with user admin . The password is stored in a Kubernetes secret in the cluster. Use the following command to retrieve and format it: oc get secret/openshift-gitops-cntk-cluster -n openshift-gitops -o json | jq -r '.data.\"admin.password\"' | base64 -D Once the UI launches, you'll see: See how there are no ArgoCD applications active at the moment. In the next section of the tutorial, we'll configure ArgoCD to create the ArgoCD applications that will in turn spin up infrastructure , service , and application resources to apply to the cluster.","title":"Installing ArgoCD for GitOps"},{"location":"snippets/gitops-install-tekton/","text":"Installing Tekton for GitOps \u00b6 Tekton is made available to your Red Hat OpenShift cluster through the Red Hat OpenShift Pipelines operator. Hence, let's see how to get that operator installed on your cluster. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Install Tekton into the cluster We use the Red Hat Pipelines operator to install Tekton into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: - argocd/operators/openshift-pipelines.yaml Your kustomization.yaml for services should match the following: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Install tekton using Red Hat OpenShift Pipelines Operator\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 524 bytes | 524.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 85a4c46..61e15b0 master -> master Activate the services in the GitOps repo Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us only deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see openshift-pipelines ArgoCD application have been created as follows: Wait for the Tekton installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/openshift-pipelines-operator -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/openshift-pipelines-operator condition met Tekton is now installed and ready to use. We\u2019ll explore Tekton in later topics. If you see something like: Error from server (NotFound): deployments.apps \"openshift-pipelines-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected.","title":"Gitops install tekton"},{"location":"snippets/gitops-install-tekton/#installing-tekton-for-gitops","text":"Tekton is made available to your Red Hat OpenShift cluster through the Red Hat OpenShift Pipelines operator. Hence, let's see how to get that operator installed on your cluster. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Install Tekton into the cluster We use the Red Hat Pipelines operator to install Tekton into the cluster. The sample repository contains the YAML necessary to do this. We\u2019ll examine it later, but first let\u2019s use it. Open 0-bootstrap/single-cluster/2-services/kustomization.yaml and uncomment the below resources: - argocd/operators/openshift-pipelines.yaml Your kustomization.yaml for services should match the following: resources : # IBM Software ## Cloud Pak for Integration #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/instances/ibm-apic-instance.yaml #- argocd/instances/ibm-apic-management-portal-instance.yaml #- argocd/instances/ibm-apic-gateway-analytics-instance.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml ## Cloud Pak for Business Automation #- argocd/operators/ibm-cp4a-operator.yaml #- argocd/operators/ibm-db2u-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml ## Cloud Pak for Data #- argocd/operators/ibm-cp4d-watson-studio-operator.yaml #- argocd/instances/ibm-cp4d-watson-studio-instance.yaml #- argocd/operators/ibm-cpd-platform-operator.yaml #- argocd/operators/ibm-cpd-scheduling-operator.yaml #- argocd/instances/ibm-cpd-instance.yaml ## Cloud Pak for Security #- argocd/operators/ibm-cp4s-operator.yaml #- argocd/instances/ibm-cp4sthreatmanagements-instance.yaml ## IBM Foundational Services / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml #- argocd/operators/ibm-automation-foundation-operator.yaml ## IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount - argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml #- argocd/instances/openshift-service-mesh-instance.yaml # Monitoring #- argocd/instances/instana-agent.yaml #- argocd/instances/instana-robot-shop.yaml # Spectrum Protect Plus #- argocd/operators/spp-catalog.yaml #- argocd/operators/spp-operator.yaml #- argocd/instances/spp-instance.yaml #- argocd/operators/oadp-operator.yaml #- argocd/instances/oadp-instance.yaml #- argocd/instances/baas-instance.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=applications,gitops.tier.source=git\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=services,gitops.tier.source=helm\" patch : |- - op: add path: /spec/source/repoURL value: https://charts.cloudnativetoolkit.dev - target : name : ibm-automation-foundation-operator patch : |- - op: add path: /spec/source/helm/parameters/- value: name: spec.channel value: v1.1 Commit and push changes to your git repository: git add . git commit -s -m \"Install tekton using Red Hat OpenShift Pipelines Operator\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 524 bytes | 524.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 85a4c46..61e15b0 master -> master Activate the services in the GitOps repo Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us only deploy services resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and uncomment 2-services/2-services.yaml as follows: resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Once we push this change to GitHub, it will be seen by the bootstrap-single-cluster application in ArgoCD, and the resources it refers to will be applied to the cluster. Push GitOps changes to GitHub Let\u2019s make these GitOps changes visible to the ArgoCD bootstrap-single-cluster application via GitHub. Add all changes in the current folder to a git index, commit them, and push them to GitHub: git add . git commit -s -m \"Deploying services\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 431 bytes | 431.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git 533602c..85a4c46 master -> master This change to the GitOps repository can now be used by ArgoCD. The bootstrap-single-cluster application detects the change and resyncs Once these changes to our GitOps repository are seen by ArgoCD, it will resync the cluster to the desired new state. Switch to the ArgoCD UI Applications view to see the start of this resync process: Notice how the bootstrap-single-cluster application has detected the changes and is automatically synching the cluster. ( You can manually sync the bootstrap-single-cluster ArgoCD application in the UI if you don't want to wait for ArgoCD to detect the change. ) The new ArgoCD applications After a short while, you'll see openshift-pipelines ArgoCD application have been created as follows: Wait for the Tekton installation to complete Installation is an asynchronous process, so we can issue a command that will complete when installation is done. Wait 30 seconds for the installation to get started, then issue the following command: sleep 30 ; oc wait --for condition = available --timeout 60s deployment.apps/openshift-pipelines-operator -n openshift-operators After a while, you should see the following message informing us that operator installation is complete: deployment.apps/openshift-pipelines-operator condition met Tekton is now installed and ready to use. We\u2019ll explore Tekton in later topics. If you see something like: Error from server (NotFound): deployments.apps \"openshift-pipelines-operator\" not found then re-issue the command; the error occurred because the subscription took a little longer to create than expected.","title":"Installing Tekton for GitOps"},{"location":"snippets/gitops-repo-argocd-connect-cp4s/","text":"Connect ArgoCD to the GitOps repository \u00b6 Let's now connect your customized GitOps repository to the instance of ArgoCD running in the cluster. Once connected, ArgoCD will use the contents of the repository to create necessary resources. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Review ArgoCD infrastructure folder Let's examine the 0-bootstrap/single-cluster/1-infra/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml We can see the contents of the kustomization.yaml : resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Notice that the resources that needs to be applied to the cluster are all inactivate and commented out. Let us enable the resources that are needed by un-commenting them. Deploy Kubernetes resources with kustomization.yaml Open 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the below resources: argocd/namespace-ci.yaml argocd/consolenotification.yaml argocd/namespace-ibm-common-services.yaml argocd/namespace-tools.yaml You will have the following resources un-commented for infrastructure: resources : #- argocd/consolelink.yaml - argocd/consolenotification.yaml - argocd/namespace-ibm-common-services.yaml - argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml - argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for infrastructure\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a900c39..e3f696d master -> master Examine bootstrap.yaml residing in 0-bootstrap/single-cluster/ . The bootstrap.yaml file is used to create our first ArgoCD application called bootstrap-single-cluster . This initial ArgoCD application will create all the other ArgoCD applications that control the application, service, and infrastructure resources (such as the ci and dev namespaces) deployed to the cluster. Examine the YAML that defines the ArgoCD bootstrap application: cat 0 -bootstrap/single-cluster/bootstrap.yaml Notice also how this ArgoCD application has been customized to use the GitOps repository repoURL: https://github.com/prod-ref-guide/multi-tenancy-gitops.git . apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-single-cluster namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/single-cluster repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Most importantly, see how path: 0-bootstrap/single-cluster refers to the 0-bootstrap/single-cluster folder within this repository. This will result in the creation of individual ArgoCD applications to manage our cluster resources. Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us for now only deploy infra resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and comment out the 2-services/2-services.yaml and 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml # - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Commit and push changes to your git repository: git add . git commit -s -m \"Using only infra\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 456 bytes | 456.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git e3f696d..ea3b43f master -> master Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. The repository contains a bootstrap-single-cluster ArgoCD application that is watching this repository and using its contents to manage the cluster. When the bootstrap-single-cluster ArgoCD application is applied to the cluster, it will continuously ensure that all the activated resources are applied to the cluster. Apply the bootstrap YAML to the cluster: oc apply -f 0 -bootstrap/single-cluster/bootstrap.yaml Kubernetes will confirm that the bootstrap ArgoCD application has been created: application.argoproj.io/bootstrap-single-cluster created The bootstrap ArgoCD application will watch the 0-bootstrap/single-cluster folder in our GitOps repository on GitHub. In this way, as resources are added to the infrastructure , service and application folders, they will be deployed to the cluster automatically. This is therefore the only direct cluster operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Verify the bootstrap deployment Verify that the bootstrap ArgoCD application is running with the following command: oc get app/bootstrap-single-cluster -n openshift-gitops You should see that the bootstrap application was recently updated: NAME SYNC STATUS HEALTH STATUS bootstrap-single-cluster Synced Healthy HEALTH_STATUS may temporarily show Missing ; simply re-issue the command to confirm it moves to Healthy . Using the UI to view the newly deployed ArgoCD applications In the previous section of this chapter you logged on to the ArgoCD web console. Switch back to that console, refresh the page and you should see the bootstrap-single-cluster ArgoCD application together with many other ArgoCD applications: (You may need to select List view rather than the Tiles view.) We can see that eleven ArgoCD applications have been deployed to the cluster as a result of applying bootstrap.yaml . In the next section of the tutorial, we'll examine these applications to see how and why they were created, but for now let's focus on one of them -- the namespace-ci ArgoCD application. Examining the namespace-ci ArgoCD application resources Let's examine the Kubernetes resources applied to the cluster by the namespace-ci ArgoCD application. In the ArgoCD application list, click on namespace-ci : (You may need to clear filters to see this screenshot.) The directed graph shows that the namespace-ci ArgoCD app has created 4 Kubernetes resources; our ci namespace and three role bindings. Verify the namespace using the oc CLI We've seen the new namespace definition in the GitOps repository and visually in the ArgoCD UI. Let's also verify it via the command line: Type the following command: oc get namespace ci -o yaml This will list the full details of the ci namespace: apiVersion : v1 kind : Namespace metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"namespace-ci\"},\"name\":\"ci\"},\"spec\":{}} openshift.io/sa.scc.mcs : s0:c27,c9 openshift.io/sa.scc.supplemental-groups : 1000720000/10000 openshift.io/sa.scc.uid-range : 1000720000/10000 creationTimestamp : \"2021-08-31T15:27:32Z\" labels : app.kubernetes.io/instance : namespace-ci managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:labels : . : {} f:app.kubernetes.io/instance : {} f:status : f:phase : {} manager : argocd-application-controller operation : Update time : \"2021-08-31T15:27:32Z\" - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : f:openshift.io/sa.scc.mcs : {} f:openshift.io/sa.scc.supplemental-groups : {} f:openshift.io/sa.scc.uid-range : {} manager : cluster-policy-controller operation : Update time : \"2021-08-31T15:27:32Z\" name : ci resourceVersion : \"2255607\" selfLink : /api/v1/namespaces/ci uid : fff6b82b-6318-4828-83bb-ade4e8e3c0cf spec : finalizers : - kubernetes status : phase : Active Notice how manager: argocd-application-controller identifies that this namespace was created by ArgoCD. It's important to understand the sequence of actions. We simply deployed the bootstrap-single-cluster ArgoCD application, and it ultimately resulted in the creation of the namespace-ci ArgoCD application which created the ci namespace. We don't directly apply resources to the cluster once the bootstrap-single-cluster ArgoCD application has been applied; the cluster state is determined by an ArgoCD application YAML in the corresponding application , service or infrastructure related folders. It's these ArgoCD applications that create and manage the underlying Kubernetes resources using the GitOps repository as the source of truth. Understanding the namespace-ci ArgoCD application Let's examine the ArgoCD application namespace-ci to see how it created the ci namespace and three role bindings in the cluster. Issue the following command to examine its YAML: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-ci.yaml Notice that apiVersion and kind identify this as an ArgoCD application: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ci labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" spec : destination : namespace : ci server : https://kubernetes.default.svc project : infra source : path : namespaces/ci syncPolicy : automated : prune : true selfHeal : true Most importantly, see how this namespace-ci ArgoCD application monitors the folder path: namespaces/ci in https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git ; it then applies the contents of this folder to the cluster whenever its content changes. Notice the SyncPolicy of automated ; any changes to this folder will automatically be applied to the cluster; we do not need to perform a manual Sync operation from the ArgoCD UI. Examine the ci namespace YAML To examine this, navigate to multi-tenancy-gitops-infra repository you cloned previously. cd $HOME /git cd multi-tenancy-gitops-infra Let's examine the ci namespace YAML in the namespaces/ci folder: cat namespaces/ci/namespace.yaml It's a very simple YAML: apiVersion : v1 kind : Namespace metadata : name : ci spec : {} This is a YAML that we would normally apply to the cluster manually or via a script. However, when we use GitOps, we push the ArgoCD application that uses this YAML to GitHub, and it applies the ci namespace YAML to the cluster. This is the essence of GitOps; we declare what we want to appear in the cluster using Git and ArgoCD synchronizes the cluster with this declaration. Examine the ci rolebinding YAML Now that we've seen how the namespace was created, let's see how the three other rolebindings were created by the namespace-ci ArgoCD application. In the same namespace/ci folder as the ci namespace YAML, there is a rolebinding.yaml file. This file will be also applied to the cluster by the namespace-ci ArgoCD application which is continuously watching this folder. Examine this file with the following command: cat namespaces/ci/rolebinding.yaml This YAML is slightly more complex than the namespace YAML: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-dev namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:dev --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-staging namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:staging --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-prod namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:prod --- However its structure is quite straightforward; look carefully and you'll see that there are indeed three rolebindings defined in this YAML. Each of these rolebindings binds different roles to different service accounts associated with components that will run within the ci namespace, such as Tekton pipelines for example. We'll see later how these rolebindings are important; they limit the operations that can be performed by the ci namespace, creating a well governed cluster. This confirms why we saw four resources created by the namespace-ci ArgoCD application in the ArgoCD UI: one namespace and three rolebindings. Again, notice the pattern: a single ArgoCD application manages one or more Kubernetes resources in the cluster -- using one or more YAML files in which those resources are defined. The bootstrap-single-cluster ArgoCD application in more detail In the ArgoCD UI Applications view, click on the bootstrap-single-cluster application: You can see the bootstrap application creates two Kubernetes resources, the infra ArgoCD application and the infra ArgoCD project. An ArgoCD project is a mechanism by which we can group related resources; we keep all our ArgoCD applications that manage infrastructure in the infra project. Later, we'll create a services project for the ArgoCD applications that manage the services we want deployed to the cluster such as the CP4S application. The infra ArgoCD application Let's examine the infra ArgoCD application in more detail to see how it works. In the ArgoCD UI Applications view, click on the open application icon for the infra application: We can see that the infra ArgoCD application creates 9 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It's the infra ArgoCD application that watches the 0-bootstrap/single-cluster/1-infra/argocd folder for ArgoCD applications that apply infrastructure resources to our cluster. It was the infra application that created the namespace-ci ArgoCD application which manages the ci namespace that we've been exploring in this section of the tutorial. We'll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it's not necessary, as you'll get lots of practice as we proceed.","title":"Gitops repo argocd connect cp4s"},{"location":"snippets/gitops-repo-argocd-connect-cp4s/#connect-argocd-to-the-gitops-repository","text":"Let's now connect your customized GitOps repository to the instance of ArgoCD running in the cluster. Once connected, ArgoCD will use the contents of the repository to create necessary resources. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Review ArgoCD infrastructure folder Let's examine the 0-bootstrap/single-cluster/1-infra/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml We can see the contents of the kustomization.yaml : resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Notice that the resources that needs to be applied to the cluster are all inactivate and commented out. Let us enable the resources that are needed by un-commenting them. Deploy Kubernetes resources with kustomization.yaml Open 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the below resources: argocd/namespace-ci.yaml argocd/consolenotification.yaml argocd/namespace-ibm-common-services.yaml argocd/namespace-tools.yaml You will have the following resources un-commented for infrastructure: resources : #- argocd/consolelink.yaml - argocd/consolenotification.yaml - argocd/namespace-ibm-common-services.yaml - argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml - argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for infrastructure\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a900c39..e3f696d master -> master Examine bootstrap.yaml residing in 0-bootstrap/single-cluster/ . The bootstrap.yaml file is used to create our first ArgoCD application called bootstrap-single-cluster . This initial ArgoCD application will create all the other ArgoCD applications that control the application, service, and infrastructure resources (such as the ci and dev namespaces) deployed to the cluster. Examine the YAML that defines the ArgoCD bootstrap application: cat 0 -bootstrap/single-cluster/bootstrap.yaml Notice also how this ArgoCD application has been customized to use the GitOps repository repoURL: https://github.com/prod-ref-guide/multi-tenancy-gitops.git . apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-single-cluster namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/single-cluster repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Most importantly, see how path: 0-bootstrap/single-cluster refers to the 0-bootstrap/single-cluster folder within this repository. This will result in the creation of individual ArgoCD applications to manage our cluster resources. Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us for now only deploy infra resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and comment out the 2-services/2-services.yaml and 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml # - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Commit and push changes to your git repository: git add . git commit -s -m \"Using only infra\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 456 bytes | 456.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git e3f696d..ea3b43f master -> master Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. The repository contains a bootstrap-single-cluster ArgoCD application that is watching this repository and using its contents to manage the cluster. When the bootstrap-single-cluster ArgoCD application is applied to the cluster, it will continuously ensure that all the activated resources are applied to the cluster. Apply the bootstrap YAML to the cluster: oc apply -f 0 -bootstrap/single-cluster/bootstrap.yaml Kubernetes will confirm that the bootstrap ArgoCD application has been created: application.argoproj.io/bootstrap-single-cluster created The bootstrap ArgoCD application will watch the 0-bootstrap/single-cluster folder in our GitOps repository on GitHub. In this way, as resources are added to the infrastructure , service and application folders, they will be deployed to the cluster automatically. This is therefore the only direct cluster operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Verify the bootstrap deployment Verify that the bootstrap ArgoCD application is running with the following command: oc get app/bootstrap-single-cluster -n openshift-gitops You should see that the bootstrap application was recently updated: NAME SYNC STATUS HEALTH STATUS bootstrap-single-cluster Synced Healthy HEALTH_STATUS may temporarily show Missing ; simply re-issue the command to confirm it moves to Healthy . Using the UI to view the newly deployed ArgoCD applications In the previous section of this chapter you logged on to the ArgoCD web console. Switch back to that console, refresh the page and you should see the bootstrap-single-cluster ArgoCD application together with many other ArgoCD applications: (You may need to select List view rather than the Tiles view.) We can see that eleven ArgoCD applications have been deployed to the cluster as a result of applying bootstrap.yaml . In the next section of the tutorial, we'll examine these applications to see how and why they were created, but for now let's focus on one of them -- the namespace-ci ArgoCD application. Examining the namespace-ci ArgoCD application resources Let's examine the Kubernetes resources applied to the cluster by the namespace-ci ArgoCD application. In the ArgoCD application list, click on namespace-ci : (You may need to clear filters to see this screenshot.) The directed graph shows that the namespace-ci ArgoCD app has created 4 Kubernetes resources; our ci namespace and three role bindings. Verify the namespace using the oc CLI We've seen the new namespace definition in the GitOps repository and visually in the ArgoCD UI. Let's also verify it via the command line: Type the following command: oc get namespace ci -o yaml This will list the full details of the ci namespace: apiVersion : v1 kind : Namespace metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"namespace-ci\"},\"name\":\"ci\"},\"spec\":{}} openshift.io/sa.scc.mcs : s0:c27,c9 openshift.io/sa.scc.supplemental-groups : 1000720000/10000 openshift.io/sa.scc.uid-range : 1000720000/10000 creationTimestamp : \"2021-08-31T15:27:32Z\" labels : app.kubernetes.io/instance : namespace-ci managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:labels : . : {} f:app.kubernetes.io/instance : {} f:status : f:phase : {} manager : argocd-application-controller operation : Update time : \"2021-08-31T15:27:32Z\" - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : f:openshift.io/sa.scc.mcs : {} f:openshift.io/sa.scc.supplemental-groups : {} f:openshift.io/sa.scc.uid-range : {} manager : cluster-policy-controller operation : Update time : \"2021-08-31T15:27:32Z\" name : ci resourceVersion : \"2255607\" selfLink : /api/v1/namespaces/ci uid : fff6b82b-6318-4828-83bb-ade4e8e3c0cf spec : finalizers : - kubernetes status : phase : Active Notice how manager: argocd-application-controller identifies that this namespace was created by ArgoCD. It's important to understand the sequence of actions. We simply deployed the bootstrap-single-cluster ArgoCD application, and it ultimately resulted in the creation of the namespace-ci ArgoCD application which created the ci namespace. We don't directly apply resources to the cluster once the bootstrap-single-cluster ArgoCD application has been applied; the cluster state is determined by an ArgoCD application YAML in the corresponding application , service or infrastructure related folders. It's these ArgoCD applications that create and manage the underlying Kubernetes resources using the GitOps repository as the source of truth. Understanding the namespace-ci ArgoCD application Let's examine the ArgoCD application namespace-ci to see how it created the ci namespace and three role bindings in the cluster. Issue the following command to examine its YAML: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-ci.yaml Notice that apiVersion and kind identify this as an ArgoCD application: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ci labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" spec : destination : namespace : ci server : https://kubernetes.default.svc project : infra source : path : namespaces/ci syncPolicy : automated : prune : true selfHeal : true Most importantly, see how this namespace-ci ArgoCD application monitors the folder path: namespaces/ci in https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git ; it then applies the contents of this folder to the cluster whenever its content changes. Notice the SyncPolicy of automated ; any changes to this folder will automatically be applied to the cluster; we do not need to perform a manual Sync operation from the ArgoCD UI. Examine the ci namespace YAML To examine this, navigate to multi-tenancy-gitops-infra repository you cloned previously. cd $HOME /git cd multi-tenancy-gitops-infra Let's examine the ci namespace YAML in the namespaces/ci folder: cat namespaces/ci/namespace.yaml It's a very simple YAML: apiVersion : v1 kind : Namespace metadata : name : ci spec : {} This is a YAML that we would normally apply to the cluster manually or via a script. However, when we use GitOps, we push the ArgoCD application that uses this YAML to GitHub, and it applies the ci namespace YAML to the cluster. This is the essence of GitOps; we declare what we want to appear in the cluster using Git and ArgoCD synchronizes the cluster with this declaration. Examine the ci rolebinding YAML Now that we've seen how the namespace was created, let's see how the three other rolebindings were created by the namespace-ci ArgoCD application. In the same namespace/ci folder as the ci namespace YAML, there is a rolebinding.yaml file. This file will be also applied to the cluster by the namespace-ci ArgoCD application which is continuously watching this folder. Examine this file with the following command: cat namespaces/ci/rolebinding.yaml This YAML is slightly more complex than the namespace YAML: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-dev namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:dev --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-staging namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:staging --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-prod namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:prod --- However its structure is quite straightforward; look carefully and you'll see that there are indeed three rolebindings defined in this YAML. Each of these rolebindings binds different roles to different service accounts associated with components that will run within the ci namespace, such as Tekton pipelines for example. We'll see later how these rolebindings are important; they limit the operations that can be performed by the ci namespace, creating a well governed cluster. This confirms why we saw four resources created by the namespace-ci ArgoCD application in the ArgoCD UI: one namespace and three rolebindings. Again, notice the pattern: a single ArgoCD application manages one or more Kubernetes resources in the cluster -- using one or more YAML files in which those resources are defined. The bootstrap-single-cluster ArgoCD application in more detail In the ArgoCD UI Applications view, click on the bootstrap-single-cluster application: You can see the bootstrap application creates two Kubernetes resources, the infra ArgoCD application and the infra ArgoCD project. An ArgoCD project is a mechanism by which we can group related resources; we keep all our ArgoCD applications that manage infrastructure in the infra project. Later, we'll create a services project for the ArgoCD applications that manage the services we want deployed to the cluster such as the CP4S application. The infra ArgoCD application Let's examine the infra ArgoCD application in more detail to see how it works. In the ArgoCD UI Applications view, click on the open application icon for the infra application: We can see that the infra ArgoCD application creates 9 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It's the infra ArgoCD application that watches the 0-bootstrap/single-cluster/1-infra/argocd folder for ArgoCD applications that apply infrastructure resources to our cluster. It was the infra application that created the namespace-ci ArgoCD application which manages the ci namespace that we've been exploring in this section of the tutorial. We'll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it's not necessary, as you'll get lots of practice as we proceed.","title":"Connect ArgoCD to the GitOps repository"},{"location":"snippets/gitops-repo-argocd-connect/","text":"Connect ArgoCD to the GitOps repository \u00b6 Let's now connect your customized GitOps repository to the instance of ArgoCD running in the cluster. Once connected, ArgoCD will use the contents of the repository to create necessary resources. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Review ArgoCD infrastructure folder Let's examine the 0-bootstrap/single-cluster/1-infra/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml We can see the contents of the kustomization.yaml : resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Notice that the resources that needs to be applied to the cluster are all inactivate and commented out. Let us enable the resources that are needed by un-commenting them. Deploy Kubernetes resources with kustomization.yaml Open 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the below resources: argocd/consolelink.yaml argocd/consolenotification.yaml argocd/namespace-ibm-common-services.yaml argocd/namespace-ci.yaml argocd/namespace-dev.yaml argocd/namespace-staging.yaml argocd/namespace-prod.yaml argocd/namespace-sealed-secrets.yaml argocd/namespace-tools.yaml You will have the following resources un-commented for infrastructure: resources : - argocd/consolelink.yaml - argocd/consolenotification.yaml - argocd/namespace-ibm-common-services.yaml - argocd/namespace-ci.yaml - argocd/namespace-dev.yaml - argocd/namespace-staging.yaml - argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml - argocd/namespace-sealed-secrets.yaml - argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for infrastructure\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a900c39..e3f696d master -> master Examine bootstrap.yaml residing in 0-bootstrap/single-cluster/ . The bootstrap.yaml file is used to create our first ArgoCD application called bootstrap-single-cluster . This initial ArgoCD application will create all the other ArgoCD applications that control the application, service, and infrastructure resources (such as the ci and dev namespaces) deployed to the cluster. Examine the YAML that defines the ArgoCD bootstrap application: cat 0 -bootstrap/single-cluster/bootstrap.yaml Notice also how this ArgoCD application has been customized to use the GitOps repository repoURL: https://github.com/prod-ref-guide/multi-tenancy-gitops.git . apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-single-cluster namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/single-cluster repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Most importantly, see how path: 0-bootstrap/single-cluster refers to the 0-bootstrap/single-cluster folder within this repository. This will result in the creation of individual ArgoCD applications to manage our cluster resources. Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us for now only deploy infra resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and comment out the 2-services/2-services.yaml and 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml # - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Commit and push changes to your git repository: git add . git commit -s -m \"Using only infra\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 456 bytes | 456.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git e3f696d..ea3b43f master -> master Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. The repository contains a bootstrap-single-cluster ArgoCD application that is watching this repository and using its contents to manage the cluster. When the bootstrap-single-cluster ArgoCD application is applied to the cluster, it will continuously ensure that all the activated resources are applied to the cluster. Apply the bootstrap YAML to the cluster: oc apply -f 0 -bootstrap/single-cluster/bootstrap.yaml Kubernetes will confirm that the bootstrap ArgoCD application has been created: application.argoproj.io/bootstrap-single-cluster created The bootstrap ArgoCD application will watch the 0-bootstrap/single-cluster folder in our GitOps repository on GitHub. In this way, as resources are added to the infrastructure , service and application folders, they will be deployed to the cluster automatically. This is therefore the only direct cluster operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Verify the bootstrap deployment Verify that the bootstrap ArgoCD application is running with the following command: oc get app/bootstrap-single-cluster -n openshift-gitops You should see that the bootstrap application was recently updated: NAME SYNC STATUS HEALTH STATUS bootstrap-single-cluster Synced Healthy HEALTH_STATUS may temporarily show Missing ; simply re-issue the command to confirm it moves to Healthy . Using the UI to view the newly deployed ArgoCD applications In the previous section of this chapter you logged on to the ArgoCD web console. Switch back to that console, refresh the page and you should see the bootstrap-single-cluster ArgoCD application together with many other ArgoCD applications: (You may need to select List view rather than the Tiles view.) We can see that eleven ArgoCD applications have been deployed to the cluster as a result of applying bootstrap.yaml . In the next section of the tutorial, we'll examine these applications to see how and why they were created, but for now let's focus on one of them -- the namespace-ci ArgoCD application. Examining the namespace-ci ArgoCD application resources Let's examine the Kubernetes resources applied to the cluster by the namespace-ci ArgoCD application. In the ArgoCD application list, click on namespace-ci : (You may need to clear filters to see this screenshot.) The directed graph shows that the namespace-ci ArgoCD app has created 4 Kubernetes resources; our ci namespace and three role bindings. Verify the namespace using the oc CLI We've seen the new namespace definition in the GitOps repository and visually in the ArgoCD UI. Let's also verify it via the command line: Type the following command: oc get namespace ci -o yaml This will list the full details of the ci namespace: apiVersion : v1 kind : Namespace metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"namespace-ci\"},\"name\":\"ci\"},\"spec\":{}} openshift.io/sa.scc.mcs : s0:c27,c9 openshift.io/sa.scc.supplemental-groups : 1000720000/10000 openshift.io/sa.scc.uid-range : 1000720000/10000 creationTimestamp : \"2021-08-31T15:27:32Z\" labels : app.kubernetes.io/instance : namespace-ci managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:labels : . : {} f:app.kubernetes.io/instance : {} f:status : f:phase : {} manager : argocd-application-controller operation : Update time : \"2021-08-31T15:27:32Z\" - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : f:openshift.io/sa.scc.mcs : {} f:openshift.io/sa.scc.supplemental-groups : {} f:openshift.io/sa.scc.uid-range : {} manager : cluster-policy-controller operation : Update time : \"2021-08-31T15:27:32Z\" name : ci resourceVersion : \"2255607\" selfLink : /api/v1/namespaces/ci uid : fff6b82b-6318-4828-83bb-ade4e8e3c0cf spec : finalizers : - kubernetes status : phase : Active Notice how manager: argocd-application-controller identifies that this namespace was created by ArgoCD. It's important to understand the sequence of actions. We simply deployed the bootstrap-single-cluster ArgoCD application, and it ultimately resulted in the creation of the namespace-ci ArgoCD application which created the ci namespace. We don't directly apply resources to the cluster once the bootstrap-single-cluster ArgoCD application has been applied; the cluster state is determined by an ArgoCD application YAML in the corresponding application , service or infrastructure related folders. It's these ArgoCD applications that create and manage the underlying Kubernetes resources using the GitOps repository as the source of truth. Understanding the namespace-ci ArgoCD application Let's examine the ArgoCD application namespace-ci to see how it created the ci namespace and three role bindings in the cluster. Issue the following command to examine its YAML: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-ci.yaml Notice that apiVersion and kind identify this as an ArgoCD application: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ci labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" spec : destination : namespace : ci server : https://kubernetes.default.svc project : infra source : path : namespaces/ci syncPolicy : automated : prune : true selfHeal : true Most importantly, see how this namespace-ci ArgoCD application monitors the folder path: namespaces/ci in https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git ; it then applies the contents of this folder to the cluster whenever its content changes. Notice the SyncPolicy of automated ; any changes to this folder will automatically be applied to the cluster; we do not need to perform a manual Sync operation from the ArgoCD UI. Examine the ci namespace YAML To examine this, navigate to multi-tenancy-gitops-infra repository you cloned previously. cd $HOME /git cd multi-tenancy-gitops-infra Let's examine the ci namespace YAML in the namespaces/ci folder: cat namespaces/ci/namespace.yaml It's a very simple YAML: apiVersion : v1 kind : Namespace metadata : name : ci spec : {} This is a YAML that we would normally apply to the cluster manually or via a script. However, when we use GitOps, we push the ArgoCD application that uses this YAML to GitHub, and it applies the ci namespace YAML to the cluster. This is the essence of GitOps; we declare what we want to appear in the cluster using Git and ArgoCD synchronizes the cluster with this declaration. Examine the ci rolebinding YAML Now that we've seen how the namespace was created, let's see how the three other rolebindings were created by the namespace-ci ArgoCD application. In the same namespace/ci folder as the ci namespace YAML, there is a rolebinding.yaml file. This file will be also applied to the cluster by the namespace-ci ArgoCD application which is continuously watching this folder. Examine this file with the following command: cat namespaces/ci/rolebinding.yaml This YAML is slightly more complex than the namespace YAML: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-dev namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:dev --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-staging namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:staging --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-prod namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:prod --- However its structure is quite straightforward; look carefully and you'll see that there are indeed three rolebindings defined in this YAML. Each of these rolebindings binds different roles to different service accounts associated with components that will run within the ci namespace, such as Tekton pipelines for example. We'll see later how these rolebindings are important; they limit the operations that can be performed by the ci namespace, creating a well governed cluster. This confirms why we saw four resources created by the namespace-ci ArgoCD application in the ArgoCD UI: one namespace and three rolebindings. Again, notice the pattern: a single ArgoCD application manages one or more Kubernetes resources in the cluster -- using one or more YAML files in which those resources are defined. The bootstrap-single-cluster ArgoCD application in more detail In the ArgoCD UI Applications view, click on the bootstrap-single-cluster application: You can see the bootstrap application creates two Kubernetes resources, the infra ArgoCD application and the infra ArgoCD project. An ArgoCD project is a mechanism by which we can group related resources; we keep all our ArgoCD applications that manage infrastructure in the infra project. Later, we'll create a services project for the ArgoCD applications that manage the services we want deployed to the cluster such as the MQ application and queue manager pipelines. The infra ArgoCD application Let's examine the infra ArgoCD application in more detail to see how it works. In the ArgoCD UI Applications view, click on the open application icon for the infra application: We can see that the infra ArgoCD application creates 9 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It's the infra ArgoCD application that watches the 0-bootstrap/single-cluster/1-infra/argocd folder for ArgoCD applications that apply infrastructure resources to our cluster. It was the infra application that created the namespace-ci ArgoCD application which manages the ci namespace that we've been exploring in this section of the tutorial. We'll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it's not necessary, as you'll get lots of practice as we proceed.","title":"Gitops repo argocd connect"},{"location":"snippets/gitops-repo-argocd-connect/#connect-argocd-to-the-gitops-repository","text":"Let's now connect your customized GitOps repository to the instance of ArgoCD running in the cluster. Once connected, ArgoCD will use the contents of the repository to create necessary resources. Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Review ArgoCD infrastructure folder Let's examine the 0-bootstrap/single-cluster/1-infra/kustomization.yaml to see how ArgoCD manages the resources deployed to the cluster. Issue the following command: cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml We can see the contents of the kustomization.yaml : resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Notice that the resources that needs to be applied to the cluster are all inactivate and commented out. Let us enable the resources that are needed by un-commenting them. Deploy Kubernetes resources with kustomization.yaml Open 0-bootstrap/single-cluster/1-infra/kustomization.yaml and uncomment the below resources: argocd/consolelink.yaml argocd/consolenotification.yaml argocd/namespace-ibm-common-services.yaml argocd/namespace-ci.yaml argocd/namespace-dev.yaml argocd/namespace-staging.yaml argocd/namespace-prod.yaml argocd/namespace-sealed-secrets.yaml argocd/namespace-tools.yaml You will have the following resources un-commented for infrastructure: resources : - argocd/consolelink.yaml - argocd/consolenotification.yaml - argocd/namespace-ibm-common-services.yaml - argocd/namespace-ci.yaml - argocd/namespace-dev.yaml - argocd/namespace-staging.yaml - argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml - argocd/namespace-sealed-secrets.yaml - argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - op: add path: /spec/source/targetRevision value: master Commit and push changes to your git repository: git add . git commit -s -m \"Intial boostrap setup for infrastructure\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git a900c39..e3f696d master -> master Examine bootstrap.yaml residing in 0-bootstrap/single-cluster/ . The bootstrap.yaml file is used to create our first ArgoCD application called bootstrap-single-cluster . This initial ArgoCD application will create all the other ArgoCD applications that control the application, service, and infrastructure resources (such as the ci and dev namespaces) deployed to the cluster. Examine the YAML that defines the ArgoCD bootstrap application: cat 0 -bootstrap/single-cluster/bootstrap.yaml Notice also how this ArgoCD application has been customized to use the GitOps repository repoURL: https://github.com/prod-ref-guide/multi-tenancy-gitops.git . apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : bootstrap-single-cluster namespace : openshift-gitops spec : destination : namespace : openshift-gitops server : https://kubernetes.default.svc project : default source : path : 0-bootstrap/single-cluster repoURL : https://github.com/prod-ref-guide/multi-tenancy-gitops.git targetRevision : master syncPolicy : automated : prune : true selfHeal : true Most importantly, see how path: 0-bootstrap/single-cluster refers to the 0-bootstrap/single-cluster folder within this repository. This will result in the creation of individual ArgoCD applications to manage our cluster resources. Access the 0-bootstrap/single-cluster/kustomization.yaml : cat 0 -bootstrap/single-cluster/kustomization.yaml Let us for now only deploy infra resources to the cluster. Open 0-bootstrap/single-cluster/kustomization.yaml and comment out the 2-services/2-services.yaml and 3-apps/3-apps.yaml as follows: resources : - 1-infra/1-infra.yaml # - 2-services/2-services.yaml # - 3-apps/3-apps.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=gitops\" patch : |- - op: add path: /spec/source/repoURL value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/source/targetRevision value: master - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=services\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git - target : group : argoproj.io kind : AppProject labelSelector : \"gitops.tier.layer=applications\" patch : |- - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops.git - op: add path: /spec/sourceRepos/- value: https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git Commit and push changes to your git repository: git add . git commit -s -m \"Using only infra\" git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 456 bytes | 456.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git e3f696d..ea3b43f master -> master Apply ArgoCD bootstrap.yaml Recall that you pushed the customized local copy of the GitOps repository to your GitHub account. The repository contains a bootstrap-single-cluster ArgoCD application that is watching this repository and using its contents to manage the cluster. When the bootstrap-single-cluster ArgoCD application is applied to the cluster, it will continuously ensure that all the activated resources are applied to the cluster. Apply the bootstrap YAML to the cluster: oc apply -f 0 -bootstrap/single-cluster/bootstrap.yaml Kubernetes will confirm that the bootstrap ArgoCD application has been created: application.argoproj.io/bootstrap-single-cluster created The bootstrap ArgoCD application will watch the 0-bootstrap/single-cluster folder in our GitOps repository on GitHub. In this way, as resources are added to the infrastructure , service and application folders, they will be deployed to the cluster automatically. This is therefore the only direct cluster operation we need to perform; from now on, all cluster operations will be performed via Git operations to this repository. Verify the bootstrap deployment Verify that the bootstrap ArgoCD application is running with the following command: oc get app/bootstrap-single-cluster -n openshift-gitops You should see that the bootstrap application was recently updated: NAME SYNC STATUS HEALTH STATUS bootstrap-single-cluster Synced Healthy HEALTH_STATUS may temporarily show Missing ; simply re-issue the command to confirm it moves to Healthy . Using the UI to view the newly deployed ArgoCD applications In the previous section of this chapter you logged on to the ArgoCD web console. Switch back to that console, refresh the page and you should see the bootstrap-single-cluster ArgoCD application together with many other ArgoCD applications: (You may need to select List view rather than the Tiles view.) We can see that eleven ArgoCD applications have been deployed to the cluster as a result of applying bootstrap.yaml . In the next section of the tutorial, we'll examine these applications to see how and why they were created, but for now let's focus on one of them -- the namespace-ci ArgoCD application. Examining the namespace-ci ArgoCD application resources Let's examine the Kubernetes resources applied to the cluster by the namespace-ci ArgoCD application. In the ArgoCD application list, click on namespace-ci : (You may need to clear filters to see this screenshot.) The directed graph shows that the namespace-ci ArgoCD app has created 4 Kubernetes resources; our ci namespace and three role bindings. Verify the namespace using the oc CLI We've seen the new namespace definition in the GitOps repository and visually in the ArgoCD UI. Let's also verify it via the command line: Type the following command: oc get namespace ci -o yaml This will list the full details of the ci namespace: apiVersion : v1 kind : Namespace metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"namespace-ci\"},\"name\":\"ci\"},\"spec\":{}} openshift.io/sa.scc.mcs : s0:c27,c9 openshift.io/sa.scc.supplemental-groups : 1000720000/10000 openshift.io/sa.scc.uid-range : 1000720000/10000 creationTimestamp : \"2021-08-31T15:27:32Z\" labels : app.kubernetes.io/instance : namespace-ci managedFields : - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : . : {} f:kubectl.kubernetes.io/last-applied-configuration : {} f:labels : . : {} f:app.kubernetes.io/instance : {} f:status : f:phase : {} manager : argocd-application-controller operation : Update time : \"2021-08-31T15:27:32Z\" - apiVersion : v1 fieldsType : FieldsV1 fieldsV1 : f:metadata : f:annotations : f:openshift.io/sa.scc.mcs : {} f:openshift.io/sa.scc.supplemental-groups : {} f:openshift.io/sa.scc.uid-range : {} manager : cluster-policy-controller operation : Update time : \"2021-08-31T15:27:32Z\" name : ci resourceVersion : \"2255607\" selfLink : /api/v1/namespaces/ci uid : fff6b82b-6318-4828-83bb-ade4e8e3c0cf spec : finalizers : - kubernetes status : phase : Active Notice how manager: argocd-application-controller identifies that this namespace was created by ArgoCD. It's important to understand the sequence of actions. We simply deployed the bootstrap-single-cluster ArgoCD application, and it ultimately resulted in the creation of the namespace-ci ArgoCD application which created the ci namespace. We don't directly apply resources to the cluster once the bootstrap-single-cluster ArgoCD application has been applied; the cluster state is determined by an ArgoCD application YAML in the corresponding application , service or infrastructure related folders. It's these ArgoCD applications that create and manage the underlying Kubernetes resources using the GitOps repository as the source of truth. Understanding the namespace-ci ArgoCD application Let's examine the ArgoCD application namespace-ci to see how it created the ci namespace and three role bindings in the cluster. Issue the following command to examine its YAML: cat 0 -bootstrap/single-cluster/1-infra/argocd/namespace-ci.yaml Notice that apiVersion and kind identify this as an ArgoCD application: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ci labels : gitops.tier.layer : infra annotations : argocd.argoproj.io/sync-wave : \"100\" spec : destination : namespace : ci server : https://kubernetes.default.svc project : infra source : path : namespaces/ci syncPolicy : automated : prune : true selfHeal : true Most importantly, see how this namespace-ci ArgoCD application monitors the folder path: namespaces/ci in https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git ; it then applies the contents of this folder to the cluster whenever its content changes. Notice the SyncPolicy of automated ; any changes to this folder will automatically be applied to the cluster; we do not need to perform a manual Sync operation from the ArgoCD UI. Examine the ci namespace YAML To examine this, navigate to multi-tenancy-gitops-infra repository you cloned previously. cd $HOME /git cd multi-tenancy-gitops-infra Let's examine the ci namespace YAML in the namespaces/ci folder: cat namespaces/ci/namespace.yaml It's a very simple YAML: apiVersion : v1 kind : Namespace metadata : name : ci spec : {} This is a YAML that we would normally apply to the cluster manually or via a script. However, when we use GitOps, we push the ArgoCD application that uses this YAML to GitHub, and it applies the ci namespace YAML to the cluster. This is the essence of GitOps; we declare what we want to appear in the cluster using Git and ArgoCD synchronizes the cluster with this declaration. Examine the ci rolebinding YAML Now that we've seen how the namespace was created, let's see how the three other rolebindings were created by the namespace-ci ArgoCD application. In the same namespace/ci folder as the ci namespace YAML, there is a rolebinding.yaml file. This file will be also applied to the cluster by the namespace-ci ArgoCD application which is continuously watching this folder. Examine this file with the following command: cat namespaces/ci/rolebinding.yaml This YAML is slightly more complex than the namespace YAML: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-dev namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:dev --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-staging namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:staging --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : system:image-puller-prod namespace : ci roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : system:image-puller subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:prod --- However its structure is quite straightforward; look carefully and you'll see that there are indeed three rolebindings defined in this YAML. Each of these rolebindings binds different roles to different service accounts associated with components that will run within the ci namespace, such as Tekton pipelines for example. We'll see later how these rolebindings are important; they limit the operations that can be performed by the ci namespace, creating a well governed cluster. This confirms why we saw four resources created by the namespace-ci ArgoCD application in the ArgoCD UI: one namespace and three rolebindings. Again, notice the pattern: a single ArgoCD application manages one or more Kubernetes resources in the cluster -- using one or more YAML files in which those resources are defined. The bootstrap-single-cluster ArgoCD application in more detail In the ArgoCD UI Applications view, click on the bootstrap-single-cluster application: You can see the bootstrap application creates two Kubernetes resources, the infra ArgoCD application and the infra ArgoCD project. An ArgoCD project is a mechanism by which we can group related resources; we keep all our ArgoCD applications that manage infrastructure in the infra project. Later, we'll create a services project for the ArgoCD applications that manage the services we want deployed to the cluster such as the MQ application and queue manager pipelines. The infra ArgoCD application Let's examine the infra ArgoCD application in more detail to see how it works. In the ArgoCD UI Applications view, click on the open application icon for the infra application: We can see that the infra ArgoCD application creates 9 ArgoCD applications, each of which is responsible for applying specific YAMLs to the cluster according to the folder the ArgoCD application is watching. It's the infra ArgoCD application that watches the 0-bootstrap/single-cluster/1-infra/argocd folder for ArgoCD applications that apply infrastructure resources to our cluster. It was the infra application that created the namespace-ci ArgoCD application which manages the ci namespace that we've been exploring in this section of the tutorial. We'll continually reinforce these relationships as we work through the tutorial. You might like to spend some time exploring the ArgoCD UI and ArgoCD YAMLs before you proceed, though it's not necessary, as you'll get lots of practice as we proceed.","title":"Connect ArgoCD to the GitOps repository"},{"location":"snippets/gitops-sample-repo-creation/","text":"Downloading the sample GitOps repository \u00b6 In this topic section, we're going to create the GitOps repository for our cluster. We'll use a sample GitOps repository as a starting point; we'll make our own copy and review its contents. Later in the tutorial, we'll customize it for our organization and cluster. ( If you'd like to re-familiarize yourself with git, GitHub or managing repositories, you'll find the following short video and short read helpful. ) Create a new GitHub organization Let's start by creating our own GitHub organization. In your browser, navigate to https://github.com/ . In the upper-right corner, click on the profile photo, and then click Settings . In user settings sidebar, click Organizations . In the \"Organizations\" section, click New organization. Create a new organization. Create the GitOps repositories in the new GitHub organization Let's start by making our own copy of the GitOps repository. In your browser, navigate to https://github.com/cloud-native-toolkit/multi-tenancy-gitops : Click on Use this template at the top RHS of the screen and you'll be asked where you'd like to use this repository: Select your organization, name the repository and click Create repository from template . After a few seconds the repository will be present in your chosen account: You now have a copy of the sample GitOps repository in your GitHub account. Create the infrastructure, services and applications GitOps repositories from the templates into the organization your GitHub organization As shown above, follow similar steps and copy the the repository templates into your organization. GitOps Infrastructure Repository GitOps Services Repository GitOps Application Repository Your organization now will have the below repositories in it. Set up environment variable for GitHub Organization This tutorial uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Replace <git-organization> in the following command: export GIT_ORG = <git-organization> You can verify your $GIT_ORG as follows: echo $GIT_ORG Change to your working git folder Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . We've used this default in the following command, but you may have a different location on your local machine. Change to your working git folder: cd $HOME /git You may need to create this folder if it's not already created. Clone the GitOps repository We're going to work on a local clone of the four repositories. Clone the GitOps repositories to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops'... remote: Enumerating objects: 132, done. remote: Counting objects: 100% (132/132), done. remote: Compressing objects: 100% (54/54), done. remote: Total 132 (delta 72), reused 118 (delta 67), pack-reused 0 Receiving objects: 100% (132/132), 310.92 KiB | 3.24 MiB/s, done. Resolving deltas: 100% (72/72), done. Clone the GitOps Infra repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-infra.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-infra'... remote: Enumerating objects: 62, done. remote: Counting objects: 100% (62/62), done. remote: Compressing objects: 100% (44/44), done. remote: Total 62 (delta 7), reused 53 (delta 3), pack-reused 0 Unpacking objects: 100% (62/62), done. Clone the GitOps Services repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-services.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-services'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Clone the GitOps Applications repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-apps'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Info Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the folder containing the local copy of your new repository: cd multi-tenancy-gitops Review the GitOps folder structure Show the folder structure with the following command: tree . -L 1 This will list all the top level folders and files: . \u251c\u2500\u2500 0-bootstrap \u251c\u2500\u2500 README.md \u251c\u2500\u2500 doc \u251c\u2500\u2500 scripts \u2514\u2500\u2500 setup The main folders perform the following roles: 0-bootstrap contains different profiles that can be used to initialize the cluster ready. We can choose one of the available profiles. scripts and setup contain useful scripts that we'll use during this tutorial. Feel free to explore this structure. We'll examine it in much more detail throughout this tutorial, and you'll become very familiar with it by trying it out.","title":"Gitops sample repo creation"},{"location":"snippets/gitops-sample-repo-creation/#downloading-the-sample-gitops-repository","text":"In this topic section, we're going to create the GitOps repository for our cluster. We'll use a sample GitOps repository as a starting point; we'll make our own copy and review its contents. Later in the tutorial, we'll customize it for our organization and cluster. ( If you'd like to re-familiarize yourself with git, GitHub or managing repositories, you'll find the following short video and short read helpful. ) Create a new GitHub organization Let's start by creating our own GitHub organization. In your browser, navigate to https://github.com/ . In the upper-right corner, click on the profile photo, and then click Settings . In user settings sidebar, click Organizations . In the \"Organizations\" section, click New organization. Create a new organization. Create the GitOps repositories in the new GitHub organization Let's start by making our own copy of the GitOps repository. In your browser, navigate to https://github.com/cloud-native-toolkit/multi-tenancy-gitops : Click on Use this template at the top RHS of the screen and you'll be asked where you'd like to use this repository: Select your organization, name the repository and click Create repository from template . After a few seconds the repository will be present in your chosen account: You now have a copy of the sample GitOps repository in your GitHub account. Create the infrastructure, services and applications GitOps repositories from the templates into the organization your GitHub organization As shown above, follow similar steps and copy the the repository templates into your organization. GitOps Infrastructure Repository GitOps Services Repository GitOps Application Repository Your organization now will have the below repositories in it. Set up environment variable for GitHub Organization This tutorial uses environment variables to save typing and reduce errors. Let's set up an environment variable, $GIT_ORG , that contains your GitHub organization name. We'll use this variable in many subsequent commands. Replace <git-organization> in the following command: export GIT_ORG = <git-organization> You can verify your $GIT_ORG as follows: echo $GIT_ORG Change to your working git folder Most users keep all of their locally cloned git repositories under a common folder, usually a child folder of their home folder, i.e. $HOME/git . We've used this default in the following command, but you may have a different location on your local machine. Change to your working git folder: cd $HOME /git You may need to create this folder if it's not already created. Clone the GitOps repository We're going to work on a local clone of the four repositories. Clone the GitOps repositories to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops'... remote: Enumerating objects: 132, done. remote: Counting objects: 100% (132/132), done. remote: Compressing objects: 100% (54/54), done. remote: Total 132 (delta 72), reused 118 (delta 67), pack-reused 0 Receiving objects: 100% (132/132), 310.92 KiB | 3.24 MiB/s, done. Resolving deltas: 100% (72/72), done. Clone the GitOps Infra repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-infra.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-infra'... remote: Enumerating objects: 62, done. remote: Counting objects: 100% (62/62), done. remote: Compressing objects: 100% (44/44), done. remote: Total 62 (delta 7), reused 53 (delta 3), pack-reused 0 Unpacking objects: 100% (62/62), done. Clone the GitOps Services repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-services.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-services'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Clone the GitOps Applications repository to your local machine: git clone https://github.com/ $GIT_ORG /multi-tenancy-gitops-apps.git A local copy will be made on your machine as follows: Cloning into 'multi-tenancy-gitops-apps'... remote: Enumerating objects: 137, done. remote: Counting objects: 100% (137/137), done. remote: Compressing objects: 100% (96/96), done. remote: Total 137 (delta 39), reused 122 (delta 33), pack-reused 0 Receiving objects: 100% (137/137), 30.01 KiB | 5.00 MiB/s, done. Resolving deltas: 100% (39/39), done. Info Throughout this tutorial we use the HTTPS URL to clone repositories, however you can also use the SSH option. Work on the local copy of the GitOps repository Change to the folder containing the local copy of your new repository: cd multi-tenancy-gitops Review the GitOps folder structure Show the folder structure with the following command: tree . -L 1 This will list all the top level folders and files: . \u251c\u2500\u2500 0-bootstrap \u251c\u2500\u2500 README.md \u251c\u2500\u2500 doc \u251c\u2500\u2500 scripts \u2514\u2500\u2500 setup The main folders perform the following roles: 0-bootstrap contains different profiles that can be used to initialize the cluster ready. We can choose one of the available profiles. scripts and setup contain useful scripts that we'll use during this tutorial. Feel free to explore this structure. We'll examine it in much more detail throughout this tutorial, and you'll become very familiar with it by trying it out.","title":"Downloading the sample GitOps repository"},{"location":"snippets/gitops-sample-repo-kustomize/","text":"Customizing the GitOps repositories \u00b6 The cloned GitOps configuration repositories (specifically, multi-tenancy-gitops and multi-tenancy-gitops-apps ) needs to be customized for your cluster. The ArgoCD Application and AppProject resources and kustomization.yaml resources contains environment variables which will need to be updated to your specific GitHub organization and branch. Update multi-tenancy-gitops repository \u00b6 Have a look at the following kustomization.yaml as an example of what will need to be updated. cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS_INFRA} - op: add path: /spec/source/targetRevision value: ${GIT_GITOPS_INFRA_BRANCH} Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Run the customization script Let's customize the cloned multi-tenancy-gitops repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. This script has also made some customizations that we'll use much later in the tutorial; we'll refer to those at the relevant time. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 199 files changed, 290 insertions ( + ) , 290 deletions ( - ) Set your GitOps repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (48/48), 4.70 KiB | 1.57 MiB/s, done. Total 48 (delta 32), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 23 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git d95eca5..a900c39 master -> master Update multi-tenancy-gitops-apps repository \u00b6 Similar to the previous section, there are corresponding ArgoCD Applications resources that will need to be updated in the multi-tenancy-gitops-apps repository. Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Locate the multi-tenancy-gitops-apps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops-apps Run the customization script Let's customize the cloned multi-tenancy-gitops-apps repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops-apps repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 21 files changed, 42 insertions ( + ) , 42 deletions ( - ) Set your repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops-apps repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (42/42), 4.70 KiB | 1.57 MiB/s, done. Total 42 (delta 42), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 21 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git d95eca5..a900c39 master -> master","title":"Gitops sample repo kustomize"},{"location":"snippets/gitops-sample-repo-kustomize/#customizing-the-gitops-repositories","text":"The cloned GitOps configuration repositories (specifically, multi-tenancy-gitops and multi-tenancy-gitops-apps ) needs to be customized for your cluster. The ArgoCD Application and AppProject resources and kustomization.yaml resources contains environment variables which will need to be updated to your specific GitHub organization and branch.","title":"Customizing the GitOps repositories"},{"location":"snippets/gitops-sample-repo-kustomize/#update-multi-tenancy-gitops-repository","text":"Have a look at the following kustomization.yaml as an example of what will need to be updated. cat 0 -bootstrap/single-cluster/1-infra/kustomization.yaml resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-cloudpak.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-instana-agent.yaml #- argocd/namespace-robot-shop.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/namespace-spp.yaml #- argocd/namespace-spp-velero.yaml #- argocd/namespace-baas.yaml #- argocd/serviceaccounts-tools.yaml #- argocd/storage.yaml #- argocd/infraconfig.yaml #- argocd/machinesets.yaml patches : - target : group : argoproj.io kind : Application labelSelector : \"gitops.tier.layer=infra\" patch : |- - op: add path: /spec/source/repoURL value: ${GIT_BASEURL}/${GIT_ORG}/${GIT_GITOPS_INFRA} - op: add path: /spec/source/targetRevision value: ${GIT_GITOPS_INFRA_BRANCH} Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Run the customization script Let's customize the cloned multi-tenancy-gitops repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. This script has also made some customizations that we'll use much later in the tutorial; we'll refer to those at the relevant time. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 199 files changed, 290 insertions ( + ) , 290 deletions ( - ) Set your GitOps repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (48/48), 4.70 KiB | 1.57 MiB/s, done. Total 48 (delta 32), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 23 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops.git d95eca5..a900c39 master -> master","title":"Update multi-tenancy-gitops repository"},{"location":"snippets/gitops-sample-repo-kustomize/#update-multi-tenancy-gitops-apps-repository","text":"Similar to the previous section, there are corresponding ArgoCD Applications resources that will need to be updated in the multi-tenancy-gitops-apps repository. Once we've customized the local repository, we'll push our updates back to our repository branch on GitHub where it can be accessed by ArgoCD. Locate the multi-tenancy-gitops-apps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops-apps Run the customization script Let's customize the cloned multi-tenancy-gitops-apps repository with the relevant values for our cluster. The sample GitOps repository provides the set-git-source.sh script to make this task easy. Run script to replace the git url and branch to your git organization where you created the git repositories: GIT_ORG = <Your Organization> GIT_BRANCH = master ./scripts/set-git-source.sh The script will list customizations it will use and all the files that it customizes: Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-infra.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-services.git on branch master Setting kustomization patches to https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git on branch master done replacing variables in kustomization.yaml files git commit and push changes now You can examine your local clone of the GitOps repository to verify these customizations. You should see lots of amended ArgoCD YAMLs that point to your GitOps repository on GitHub. Add the changes to a git index, ready to push to GitHub We've now customized our local clone of the multi-tenancy-gitops-apps repository. Let's commit these changes and make the customized repository available to the cluster via GitHub. Add all changes in the current folder to a git index: git add . Commit the changes to git Use the following command to create a commit record: git commit -s -m \"GitOps customizations for organization and cluster\" See the Git commit message for the customized files: [ master a900c39 ] GitOps customizations for organization and cluster 21 files changed, 42 insertions ( + ) , 42 deletions ( - ) Set your repository branch For this guide, we will be using the master branch for the multi-tenancy-gitops-apps repository. export GIT_BRANCH = master You can verify your $GIT_BRANCH as follows: echo $GIT_BRANCH Push changes to GitHub Push this commit back to the branch on GitHub: git push origin $GIT_BRANCH The changes have now been pushed to your GitOps repository: Enumerating objects: 90, done. Counting objects: 100% (90/90), done. Delta compression using up to 8 threads Compressing objects: 100% (47/47), done. Writing objects: 100% (42/42), 4.70 KiB | 1.57 MiB/s, done. Total 42 (delta 42), reused 0 (delta 0) remote: Resolving deltas: 100% (32/32), completed with 21 local objects. To https://github.com/prod-ref-guide/multi-tenancy-gitops-apps.git d95eca5..a900c39 master -> master","title":"Update multi-tenancy-gitops-apps repository"},{"location":"snippets/gitops-sample-repo-structure/","text":"The sample GitOps repository \u00b6 Let's understand how the GitOps repository works by using it to install the components we've highlighted in the above diagram. In the first part of this topic, let's look at the high level structure of the GitOps repository. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Explore the high level folder structure Use the following command to display the folder structure: tree . -d -L 2 We can see the different folders in the GitOps repository: . \u251c\u2500\u2500 0-bootstrap \u2502 \u251c\u2500\u2500 others \u2502 \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 doc \u2502 \u251c\u2500\u2500 diagrams \u2502 \u251c\u2500\u2500 experimental \u2502 \u251c\u2500\u2500 images \u2502 \u2514\u2500\u2500 scenarios \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 bom \u2514\u2500\u2500 setup \u2514\u2500\u2500 ocp47 The 0-bootstrap folder is the key folder that contains all the available profiles. Within this folders are Cluster Profiles that control which resources are deployed to the cluster. There are other folders, but these contain utility scripts; and some document related information. Let us examine the 0-bootstrap folder structure: tree ./0-bootstrap/ -d -L 2 We can see the different folders: ./0-bootstrap/ \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u2514\u2500\u2500 3-apps If we consider single-cluster profile, you can see the three layers under that covering infrastructure, services, and applications. Use single-cluster profile to configure the cluster Use the following command to display the folder structure: tree ./0-bootstrap/single-cluster/ -L 3 We can see the different folders: ./0-bootstrap/single-cluster/ \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 1-infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 2-services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3-apps \u2502 \u251c\u2500\u2500 3-apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 cp4a \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Notice that each high level folder has an argocd folder. Within these argocd folders are ArgoCD applications that control which resources are deployed to the cluster. Sample ArgoCD application We'll see later how these ArgoCD applications work in detail. For now, notice how each layer of the architecture ( infrastructure , service , application ) has a set of ArgoCD applications. Type the following command: tree 0 -bootstrap/single-cluster/1-infra/ It shows a list of ArgoCD applications that are used to manage Kubernetes infrastructure resources: 0-bootstrap/single-cluster/1-infra/ \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Notice the namespace- YAMLs; we'll see in a moment how these each define an ArgoCD application dedicated to managing a Kubernetes namespace in our reference architecture. A word on terminology As we get started with ArgoCD, it can be easy to confuse the term ArgoCD application with your application. That's because ArgoCD uses the term application to refer to a Kubernetes custom resource that was initially designed to automate application management. However, an ArgoCD application can automate the deployment of any Kubernetes resource within a cluster, such as namespaces or queue managers, as we'll see a little later. Let's now customize this repository to deploy the highlighted Kubernetes resources to our cluster.","title":"Gitops sample repo structure"},{"location":"snippets/gitops-sample-repo-structure/#the-sample-gitops-repository","text":"Let's understand how the GitOps repository works by using it to install the components we've highlighted in the above diagram. In the first part of this topic, let's look at the high level structure of the GitOps repository. Ensure you're logged in to the cluster Start a terminal window and log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select \"Copy Login Command\". Locate your GitOps repository If necessary, change to the root of your GitOps repository, which is typically $HOME/git . Issue the following command to change to your GitOps repository: cd $HOME /git cd multi-tenancy-gitops Explore the high level folder structure Use the following command to display the folder structure: tree . -d -L 2 We can see the different folders in the GitOps repository: . \u251c\u2500\u2500 0-bootstrap \u2502 \u251c\u2500\u2500 others \u2502 \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 doc \u2502 \u251c\u2500\u2500 diagrams \u2502 \u251c\u2500\u2500 experimental \u2502 \u251c\u2500\u2500 images \u2502 \u2514\u2500\u2500 scenarios \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 bom \u2514\u2500\u2500 setup \u2514\u2500\u2500 ocp47 The 0-bootstrap folder is the key folder that contains all the available profiles. Within this folders are Cluster Profiles that control which resources are deployed to the cluster. There are other folders, but these contain utility scripts; and some document related information. Let us examine the 0-bootstrap folder structure: tree ./0-bootstrap/ -d -L 2 We can see the different folders: ./0-bootstrap/ \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u2514\u2500\u2500 3-apps If we consider single-cluster profile, you can see the three layers under that covering infrastructure, services, and applications. Use single-cluster profile to configure the cluster Use the following command to display the folder structure: tree ./0-bootstrap/single-cluster/ -L 3 We can see the different folders: ./0-bootstrap/single-cluster/ \u251c\u2500\u2500 1-infra \u2502 \u251c\u2500\u2500 1-infra.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2502 \u2514\u2500\u2500 storage.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 2-services \u2502 \u251c\u2500\u2500 2-services.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u2514\u2500\u2500 operators \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 3-apps \u2502 \u251c\u2500\u2500 3-apps.yaml \u2502 \u251c\u2500\u2500 argocd \u2502 \u2502 \u251c\u2500\u2500 ace \u2502 \u2502 \u251c\u2500\u2500 apic \u2502 \u2502 \u251c\u2500\u2500 bookinfo \u2502 \u2502 \u251c\u2500\u2500 cp4a \u2502 \u2502 \u251c\u2500\u2500 mq \u2502 \u2502 \u2514\u2500\u2500 soapserver \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Notice that each high level folder has an argocd folder. Within these argocd folders are ArgoCD applications that control which resources are deployed to the cluster. Sample ArgoCD application We'll see later how these ArgoCD applications work in detail. For now, notice how each layer of the architecture ( infrastructure , service , application ) has a set of ArgoCD applications. Type the following command: tree 0 -bootstrap/single-cluster/1-infra/ It shows a list of ArgoCD applications that are used to manage Kubernetes infrastructure resources: 0-bootstrap/single-cluster/1-infra/ \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-baas.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-cloudpak.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-instana-agent.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-robot-shop.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-spp-velero.yaml \u2502 \u251c\u2500\u2500 namespace-spp.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u251c\u2500\u2500 serviceaccounts-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Notice the namespace- YAMLs; we'll see in a moment how these each define an ArgoCD application dedicated to managing a Kubernetes namespace in our reference architecture. A word on terminology As we get started with ArgoCD, it can be easy to confuse the term ArgoCD application with your application. That's because ArgoCD uses the term application to refer to a Kubernetes custom resource that was initially designed to automate application management. However, an ArgoCD application can automate the deployment of any Kubernetes resource within a cluster, such as namespaces or queue managers, as we'll see a little later. Let's now customize this repository to deploy the highlighted Kubernetes resources to our cluster.","title":"The sample GitOps repository"},{"location":"snippets/quickstart-install-required-cli/","text":"Install required CLIs \u00b6 Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI","title":"Quickstart install required cli"},{"location":"snippets/quickstart-install-required-cli/#install-required-clis","text":"Install the Github CLI (version 1.14.0+) Install the OpenShift CLI oc (version 4.7 or 4.8) Log into your OCP cluster, substituting the --token and --server parameters with your values: oc login --token = <token> --server = <server> If you are unsure of these values, click your user ID in the OpenShift web console and select Copy login command . Install the kubeseal CLI","title":"Install required CLIs"},{"location":"snippets/quickstart-overview/","text":"Introduction \u00b6 The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers.","title":"Quickstart overview"},{"location":"snippets/quickstart-overview/#introduction","text":"The instructions in this section will walk you through the steps necessary for setting up a demo environment that can be quickly shown to customers to showcase the art of the possible. This demo in no way removes the need for following the tutorial style instructions that are documented in other sections of this guide. The instructions below will set up a pre-canned application demo environment to show to the customers.","title":"Introduction"},{"location":"snippets/quickstart-select-resources/","text":"Select resources to deploy \u00b6 By now, you should already have a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration bootstrapped for you. If you open ArgoCD, which is the GitOps tool being installed by the Red Hat OpenShift GitOps Operator, using the Argo CD URL provided in the email shown in the previous section, you will see that your ArgoCD GitOps application has been bootstrapped to monitor the multi-tenancy-gitops repository that has been forked into the GitHub organization you provided when requesting the quickstart environment. As a result, anything you want to apply/do to your quickstart environment will be done through code changes on the aforementioned forked GitHub repository . You can see in the image above of your ArgoCD web console that the profile within the multi-tenancy-gitops repository ArgoCD has been bootstrapped with is the single-cluster . As a result, anything you want to apply/do to your quickstart environment will be done within that GitOps profile. You can also see that the ArgoCD applications for the infrastructure , services and applications layers are already created so that these will pick up any changes done, through code, at their respective layers.","title":"Quickstart select resources"},{"location":"snippets/quickstart-select-resources/#select-resources-to-deploy","text":"By now, you should already have a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration bootstrapped for you. If you open ArgoCD, which is the GitOps tool being installed by the Red Hat OpenShift GitOps Operator, using the Argo CD URL provided in the email shown in the previous section, you will see that your ArgoCD GitOps application has been bootstrapped to monitor the multi-tenancy-gitops repository that has been forked into the GitHub organization you provided when requesting the quickstart environment. As a result, anything you want to apply/do to your quickstart environment will be done through code changes on the aforementioned forked GitHub repository . You can see in the image above of your ArgoCD web console that the profile within the multi-tenancy-gitops repository ArgoCD has been bootstrapped with is the single-cluster . As a result, anything you want to apply/do to your quickstart environment will be done within that GitOps profile. You can also see that the ArgoCD applications for the infrastructure , services and applications layers are already created so that these will pick up any changes done, through code, at their respective layers.","title":"Select resources to deploy"},{"location":"snippets/techzone-roks-bootstrapped-cluster-use/","text":"Use the cluster \u00b6 You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy.","title":"Techzone roks bootstrapped cluster use"},{"location":"snippets/techzone-roks-bootstrapped-cluster-use/#use-the-cluster","text":"You will receive an email once the cluster provisioning is complete. The email will contain details on the cluster including the ArgoCD Console URL and admin credentials. This same information can also be found on the My reservations from IBM Technology Zone . Once your cluster is ready, proceed to the next step to select resources to deploy.","title":"Use the cluster"},{"location":"snippets/techzone-roks-bootstrapped-cluster/","text":"Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration \u00b6 Create the cluster \u00b6 Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit .","title":"Techzone roks bootstrapped cluster"},{"location":"snippets/techzone-roks-bootstrapped-cluster/#red-hat-openshift-on-ibm-cloud-cluster-with-gitops-configuration","text":"","title":"Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration"},{"location":"snippets/techzone-roks-bootstrapped-cluster/#create-the-cluster","text":"Provision a Red Hat OpenShift on IBM Cloud cluster with GitOps Configuration from IBM Technology Zone . Select the ROKS Cluster with GitOps Configuration tile. Click the Reserve now radio button. Provide a name for the cluster, select Practice / Self-Education for the purpose and choose the region to provision the cluster. Once a Preferred Geography has been selected, provide the GitHub Access API Token and GitHub Organization created from the Pre-requisites section as well as the appropriate Worker Node Count and Worker Node Flavor values based on the requirements for this quickstart listed in the note below. Finally, click Submit .","title":"Create the cluster"},{"location":"snippets/techzone-roks-empty-cluster/","text":"OpenShift on IBM Cloud \u00b6 Navigate to the IBM Technology Zone In your browser, navigate to the IBM Technology Zone to create your cluster. You will need to sign-in with your IBM credentials if your browser has not authenticated. You will need to read and accept Terms and conditions to proceed to the IBM Technology Zone. Reserve a cluster You can reserve a cluster immediately on IBM Cloud. You'll see the following options: Click the Reserve now radio button. It takes between 30 and 60 minutes to provision a cluster. Moreover, once a cluster has been provisioned, by default, it will be deleted after 3 days -- unless you renew it. It can therefore be helpful to schedule the environment to be created later if you are not going to be using it for a while -- such as for a future demonstration or classroom session. Come back and explore the Schedule for later option another time. Complete reservation details - step 1 You will be asked to complete your reservations details in two steps. Firstly, when you make a cluster reservation, you can supply a set of details that describe: The cluster name The purpose of the reservation from a pre-defined set A description, which will appear for a Practice/Self-Education reservation Notes to help you identify this reservation In the following web page: Complete the details as follows: Type a helpful Name such as My cluster 1 Select Practice/Self-Education from the Purpose drop-down list Type a helpful Purpose Description such as Education Do not select Preferred Geography yet; we will do that in a moment Add some helpful Notes such as Learn how to build a cloud native production reference deployment using a CloudPak and OpenShift on IBM Cloud Complete reservation details - step 2 The second step in registration is to specify the physical details of the compute infrastructure for your cluster. When you make a cluster reservation, you can supply a set of details that describe: The cluster's geographical location The version of OpenShift Container Platform used by the cluster The number of CPUs in the cluster and their memory size The number of Kubernetes worker nodes in the cluster The NFS storage size for the cluster The End date and time for the cluster In the following web page: Complete the details as follows: Select your Preferred Geography such as London 5 , HongKong 2 or San Jose 04 Under End date and time , select a date and specify the time along with its time zone. Select Worker Node Count as 3 from the drop-down list Select Worker Node Flavor as 8CPU x 32GB from the drop-down list Select NFS Size as 500 GB from the drop-down list Select OpenShift Version as 4.8 from the drop-down-list Info By default, your cluster will be reserved for three days , after which time it will be deleted. This is more than enough time to complete this tutorial. If required, you can increase the cluster lifetime by configuring the End date and time . Don't do this unless you really need to do so; as you'll learn in the tutorial, the use of GitOps makes it quick and easy to provision a cluster when you need to do so. Create the cluster Check your reservation details and press Submit when you're ready to create the cluster: The cluster provisioning process will take between 30 and 60 minutes to complete on IBM Cloud. Provisioning email As the provisioning process starts, you will receive an email to confirm that the provisioning process has started: Checking the cluster provisioning status You can also view the status of your provisioning request on IBM Technology Zone. Click on My reservations to see: Note: The Status: field will be Ready when the cluster has been provisioned. The creation and expiry date-time for your cluster. You can return to the reservation at any time to get the details of your cluster. Cluster provisioned email You will receive another email once your cluster is ready: Your cluster is now provisioned and ready to use. Note the expiry date-time. Navigate to your cluster You can use the information in the notification email to access your cluster. Click on the Cluster URL link to login to IBM Cloud and locate your OpenShift cluster: Note: The Cluster ID that has been assigned by IBM Cloud. The Zone matches your requested geography. The OCP Version matches your requested version of OCP. The Node status identifies the 3 worker nodes in the Normal state. Feel free to explore this web page. Receiving cluster deletion email and extending your cluster's lifetime When your cluster is within 3 days of deletion, you will receive a daily email: Note: You can extend your cluster if required. This option may be limited to certain types of reservations. The My reservations page allows you to manage your cluster reservation","title":"Techzone roks empty cluster"},{"location":"snippets/techzone-roks-empty-cluster/#openshift-on-ibm-cloud","text":"Navigate to the IBM Technology Zone In your browser, navigate to the IBM Technology Zone to create your cluster. You will need to sign-in with your IBM credentials if your browser has not authenticated. You will need to read and accept Terms and conditions to proceed to the IBM Technology Zone. Reserve a cluster You can reserve a cluster immediately on IBM Cloud. You'll see the following options: Click the Reserve now radio button. It takes between 30 and 60 minutes to provision a cluster. Moreover, once a cluster has been provisioned, by default, it will be deleted after 3 days -- unless you renew it. It can therefore be helpful to schedule the environment to be created later if you are not going to be using it for a while -- such as for a future demonstration or classroom session. Come back and explore the Schedule for later option another time. Complete reservation details - step 1 You will be asked to complete your reservations details in two steps. Firstly, when you make a cluster reservation, you can supply a set of details that describe: The cluster name The purpose of the reservation from a pre-defined set A description, which will appear for a Practice/Self-Education reservation Notes to help you identify this reservation In the following web page: Complete the details as follows: Type a helpful Name such as My cluster 1 Select Practice/Self-Education from the Purpose drop-down list Type a helpful Purpose Description such as Education Do not select Preferred Geography yet; we will do that in a moment Add some helpful Notes such as Learn how to build a cloud native production reference deployment using a CloudPak and OpenShift on IBM Cloud Complete reservation details - step 2 The second step in registration is to specify the physical details of the compute infrastructure for your cluster. When you make a cluster reservation, you can supply a set of details that describe: The cluster's geographical location The version of OpenShift Container Platform used by the cluster The number of CPUs in the cluster and their memory size The number of Kubernetes worker nodes in the cluster The NFS storage size for the cluster The End date and time for the cluster In the following web page: Complete the details as follows: Select your Preferred Geography such as London 5 , HongKong 2 or San Jose 04 Under End date and time , select a date and specify the time along with its time zone. Select Worker Node Count as 3 from the drop-down list Select Worker Node Flavor as 8CPU x 32GB from the drop-down list Select NFS Size as 500 GB from the drop-down list Select OpenShift Version as 4.8 from the drop-down-list Info By default, your cluster will be reserved for three days , after which time it will be deleted. This is more than enough time to complete this tutorial. If required, you can increase the cluster lifetime by configuring the End date and time . Don't do this unless you really need to do so; as you'll learn in the tutorial, the use of GitOps makes it quick and easy to provision a cluster when you need to do so. Create the cluster Check your reservation details and press Submit when you're ready to create the cluster: The cluster provisioning process will take between 30 and 60 minutes to complete on IBM Cloud. Provisioning email As the provisioning process starts, you will receive an email to confirm that the provisioning process has started: Checking the cluster provisioning status You can also view the status of your provisioning request on IBM Technology Zone. Click on My reservations to see: Note: The Status: field will be Ready when the cluster has been provisioned. The creation and expiry date-time for your cluster. You can return to the reservation at any time to get the details of your cluster. Cluster provisioned email You will receive another email once your cluster is ready: Your cluster is now provisioned and ready to use. Note the expiry date-time. Navigate to your cluster You can use the information in the notification email to access your cluster. Click on the Cluster URL link to login to IBM Cloud and locate your OpenShift cluster: Note: The Cluster ID that has been assigned by IBM Cloud. The Zone matches your requested geography. The OCP Version matches your requested version of OCP. The Node status identifies the 3 worker nodes in the Normal state. Feel free to explore this web page. Receiving cluster deletion email and extending your cluster's lifetime When your cluster is within 3 days of deletion, you will receive a daily email: Note: You can extend your cluster if required. This option may be limited to certain types of reservations. The My reservations page allows you to manage your cluster reservation","title":"OpenShift on IBM Cloud"},{"location":"snippets/test/","text":"import snippet test \u00b6 this is a test","title":"Test"},{"location":"snippets/test/#import-snippet-test","text":"this is a test","title":"import snippet test"}]}